{
  "papers": [
    {
      "id": "2602.17599",
      "arxivId": "2602.17599",
      "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
      "authors": [
        "Ivan Rinaldi",
        "Matteo Mendula",
        "Nicola Fanelli",
        "Florence Levé",
        "Matteo Testi",
        "Giovanna Castellano",
        "Gennaro Vessio"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.",
      "url": "https://arxiv.org/abs/2602.17599",
      "pdfUrl": "https://arxiv.org/pdf/2602.17599.pdf",
      "titleJa": "Art2Mus: 視覚条件付けと大規模クロスモーダルアライメントによるアートワークから音楽への生成"
    },
    {
      "id": "2602.17394",
      "arxivId": "2602.17394",
      "title": "Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks",
      "authors": [
        "Nuno Saavedra",
        "Pedro Ribeiro",
        "André Coelho",
        "Rui Campos"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Unmanned Aerial Vehicle (UAV)-assisted networks are increasingly foreseen as a promising approach for emergency response, providing rapid, flexible, and resilient communications in environments where terrestrial infrastructure is degraded or unavailable. In such scenarios, voice radio communications remain essential for first responders due to their robustness; however, their unstructured nature prevents direct integration with automated UAV-assisted network management. This paper proposes SIREN, an AI-driven framework that enables voice-driven perception for UAV-assisted networks. By integrating Automatic Speech Recognition (ASR) with Large Language Model (LLM)-based semantic extraction and Natural Language Processing (NLP) validation, SIREN converts emergency voice traffic into structured, machine-readable information, including responding units, location references, emergency severity, and Quality-of-Service (QoS) requirements. SIREN is evaluated using synthetic emergency scenarios with controlled variations in language, speaker count, background noise, and message complexity. The results demonstrate robust transcription and reliable semantic extraction across diverse operating conditions, while highlighting speaker diarization and geographic ambiguity as the main limiting factors. These findings establish the feasibility of voice-driven situational awareness for UAV-assisted networks and show a practical foundation for human-in-the-loop decision support and adaptive network management in emergency response operations.",
      "url": "https://arxiv.org/abs/2602.17394",
      "pdfUrl": "https://arxiv.org/pdf/2602.17394.pdf",
      "titleJa": "UAV支援緊急ネットワークのための音声駆動型意味認識"
    },
    {
      "id": "2602.17097",
      "arxivId": "2602.17097",
      "title": "AudioChat: Unified Audio Storytelling, Editing, and Understanding with Transfusion Forcing",
      "authors": [
        "William Chen",
        "Prem Seetharaman",
        "Rithesh Kumar",
        "Oriol Nieto",
        "Shinji Watanabe",
        "Justin Salamon",
        "Zeyu Jin"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Despite recent breakthroughs, audio foundation models struggle in processing complex multi-source acoustic scenes. We refer to this challenging domain as audio stories, which can have multiple speakers and background/foreground sound effects. Compared to traditional audio processing tasks, audio stories introduce new layers of semantic, temporal, and physical complexity. To address this challenge, we propose AudioChat, a framework for developing audio foundation models that can generate, edit, and understand audio stories. AudioChat introduces a new paradigm in which LLM-based toolcalling agents simulate interactions between users and the system, and these simulated dialogues are used as training data. We also introduce a novel Audio Transfusion Forcing objective to train the AudioChat model, allowing it to simultaneously decompose high-level instructions via structured chain-of-thought reasoning and perform interactive multi-turn audio understanding/generation. To evaluate generation and editing performance, we develop three new metrics that directly measure task performance instead of relying upon distribution-based scoring. We highly encourage readers to visit our demo to better understand the capabilities of AudioChat: https://wanchichen.github.io/audiochat/.",
      "url": "https://arxiv.org/abs/2602.17097",
      "pdfUrl": "https://arxiv.org/pdf/2602.17097.pdf",
      "titleJa": "AudioChat: Transfusion Forcingによる統合オーディオストーリーテリング、編集、理解"
    },
    {
      "id": "2602.16790",
      "arxivId": "2602.16790",
      "title": "Generative Audio Extension and Morphing",
      "authors": [
        "Prem Seetharaman",
        "Oriol Nieto",
        "Justin Salamon"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.MM"
      ],
      "abstract": "In audio-related creative tasks, sound designers often seek to extend and morph different sounds from their libraries. Generative audio models, capable of creating audio using examples as references, offer promising solutions. By masking the noisy latents of a DiT and applying a novel variant of classifier-free guidance on such masked latents, we demonstrate that: (i) given an audio reference, we can extend it both forward and backward for a specified duration, and (ii) given two audio references, we can morph them seamlessly for the desired duration. Furthermore, we show that by fine-tuning the model on different types of stationary audio data we mitigate potential hallucinations. The effectiveness of our method is supported by objective metrics, with the generated audio achieving Fréchet Audio Distances (FADs) comparable to those of real samples from the training data. Additionally, we validate our results through a subjective listener test, where subjects gave positive ratings to the proposed model generations. This technique paves the way for more controllable and expressive generative sound frameworks, enabling sound designers to focus less on tedious, repetitive tasks and more on their actual creative process.",
      "url": "https://arxiv.org/abs/2602.16790",
      "pdfUrl": "https://arxiv.org/pdf/2602.16790.pdf",
      "titleJa": "生成オーディオの拡張とモーフィング"
    },
    {
      "id": "2602.16687",
      "arxivId": "2602.16687",
      "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
      "authors": [
        "Potsawee Manakul",
        "Woody Haosheng Gan",
        "Martijn Bartelds",
        "Guangzhi Sun",
        "William Held",
        "Diyi Yang"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.",
      "url": "https://arxiv.org/abs/2602.16687",
      "pdfUrl": "https://arxiv.org/pdf/2602.16687.pdf",
      "titleJa": "インターリーブされたセマンティック、音響、テキストトークンによるオープン離散オーディオ基盤モデルのスケーリング"
    },
    {
      "id": "2602.16442",
      "arxivId": "2602.16442",
      "title": "Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA",
      "authors": [
        "Kamil Jeziorek",
        "Piotr Wzorek",
        "Krzysztof Blachut",
        "Hiroshi Nakano",
        "Manon Dampfhoffer",
        "Thomas Mesquida",
        "Hiroaki Nishi",
        "Thomas Dalgaty",
        "Tomasz Kryjak"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA implementation of event-graph neural networks for audio processing. We utilise an artificial cochlea that converts time-series signals into sparse event data, reducing memory and computation costs. Our architecture was implemented on a SoC FPGA and evaluated on two open-source datasets. For classification task, our baseline floating-point model achieves 92.7% accuracy on SHD dataset - only 2.4% below the state of the art - while requiring over 10x and 67x fewer parameters. On SSC, our models achieve 66.9-71.0% accuracy. Compared to FPGA-based spiking neural networks, our quantised model reaches 92.3% accuracy, outperforming them by up to 19.3% while reducing resource usage and latency. For SSC, we report the first hardware-accelerated evaluation. We further demonstrate the first end-to-end FPGA implementation of event-audio keyword spotting, combining graph convolutional layers with recurrent sequence modelling. The system achieves up to 95% word-end detection accuracy, with only 10.53 microsecond latency and 1.18 W power consumption, establishing a strong benchmark for energy-efficient event-driven KWS.",
      "url": "https://arxiv.org/abs/2602.16442",
      "pdfUrl": "https://arxiv.org/pdf/2602.16442.pdf",
      "titleJa": "ハードウェアアクセラレーショングラフニューラルネットワーク：SoC FPGA上でのニューロモルフィックイベントベースのオーディオ分類とキーワードスポッティングの代替アプローチ"
    },
    {
      "id": "2602.16421",
      "arxivId": "2602.16421",
      "title": "SELEBI: Percussion-aware Time Stretching via Selective Magnitude Spectrogram Compression by Nonstationary Gabor Transform",
      "authors": [
        "Natsuki Akaishi",
        "Nicki Holighaus",
        "Kohei Yatabe"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Phase vocoder-based time-stretching is a widely used technique for the time-scale modification of audio signals. However, conventional implementations suffer from ``percussion smearing,'' a well-known artifact that significantly degrades the quality of percussive components. We attribute this artifact to a fundamental time-scale mismatch between the temporally smeared magnitude spectrogram and the localized, newly generated phase. To address this, we propose SELEBI, a signal-adaptive phase vocoder algorithm that significantly reduces percussion smearing while preserving stability and the perfect reconstruction property. Unlike conventional methods that rely on heuristic processing or component separation, our approach leverages the nonstationary Gabor transform. By dynamically adapting analysis window lengths to assign short windows to intervals containing significant energy associated with percussive components, we directly compute a temporally localized magnitude spectrogram from the time-domain signal. This approach ensures greater consistency between the temporal structures of the magnitude and phase. Furthermore, the perfect reconstruction property of the nonstationary Gabor transform guarantees stable, high-fidelity signal synthesis, in contrast to previous heuristic approaches. Experimental results demonstrate that the proposed method effectively mitigates percussion smearing and yields natural sound quality.",
      "url": "https://arxiv.org/abs/2602.16421",
      "pdfUrl": "https://arxiv.org/pdf/2602.16421.pdf",
      "titleJa": "SELEBI: 非定常ガボール変換による選択的振幅スペクトログラム圧縮による打楽器を考慮した時間伸縮"
    },
    {
      "id": "2602.16416",
      "arxivId": "2602.16416",
      "title": "Online Single-Channel Audio-Based Sound Speed Estimation for Robust Multi-Channel Audio Control",
      "authors": [
        "Andreas Jonas Fuglsig",
        "Mads Græsbøll Christensen",
        "Jesper Rindom Jensen"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Robust spatial audio control relies on accurate acoustic propagation models, yet environmental variations, especially changes in the speed of sound, cause systematic mismatches that degrade performance. Existing methods either assume known sound speed, require multiple microphones, or rely on separate calibration, making them impractical for systems with minimal sensing. We propose an online sound speed estimator that operates during general multichannel audio playback and requires only a single observation microphone. The method exploits the structured effect of sound speed on the reproduced signal and estimates it by minimizing the mismatch between the measured audio and a parametric acoustic model. Simulations show accurate tracking of sound speed for diverse input signals and improved spatial control performance when the estimates are used to compensate propagation errors in a sound zone control framework.",
      "url": "https://arxiv.org/abs/2602.16416",
      "pdfUrl": "https://arxiv.org/pdf/2602.16416.pdf",
      "titleJa": "堅牢なマルチチャンネルオーディオ制御のためのオンラインシングルチャンネルオーディオベースの音速推定"
    },
    {
      "id": "2602.16399",
      "arxivId": "2602.16399",
      "title": "Multi-Channel Replay Speech Detection using Acoustic Maps",
      "authors": [
        "Michael Neri",
        "Tuomas Virtanen"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Replay attacks remain a critical vulnerability for automatic speaker verification systems, particularly in real-time voice assistant applications. In this work, we propose acoustic maps as a novel spatial feature representation for replay speech detection from multi-channel recordings. Derived from classical beamforming over discrete azimuth and elevation grids, acoustic maps encode directional energy distributions that reflect physical differences between human speech radiation and loudspeaker-based replay. A lightweight convolutional neural network is designed to operate on this representation, achieving competitive performance on the ReMASC dataset with approximately 6k trainable parameters. Experimental results show that acoustic maps provide a compact and physically interpretable feature space for replay attack detection across different devices and acoustic environments.",
      "url": "https://arxiv.org/abs/2602.16399",
      "pdfUrl": "https://arxiv.org/pdf/2602.16399.pdf",
      "titleJa": "音響マップを用いたマルチチャンネル再生音声検出"
    },
    {
      "id": "2602.16343",
      "arxivId": "2602.16343",
      "title": "How to Label Resynthesized Audio: The Dual Role of Neural Audio Codecs in Audio Deepfake Detection",
      "authors": [
        "Yixuan Xiao",
        "Florian Lux",
        "Alejandro Pérez-González-de-Martos",
        "Ngoc Thang Vu"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Since Text-to-Speech systems typically don't produce waveforms directly, recent spoof detection studies use resynthesized waveforms from vocoders and neural audio codecs to simulate an attacker. Unlike vocoders, which are specifically designed for speech synthesis, neural audio codecs were originally developed for compressing audio for storage and transmission. However, their ability to discretize speech also sparked interest in language-modeling-based speech synthesis. Owing to this dual functionality, codec resynthesized data may be labeled as either bonafide or spoof. So far, very little research has addressed this issue. In this study, we present a challenging extension of the ASVspoof 5 dataset constructed for this purpose. We examine how different labeling choices affect detection performance and provide insights into labeling strategies.",
      "url": "https://arxiv.org/abs/2602.16343",
      "pdfUrl": "https://arxiv.org/pdf/2602.16343.pdf",
      "titleJa": "再合成されたオーディオのラベル付け方法：オーディオディープフェイク検出におけるニューラルオーディオコーデックの二重の役割"
    },
    {
      "id": "2602.16334",
      "arxivId": "2602.16334",
      "title": "Spatial Audio Question Answering and Reasoning on Dynamic Source Movements",
      "authors": [
        "Arvind Krishna Sridhar",
        "Yinyi Guo",
        "Erik Visser"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Spatial audio understanding aims to enable machines to interpret complex auditory scenes, particularly when sound sources move over time. In this work, we study Spatial Audio Question Answering (Spatial AQA) with a focus on movement reasoning, where a model must infer object motion, position, and directional changes directly from stereo audio. First, we introduce a movement-centric spatial audio augmentation framework that synthesizes diverse motion patterns from isolated mono audio events, enabling controlled and scalable training data generation. Second, we propose an end-to-end multimodal finetuning approach with a thinking mode, which allows audio-language models to produce explicit intermediate reasoning steps before predicting an answer. Third, we investigate the impact of query-conditioned source separation as a preprocessing stage and compare three inference regimes: no masking, an audio grounding model (AGM), and ground-truth masks. Our results show that reasoning amplifies the benefits of source separation, with thinking mode showing significant improvement of +5.1% when a single event is present in the question. These findings highlight the interplay between movement modeling, reasoning, and separation quality, offering new insights for advancing spatial audio understanding.",
      "url": "https://arxiv.org/abs/2602.16334",
      "pdfUrl": "https://arxiv.org/pdf/2602.16334.pdf",
      "titleJa": "動的音源移動に関する空間オーディオ質問応答と推論"
    },
    {
      "id": "2602.16305",
      "arxivId": "2602.16305",
      "title": "BAT: Better Audio Transformer Guided by Convex Gated Probing",
      "authors": [
        "Houtan Ghaffari",
        "Lukas Rauch",
        "Christoph Scholz",
        "Paul Devos"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Probing is widely adopted in computer vision to faithfully evaluate self-supervised learning (SSL) embeddings, as fine-tuning may misrepresent their inherent quality. In contrast, audio SSL models still rely on fine-tuning because simple probing fails to unlock their full potential and alters their rankings when competing for SOTA on AudioSet. Hence, a robust and efficient probing mechanism is required to guide the trajectory of audio SSL towards reliable and reproducible methods. We introduce Convex Gated Probing (CGP), a prototype-based method that drastically closes the gap between fine-tuning and probing in audio. CGP efficiently utilizes all frozen layers via a gating mechanism and exposes the location of latent task-relevant information. Guided by CGP, we rework the entire SSL pipeline of current SOTA audio models that use legacy implementations of prior SSL methods. By refining data preprocessing, model architecture, and pre-training recipe, we introduce Better Audio Transformer (BAT), and establish new SOTA on audio benchmarks.",
      "url": "https://arxiv.org/abs/2602.16305",
      "pdfUrl": "https://arxiv.org/pdf/2602.16305.pdf",
      "titleJa": "BAT: 凸型ゲートプローブによる優れたオーディオトランスフォーマー"
    },
    {
      "id": "2602.16256",
      "arxivId": "2602.16256",
      "title": "Color-based Emotion Representation for Speech Emotion Recognition",
      "authors": [
        "Ryotaro Nagase",
        "Ryoichi Takashima",
        "Yoichi Yamashita"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Speech emotion recognition (SER) has traditionally relied on categorical or dimensional labels. However, this technique is limited in representing both the diversity and interpretability of emotions. To overcome this limitation, we focus on color attributes, such as hue, saturation, and value, to represent emotions as continuous and interpretable scores. We annotated an emotional speech corpus with color attributes via crowdsourcing and analyzed them. Moreover, we built regression models for color attributes in SER using machine learning and deep learning, and explored the multitask learning of color attribute regression and emotion classification. As a result, we demonstrated the relationship between color attributes and emotions in speech, and successfully developed color attribute regression models for SER. We also showed that multitask learning improved the performance of each task.",
      "url": "https://arxiv.org/abs/2602.16256",
      "pdfUrl": "https://arxiv.org/pdf/2602.16256.pdf",
      "titleJa": "音声感情認識のための色彩に基づく感情表現"
    },
    {
      "id": "2602.16253",
      "arxivId": "2602.16253",
      "title": "How Much Does Machine Identity Matter in Anomalous Sound Detection at Test Time?",
      "authors": [
        "Kevin Wilkinghoff",
        "Keisuke Imoto",
        "Zheng-Hua Tan"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Anomalous sound detection (ASD) benchmarks typically assume that the identity of the monitored machine is known at test time and that recordings are evaluated in a machine-wise manner. However, in realistic monitoring scenarios with multiple known machines operating concurrently, test recordings may not be reliably attributable to a specific machine, and requiring machine identity imposes deployment constraints such as dedicated sensors per machine. To reveal performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, we consider a minimal modification of the ASD evaluation protocol in which test recordings from multiple machines are merged and evaluated jointly without access to machine identity at inference time. Training data and evaluation metrics remain unchanged, and machine identity labels are used only for post hoc evaluation. Experiments with representative ASD methods show that relaxing this assumption reveals performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, and that these degradations are strongly related to implicit machine identification accuracy.",
      "url": "https://arxiv.org/abs/2602.16253",
      "pdfUrl": "https://arxiv.org/pdf/2602.16253.pdf",
      "titleJa": "テスト時の異常音検出において、マシン ID はどの程度重要ですか?"
    },
    {
      "id": "2602.16118",
      "arxivId": "2602.16118",
      "title": "Real time fault detection in 3D printers using Convolutional Neural Networks and acoustic signals",
      "authors": [
        "Muhammad Fasih Waheed",
        "Shonda Bernadin"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The reliability and quality of 3D printing processes are critically dependent on the timely detection of mechanical faults. Traditional monitoring methods often rely on visual inspection and hardware sensors, which can be both costly and limited in scope. This paper explores a scalable and contactless method for the use of real-time audio signal analysis for detecting mechanical faults in 3D printers. By capturing and classifying acoustic emissions during the printing process, we aim to identify common faults such as nozzle clogging, filament breakage, pully skipping and various other mechanical faults. Utilizing Convolutional neural networks, we implement algorithms capable of real-time audio classification to detect these faults promptly. Our methodology involves conducting a series of controlled experiments to gather audio data, followed by the application of advanced machine learning models for fault detection. Additionally, we review existing literature on audio-based fault detection in manufacturing and 3D printing to contextualize our research within the broader field. Preliminary results demonstrate that audio signals, when analyzed with machine learning techniques, provide a reliable and cost-effective means of enhancing real-time fault detection.",
      "url": "https://arxiv.org/abs/2602.16118",
      "pdfUrl": "https://arxiv.org/pdf/2602.16118.pdf",
      "titleJa": "畳み込みニューラルネットワークと音響信号を用いた3Dプリンターのリアルタイム故障検出"
    },
    {
      "id": "2602.16008",
      "arxivId": "2602.16008",
      "title": "MAEB: Massive Audio Embedding Benchmark",
      "authors": [
        "Adnan El Assadi",
        "Isaac Chung",
        "Chenghao Xiao",
        "Roman Solomatin",
        "Animesh Jha",
        "Rahul Chand",
        "Silky Singh",
        "Kaitlyn Wang",
        "Ali Sartaz Khan",
        "Marc Moussa Nasser",
        "Sufen Fong",
        "Pengfei He",
        "Alan Xiao",
        "Ayush Sunil Munot",
        "Aditya Shrivastava",
        "Artem Gazizov",
        "Niklas Muennighoff",
        "Kenneth Enevoldsen"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.",
      "url": "https://arxiv.org/abs/2602.16008",
      "pdfUrl": "https://arxiv.org/pdf/2602.16008.pdf",
      "titleJa": "MAEB: 大規模オーディオ埋め込みベンチマーク"
    },
    {
      "id": "2602.15766",
      "arxivId": "2602.15766",
      "title": "TAC: Timestamped Audio Captioning",
      "authors": [
        "Sonal Kumar",
        "Prem Seetharaman",
        "Ke Chen",
        "Oriol Nieto",
        "Jiaqi Su",
        "Zhepei Wang",
        "Rithesh Kumar",
        "Dinesh Manocha",
        "Nicholas J. Bryan",
        "Zeyu Jin",
        "Justin Salamon"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Large Audio Language Models struggle to disentangle overlapping events in complex acoustic scenes, yielding temporally inconsistent captions and frequent hallucinations. We introduce Timestamped Audio Captioner (TAC), a model that produces temporally grounded audio descriptions at varying degrees of detail and resolution. TAC is trained with a synthetic data pipeline that constructs challenging and dynamic mixtures from real-world audio sources, enabling robust learning under realistic polyphonic conditions. Across event detection and dense captioning, TAC outperforms all competing methods, with a low hallucination rate and accurate temporal grounding. We also introduce TAC-V, an audio-visual pipeline to generate semantically rich audio-visual descriptions. We then show that TAC and TAC-V serves as a \"semantic bridge\" for a text-only reasoner: a simple TAC$\\rightarrow$LLM and TAC-V$\\rightarrow$LLM cascade achieves state-of-the-art scores on benchmarks for both audio (MMAU-Pro, MMSU, MMAR) and audio-visual (DailyOmni, VideoHolmes) understanding and reasoning respectively.",
      "url": "https://arxiv.org/abs/2602.15766",
      "pdfUrl": "https://arxiv.org/pdf/2602.15766.pdf",
      "titleJa": "TAC: タイムスタンプ付き音声字幕"
    },
    {
      "id": "2602.15749",
      "arxivId": "2602.15749",
      "title": "A Generative-First Neural Audio Autoencoder",
      "authors": [
        "Jonah Casebeer",
        "Ge Zhu",
        "Zhepei Wang",
        "Nicholas J. Bryan"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Neural autoencoders underpin generative models. Practical, large-scale use of neural autoencoders for generative modeling necessitates fast encoding, low latent rates, and a single model across representations. Existing approaches are reconstruction-first: they incur high latent rates, slow encoding, and separate architectures for discrete vs. continuous latents and for different audio channel formats, hindering workflows from preprocessing to inference conditioning. We introduce a generative-first architecture for audio autoencoding that increases temporal downsampling from 2048x to 3360x and supports continuous and discrete representations and common audio channel formats in one model. By balancing compression, quality, and speed, it delivers 10x faster encoding, 1.6x lower rates, and eliminates channel-format-specific variants while maintaining competitive reconstruction quality. This enables applications previously constrained by processing costs: a 60-second mono signal compresses to 788 tokens, making generative modeling more tractable.",
      "url": "https://arxiv.org/abs/2602.15749",
      "pdfUrl": "https://arxiv.org/pdf/2602.15749.pdf",
      "titleJa": "ジェネレーティブファーストのニューラルオーディオオートエンコーダー"
    },
    {
      "id": "2602.15651",
      "arxivId": "2602.15651",
      "title": "UniTAF: A Modular Framework for Joint Text-to-Speech and Audio-to-Face Modeling",
      "authors": [
        "Qiangong Zhou",
        "Nagasaka Tomohiro"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "cs.CV",
        "eess.AS"
      ],
      "abstract": "This work considers merging two independent models, TTS and A2F, into a unified model to enable internal feature transfer, thereby improving the consistency between audio and facial expressions generated from text. We also discuss the extension of the emotion control mechanism from TTS to the joint model. This work does not aim to showcase generation quality; instead, from a system design perspective, it validates the feasibility of reusing intermediate representations from TTS for joint modeling of speech and facial expressions, and provides engineering practice references for subsequent speech expression co-design. The project code has been open source at: https://github.com/GoldenFishes/UniTAF",
      "url": "https://arxiv.org/abs/2602.15651",
      "pdfUrl": "https://arxiv.org/pdf/2602.15651.pdf",
      "titleJa": "UniTAF: テキスト音声合成と音声顔認識の統合モデリングのためのモジュール式フレームワーク"
    },
    {
      "id": "2602.15519",
      "arxivId": "2602.15519",
      "title": "Enroll-on-Wakeup: A First Comparative Study of Target Speech Extraction for Seamless Interaction in Real Noisy Human-Machine Dialogue Scenarios",
      "authors": [
        "Yiming Yang",
        "Guangyong Wang",
        "Haixin Guan",
        "Yanhua Long"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Target speech extraction (TSE) typically relies on pre-recorded high-quality enrollment speech, which disrupts user experience and limits feasibility in spontaneous interaction. In this paper, we propose Enroll-on-Wakeup (EoW), a novel framework where the wake-word segment, captured naturally during human-machine interaction, is automatically utilized as the enrollment reference. This eliminates the need for pre-collected speech to enable a seamless experience. We perform the first systematic study of EoW-TSE, evaluating advanced discriminative and generative models under real diverse acoustic conditions. Given the short and noisy nature of wake-word segments, we investigate enrollment augmentation using LLM-based TTS. Results show that while current TSE models face performance degradation in EoW-TSE, TTS-based assistance significantly enhances the listening experience, though gaps remain in speech recognition accuracy.",
      "url": "https://arxiv.org/abs/2602.15519",
      "pdfUrl": "https://arxiv.org/pdf/2602.15519.pdf",
      "titleJa": "エンロールオンウェイクアップ：実際のノイズ環境における人間と機械の対話シナリオにおけるシームレスなインタラクションのためのターゲット音声抽出に関する初の比較研究"
    },
    {
      "id": "2602.17664",
      "arxivId": "2602.17664",
      "title": "Sink-Aware Pruning for Diffusion Language Models",
      "authors": [
        "Aidar Myrzakhan",
        "Tianyi Li",
        "Bowei Guo",
        "Shengkun Tang",
        "Zhiqiang Shen"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\\bf \\texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.",
      "url": "https://arxiv.org/abs/2602.17664",
      "pdfUrl": "https://arxiv.org/pdf/2602.17664.pdf",
      "titleJa": "拡散言語モデルのためのシンクを考慮した枝刈り"
    },
    {
      "id": "2602.17663",
      "arxivId": "2602.17663",
      "title": "CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts",
      "authors": [
        "Juri Opitz",
        "Corina Raclé",
        "Emanuela Boros",
        "Andrianos Michail",
        "Matteo Romanello",
        "Maud Ehrmann",
        "Simon Clematide"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "abstract": "HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ (\"Has the person ever been at this place?\") and $isAt$ (\"Is the person located at this place around publication time?\") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.",
      "url": "https://arxiv.org/abs/2602.17663",
      "pdfUrl": "https://arxiv.org/pdf/2602.17663.pdf",
      "titleJa": "CLEF HIPE-2026: 多言語歴史文書からの正確かつ効率的な人・場所関係抽出の評価"
    },
    {
      "id": "2602.17658",
      "arxivId": "2602.17658",
      "title": "MARS: Margin-Aware Reward-Modeling with Self-Refinement",
      "authors": [
        "Payel Bhattacharjee",
        "Osvaldo Simeone",
        "Ravi Tandon"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "abstract": "Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.",
      "url": "https://arxiv.org/abs/2602.17658",
      "pdfUrl": "https://arxiv.org/pdf/2602.17658.pdf",
      "titleJa": "MARS: 自己改良によるマージンを考慮した報酬モデリング"
    },
    {
      "id": "2602.17645",
      "arxivId": "2602.17645",
      "title": "Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting",
      "authors": [
        "Xiaohan Zhao",
        "Zhaoyi Li",
        "Yaxin Luo",
        "Jiacheng Cui",
        "Zhiqiang Shen"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "abstract": "Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.",
      "url": "https://arxiv.org/abs/2602.17645",
      "pdfUrl": "https://arxiv.org/pdf/2602.17645.pdf",
      "titleJa": "きめ細かな詳細ターゲティングによるブラックボックスLVLM攻撃の限界を押し広げる"
    },
    {
      "id": "2602.17641",
      "arxivId": "2602.17641",
      "title": "FAMOSE: A ReAct Approach to Automated Feature Discovery",
      "authors": [
        "Keith Burghardt",
        "Jienan Liu",
        "Sadman Sakib",
        "Yuning Hao",
        "Bo Li"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.",
      "url": "https://arxiv.org/abs/2602.17641",
      "pdfUrl": "https://arxiv.org/pdf/2602.17641.pdf",
      "titleJa": "FAMOSE: 自動特徴検出のためのReActアプローチ"
    },
    {
      "id": "2602.17634",
      "arxivId": "2602.17634",
      "title": "Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting",
      "authors": [
        "Xinghong Fu",
        "Yanhong Li",
        "Georgios Papaioannou",
        "Yoon Kim"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.",
      "url": "https://arxiv.org/abs/2602.17634",
      "pdfUrl": "https://arxiv.org/pdf/2602.17634.pdf",
      "titleJa": "Reverso: ゼロショット予測のための効率的な時系列基礎モデル"
    },
    {
      "id": "2602.17633",
      "arxivId": "2602.17633",
      "title": "When to Trust the Cheap Check: Weak and Strong Verification for Reasoning",
      "authors": [
        "Shayan Kiyani",
        "Sima Noorani",
        "George Pappas",
        "Hamed Hassani"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "abstract": "Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier.",
      "url": "https://arxiv.org/abs/2602.17633",
      "pdfUrl": "https://arxiv.org/pdf/2602.17633.pdf",
      "titleJa": "安価なチェックをいつ信頼するか：推論のための弱い検証と強い検証"
    },
    {
      "id": "2602.17632",
      "arxivId": "2602.17632",
      "title": "SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer",
      "authors": [
        "Nathan S. de Lara",
        "Florian Shkurti"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces regret by 34-58% over the best baseline.",
      "url": "https://arxiv.org/abs/2602.17632",
      "pdfUrl": "https://arxiv.org/pdf/2602.17632.pdf",
      "titleJa": "SMAC: 堅牢なオフラインからオンラインへの転送のためのスコアマッチングされたアクター・クリティック"
    },
    {
      "id": "2602.17616",
      "arxivId": "2602.17616",
      "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
      "authors": [
        "Luke Huang",
        "Zhuoyang Zhang",
        "Qinghao Hu",
        "Shang Yang",
        "Song Han"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\\textbf{V}$ariance $\\textbf{C}$ontrolled $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.",
      "url": "https://arxiv.org/abs/2602.17616",
      "pdfUrl": "https://arxiv.org/pdf/2602.17616.pdf",
      "titleJa": "安定した非同期性：LLMのための分散制御オフポリシー強化学習"
    },
    {
      "id": "2602.17608",
      "arxivId": "2602.17608",
      "title": "Towards Anytime-Valid Statistical Watermarking",
      "authors": [
        "Baihe Huang",
        "Eric Xu",
        "Kannan Ramchandran",
        "Jiantao Jiao",
        "Michael I. Jordan"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.",
      "url": "https://arxiv.org/abs/2602.17608",
      "pdfUrl": "https://arxiv.org/pdf/2602.17608.pdf",
      "titleJa": "いつでも有効な統計的透かしに向けて"
    },
    {
      "id": "2602.17607",
      "arxivId": "2602.17607",
      "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
      "authors": [
        "Jianda Du",
        "Youran Sun",
        "Haizhao Yang"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "abstract": "PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \\texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \\texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.",
      "url": "https://arxiv.org/abs/2602.17607",
      "pdfUrl": "https://arxiv.org/pdf/2602.17607.pdf",
      "titleJa": "AutoNumerics: 科学計算のための自律型、偏微分方程式非依存型マルチエージェントパイプライン"
    },
    {
      "id": "2602.17605",
      "arxivId": "2602.17605",
      "title": "Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery",
      "authors": [
        "Jowaria Khan",
        "Anindya Sarkar",
        "Yevgeniy Vorobeychik",
        "Elizabeth Bondi-Kelly"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "abstract": "In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.",
      "url": "https://arxiv.org/abs/2602.17605",
      "pdfUrl": "https://arxiv.org/pdf/2602.17605.pdf",
      "titleJa": "リアルタイムで能動的に適応する：地理空間発見のための潜在概念を用いた関連性誘導型オンラインメタ学習"
    },
    {
      "id": "2602.17602",
      "arxivId": "2602.17602",
      "title": "MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models",
      "authors": [
        "Hojung Jung",
        "Rodrigo Hormazabal",
        "Jaehyeong Jo",
        "Youngrok Park",
        "Kyunggeun Roh",
        "Se-Young Yun",
        "Sehui Han",
        "Dae-Woong Jeong"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.",
      "url": "https://arxiv.org/abs/2602.17602",
      "pdfUrl": "https://arxiv.org/pdf/2602.17602.pdf",
      "titleJa": "MolHIT: 階層的離散拡散モデルを用いた分子グラフ生成の高度化"
    },
    {
      "id": "2602.17598",
      "arxivId": "2602.17598",
      "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
      "authors": [
        "Jayadev Billa"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.",
      "url": "https://arxiv.org/abs/2602.17598",
      "pdfUrl": "https://arxiv.org/pdf/2602.17598.pdf",
      "titleJa": "カスケード等価性仮説: 音声 LLM が ASR$\\rightarrow$LLM パイプラインのように動作するのはどのような場合ですか?"
    },
    {
      "id": "2602.17594",
      "arxivId": "2602.17594",
      "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
      "authors": [
        "Lance Ying",
        "Ryan Truong",
        "Prafull Sharma",
        "Kaiya Ivy Zhao",
        "Nathan Cloos",
        "Kelsey R. Allen",
        "Thomas L. Griffiths",
        "Katherine M. Collins",
        "José Hernández-Orallo",
        "Phillip Isola",
        "Samuel J. Gershman",
        "Joshua B. Tenenbaum"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \\textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.",
      "url": "https://arxiv.org/abs/2602.17594",
      "pdfUrl": "https://arxiv.org/pdf/2602.17594.pdf",
      "titleJa": "AIゲームストア：人間のゲームによる機械の汎用知能のスケーラブルでオープンエンドな評価"
    },
    {
      "id": "2602.17586",
      "arxivId": "2602.17586",
      "title": "Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space",
      "authors": [
        "Antonio Guillen-Perez"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.",
      "url": "https://arxiv.org/abs/2602.17586",
      "pdfUrl": "https://arxiv.org/pdf/2602.17586.pdf",
      "titleJa": "多様体を考慮したスペクトル空間における自動運転における連続異常検出のための条件付きフローマッチング"
    },
    {
      "id": "2602.17568",
      "arxivId": "2602.17568",
      "title": "Be Wary of Your Time Series Preprocessing",
      "authors": [
        "Sofiane Ennadir",
        "Tianze Wang",
        "Oleg Smirnov",
        "Sahar Asadi",
        "Lele Cao"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model's representational capacity, depending on the task and data characteristics. We complement our theory with empirical validation on classification and forecasting benchmarks using multiple Transformer-based models. Our results show that no single normalization method consistently outperforms others, and in some cases, omitting normalization entirely leads to superior performance. These findings highlight the critical role of preprocessing in time series learning and motivate the need for more principled normalization strategies tailored to specific tasks and datasets.",
      "url": "https://arxiv.org/abs/2602.17568",
      "pdfUrl": "https://arxiv.org/pdf/2602.17568.pdf",
      "titleJa": "時系列の前処理に注意する"
    },
    {
      "id": "2602.17566",
      "arxivId": "2602.17566",
      "title": "A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN",
      "authors": [
        "Asif Hasan Chowdhury",
        "Md. Fahim Islam",
        "M Ragib Anjum Riad",
        "Faiyaz Bin Hashem",
        "Md Tanzim Reza",
        "Md. Golam Rabiul Alam"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.AI"
      ],
      "abstract": "The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.",
      "url": "https://arxiv.org/abs/2602.17566",
      "pdfUrl": "https://arxiv.org/pdf/2602.17566.pdf",
      "titleJa": "SWINトランスフォーマーとCNNの融合を活用した肺疾患診断のためのハイブリッド連合学習ベースのアンサンブルアプローチ"
    },
    {
      "id": "2602.17560",
      "arxivId": "2602.17560",
      "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
      "authors": [
        "Hongjue Zhao",
        "Haosen Sun",
        "Jiangtao Kong",
        "Xiaochang Li",
        "Qineng Wang",
        "Liwei Jiang",
        "Qi Zhu",
        "Tarek Abdelzaher",
        "Yejin Choi",
        "Manling Li",
        "Huajie Shao"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \\textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \\textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \\textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \\textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\\%$ improvement over TruthfulQA, $2.5\\%$ over UltraFeedback, and $2.4\\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.",
      "url": "https://arxiv.org/abs/2602.17560",
      "pdfUrl": "https://arxiv.org/pdf/2602.17560.pdf",
      "titleJa": "ODESteer: LLMアライメントのための統合ODEベースのステアリングフレームワーク"
    },
    {
      "id": "2602.17557",
      "arxivId": "2602.17557",
      "title": "Probability-Invariant Random Walk Learning on Gyral Folding-Based Cortical Similarity Networks for Alzheimer's and Lewy Body Dementia Diagnosis",
      "authors": [
        "Minheng Chen",
        "Jing Zhang",
        "Tong Chen",
        "Chao Cao",
        "Tianming Liu",
        "Li Su",
        "Dajiang Zhu"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV"
      ],
      "abstract": "Alzheimer's disease (AD) and Lewy body dementia (LBD) present overlapping clinical features yet require distinct diagnostic strategies. While neuroimaging-based brain network analysis is promising, atlas-based representations may obscure individualized anatomy. Gyral folding-based networks using three-hinge gyri provide a biologically grounded alternative, but inter-individual variability in cortical folding results in inconsistent landmark correspondence and highly irregular network sizes, violating the fixed-topology and node-alignment assumptions of most existing graph learning methods, particularly in clinical datasets where pathological changes further amplify anatomical heterogeneity. We therefore propose a probability-invariant random-walk-based framework that classifies individualized gyral folding networks without explicit node alignment. Cortical similarity networks are built from local morphometric features and represented by distributions of anonymized random walks, with an anatomy-aware encoding that preserves permutation invariance. Experiments on a large clinical cohort of AD and LBD subjects show consistent improvements over existing gyral folding and atlas-based models, demonstrating robustness and potential for dementia diagnosis.",
      "url": "https://arxiv.org/abs/2602.17557",
      "pdfUrl": "https://arxiv.org/pdf/2602.17557.pdf",
      "titleJa": "アルツハイマー病およびレビー小体型認知症の診断のための回旋折り畳みに基づく皮質類似性ネットワークにおける確率不変ランダムウォーク学習"
    }
  ],
  "lastUpdated": "2026-02-21T01:03:23.208504",
  "totalCount": 40
}