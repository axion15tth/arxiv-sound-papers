{
  "papers": [
    {
      "id": "2602.16687",
      "arxivId": "2602.16687",
      "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
      "authors": [
        "Potsawee Manakul",
        "Woody Haosheng Gan",
        "Martijn Bartelds",
        "Guangzhi Sun",
        "William Held",
        "Diyi Yang"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.",
      "url": "https://arxiv.org/abs/2602.16687",
      "pdfUrl": "https://arxiv.org/pdf/2602.16687.pdf",
      "titleJa": "インターリーブされたセマンティック、音響、テキストトークンによるオープン離散オーディオ基盤モデルのスケーリング"
    },
    {
      "id": "2602.16442",
      "arxivId": "2602.16442",
      "title": "Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA",
      "authors": [
        "Kamil Jeziorek",
        "Piotr Wzorek",
        "Krzysztof Blachut",
        "Hiroshi Nakano",
        "Manon Dampfhoffer",
        "Thomas Mesquida",
        "Hiroaki Nishi",
        "Thomas Dalgaty",
        "Tomasz Kryjak"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA implementation of event-graph neural networks for audio processing. We utilise an artificial cochlea that converts time-series signals into sparse event data, reducing memory and computation costs. Our architecture was implemented on a SoC FPGA and evaluated on two open-source datasets. For classification task, our baseline floating-point model achieves 92.7% accuracy on SHD dataset - only 2.4% below the state of the art - while requiring over 10x and 67x fewer parameters. On SSC, our models achieve 66.9-71.0% accuracy. Compared to FPGA-based spiking neural networks, our quantised model reaches 92.3% accuracy, outperforming them by up to 19.3% while reducing resource usage and latency. For SSC, we report the first hardware-accelerated evaluation. We further demonstrate the first end-to-end FPGA implementation of event-audio keyword spotting, combining graph convolutional layers with recurrent sequence modelling. The system achieves up to 95% word-end detection accuracy, with only 10.53 microsecond latency and 1.18 W power consumption, establishing a strong benchmark for energy-efficient event-driven KWS.",
      "url": "https://arxiv.org/abs/2602.16442",
      "pdfUrl": "https://arxiv.org/pdf/2602.16442.pdf",
      "titleJa": "ハードウェアアクセラレーショングラフニューラルネットワーク：SoC FPGA上でのニューロモルフィックイベントベースのオーディオ分類とキーワードスポッティングの代替アプローチ"
    },
    {
      "id": "2602.16421",
      "arxivId": "2602.16421",
      "title": "SELEBI: Percussion-aware Time Stretching via Selective Magnitude Spectrogram Compression by Nonstationary Gabor Transform",
      "authors": [
        "Natsuki Akaishi",
        "Nicki Holighaus",
        "Kohei Yatabe"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Phase vocoder-based time-stretching is a widely used technique for the time-scale modification of audio signals. However, conventional implementations suffer from ``percussion smearing,'' a well-known artifact that significantly degrades the quality of percussive components. We attribute this artifact to a fundamental time-scale mismatch between the temporally smeared magnitude spectrogram and the localized, newly generated phase. To address this, we propose SELEBI, a signal-adaptive phase vocoder algorithm that significantly reduces percussion smearing while preserving stability and the perfect reconstruction property. Unlike conventional methods that rely on heuristic processing or component separation, our approach leverages the nonstationary Gabor transform. By dynamically adapting analysis window lengths to assign short windows to intervals containing significant energy associated with percussive components, we directly compute a temporally localized magnitude spectrogram from the time-domain signal. This approach ensures greater consistency between the temporal structures of the magnitude and phase. Furthermore, the perfect reconstruction property of the nonstationary Gabor transform guarantees stable, high-fidelity signal synthesis, in contrast to previous heuristic approaches. Experimental results demonstrate that the proposed method effectively mitigates percussion smearing and yields natural sound quality.",
      "url": "https://arxiv.org/abs/2602.16421",
      "pdfUrl": "https://arxiv.org/pdf/2602.16421.pdf",
      "titleJa": "SELEBI: 非定常ガボール変換による選択的振幅スペクトログラム圧縮による打楽器を考慮した時間伸縮"
    },
    {
      "id": "2602.16416",
      "arxivId": "2602.16416",
      "title": "Online Single-Channel Audio-Based Sound Speed Estimation for Robust Multi-Channel Audio Control",
      "authors": [
        "Andreas Jonas Fuglsig",
        "Mads Græsbøll Christensen",
        "Jesper Rindom Jensen"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Robust spatial audio control relies on accurate acoustic propagation models, yet environmental variations, especially changes in the speed of sound, cause systematic mismatches that degrade performance. Existing methods either assume known sound speed, require multiple microphones, or rely on separate calibration, making them impractical for systems with minimal sensing. We propose an online sound speed estimator that operates during general multichannel audio playback and requires only a single observation microphone. The method exploits the structured effect of sound speed on the reproduced signal and estimates it by minimizing the mismatch between the measured audio and a parametric acoustic model. Simulations show accurate tracking of sound speed for diverse input signals and improved spatial control performance when the estimates are used to compensate propagation errors in a sound zone control framework.",
      "url": "https://arxiv.org/abs/2602.16416",
      "pdfUrl": "https://arxiv.org/pdf/2602.16416.pdf",
      "titleJa": "堅牢なマルチチャンネルオーディオ制御のためのオンラインシングルチャンネルオーディオベースの音速推定"
    },
    {
      "id": "2602.16399",
      "arxivId": "2602.16399",
      "title": "Multi-Channel Replay Speech Detection using Acoustic Maps",
      "authors": [
        "Michael Neri",
        "Tuomas Virtanen"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Replay attacks remain a critical vulnerability for automatic speaker verification systems, particularly in real-time voice assistant applications. In this work, we propose acoustic maps as a novel spatial feature representation for replay speech detection from multi-channel recordings. Derived from classical beamforming over discrete azimuth and elevation grids, acoustic maps encode directional energy distributions that reflect physical differences between human speech radiation and loudspeaker-based replay. A lightweight convolutional neural network is designed to operate on this representation, achieving competitive performance on the ReMASC dataset with approximately 6k trainable parameters. Experimental results show that acoustic maps provide a compact and physically interpretable feature space for replay attack detection across different devices and acoustic environments.",
      "url": "https://arxiv.org/abs/2602.16399",
      "pdfUrl": "https://arxiv.org/pdf/2602.16399.pdf",
      "titleJa": "音響マップを用いたマルチチャンネル再生音声検出"
    },
    {
      "id": "2602.16343",
      "arxivId": "2602.16343",
      "title": "How to Label Resynthesized Audio: The Dual Role of Neural Audio Codecs in Audio Deepfake Detection",
      "authors": [
        "Yixuan Xiao",
        "Florian Lux",
        "Alejandro Pérez-González-de-Martos",
        "Ngoc Thang Vu"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Since Text-to-Speech systems typically don't produce waveforms directly, recent spoof detection studies use resynthesized waveforms from vocoders and neural audio codecs to simulate an attacker. Unlike vocoders, which are specifically designed for speech synthesis, neural audio codecs were originally developed for compressing audio for storage and transmission. However, their ability to discretize speech also sparked interest in language-modeling-based speech synthesis. Owing to this dual functionality, codec resynthesized data may be labeled as either bonafide or spoof. So far, very little research has addressed this issue. In this study, we present a challenging extension of the ASVspoof 5 dataset constructed for this purpose. We examine how different labeling choices affect detection performance and provide insights into labeling strategies.",
      "url": "https://arxiv.org/abs/2602.16343",
      "pdfUrl": "https://arxiv.org/pdf/2602.16343.pdf",
      "titleJa": "再合成されたオーディオのラベル付け方法：オーディオディープフェイク検出におけるニューラルオーディオコーデックの二重の役割"
    },
    {
      "id": "2602.16334",
      "arxivId": "2602.16334",
      "title": "Spatial Audio Question Answering and Reasoning on Dynamic Source Movements",
      "authors": [
        "Arvind Krishna Sridhar",
        "Yinyi Guo",
        "Erik Visser"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Spatial audio understanding aims to enable machines to interpret complex auditory scenes, particularly when sound sources move over time. In this work, we study Spatial Audio Question Answering (Spatial AQA) with a focus on movement reasoning, where a model must infer object motion, position, and directional changes directly from stereo audio. First, we introduce a movement-centric spatial audio augmentation framework that synthesizes diverse motion patterns from isolated mono audio events, enabling controlled and scalable training data generation. Second, we propose an end-to-end multimodal finetuning approach with a thinking mode, which allows audio-language models to produce explicit intermediate reasoning steps before predicting an answer. Third, we investigate the impact of query-conditioned source separation as a preprocessing stage and compare three inference regimes: no masking, an audio grounding model (AGM), and ground-truth masks. Our results show that reasoning amplifies the benefits of source separation, with thinking mode showing significant improvement of +5.1% when a single event is present in the question. These findings highlight the interplay between movement modeling, reasoning, and separation quality, offering new insights for advancing spatial audio understanding.",
      "url": "https://arxiv.org/abs/2602.16334",
      "pdfUrl": "https://arxiv.org/pdf/2602.16334.pdf",
      "titleJa": "動的音源移動に関する空間オーディオ質問応答と推論"
    },
    {
      "id": "2602.16305",
      "arxivId": "2602.16305",
      "title": "BAT: Better Audio Transformer Guided by Convex Gated Probing",
      "authors": [
        "Houtan Ghaffari",
        "Lukas Rauch",
        "Christoph Scholz",
        "Paul Devos"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Probing is widely adopted in computer vision to faithfully evaluate self-supervised learning (SSL) embeddings, as fine-tuning may misrepresent their inherent quality. In contrast, audio SSL models still rely on fine-tuning because simple probing fails to unlock their full potential and alters their rankings when competing for SOTA on AudioSet. Hence, a robust and efficient probing mechanism is required to guide the trajectory of audio SSL towards reliable and reproducible methods. We introduce Convex Gated Probing (CGP), a prototype-based method that drastically closes the gap between fine-tuning and probing in audio. CGP efficiently utilizes all frozen layers via a gating mechanism and exposes the location of latent task-relevant information. Guided by CGP, we rework the entire SSL pipeline of current SOTA audio models that use legacy implementations of prior SSL methods. By refining data preprocessing, model architecture, and pre-training recipe, we introduce Better Audio Transformer (BAT), and establish new SOTA on audio benchmarks.",
      "url": "https://arxiv.org/abs/2602.16305",
      "pdfUrl": "https://arxiv.org/pdf/2602.16305.pdf",
      "titleJa": "BAT: 凸型ゲートプローブによる優れたオーディオトランスフォーマー"
    },
    {
      "id": "2602.16256",
      "arxivId": "2602.16256",
      "title": "Color-based Emotion Representation for Speech Emotion Recognition",
      "authors": [
        "Ryotaro Nagase",
        "Ryoichi Takashima",
        "Yoichi Yamashita"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Speech emotion recognition (SER) has traditionally relied on categorical or dimensional labels. However, this technique is limited in representing both the diversity and interpretability of emotions. To overcome this limitation, we focus on color attributes, such as hue, saturation, and value, to represent emotions as continuous and interpretable scores. We annotated an emotional speech corpus with color attributes via crowdsourcing and analyzed them. Moreover, we built regression models for color attributes in SER using machine learning and deep learning, and explored the multitask learning of color attribute regression and emotion classification. As a result, we demonstrated the relationship between color attributes and emotions in speech, and successfully developed color attribute regression models for SER. We also showed that multitask learning improved the performance of each task.",
      "url": "https://arxiv.org/abs/2602.16256",
      "pdfUrl": "https://arxiv.org/pdf/2602.16256.pdf",
      "titleJa": "音声感情認識のための色彩に基づく感情表現"
    },
    {
      "id": "2602.16253",
      "arxivId": "2602.16253",
      "title": "How Much Does Machine Identity Matter in Anomalous Sound Detection at Test Time?",
      "authors": [
        "Kevin Wilkinghoff",
        "Keisuke Imoto",
        "Zheng-Hua Tan"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Anomalous sound detection (ASD) benchmarks typically assume that the identity of the monitored machine is known at test time and that recordings are evaluated in a machine-wise manner. However, in realistic monitoring scenarios with multiple known machines operating concurrently, test recordings may not be reliably attributable to a specific machine, and requiring machine identity imposes deployment constraints such as dedicated sensors per machine. To reveal performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, we consider a minimal modification of the ASD evaluation protocol in which test recordings from multiple machines are merged and evaluated jointly without access to machine identity at inference time. Training data and evaluation metrics remain unchanged, and machine identity labels are used only for post hoc evaluation. Experiments with representative ASD methods show that relaxing this assumption reveals performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, and that these degradations are strongly related to implicit machine identification accuracy.",
      "url": "https://arxiv.org/abs/2602.16253",
      "pdfUrl": "https://arxiv.org/pdf/2602.16253.pdf",
      "titleJa": "テスト時の異常音検出において、マシン ID はどの程度重要ですか?"
    },
    {
      "id": "2602.16118",
      "arxivId": "2602.16118",
      "title": "Real time fault detection in 3D printers using Convolutional Neural Networks and acoustic signals",
      "authors": [
        "Muhammad Fasih Waheed",
        "Shonda Bernadin"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The reliability and quality of 3D printing processes are critically dependent on the timely detection of mechanical faults. Traditional monitoring methods often rely on visual inspection and hardware sensors, which can be both costly and limited in scope. This paper explores a scalable and contactless method for the use of real-time audio signal analysis for detecting mechanical faults in 3D printers. By capturing and classifying acoustic emissions during the printing process, we aim to identify common faults such as nozzle clogging, filament breakage, pully skipping and various other mechanical faults. Utilizing Convolutional neural networks, we implement algorithms capable of real-time audio classification to detect these faults promptly. Our methodology involves conducting a series of controlled experiments to gather audio data, followed by the application of advanced machine learning models for fault detection. Additionally, we review existing literature on audio-based fault detection in manufacturing and 3D printing to contextualize our research within the broader field. Preliminary results demonstrate that audio signals, when analyzed with machine learning techniques, provide a reliable and cost-effective means of enhancing real-time fault detection.",
      "url": "https://arxiv.org/abs/2602.16118",
      "pdfUrl": "https://arxiv.org/pdf/2602.16118.pdf",
      "titleJa": "畳み込みニューラルネットワークと音響信号を用いた3Dプリンターのリアルタイム故障検出"
    },
    {
      "id": "2602.16008",
      "arxivId": "2602.16008",
      "title": "MAEB: Massive Audio Embedding Benchmark",
      "authors": [
        "Adnan El Assadi",
        "Isaac Chung",
        "Chenghao Xiao",
        "Roman Solomatin",
        "Animesh Jha",
        "Rahul Chand",
        "Silky Singh",
        "Kaitlyn Wang",
        "Ali Sartaz Khan",
        "Marc Moussa Nasser",
        "Sufen Fong",
        "Pengfei He",
        "Alan Xiao",
        "Ayush Sunil Munot",
        "Aditya Shrivastava",
        "Artem Gazizov",
        "Niklas Muennighoff",
        "Kenneth Enevoldsen"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.",
      "url": "https://arxiv.org/abs/2602.16008",
      "pdfUrl": "https://arxiv.org/pdf/2602.16008.pdf",
      "titleJa": "MAEB: 大規模オーディオ埋め込みベンチマーク"
    },
    {
      "id": "2602.15766",
      "arxivId": "2602.15766",
      "title": "TAC: Timestamped Audio Captioning",
      "authors": [
        "Sonal Kumar",
        "Prem Seetharaman",
        "Ke Chen",
        "Oriol Nieto",
        "Jiaqi Su",
        "Zhepei Wang",
        "Rithesh Kumar",
        "Dinesh Manocha",
        "Nicholas J. Bryan",
        "Zeyu Jin",
        "Justin Salamon"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Large Audio Language Models struggle to disentangle overlapping events in complex acoustic scenes, yielding temporally inconsistent captions and frequent hallucinations. We introduce Timestamped Audio Captioner (TAC), a model that produces temporally grounded audio descriptions at varying degrees of detail and resolution. TAC is trained with a synthetic data pipeline that constructs challenging and dynamic mixtures from real-world audio sources, enabling robust learning under realistic polyphonic conditions. Across event detection and dense captioning, TAC outperforms all competing methods, with a low hallucination rate and accurate temporal grounding. We also introduce TAC-V, an audio-visual pipeline to generate semantically rich audio-visual descriptions. We then show that TAC and TAC-V serves as a \"semantic bridge\" for a text-only reasoner: a simple TAC$\\rightarrow$LLM and TAC-V$\\rightarrow$LLM cascade achieves state-of-the-art scores on benchmarks for both audio (MMAU-Pro, MMSU, MMAR) and audio-visual (DailyOmni, VideoHolmes) understanding and reasoning respectively.",
      "url": "https://arxiv.org/abs/2602.15766",
      "pdfUrl": "https://arxiv.org/pdf/2602.15766.pdf",
      "titleJa": "TAC: タイムスタンプ付き音声字幕"
    },
    {
      "id": "2602.15749",
      "arxivId": "2602.15749",
      "title": "A Generative-First Neural Audio Autoencoder",
      "authors": [
        "Jonah Casebeer",
        "Ge Zhu",
        "Zhepei Wang",
        "Nicholas J. Bryan"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Neural autoencoders underpin generative models. Practical, large-scale use of neural autoencoders for generative modeling necessitates fast encoding, low latent rates, and a single model across representations. Existing approaches are reconstruction-first: they incur high latent rates, slow encoding, and separate architectures for discrete vs. continuous latents and for different audio channel formats, hindering workflows from preprocessing to inference conditioning. We introduce a generative-first architecture for audio autoencoding that increases temporal downsampling from 2048x to 3360x and supports continuous and discrete representations and common audio channel formats in one model. By balancing compression, quality, and speed, it delivers 10x faster encoding, 1.6x lower rates, and eliminates channel-format-specific variants while maintaining competitive reconstruction quality. This enables applications previously constrained by processing costs: a 60-second mono signal compresses to 788 tokens, making generative modeling more tractable.",
      "url": "https://arxiv.org/abs/2602.15749",
      "pdfUrl": "https://arxiv.org/pdf/2602.15749.pdf",
      "titleJa": "ジェネレーティブファーストのニューラルオーディオオートエンコーダー"
    },
    {
      "id": "2602.15651",
      "arxivId": "2602.15651",
      "title": "UniTAF: A Modular Framework for Joint Text-to-Speech and Audio-to-Face Modeling",
      "authors": [
        "Qiangong Zhou",
        "Nagasaka Tomohiro"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "cs.CV",
        "eess.AS"
      ],
      "abstract": "This work considers merging two independent models, TTS and A2F, into a unified model to enable internal feature transfer, thereby improving the consistency between audio and facial expressions generated from text. We also discuss the extension of the emotion control mechanism from TTS to the joint model. This work does not aim to showcase generation quality; instead, from a system design perspective, it validates the feasibility of reusing intermediate representations from TTS for joint modeling of speech and facial expressions, and provides engineering practice references for subsequent speech expression co-design. The project code has been open source at: https://github.com/GoldenFishes/UniTAF",
      "url": "https://arxiv.org/abs/2602.15651",
      "pdfUrl": "https://arxiv.org/pdf/2602.15651.pdf",
      "titleJa": "UniTAF: テキスト音声合成と音声顔認識の統合モデリングのためのモジュール式フレームワーク"
    },
    {
      "id": "2602.15519",
      "arxivId": "2602.15519",
      "title": "Enroll-on-Wakeup: A First Comparative Study of Target Speech Extraction for Seamless Interaction in Real Noisy Human-Machine Dialogue Scenarios",
      "authors": [
        "Yiming Yang",
        "Guangyong Wang",
        "Haixin Guan",
        "Yanhua Long"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Target speech extraction (TSE) typically relies on pre-recorded high-quality enrollment speech, which disrupts user experience and limits feasibility in spontaneous interaction. In this paper, we propose Enroll-on-Wakeup (EoW), a novel framework where the wake-word segment, captured naturally during human-machine interaction, is automatically utilized as the enrollment reference. This eliminates the need for pre-collected speech to enable a seamless experience. We perform the first systematic study of EoW-TSE, evaluating advanced discriminative and generative models under real diverse acoustic conditions. Given the short and noisy nature of wake-word segments, we investigate enrollment augmentation using LLM-based TTS. Results show that while current TSE models face performance degradation in EoW-TSE, TTS-based assistance significantly enhances the listening experience, though gaps remain in speech recognition accuracy.",
      "url": "https://arxiv.org/abs/2602.15519",
      "pdfUrl": "https://arxiv.org/pdf/2602.15519.pdf",
      "titleJa": "エンロールオンウェイクアップ：実際のノイズ環境における人間と機械の対話シナリオにおけるシームレスなインタラクションのためのターゲット音声抽出に関する初の比較研究"
    },
    {
      "id": "2602.15491",
      "arxivId": "2602.15491",
      "title": "The Equalizer: Introducing Shape-Gain Decomposition in Neural Audio Codecs",
      "authors": [
        "Samir Sadok",
        "Laurent Girin",
        "Xavier Alameda-Pineda"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Neural audio codecs (NACs) typically encode the short-term energy (gain) and normalized structure (shape) of speech/audio signals jointly within the same latent space. As a result, they are poorly robust to a global variation of the input signal level in the sense that such variation has strong influence on the embedding vectors at the output of the encoder and their quantization. This methodology is inherently inefficient, leading to codebook redundancy and suboptimal bitrate-distortion performance. To address these limitations, we propose to introduce shape-gain decomposition, widely used in classical speech/audio coding, into the NAC framework. The principle of the proposed Equalizer methodology is to decompose the input signal -- before the NAC encoder -- into gain and normalized shape vector on a short-term basis. The shape vector is processed by the NAC, while the gain is quantized with scalar quantization and transmitted separately. The output (decoded) signal is reconstructed from the normalized output of the NAC and the quantized gain. Our experiments conducted on speech signals show that this general methodology, easily applicable to any NAC, enables a substantial gain in bitrate-distortion performance, as well as a massive reduction in complexity.",
      "url": "https://arxiv.org/abs/2602.15491",
      "pdfUrl": "https://arxiv.org/pdf/2602.15491.pdf",
      "titleJa": "イコライザー：ニューラルオーディオコーデックにおけるシェイプゲイン分解の導入"
    },
    {
      "id": "2602.15307",
      "arxivId": "2602.15307",
      "title": "What Do Neurons Listen To? A Neuron-level Dissection of a General-purpose Audio Model",
      "authors": [
        "Takao Kawamura",
        "Daisuke Niizumi",
        "Nobutaka Ono"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "In this paper, we analyze the internal representations of a general-purpose audio self-supervised learning (SSL) model from a neuron-level perspective. Despite their strong empirical performance as feature extractors, the internal mechanisms underlying the robust generalization of SSL audio models remain unclear. Drawing on the framework of mechanistic interpretability, we identify and examine class-specific neurons by analyzing conditional activation patterns across diverse tasks. Our analysis reveals that SSL models foster the emergence of class-specific neurons that provide extensive coverage across novel task classes. These neurons exhibit shared responses across different semantic categories and acoustic similarities, such as speech attributes and musical pitch. We also confirm that these neurons have a functional impact on classification performance. To our knowledge, this is the first systematic neuron-level analysis of a general-purpose audio SSL model, providing new insights into its internal representation.",
      "url": "https://arxiv.org/abs/2602.15307",
      "pdfUrl": "https://arxiv.org/pdf/2602.15307.pdf",
      "titleJa": "ニューロンは何を聞いているのか？汎用オーディオモデルのニューロンレベルの解析"
    },
    {
      "id": "2602.15909",
      "arxivId": "2602.15909",
      "title": "Resp-Agent: An Agent-Based System for Multimodal Respiratory Sound Generation and Disease Diagnosis",
      "authors": [
        "Pengfei Zhang",
        "Tianxin Xie",
        "Minghao Yang",
        "Li Liu"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.DB",
        "cs.HC",
        "cs.MA",
        "cs.SD"
      ],
      "abstract": "Deep learning-based respiratory auscultation is currently hindered by two fundamental challenges: (i) inherent information loss, as converting signals into spectrograms discards transient acoustic events and clinical context; (ii) limited data availability, exacerbated by severe class imbalance. To bridge these gaps, we present Resp-Agent, an autonomous multimodal system orchestrated by a novel Active Adversarial Curriculum Agent (Thinker-A$^2$CA). Unlike static pipelines, Thinker-A$^2$CA serves as a central controller that actively identifies diagnostic weaknesses and schedules targeted synthesis in a closed loop. To address the representation gap, we introduce a Modality-Weaving Diagnoser that weaves EHR data with audio tokens via Strategic Global Attention and sparse audio anchors, capturing both long-range clinical context and millisecond-level transients. To address the data gap, we design a Flow Matching Generator that adapts a text-only Large Language Model (LLM) via modality injection, decoupling pathological content from acoustic style to synthesize hard-to-diagnose samples. As a foundation for these efforts, we introduce Resp-229k, a benchmark corpus of 229k recordings paired with LLM-distilled clinical narratives. Extensive experiments demonstrate that Resp-Agent consistently outperforms prior approaches across diverse evaluation settings, improving diagnostic robustness under data scarcity and long-tailed class imbalance. Our code and data are available at https://github.com/zpforlove/Resp-Agent.",
      "url": "https://arxiv.org/abs/2602.15909",
      "pdfUrl": "https://arxiv.org/pdf/2602.15909.pdf",
      "titleJa": "Resp-Agent: マルチモーダル呼吸音生成と疾患診断のためのエージェントベースシステム"
    },
    {
      "id": "2602.14664",
      "arxivId": "2602.14664",
      "title": "Probing Human Articulatory Constraints in End-to-End TTS with Reverse and Mismatched Speech-Text Directions",
      "authors": [
        "Parth Khadse",
        "Sunil Kumar Kopparapu"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "cs.SD"
      ],
      "abstract": "An end-to-end (e2e) text-to-speech (TTS) system is a deep architecture that learns to associate a text string with acoustic speech patterns from a curated dataset. It is expected that all aspects associated with speech production, such as phone duration, speaker characteristics, and intonation among other things are captured in the trained TTS model to enable the synthesized speech to be natural and intelligible. Human speech is complex, involving smooth transitions between articulatory configurations (ACs). Due to anatomical constraints, some ACs are challenging to mimic or transition between. In this paper, we experimentally study if the constraints imposed by human anatomy have an implication on training an e2e-TTS systems. We experiment with two e2e-TTS architectures, namely, Tacotron-2 an autoregressive model and VITS-TTS a non-autoregressive model. In this study, we build TTS systems using (a) forward text, forward speech (conventional, e2e-TTS), (b) reverse text, reverse speech (r-e2e-TTS), and (c) reverse text, forward speech (rtfs-e2e-TTS). Experiments demonstrate that e2e-TTS systems are purely data-driven. Interestingly, the generated speech by r-e2e-TTS systems exhibits better fidelity, better perceptual intelligibility, and better naturalness",
      "url": "https://arxiv.org/abs/2602.14664",
      "pdfUrl": "https://arxiv.org/pdf/2602.14664.pdf",
      "titleJa": "逆方向および不一致な音声テキスト方向を持つエンドツーエンドTTSにおける人間の調音制約の調査"
    },
    {
      "id": "2602.15537",
      "arxivId": "2602.15537",
      "title": "ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling",
      "authors": [
        "Nicol Visser",
        "Simon Malan",
        "Danel Slabbert",
        "Herman Kamper"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Pure speech language models aim to learn language directly from raw audio without textual resources. A key challenge is that discrete tokens from self-supervised speech encoders result in excessively long sequences, motivating recent work on syllable-like units. However, methods like Sylber and SyllableLM rely on intricate multi-stage training pipelines. We propose ZeroSyl, a simple training-free method to extract syllable boundaries and embeddings directly from a frozen WavLM model. Using L2 norms of features in WavLM's intermediate layers, ZeroSyl achieves competitive syllable segmentation performance. The resulting segments are mean-pooled, discretized using K-means, and used to train a language model. ZeroSyl outperforms prior syllabic tokenizers across lexical, syntactic, and narrative benchmarks. Scaling experiments show that while finer-grained units are beneficial for lexical tasks, our discovered syllabic units exhibit better scaling behavior for syntactic modeling.",
      "url": "https://arxiv.org/abs/2602.15537",
      "pdfUrl": "https://arxiv.org/pdf/2602.15537.pdf",
      "titleJa": "ZeroSyl: 音声言語モデルのためのシンプルなゼロリソース音節トークン化"
    },
    {
      "id": "2602.15484",
      "arxivId": "2602.15484",
      "title": "Bottleneck Transformer-Based Approach for Improved Automatic STOI Score Prediction",
      "authors": [
        " Amartyaveer",
        "Murali Kadambi",
        "Chandra Mohan Sharma",
        "Anupam Mondal",
        "Prasanta Kumar Ghosh"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "eess.AS",
        "cs.LG",
        "eess.SP"
      ],
      "abstract": "In this study, we have presented a novel approach to predict the Short-Time Objective Intelligibility (STOI) metric using a bottleneck transformer architecture. Traditional methods for calculating STOI typically requires clean reference speech, which limits their applicability in the real world. To address this, numerous deep learning-based nonintrusive speech assessment models have garnered significant interest. Many studies have achieved commendable performance, but there is room for further improvement. We propose the use of bottleneck transformer, incorporating convolution blocks for learning frame-level features and a multi-head self-attention (MHSA) layer to aggregate the information. These components enable the transformer to focus on the key aspects of the input data. Our model has shown higher correlation and lower mean squared error for both seen and unseen scenarios compared to the state-of-the-art model using self-supervised learning (SSL) and spectral features as inputs.",
      "url": "https://arxiv.org/abs/2602.15484",
      "pdfUrl": "https://arxiv.org/pdf/2602.15484.pdf",
      "titleJa": "ボトルネックトランスフォーマーベースのアプローチによるSTOIスコア自動予測の改善"
    },
    {
      "id": "2602.14785",
      "arxivId": "2602.14785",
      "title": "SA-SSL-MOS: Self-supervised Learning MOS Prediction with Spectral Augmentation for Generalized Multi-Rate Speech Assessment",
      "authors": [
        "Fengyuan Cao",
        "Xinyu Liang",
        "Fredrik Cumlin",
        "Victor Ungureanu",
        "Chandan K. A. Reddy",
        "Christian Schuldt",
        "Saikat Chatterjee"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Designing a speech quality assessment (SQA) system for estimating mean-opinion-score (MOS) of multi-rate speech with varying sampling frequency (16-48 kHz) is a challenging task. The challenge arises due to the limited availability of a MOS-labeled training dataset comprising multi-rate speech samples. While self-supervised learning (SSL) models have been widely adopted in SQA to boost performance, a key limitation is that they are pretrained on 16 kHz speech and therefore discard high-frequency information present in higher sampling rates. To address this issue, we propose a spectrogram-augmented SSL method that incorporates high-frequency features (up to 48 kHz sampling rate) through a parallel-branch architecture. We further introduce a two-step training scheme: the model is first pre-trained on a large 48 kHz dataset and then fine-tuned on a smaller multi-rate dataset. Experimental results show that leveraging high-frequency information overlooked by SSL features is crucial for accurate multi-rate SQA, and that the proposed two-step training substantially improves generalization when multi-rate data is limited.",
      "url": "https://arxiv.org/abs/2602.14785",
      "pdfUrl": "https://arxiv.org/pdf/2602.14785.pdf",
      "titleJa": "SA-SSL-MOS: スペクトル拡張を用いた自己教師学習によるMOS予測と一般化マルチレート音声評価"
    },
    {
      "id": "2602.14686",
      "arxivId": "2602.14686",
      "title": "Disentangling Pitch and Creak for Speaker Identity Preservation in Speech Synthesis",
      "authors": [
        "Frederik Rautenberg",
        "Jana Wiechmann",
        "Petra Wagner",
        "Reinhold Haeb-Umbach"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We introduce a system capable of faithfully modifying the perceptual voice quality of creak while preserving the speaker's perceived identity. While it is well known that high creak probability is typically correlated with low pitch, it is important to note that this is a property observed on a population of speakers but does not necessarily hold across all situations. Disentanglement of pitch from creak is achieved by augmentation of the training dataset of a speech synthesis system with a speaker manipulation block based on conditional continuous normalizing flow. The experiments show greatly improved speaker verification performance over a range of creak manipulation strengths.",
      "url": "https://arxiv.org/abs/2602.14686",
      "pdfUrl": "https://arxiv.org/pdf/2602.14686.pdf",
      "titleJa": "音声合成における話者識別の保持のためのピッチとキーキー音の分離"
    },
    {
      "id": "2602.14671",
      "arxivId": "2602.14671",
      "title": "Data Augmentation for Pathological Speech Enhancement",
      "authors": [
        "Mingchi Hou",
        "Enno Hermann",
        "Ina Kodrasi"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS"
      ],
      "abstract": "The performance of state-of-the-art speech enhancement (SE) models considerably degrades for pathological speech due to atypical acoustic characteristics and limited data availability. This paper systematically investigates data augmentation (DA) strategies to improve SE performance for pathological speakers, evaluating both predictive and generative SE models. We examine three DA categories, i.e., transformative, generative, and noise augmentation, assessing their impact with objective SE metrics. Experimental results show that noise augmentation consistently delivers the largest and most robust gains, transformative augmentations provide moderate improvements, while generative augmentation yields limited benefits and can harm performance as the amount of synthetic data increases. Furthermore, we show that the effectiveness of DA varies depending on the SE model, with DA being more beneficial for predictive SE models. While our results demonstrate that DA improves SE performance for pathological speakers, a performance gap between neurotypical and pathological speech persists, highlighting the need for future research on targeted DA strategies for pathological speech.",
      "url": "https://arxiv.org/abs/2602.14671",
      "pdfUrl": "https://arxiv.org/pdf/2602.14671.pdf",
      "titleJa": "病的な音声強調のためのデータ拡張"
    },
    {
      "id": "2602.14612",
      "arxivId": "2602.14612",
      "title": "LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio",
      "authors": [
        "Naveen Vakada",
        "Kartik Hegde",
        "Arvind Krishna Sridhar",
        "Yinyi Guo",
        "Erik Visser"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Long-duration audio is increasingly common in industrial and consumer settings, yet reviewing multi-hour recordings is impractical, motivating systems that answer natural-language queries with precise temporal grounding and minimal hallucination. Existing audio-language models show promise, but long-audio question answering remains difficult due to context-length limits. We introduce LongAudio-RAG (LA-RAG), a hybrid framework that grounds Large Language Model (LLM) outputs in retrieved, timestamped acoustic event detections rather than raw audio. Multi-hour streams are converted into structured event records stored in an SQL database, and at inference time the system resolves natural-language time references, classifies intent, retrieves only the relevant events, and generates answers using this constrained evidence. To evaluate performance, we construct a synthetic long-audio benchmark by concatenating recordings with preserved timestamps and generating template-based question-answer pairs for detection, counting, and summarization tasks. Finally, we demonstrate the practicality of our approach by deploying it in a hybrid edge-cloud environment, where the audio grounding model runs on-device on IoT-class hardware while the LLM is hosted on a GPU-backed server. This architecture enables low-latency event extraction at the edge and high-quality language reasoning in the cloud. Experiments show that structured, event-level retrieval significantly improves accuracy compared to vanilla Retrieval-Augmented Generation (RAG) or text-to-SQL approaches.",
      "url": "https://arxiv.org/abs/2602.14612",
      "pdfUrl": "https://arxiv.org/pdf/2602.14612.pdf",
      "titleJa": "LongAudio-RAG: 数時間にわたる音声によるイベントベースの質問応答"
    },
    {
      "id": "2602.14584",
      "arxivId": "2602.14584",
      "title": "CLAP-Based Automatic Word Naming Recognition in Post-Stroke Aphasia",
      "authors": [
        "Yacouba Kaloga",
        "Marina Laganaro",
        "Ina Kodrasi"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Conventional automatic word-naming recognition systems struggle to recognize words from post-stroke patients with aphasia because of disfluencies and mispronunciations, limiting reliable automated assessment in this population. In this paper, we propose a Contrastive Language-Audio Pretraining (CLAP) based approach for automatic word-naming recognition to address this challenge by leveraging text-audio alignment. Our approach treats word-naming recognition as an audio-text matching problem, projecting speech signals and textual prompts into a shared embedding space to identify intended words even in challenging recordings. Evaluated on two speech datasets of French post-stroke patients with aphasia, our approach achieves up to 90% accuracy, outperforming existing classification-based and automatic speech recognition-based baselines.",
      "url": "https://arxiv.org/abs/2602.14584",
      "pdfUrl": "https://arxiv.org/pdf/2602.14584.pdf",
      "titleJa": "脳卒中後失語症におけるCLAPベースの自動単語命名認識"
    },
    {
      "id": "2602.16708",
      "arxivId": "2602.16708",
      "title": "Policy Compiler for Secure Agentic Systems",
      "authors": [
        "Nils Palumbo",
        "Sarthak Choudhary",
        "Jihye Choi",
        "Prasad Chalasani",
        "Mihai Christodorescu",
        "Somesh Jha"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "abstract": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement. Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning. PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "url": "https://arxiv.org/abs/2602.16708",
      "pdfUrl": "https://arxiv.org/pdf/2602.16708.pdf",
      "titleJa": "セキュアエージェントシステム向けポリシーコンパイラ"
    },
    {
      "id": "2602.16703",
      "arxivId": "2602.16703",
      "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
      "authors": [
        "Shen Zhou Hong",
        "Alex Kleinman",
        "Alyssa Mathiowetz",
        "Adam Howes",
        "Julian Cohen",
        "Suveer Ganta",
        "Alex Letizia",
        "Dora Liao",
        "Deepika Pahari",
        "Xavier Roberts-Gaal",
        "Luca Righetti",
        "Joe Torres"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "abstract": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.",
      "url": "https://arxiv.org/abs/2602.16703",
      "pdfUrl": "https://arxiv.org/pdf/2602.16703.pdf",
      "titleJa": "2025年半ばの生物学初心者のパフォーマンスに関するLLM支援の測定"
    },
    {
      "id": "2602.16699",
      "arxivId": "2602.16699",
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
      "authors": [
        "Wenxuan Ding",
        "Nicholas Tomlin",
        "Greg Durrett"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
      "url": "https://arxiv.org/abs/2602.16699",
      "pdfUrl": "https://arxiv.org/pdf/2602.16699.pdf",
      "titleJa": "調整してから行動する: LLM エージェントにおけるコストを考慮した探索"
    },
    {
      "id": "2602.16671",
      "arxivId": "2602.16671",
      "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
      "authors": [
        "Jaid Monwar Chowdhury",
        "Chi-An Fu",
        "Reyhaneh Jabbarvand"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "abstract": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",
      "url": "https://arxiv.org/abs/2602.16671",
      "pdfUrl": "https://arxiv.org/pdf/2602.16671.pdf",
      "titleJa": "SPARC: 自動 C ユニットテスト生成のためのシナリオ計画と推論"
    },
    {
      "id": "2602.16666",
      "arxivId": "2602.16666",
      "title": "Towards a Science of AI Agent Reliability",
      "authors": [
        "Stephan Rabanser",
        "Sayash Kapoor",
        "Peter Kirgis",
        "Kangheng Liu",
        "Saiteja Utpala",
        "Arvind Narayanan"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "abstract": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "url": "https://arxiv.org/abs/2602.16666",
      "pdfUrl": "https://arxiv.org/pdf/2602.16666.pdf",
      "titleJa": "AIエージェントの信頼性の科学に向けて"
    },
    {
      "id": "2602.16660",
      "arxivId": "2602.16660",
      "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
      "authors": [
        "Yuyan Bu",
        "Xiaohao Liu",
        "ZhaoXing Ren",
        "Yaodong Yang",
        "Juntao Dai"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.",
      "url": "https://arxiv.org/abs/2602.16660",
      "pdfUrl": "https://arxiv.org/pdf/2602.16660.pdf",
      "titleJa": "一度整合すれば多言語でメリットを享受できる：LLM 安全性整合のための多言語一貫性の強化"
    },
    {
      "id": "2602.16653",
      "arxivId": "2602.16653",
      "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
      "authors": [
        "Yangjie Xu",
        "Lujun Li",
        "Lama Sleem",
        "Niccolo Gentile",
        "Yewei Song",
        "Yiqun Wang",
        "Siming Ji",
        "Wenbo Wu",
        "Radu State"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.",
      "url": "https://arxiv.org/abs/2602.16653",
      "pdfUrl": "https://arxiv.org/pdf/2602.16653.pdf",
      "titleJa": "エージェントスキルフレームワーク：産業環境における小規模言語モデルの可能性に関する展望"
    },
    {
      "id": "2602.16650",
      "arxivId": "2602.16650",
      "title": "Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System",
      "authors": [
        "Sonakshi Gupta",
        "Akhlak Mahmood",
        "Wei Xiong",
        "Rampi Ramprasad"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "abstract": "Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. Retrieval-augmented generation (RAG) offers a promising way to overcome this limitation by combining large language models (LLMs) with external retrieval, but its effectiveness depends strongly on how domain knowledge is represented. In this work, we develop two retrieval pipelines: a dense semantic vector-based approach (VectorRAG) and a graph-based approach (GraphRAG). Using over 1,000 polyhydroxyalkanoate (PHA) papers, we construct context-preserving paragraph embeddings and a canonicalized structured knowledge graph supporting entity disambiguation and multi-hop reasoning. We evaluate these pipelines through standard retrieval metrics, comparisons with general state-of-the-art systems such as GPT and Gemini, and qualitative validation by a domain chemist. The results show that GraphRAG achieves higher precision and interpretability, while VectorRAG provides broader recall, highlighting complementary trade-offs. Expert validation further confirms that the tailored pipelines, particularly GraphRAG, produce well-grounded, citation-reliable responses with strong domain relevance. By grounding every statement in evidence, these systems enable researchers to navigate the literature, compare findings across studies, and uncover patterns that are difficult to extract manually. More broadly, this work establishes a practical framework for building materials science assistants using curated corpora and retrieval design, reducing reliance on proprietary models while enabling trustworthy literature analysis at scale.",
      "url": "https://arxiv.org/abs/2602.16650",
      "pdfUrl": "https://arxiv.org/pdf/2602.16650.pdf",
      "titleJa": "文献に基づくポリマー知識の検索と生成の強化：生分解性ポリマーエキスパートシステムの例"
    },
    {
      "id": "2602.16634",
      "arxivId": "2602.16634",
      "title": "Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models",
      "authors": [
        "Yu Xie",
        "Ludwig Winkler",
        "Lixin Sun",
        "Sarah Lewis",
        "Adam E. Foster",
        "José Jiménez Luna",
        "Tim Hempel",
        "Michael Gastegger",
        "Yaoyi Chen",
        "Iryna Zaporozhets",
        "Cecilia Clementi",
        "Christopher M. Bishop",
        "Frank Noé"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "physics.bio-ph",
        "physics.chem-ph"
      ],
      "abstract": "The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions, eliminating the cost of sampling rare transition events. However, a sampling problem remains when computing observables that rely on states which are rare in equilibrium, for example folding free energies. Here, we introduce enhanced diffusion sampling, enabling efficient exploration of rare-event regions while preserving unbiased thermodynamic estimators. The key idea is to perform quantitatively accurate steering protocols to generate biased ensembles and subsequently recover equilibrium statistics via exact reweighting. We instantiate our framework in three algorithms: UmbrellaDiff (umbrella sampling with diffusion models), $Δ$G-Diff (free-energy differences via tilted ensembles), and MetaDiff (a batchwise analogue for metadynamics). Across toy systems, protein folding landscapes and folding free energies, our methods achieve fast, accurate, and scalable estimation of equilibrium properties within GPU-minutes to hours per system -- closing the rare-event sampling gap that remained after the advent of diffusion-model equilibrium samplers.",
      "url": "https://arxiv.org/abs/2602.16634",
      "pdfUrl": "https://arxiv.org/pdf/2602.16634.pdf",
      "titleJa": "拡張拡散サンプリング：拡散モデルを用いた効率的な稀少事象サンプリングと自由エネルギー計算"
    },
    {
      "id": "2602.16629",
      "arxivId": "2602.16629",
      "title": "Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes",
      "authors": [
        "Ethan Blaser",
        "Jiuqi Wang",
        "Shangtong Zhang"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.",
      "url": "https://arxiv.org/abs/2602.16629",
      "pdfUrl": "https://arxiv.org/pdf/2602.16629.pdf",
      "titleJa": "平均報酬マルコフ決定過程における微分時間差分学習のほぼ確実な収束"
    },
    {
      "id": "2602.16626",
      "arxivId": "2602.16626",
      "title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models",
      "authors": [
        "SungJun Cho",
        "Chetan Gohil",
        "Rukuang Huang",
        "Oiwi Parker Jones",
        "Mark W. Woolrich"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC"
      ],
      "abstract": "Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.",
      "url": "https://arxiv.org/abs/2602.16626",
      "pdfUrl": "https://arxiv.org/pdf/2602.16626.pdf",
      "titleJa": "MEG基礎モデルにおけるサンプルレベルトークン化戦略の体系的評価"
    },
    {
      "id": "2602.16612",
      "arxivId": "2602.16612",
      "title": "Causal and Compositional Abstraction",
      "authors": [
        "Robin Lorenz",
        "Sean Tull"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.LO",
        "cs.AI",
        "math.CT",
        "quant-ph"
      ],
      "abstract": "Abstracting from a low level to a more explanatory high level of description, and ideally while preserving causal structure, is fundamental to scientific practice, to causal inference problems, and to robust, efficient and interpretable AI. We present a general account of abstractions between low and high level models as natural transformations, focusing on the case of causal models. This provides a new formalisation of causal abstraction, unifying several notions in the literature, including constructive causal abstraction, Q-$τ$ consistency, abstractions based on interchange interventions, and `distributed' causal abstractions. Our approach is formalised in terms of category theory, and uses the general notion of a compositional model with a given set of queries and semantics in a monoidal, cd- or Markov category; causal models and their queries such as interventions being special cases. We identify two basic notions of abstraction: downward abstractions mapping queries from high to low level; and upward abstractions, mapping concrete queries such as Do-interventions from low to high. Although usually presented as the latter, we show how common causal abstractions may, more fundamentally, be understood in terms of the former. Our approach also leads us to consider a new stronger notion of `component-level' abstraction, applying to the individual components of a model. In particular, this yields a novel, strengthened form of constructive causal abstraction at the mechanism-level, for which we prove characterisation results. Finally, we show that abstraction can be generalised to further compositional models, including those with a quantum semantics implemented by quantum circuits, and we take first steps in exploring abstractions between quantum compositional circuit models and high-level classical causal models as a means to explainable quantum AI.",
      "url": "https://arxiv.org/abs/2602.16612",
      "pdfUrl": "https://arxiv.org/pdf/2602.16612.pdf",
      "titleJa": "因果的抽象と構成的抽象"
    },
    {
      "id": "2602.16610",
      "arxivId": "2602.16610",
      "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
      "authors": [
        "Mengjie Qian",
        "Guangzhi Sun",
        "Mark J. F. Gales",
        "Kate M. Knill"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.",
      "url": "https://arxiv.org/abs/2602.16610",
      "pdfUrl": "https://arxiv.org/pdf/2602.16610.pdf",
      "titleJa": "誰を信頼できるのか？比較評価における陪審員としての法学修士"
    },
    {
      "id": "2602.16608",
      "arxivId": "2602.16608",
      "title": "Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models",
      "authors": [
        "Melkamu Abay Mersha",
        "Jugal Kalita"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "abstract": "Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attention patterns without unification, and lack context-awareness of inter-token dependencies and structural components. They also fail to capture how relevance evolves across layers and how structural components shape decision-making. To address these limitations, we proposed the \\textbf{Context-Aware Layer-wise Integrated Gradients (CA-LIG) Framework}, a unified hierarchical attribution framework that computes layer-wise Integrated Gradients within each Transformer block and fuses these token-level attributions with class-specific attention gradients. This integration yields signed, context-sensitive attribution maps that capture supportive and opposing evidence while tracing the hierarchical flow of relevance through the Transformer layers. We evaluate the CA-LIG Framework across diverse tasks, domains, and transformer model families, including sentiment analysis and long and multi-class document classification with BERT, hate speech detection in a low-resource language setting with XLM-R and AfroLM, and image classification with Masked Autoencoder vision Transformer model. Across all tasks and architectures, CA-LIG provides more faithful attributions, shows stronger sensitivity to contextual dependencies, and produces clearer, more semantically coherent visualizations than established explainability methods. These results indicate that CA-LIG provides a more comprehensive, context-aware, and reliable explanation of Transformer decision-making, advancing both the practical interpretability and conceptual understanding of deep neural models.",
      "url": "https://arxiv.org/abs/2602.16608",
      "pdfUrl": "https://arxiv.org/pdf/2602.16608.pdf",
      "titleJa": "説明可能なAI: Transformerモデルを説明するためのコンテキスト認識型レイヤーワイズ統合勾配"
    },
    {
      "id": "2602.16603",
      "arxivId": "2602.16603",
      "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
      "authors": [
        "Chia-chi Hsieh",
        "Zan Zong",
        "Xinyang Chen",
        "Jianjiang Li",
        "Jidong Zhai",
        "Lijie Wen"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "abstract": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge. In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.",
      "url": "https://arxiv.org/abs/2602.16603",
      "pdfUrl": "https://arxiv.org/pdf/2602.16603.pdf",
      "titleJa": "FlowPrefill: プリエンプションとプレフィルスケジューリング粒度を分離して LLM サービングにおけるヘッドオブラインブロッキングを軽減する"
    },
    {
      "id": "2602.16590",
      "arxivId": "2602.16590",
      "title": "A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification",
      "authors": [
        "Qi You",
        "Yitai Cheng",
        "Zichao Zeng",
        "James Haworth"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.",
      "url": "https://arxiv.org/abs/2602.16590",
      "pdfUrl": "https://arxiv.org/pdf/2602.16590.pdf",
      "titleJa": "ストリートビュー画像分類のための注目度ベースの特徴適応を活用した対照学習フレームワーク"
    },
    {
      "id": "2602.16585",
      "arxivId": "2602.16585",
      "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
      "authors": [
        "Dimitri Yatsenko",
        "Thinh T. Nguyen"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "abstract": "Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a single formal system where data structure, computational dependencies, and integrity constraints are all queryable, enforceable, and machine-readable. Four technical innovations extend this foundation: object-augmented schemas integrating relational metadata with scalable object storage, semantic matching using attribute lineage to prevent erroneous joins, an extensible type system for domain-specific formats, and distributed job coordination designed for composability with external orchestration. By unifying data structure, data, and computational transformations, DataJoint creates a substrate for SciOps where agents can participate in scientific workflows without risking data corruption.",
      "url": "https://arxiv.org/abs/2602.16585",
      "pdfUrl": "https://arxiv.org/pdf/2602.16585.pdf",
      "titleJa": "DataJoint 2.0: エージェント型科学ワークフローのための計算基盤"
    },
    {
      "id": "2602.16579",
      "arxivId": "2602.16579",
      "title": "AIFL: A Global Daily Streamflow Forecasting Model Using Deterministic LSTM Pre-trained on ERA5-Land and Fine-tuned on IFS",
      "authors": [
        "Maria Luisa Taccari",
        "Kenza Tazi",
        "Oisín M. Morrison",
        "Andreas Grafberger",
        "Juan Colonese",
        "Corentin Carton de Wiart",
        "Christel Prudhomme",
        "Cinzia Mazzetti",
        "Matthew Chantry",
        "Florian Pappenberger"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.app-ph"
      ],
      "abstract": "Reliable global streamflow forecasting is essential for flood preparedness and water resource management, yet data-driven models often suffer from a performance gap when transitioning from historical reanalysis to operational forecast products. This paper introduces AIFL (Artificial Intelligence for Floods), a deterministic LSTM-based model designed for global daily streamflow forecasting. Trained on 18,588 basins curated from the CARAVAN dataset, AIFL utilises a novel two-stage training strategy to bridge the reanalysis-to-forecast domain shift. The model is first pre-trained on 40 years of ERA5-Land reanalysis (1980-2019) to capture robust hydrological processes, then fine-tuned on operational Integrated Forecasting System (IFS) control forecasts (2016-2019) to adapt to the specific error structures and biases of operational numerical weather prediction. To our knowledge, this is the first global model trained end-to-end within the CARAVAN ecosystem. On an independent temporal test set (2021-2024), AIFL achieves high predictive skill with a median modified Kling-Gupta Efficiency (KGE') of 0.66 and a median Nash-Sutcliffe Efficiency (NSE) of 0.53. Benchmarking results show that AIFL is highly competitive with current state-of-the-art global systems, achieving comparable accuracy while maintaining a transparent and reproducible forcing pipeline. The model demonstrates exceptional reliability in extreme-event detection, providing a streamlined and operationally robust baseline for the global hydrological community.",
      "url": "https://arxiv.org/abs/2602.16579",
      "pdfUrl": "https://arxiv.org/pdf/2602.16579.pdf",
      "titleJa": "AIFL: ERA5-Landで事前学習し、IFSで微調整した決定論的LSTMを用いた世界日流量予測モデル"
    },
    {
      "id": "2602.16578",
      "arxivId": "2602.16578",
      "title": "Creating a digital poet",
      "authors": [
        "Vered Tohar",
        "Tsahi Hayat",
        "Amir Leshem"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.",
      "url": "https://arxiv.org/abs/2602.16578",
      "pdfUrl": "https://arxiv.org/pdf/2602.16578.pdf",
      "titleJa": "デジタル詩人の創造"
    },
    {
      "id": "2602.16554",
      "arxivId": "2602.16554",
      "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
      "authors": [
        "Yuanjie Ren",
        "Jinzheng Li",
        "Yidi Qi"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.ET",
        "quant-ph"
      ],
      "abstract": "We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for semantic review. We evaluate MerLean on three theoretical quantum computing papers producing 2,050 Lean declarations from 114 statements in total. MerLean achieves end-to-end formalization on all three papers, reducing the verification burden to only the newly introduced definitions and axioms. Our results demonstrate that agentic autoformalization can scale to frontier research, offering both a practical tool for machine-verified peer review and a scalable engine for mining high-quality synthetic data to train future reasoning models. Our approach can also be generalized to any other rigorous research in mathematics and theoretical physics.",
      "url": "https://arxiv.org/abs/2602.16554",
      "pdfUrl": "https://arxiv.org/pdf/2602.16554.pdf",
      "titleJa": "MerLean: 量子計算における自動形式化のためのエージェントフレームワーク"
    },
    {
      "id": "2602.13928",
      "arxivId": "2602.13928",
      "title": "voice2mode: Phonation Mode Classification in Singing using Self-Supervised Speech Models",
      "authors": [
        "Aju Ani Justus",
        "Ruchit Agrawal",
        "Sudarsana Reddy Kadiri",
        "Shrikanth Narayanan"
      ],
      "publishedDate": "2026-02-14",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "We present voice2mode, a method for classification of four singing phonation modes (breathy, neutral (modal), flow, and pressed) using embeddings extracted from large self-supervised speech models. Prior work on singing phonation has relied on handcrafted signal features or task-specific neural nets; this work evaluates the transferability of speech foundation models to singing phonation classification. voice2mode extracts layer-wise representations from HuBERT and two wav2vec2 variants, applies global temporal pooling, and classifies the pooled embeddings with lightweight classifiers (SVM, XGBoost). Experiments on a publicly available soprano dataset (763 sustained vowel recordings, four labels) show that foundation-model features substantially outperform conventional spectral baselines (spectrogram, mel-spectrogram, MFCC). HuBERT embeddings obtained from early layers yield the best result (~95.7% accuracy with SVM), an absolute improvement of ~12-15% over the best traditional baseline. We also show layer-wise behaviour: lower layers, which retain acoustic/phonetic detail, are more effective than top layers specialized for Automatic Speech Recognition (ASR).",
      "url": "https://arxiv.org/abs/2602.13928",
      "pdfUrl": "https://arxiv.org/pdf/2602.13928.pdf",
      "titleJa": "voice2mode: 自己教師あり音声モデルを用いた歌唱における発声モードの分類"
    },
    {
      "id": "2602.11910",
      "arxivId": "2602.11910",
      "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
      "authors": [
        "Łukasz Staniszewski",
        "Katarzyna Zaleska",
        "Mateusz Modrzejewski",
        "Kamil Deja"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood.",
      "url": "https://arxiv.org/abs/2602.11910",
      "pdfUrl": "https://arxiv.org/pdf/2602.11910.pdf",
      "titleJa": "TADA! アクティベーションステアリングによるオーディオ拡散モデルのチューニング"
    },
    {
      "id": "2602.11896",
      "arxivId": "2602.11896",
      "title": "Musical Metamerism with Time--Frequency Scattering",
      "authors": [
        "Vincent Lostanlen",
        "Han Han"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The concept of metamerism originates from colorimetry, where it describes a sensation of visual similarity between two colored lights despite significant differences in spectral content. Likewise, we propose to call ``musical metamerism'' the sensation of auditory similarity which is elicited by two music fragments which differ in terms of underlying waveforms. In this technical report, we describe a method to generate musical metamers from any audio recording. Our method is based on joint time--frequency scattering in Kymatio, an open-source software in Python which enables GPU computing and automatic differentiation. The advantage of our method is that it does not require any manual preprocessing, such as transcription, beat tracking, or source separation. We provide a mathematical description of JTFS as well as some excerpts from the Kymatio source code. Lastly, we review the prior work on JTFS and draw connections with closely related algorithms, such as spectrotemporal receptive fields (STRF), modulation power spectra (MPS), and Gabor filterbank (GBFB).",
      "url": "https://arxiv.org/abs/2602.11896",
      "pdfUrl": "https://arxiv.org/pdf/2602.11896.pdf",
      "titleJa": "時間周波数散乱を伴う音楽的メタメリズム"
    },
    {
      "id": "2602.10934",
      "arxivId": "2602.10934",
      "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
      "authors": [
        "Yitian Gong",
        "Kuangwei Chen",
        "Zhaoye Fei",
        "Xiaogui Yang",
        "Ke Chen",
        "Yang Wang",
        "Kexin Huang",
        "Mingshu Chen",
        "Ruixiao Li",
        "Qingyuan Cheng",
        "Shimin Li",
        "Xipeng Qiu"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.",
      "url": "https://arxiv.org/abs/2602.10934",
      "pdfUrl": "https://arxiv.org/pdf/2602.10934.pdf",
      "titleJa": "MOSS-Audio-Tokenizer: 将来のオーディオ基盤モデルに向けたオーディオトークナイザーのスケーリング"
    },
    {
      "id": "2602.12301",
      "arxivId": "2602.12301",
      "title": "Beyond Musical Descriptors: Extracting Preference-Bearing Intent in Music Queries",
      "authors": [
        "Marion Baranes",
        "Romain Hennequin",
        "Elena V. Epure"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.IR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Although annotated music descriptor datasets for user queries are increasingly common, few consider the user's intent behind these descriptors, which is essential for effectively meeting their needs. We introduce MusicRecoIntent, a manually annotated corpus of 2,291 Reddit music requests, labeling musical descriptors across seven categories with positive, negative, or referential preference-bearing roles. We then investigate how reliably large language models (LLMs) can extract these music descriptors, finding that they do capture explicit descriptors but struggle with context-dependent ones. This work can further serve as a benchmark for fine-grained modeling of user intent and for gaining insights into improving LLM-based music understanding systems.",
      "url": "https://arxiv.org/abs/2602.12301",
      "pdfUrl": "https://arxiv.org/pdf/2602.12301.pdf",
      "titleJa": "音楽記述子を超えて：音楽検索クエリにおける嗜好意図の抽出"
    },
    {
      "id": "2602.10656",
      "arxivId": "2602.10656",
      "title": "AudioRAG: A Challenging Benchmark for Audio Reasoning and Information Retrieval",
      "authors": [
        "Jingru Lin",
        "Chen Zhang",
        "Tianrui Wang",
        "Haizhou Li"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Due to recent advancements in Large Audio-Language Models (LALMs) that demonstrate remarkable performance across a range of sound-, speech- and music-related tasks, there is a growing interest in proposing benchmarks to assess these models. Existing benchmarks generally focus only on reasoning with internal knowledge, neglecting real-world scenarios that require external information grounding. To bridge this gap, we introduce AudioRAG, a novel benchmark designed to evaluate audio-based reasoning augmented by information retrieval in realistic web environments. This benchmark comprises both LLM-generated and manually curated question-answer pairs. Our evaluations reveal that even the state-of-the-art LALMs struggle to answer these questions. We therefore propose an agentic pipeline that integrates audio reasoning with retrieval-augmented generation, providing a stronger baseline for future research.",
      "url": "https://arxiv.org/abs/2602.10656",
      "pdfUrl": "https://arxiv.org/pdf/2602.10656.pdf",
      "titleJa": "AudioRAG: オーディオ推論と情報検索のための挑戦的なベンチマーク"
    },
    {
      "id": "2602.10058",
      "arxivId": "2602.10058",
      "title": "Evaluating Disentangled Representations for Controllable Music Generation",
      "authors": [
        "Laura Ibáñez-Martínez",
        "Chukwuemeka Nkama",
        "Andrea Poltronieri",
        "Xavier Serra",
        "Martín Rocamora"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Recent approaches in music generation rely on disentangled representations, often labeled as structure and timbre or local and global, to enable controllable synthesis. Yet the underlying properties of these embeddings remain underexplored. In this work, we evaluate such disentangled representations in a set of music audio models for controllable generation using a probing-based framework that goes beyond standard downstream tasks. The selected models reflect diverse unsupervised disentanglement strategies, including inductive biases, data augmentations, adversarial objectives, and staged training procedures. We further isolate specific strategies to analyze their effect. Our analysis spans four key axes: informativeness, equivariance, invariance, and disentanglement, which are assessed across datasets, tasks, and controlled transformations. Our findings reveal inconsistencies between intended and actual semantics of the embeddings, suggesting that current strategies fall short of producing truly disentangled representations, and prompting a re-examination of how controllability is approached in music generation.",
      "url": "https://arxiv.org/abs/2602.10058",
      "pdfUrl": "https://arxiv.org/pdf/2602.10058.pdf",
      "titleJa": "制御可能な音楽生成のための分離表現の評価"
    },
    {
      "id": "2602.09891",
      "arxivId": "2602.09891",
      "title": "Stemphonic: All-at-once Flexible Multi-stem Music Generation",
      "authors": [
        "Shih-Lun Wu",
        "Ge Zhu",
        "Juan-Pablo Caceres",
        "Cheng-Zhi Anna Huang",
        "Nicholas J. Bryan"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM"
      ],
      "abstract": "Music stem generation, the task of producing musically-synchronized and isolated instrument audio clips, offers the potential of greater user control and better alignment with musician workflows compared to conventional text-to-music models. Existing stem generation approaches, however, either rely on fixed architectures that output a predefined set of stems in parallel, or generate only one stem at a time, resulting in slow inference despite flexibility in stem combination. We propose Stemphonic, a diffusion-/flow-based framework that overcomes this trade-off and generates a variable set of synchronized stems in one inference pass. During training, we treat each stem as a batch element, group synchronized stems in a batch, and apply a shared noise latent to each group. At inference-time, we use a shared initial noise latent and stem-specific text inputs to generate synchronized multi-stem outputs in one pass. We further expand our approach to enable one-pass conditional multi-stem generation and stem-wise activity controls to empower users to iteratively generate and orchestrate the temporal layering of a mix. We benchmark our results on multiple open-source stem evaluation sets and show that Stemphonic produces higher-quality outputs while accelerating the full mix generation process by 25 to 50%. Demos at: https://stemphonic-demo.vercel.app.",
      "url": "https://arxiv.org/abs/2602.09891",
      "pdfUrl": "https://arxiv.org/pdf/2602.09891.pdf",
      "titleJa": "Stemphonic: 一度に柔軟なマルチステム音楽生成"
    },
    {
      "id": "2602.08794",
      "arxivId": "2602.08794",
      "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
      "authors": [
        "SII-OpenMOSS Team",
        " :",
        "Donghua Yu",
        "Mingshu Chen",
        "Qi Chen",
        "Qi Luo",
        "Qianyi Wu",
        "Qinyuan Cheng",
        "Ruixiao Li",
        "Tianyi Liang",
        "Wenbo Zhang",
        "Wenming Tu",
        "Xiangyu Peng",
        "Yang Gao",
        "Yanru Huo",
        "Ying Zhu",
        "Yinze Luo",
        "Yiyang Zhang",
        "Yuerong Song",
        "Zhe Xu",
        "Zhiyu Zhang",
        "Chenchen Yang",
        "Cheng Chang",
        "Chushu Zhou",
        "Hanfu Chen",
        "Hongnan Ma",
        "Jiaxi Li",
        "Jingqi Tong",
        "Junxi Liu",
        "Ke Chen",
        "Shimin Li",
        "Shiqi Jiang",
        "Songlin Wang",
        "Wei Jiang",
        "Zhaoye Fei",
        "Zhiyuan Ning",
        "Chunguo Li",
        "Chenhui Li",
        "Ziwei He",
        "Zengfeng Huang",
        "Xie Chen",
        "Xipeng Qiu"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.CV",
        "cs.SD"
      ],
      "abstract": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
      "url": "https://arxiv.org/abs/2602.08794",
      "pdfUrl": "https://arxiv.org/pdf/2602.08794.pdf",
      "titleJa": "MOVA: スケーラブルで同期したビデオ・オーディオ生成に向けて"
    },
    {
      "id": "2602.08671",
      "arxivId": "2602.08671",
      "title": "Input-Adaptive Spectral Feature Compression by Sequence Modeling for Source Separation",
      "authors": [
        "Kohei Saijo",
        "Yoshiaki Bando"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Time-frequency domain dual-path models have demonstrated strong performance and are widely used in source separation. Because their computational cost grows with the number of frequency bins, these models often use the band-split (BS) module in high-sampling-rate tasks such as music source separation (MSS) and cinematic audio source separation (CASS). The BS encoder compresses frequency information by encoding features for each predefined subband. It achieves effective compression by introducing an inductive bias that places greater emphasis on low-frequency parts. Despite its success, the BS module has two inherent limitations: (i) it is not input-adaptive, preventing the use of input-dependent information, and (ii) the parameter count is large, since each subband requires a dedicated module. To address these issues, we propose Spectral Feature Compression (SFC). SFC compresses the input using a single sequence modeling module, making it both input-adaptive and parameter-efficient. We investigate two variants of SFC, one based on cross-attention and the other on Mamba, and introduce inductive biases inspired by the BS module to make them suitable for frequency information compression. Experiments on MSS and CASS tasks demonstrate that the SFC module consistently outperforms the BS module across different separator sizes and compression ratios. We also provide an analysis showing that SFC adaptively captures frequency patterns from the input.",
      "url": "https://arxiv.org/abs/2602.08671",
      "pdfUrl": "https://arxiv.org/pdf/2602.08671.pdf",
      "titleJa": "音源分離のためのシーケンスモデリングによる入力適応型スペクトル特徴圧縮"
    },
    {
      "id": "2602.09070",
      "arxivId": "2602.09070",
      "title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
      "authors": [
        "Yufan Wen",
        "Zhaocheng Liu",
        "YeGuo Hua",
        "Ziyi Guo",
        "Lihua Zhang",
        "Chun Yuan",
        "Jian Wu"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a \\textit{Global Semantic Anchor} ensures stylistic stability, while a surgical \\textit{Token-Level Affective Adapter} modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation.",
      "url": "https://arxiv.org/abs/2602.09070",
      "pdfUrl": "https://arxiv.org/pdf/2602.09070.pdf",
      "titleJa": "NarraScore: 階層的感情制御による視覚的物語と音楽的ダイナミクスの橋渡し"
    },
    {
      "id": "2602.08233",
      "arxivId": "2602.08233",
      "title": "Tutti: Expressive Multi-Singer Synthesis via Structure-Level Timbre Control and Vocal Texture Modeling",
      "authors": [
        "Jiatao Chen",
        "Xing Tang",
        "Xiaoyue Duan",
        "Yutang Feng",
        "Jinchao Zhang",
        "Jie Zhou"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "While existing Singing Voice Synthesis systems achieve high-fidelity solo performances, they are constrained by global timbre control, failing to address dynamic multi-singer arrangement and vocal texture within a single song. To address this, we propose Tutti, a unified framework designed for structured multi-singer generation. Specifically, we introduce a Structure-Aware Singer Prompt to enable flexible singer scheduling evolving with musical structure, and propose Complementary Texture Learning via Condition-Guided VAE to capture implicit acoustic textures (e.g., spatial reverberation and spectral fusion) that are complementary to explicit controls. Experiments demonstrate that Tutti excels in precise multi-singer scheduling and significantly enhances the acoustic realism of choral generation, offering a novel paradigm for complex multi-singer arrangement. Audio samples are available at https://annoauth123-ctrl.github.io/Tutii_Demo/.",
      "url": "https://arxiv.org/abs/2602.08233",
      "pdfUrl": "https://arxiv.org/pdf/2602.08233.pdf",
      "titleJa": "Tutti: 構造レベルの音色制御とボーカルテクスチャモデリングによる表現力豊かなマルチシンガー合成"
    },
    {
      "id": "2602.08148",
      "arxivId": "2602.08148",
      "title": "SNC: A Stem-Native Codec for Efficient Lossless Audio Storage with Adaptive Playback Capabilities",
      "authors": [
        "Shaad Sufi"
      ],
      "publishedDate": "2026-02-08",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Current audio formats present a fundamental trade-off between file size and functionality: lossless formats like FLAC preserve quality but lack adaptability, while lossy formats reduce size at the cost of fidelity and offer no stem-level access.We introduce the Stem-Native Codec (SNC), a novel audio container format that stores music as independently encoded stems plus a low-energy mastering residual. By exploiting the lower information entropy of separated stems compared to mixed audio, SNC achieves a 38.2% file size reduction versus FLAC (7.76 MB vs. 12.55 MB for a 2:18 test track) while maintaining perceptual transparency (STOI = 0.996). Unlike existing formats, SNC enables context-aware adaptive playback, spatial audio rendering, and user-controlled remixing without requiring additional storage. Our experimental validation demonstrates that the stems-plus residual architecture successfully decouples the conflicting requirements of compression efficiency and feature richness, offering a practical path toward next-generation audio distribution systems.",
      "url": "https://arxiv.org/abs/2602.08148",
      "pdfUrl": "https://arxiv.org/pdf/2602.08148.pdf",
      "titleJa": "SNC: 適応型再生機能を備えた効率的なロスレスオーディオストレージのためのステムネイティブコーデック"
    },
    {
      "id": "2602.07803",
      "arxivId": "2602.07803",
      "title": "SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis",
      "authors": [
        "Jiale Qian",
        "Hao Meng",
        "Tian Zheng",
        "Pengcheng Zhu",
        "Haopeng Lin",
        "Yuhang Dai",
        "Hanke Xie",
        "Wenxiao Cao",
        "Ruixuan Shang",
        "Jun Wu",
        "Hongmei Liu",
        "Hanlin Wen",
        "Jian Zhao",
        "Zhonglin Jiang",
        "Yong Chen",
        "Shunshun Yin",
        "Ming Tao",
        "Jianguo Wei",
        "Lei Xie",
        "Xinsheng Wang"
      ],
      "publishedDate": "2026-02-08",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.",
      "url": "https://arxiv.org/abs/2602.07803",
      "pdfUrl": "https://arxiv.org/pdf/2602.07803.pdf",
      "titleJa": "SoulX-Singer: 高品質なゼロショット歌声合成に向けて"
    },
    {
      "id": "2602.06917",
      "arxivId": "2602.06917",
      "title": "Automatic Detection and Analysis of Singing Mistakes for Music Pedagogy",
      "authors": [
        "Sumit Kumar",
        "Suraj Jaiswal",
        "Parampreet Singh",
        "Vipul Arora"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "The advancement of machine learning in audio analysis has opened new possibilities for technology-enhanced music education. This paper introduces a framework for automatic singing mistake detection in the context of music pedagogy, supported by a newly curated dataset. The dataset comprises synchronized teacher learner vocal recordings, with annotations marking different types of mistakes made by learners. Using this dataset, we develop different deep learning models for mistake detection and benchmark them. To compare the efficacy of mistake detection systems, a new evaluation methodology is proposed. Experiments indicate that the proposed learning-based methods are superior to rule-based methods. A systematic study of errors and a cross-teacher study reveal insights into music pedagogy that can be utilised for various music applications. This work sets out new directions of research in music pedagogy. The codes and dataset are publicly available.",
      "url": "https://arxiv.org/abs/2602.06917",
      "pdfUrl": "https://arxiv.org/pdf/2602.06917.pdf",
      "titleJa": "音楽教育のための歌唱ミスの自動検出と分析"
    },
    {
      "id": "2602.06823",
      "arxivId": "2602.06823",
      "title": "AI-Generated Music Detection in Broadcast Monitoring",
      "authors": [
        "David Lopez-Ayala",
        "Asier Cabello",
        "Pablo Zinemanas",
        "Emilio Molina",
        "Martin Rocamora"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "AI music generators have advanced to the point where their outputs are often indistinguishable from human compositions. While detection methods have emerged, they are typically designed and validated in music streaming contexts with clean, full-length tracks. Broadcast audio, however, poses a different challenge: music appears as short excerpts, often masked by dominant speech, conditions under which existing detectors fail. In this work, we introduce AI-OpenBMAT, the first dataset tailored to broadcast-style AI-music detection. It contains 3,294 one-minute audio excerpts (54.9 hours) that follow the duration patterns and loudness relations of real television audio, combining human-made production music with stylistically matched continuations generated with Suno v3.5. We benchmark a CNN baseline and state-of-the-art SpectTTTra models to assess SNR and duration robustness, and evaluate on a full broadcast scenario. Across all settings, models that excel in streaming scenarios suffer substantial degradation, with F1-scores dropping below 60% when music is in the background or has a short duration. These results highlight speech masking and short music length as critical open challenges for AI music detection, and position AI-OpenBMAT as a benchmark for developing detectors capable of meeting industrial broadcast requirements.",
      "url": "https://arxiv.org/abs/2602.06823",
      "pdfUrl": "https://arxiv.org/pdf/2602.06823.pdf",
      "titleJa": "放送監視におけるAI生成音楽検出"
    },
    {
      "id": "2602.07063",
      "arxivId": "2602.07063",
      "title": "Video-based Music Generation",
      "authors": [
        "Serkan Sulun"
      ],
      "publishedDate": "2026-02-05",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called \"boundary offset encodings,\" aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.",
      "url": "https://arxiv.org/abs/2602.07063",
      "pdfUrl": "https://arxiv.org/pdf/2602.07063.pdf",
      "titleJa": "ビデオベースの音楽生成"
    },
    {
      "id": "2602.05220",
      "arxivId": "2602.05220",
      "title": "Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions",
      "authors": [
        "Jinchuan Tian",
        "Haoran Wang",
        "Bo-Hao Su",
        "Chien-yu Huang",
        "Qingzheng Wang",
        "Jiatong Shi",
        "William Chen",
        "Xun Gong",
        "Siddhant Arora",
        "Chin-Jou Li",
        "Masao Someki",
        "Takashi Maekaku",
        "Yusuke Shinohara",
        "Jin Sakuma",
        "Chao-Han Huck Yang",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-02-05",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.",
      "url": "https://arxiv.org/abs/2602.05220",
      "pdfUrl": "https://arxiv.org/pdf/2602.05220.pdf",
      "titleJa": "Bagpiper: 豊富なキャプションでオープンエンドの音声タスクを解決する"
    },
    {
      "id": "2602.15082",
      "arxivId": "2602.15082",
      "title": "S-PRESSO: Ultra Low Bitrate Sound Effect Compression With Diffusion Autoencoders And Offline Quantization",
      "authors": [
        "Zineb Lahrichi",
        "Gaëtan Hadjeres",
        "Gaël Richard",
        "Geoffroy Peeters"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM"
      ],
      "abstract": "Neural audio compression models have recently achieved extreme compression rates, enabling efficient latent generative modeling. Conversely, latent generative models have been applied to compression, pushing the limits of continuous and discrete approaches. However, existing methods remain constrained to low-resolution audio and degrade substantially at very low bitrates, where audible artifacts are prominent. In this paper, we present S-PRESSO, a 48kHz sound effect compression model that produces both continuous and discrete embeddings at ultra-low bitrates, down to 0.096 kbps, via offline quantization. Our model relies on a pretrained latent diffusion model to decode compressed audio embeddings learned by a latent encoder. Leveraging the generative priors of the diffusion decoder, we achieve extremely low frame rates, down to 1Hz (750x compression rate), producing convincing and realistic reconstructions at the cost of exact fidelity. Despite operating at high compression rates, we demonstrate that S-PRESSO outperforms both continuous and discrete baselines in audio quality, acoustic similarity and reconstruction metrics.",
      "url": "https://arxiv.org/abs/2602.15082",
      "pdfUrl": "https://arxiv.org/pdf/2602.15082.pdf",
      "titleJa": "S-PRESSO: 拡散オートエンコーダとオフライン量子化による超低ビットレート効果音圧縮"
    },
    {
      "id": "2602.14560",
      "arxivId": "2602.14560",
      "title": "Preliminary sonification of ENSO using traditional Javanese gamelan scales",
      "authors": [
        "Sandy H. S. Herho",
        "Rusmawan Suwarman",
        "Nurjanna J. Trilaksono",
        "Iwan P. Anwar",
        "Faiz R. Fajary"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "physics.soc-ph",
        "cs.SD",
        "physics.ao-ph"
      ],
      "abstract": "Sonification -- the mapping of data to non-speech audio -- offers an underexplored channel for representing complex dynamical systems. We treat El Niño-Southern Oscillation (ENSO), a canonical example of low-dimensional climate chaos, as a test case for culturally-situated sonification evaluated through complex systems diagnostics. Using parameter-mapping sonification of the Niño 3.4 sea surface temperature anomaly index (1870--2024), we encode ENSO variability into two traditional Javanese gamelan pentatonic systems (pelog and slendro) across four composition strategies, then analyze the resulting audio as trajectories in a two-dimensional acoustic phase space. Recurrence-based diagnostics, convex hull geometry, and coupling analysis reveal that the sonification pipeline preserves key dynamical signatures: alternating modes produce the highest trajectory recurrence rates, echoing ENSO's quasi-periodicity; layered polyphonic modes explore the broadest phase space regions; and the two scale families induce qualitatively distinct coupling regimes between spectral brightness and energy -- predominantly anti-phase in pelog but near-independent in slendro. Phase space trajectory analysis provides a rigorous geometric framework for comparing sonification designs within a complex systems context. Perceptual validation remains necessary; we contribute the dynamical systems methodology for evaluating such mappings.",
      "url": "https://arxiv.org/abs/2602.14560",
      "pdfUrl": "https://arxiv.org/pdf/2602.14560.pdf",
      "titleJa": "伝統的なジャワガムラン音階を用いたENSOの予備的な音響化"
    },
    {
      "id": "2602.14172",
      "arxivId": "2602.14172",
      "title": "Investigation for Relative Voice Impression Estimation",
      "authors": [
        "Kenichi Fujita",
        "Yusuke Ijima"
      ],
      "publishedDate": "2026-02-15",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "Paralinguistic and non-linguistic aspects of speech strongly influence listener impressions. While most research focuses on absolute impression scoring, this study investigates relative voice impression estimation (RIE), a framework for predicting the perceptual difference between two utterances from the same speaker. The estimation target is a low-dimensional vector derived from subjective evaluations, quantifying the perceptual shift of the second utterance relative to the first along an antonymic axis (e.g., ``Dark--Bright''). To isolate expressive and prosodic variation, we used recordings of a professional speaker reading a text in various styles. We compare three modeling approaches: classical acoustic features commonly used for speech emotion recognition, self-supervised speech representations, and multimodal large language models (MLLMs). Our results demonstrate that models using self-supervised representations outperform methods with classical acoustic features, particularly in capturing complex and dynamic impressions (e.g., ``Cold--Warm'') where classical features fail. In contrast, current MLLMs prove unreliable for this fine-grained pairwise task. This study provides the first systematic investigation of RIE and demonstrates the strength of self-supervised speech models in capturing subtle perceptual variations.",
      "url": "https://arxiv.org/abs/2602.14172",
      "pdfUrl": "https://arxiv.org/pdf/2602.14172.pdf",
      "titleJa": "相対的な音声印象推定に関する調査"
    },
    {
      "id": "2602.13891",
      "arxivId": "2602.13891",
      "title": "GSRM: Generative Speech Reward Model for Speech RLHF",
      "authors": [
        "Maohao Shen",
        "Tejas Jayashankar",
        "Osama Hanna",
        "Naoyuki Kanda",
        "Yancheng Wang",
        "Kateřina Žmolíková",
        "Ruiming Xie",
        "Niko Moritz",
        "Anfeng Xu",
        "Yashesh Gaur",
        "Gregory Wornell",
        "Qing He",
        "Jilong Wu"
      ],
      "publishedDate": "2026-02-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions. Experiments show that GSRM substantially outperforms existing speech naturalness predictors, achieving model-human correlation of naturalness score prediction that approaches human inter-rater consistency. We further show how GSRM can improve the naturalness of speech LLM generations by serving as an effective verifier for online RLHF.",
      "url": "https://arxiv.org/abs/2602.13891",
      "pdfUrl": "https://arxiv.org/pdf/2602.13891.pdf",
      "titleJa": "GSRM: 音声RLHFのための生成音声報酬モデル"
    },
    {
      "id": "2602.13835",
      "arxivId": "2602.13835",
      "title": "Audiocards: Structured Metadata Improves Audio Language Models For Sound Design",
      "authors": [
        "Sripathi Sridhar",
        "Prem Seetharaman",
        "Oriol Nieto",
        "Mark Cartwright",
        "Justin Salamon"
      ],
      "publishedDate": "2026-02-14",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Sound designers search for sounds in large sound effects libraries using aspects such as sound class or visual context. However, the metadata needed for such search is often missing or incomplete, and requires significant manual effort to add. Existing solutions to automate this task by generating metadata, i.e. captioning, and search using learned embeddings, i.e. text-audio retrieval, are not trained on metadata with the structure and information pertinent to sound design. To this end we propose audiocards, structured metadata grounded in acoustic attributes and sonic descriptors, by exploiting the world knowledge of LLMs. We show that training on audiocards improves downstream text-audio retrieval, descriptive captioning, and metadata generation on professional sound effects libraries. Moreover, audiocards also improve performance on general audio captioning and retrieval over the baseline single-sentence captioning approach. We release a curated dataset of sound effects audiocards to invite further research in audio language modeling for sound design.",
      "url": "https://arxiv.org/abs/2602.13835",
      "pdfUrl": "https://arxiv.org/pdf/2602.13835.pdf",
      "titleJa": "オーディオカード：構造化メタデータがサウンドデザインのためのオーディオ言語モデルを改善"
    },
    {
      "id": "2602.13685",
      "arxivId": "2602.13685",
      "title": "AuTAgent: A Reinforcement Learning Framework for Tool-Augmented Audio Reasoning",
      "authors": [
        "Siqian Tong",
        "Xuan Li",
        "Yiwei Wang",
        "Baolong Bi",
        "Yujun Cai",
        "Shenghua Liu",
        "Yuchen He",
        "Chengpeng Hao"
      ],
      "publishedDate": "2026-02-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Large Audio Language Models (LALMs) excel at perception but struggle with complex reasoning requiring precise acoustic measurements. While external tools can extract fine-grained features like exact tempo or pitch, effective integration remains challenging: naively using all tools causes information overload, while prompt-based selection fails to assess context-dependent utility. To address this, we propose AuTAgent (Audio Tool Agent), a reinforcement learning framework that learns when and which tools to invoke. By employing a sparse-feedback training strategy with a novel Differential Reward mechanism, the agent learns to filter out irrelevant tools and invokes external assistance only when it yields a net performance gain over the base model. Experimental results confirm that AuTAgent complements the representation bottleneck of LALMs by providing verifiable acoustic evidence. It improves accuracy by 4.20% / 6.20% and 9.80% / 8.00% for open-source and closed-source backbones on the MMAU Test-mini and the MMAR benchmarks, respectively. In addition, further experiments demonstrate exceptional transferability. We highlight the complementary role of external tools in augmenting audio model reasoning.",
      "url": "https://arxiv.org/abs/2602.13685",
      "pdfUrl": "https://arxiv.org/pdf/2602.13685.pdf",
      "titleJa": "AuTAgent: ツール拡張型音声推論のための強化学習フレームワーク"
    },
    {
      "id": "2602.12241",
      "arxivId": "2602.12241",
      "title": "Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications",
      "authors": [
        "Manjunath Kudlur",
        "Evan King",
        "James Wang",
        "Pete Warden"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent \"encode-the-whole-utterance\" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.",
      "url": "https://arxiv.org/abs/2602.12241",
      "pdfUrl": "https://arxiv.org/pdf/2602.12241.pdf",
      "titleJa": "Moonshine v2: 遅延が重要な音声アプリケーション向けのエルゴディックストリーミングエンコーダASR"
    }
  ],
  "lastUpdated": "2026-02-20T01:03:42.250918",
  "totalCount": 72
}