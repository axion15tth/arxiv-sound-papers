{
  "papers": [
    {
      "id": "2602.12241",
      "arxivId": "2602.12241",
      "title": "Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications",
      "authors": [
        "Manjunath Kudlur",
        "Evan King",
        "James Wang",
        "Pete Warden"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent \"encode-the-whole-utterance\" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.",
      "url": "https://arxiv.org/abs/2602.12241",
      "pdfUrl": "https://arxiv.org/pdf/2602.12241.pdf",
      "titleJa": "Moonshine v2: 遅延が重要な音声アプリケーション向けのエルゴディックストリーミングエンコーダASR"
    },
    {
      "id": "2602.11910",
      "arxivId": "2602.11910",
      "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
      "authors": [
        "Łukasz Staniszewski",
        "Katarzyna Zaleska",
        "Mateusz Modrzejewski",
        "Kamil Deja"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood.",
      "url": "https://arxiv.org/abs/2602.11910",
      "pdfUrl": "https://arxiv.org/pdf/2602.11910.pdf",
      "titleJa": "TADA! アクティベーションステアリングによるオーディオ拡散モデルのチューニング"
    },
    {
      "id": "2602.11909",
      "arxivId": "2602.11909",
      "title": "Echo: Towards Advanced Audio Comprehension via Audio-Interleaved Reasoning",
      "authors": [
        "Daiqing Wu",
        "Xuan Zhang",
        "Dongbao Yang",
        "Jiashu Yao",
        "Longfei Chen",
        "Qingsong Liu",
        "Sicheng Zhao",
        "Can Ma",
        "Yangyang Kang",
        "Yu Zhou"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "The maturation of Large Audio Language Models (LALMs) has raised growing expectations for them to comprehend complex audio much like humans. Current efforts primarily replicate text-based reasoning by contextualizing audio content through a one-time encoding, which introduces a critical information bottleneck. Drawing inspiration from human cognition, we propose audio-interleaved reasoning to break through this bottleneck. It treats audio as an active reasoning component, enabling sustained audio engagement and perception-grounded analysis. To instantiate it, we introduce a two-stage training framework, first teaching LALMs to localize salient audio segments through supervised fine-tuning, and then incentivizing proficient re-listening via reinforcement learning. In parallel, a structured data generation pipeline is developed to produce high-quality training data. Consequently, we present Echo, a LALM capable of dynamically re-listening to audio in demand during reasoning. On audio comprehension benchmarks, Echo achieves overall superiority in both challenging expert-level and general-purpose tasks. Comprehensive analysis further confirms the efficiency and generalizability of audio-interleaved reasoning, establishing it as a promising direction for advancing audio comprehension. Project page: https://github.com/wdqqdw/Echo.",
      "url": "https://arxiv.org/abs/2602.11909",
      "pdfUrl": "https://arxiv.org/pdf/2602.11909.pdf",
      "titleJa": "Echo: 音声インターリーブ推論による高度な音声理解に向けて"
    },
    {
      "id": "2602.11896",
      "arxivId": "2602.11896",
      "title": "Musical Metamerism with Time--Frequency Scattering",
      "authors": [
        "Vincent Lostanlen",
        "Han Han"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The concept of metamerism originates from colorimetry, where it describes a sensation of visual similarity between two colored lights despite significant differences in spectral content. Likewise, we propose to call ``musical metamerism'' the sensation of auditory similarity which is elicited by two music fragments which differ in terms of underlying waveforms. In this technical report, we describe a method to generate musical metamers from any audio recording. Our method is based on joint time--frequency scattering in Kymatio, an open-source software in Python which enables GPU computing and automatic differentiation. The advantage of our method is that it does not require any manual preprocessing, such as transcription, beat tracking, or source separation. We provide a mathematical description of JTFS as well as some excerpts from the Kymatio source code. Lastly, we review the prior work on JTFS and draw connections with closely related algorithms, such as spectrotemporal receptive fields (STRF), modulation power spectra (MPS), and Gabor filterbank (GBFB).",
      "url": "https://arxiv.org/abs/2602.11896",
      "pdfUrl": "https://arxiv.org/pdf/2602.11896.pdf",
      "titleJa": "時間周波数散乱を伴う音楽的メタメリズム"
    },
    {
      "id": "2602.11488",
      "arxivId": "2602.11488",
      "title": "When Audio-LLMs Don't Listen: A Cross-Linguistic Study of Modality Arbitration",
      "authors": [
        "Jayadev Billa"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "When audio and text conflict, speech-enabled language models follow the text 10 times more often than when arbitrating between two text sources, even when explicitly instructed to trust the audio. Using ALME, a benchmark of 57,602 controlled audio-text conflict stimuli across 8 languages, we find that Gemini 2.0 Flash exhibits 16.6\\% text dominance under audio-text conflict versus 1.6\\% under text-text conflict with identical reliability cues. This gap is not explained by audio quality: audio-only accuracy (97.2\\%) exceeds cascade accuracy (93.9\\%), indicating audio embeddings preserve more information than text transcripts. We propose that text dominance reflects an asymmetry not in information content but in arbitration accessibility: how easily the model can reason over competing representations. This framework explains otherwise puzzling findings. Forcing transcription before answering increases text dominance (19\\% to 33\\%), sacrificing audio's information advantage without improving accessibility. Framing text as ``deliberately corrupted'' reduces text dominance by 80\\%. A fine-tuning ablation provides interventional evidence: training only the audio projection layer increases text dominance (+26.5\\%), while LoRA on the language model halves it ($-$23.9\\%), localizing text dominance to the LLM's reasoning rather than the audio encoder. Experiments across four state-of-the-art audio-LLMs and 8 languages show consistent trends with substantial cross-linguistic and cross-model variation, establishing modality arbitration as a distinct reliability dimension not captured by standard speech benchmarks.",
      "url": "https://arxiv.org/abs/2602.11488",
      "pdfUrl": "https://arxiv.org/pdf/2602.11488.pdf",
      "titleJa": "オーディオLLMが聞き取れない時：モダリティ仲裁に関する言語間研究"
    },
    {
      "id": "2602.11425",
      "arxivId": "2602.11425",
      "title": "Surface impedance inference via neural fields and sparse acoustic data obtained by a compact array",
      "authors": [
        "Yuanxin Xia",
        "Xinyan Li",
        "Matteo Calafà",
        "Allan P. Engsig-Karup",
        "Cheol-Ho Jeong"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Standardized laboratory characterizations for absorbing materials rely on idealized sound field assumptions, which deviate largely from real-life conditions. Consequently, \\emph{in-situ} acoustic characterization has become essential for accurate diagnosis and virtual prototyping. We propose a physics-informed neural field that reconstructs local, near-surface broadband sound fields from sparse pressure samples to directly infer complex surface impedance. A parallel, multi-frequency architecture enables a broadband impedance retrieval within runtimes on the order of seconds to minutes. To validate the method, we developed a compact microphone array with low hardware complexity. Numerical verifications and laboratory experiments demonstrate accurate impedance retrieval with a small number of sensors under realistic conditions. We further showcase the approach in a vehicle cabin to provide practical guidance on measurement locations that avoid strong interference. Here, we show that this approach offers a robust means of characterizing \\emph{in-situ} boundary conditions for architectural and automotive acoustics.",
      "url": "https://arxiv.org/abs/2602.11425",
      "pdfUrl": "https://arxiv.org/pdf/2602.11425.pdf",
      "titleJa": "コンパクトなアレイによって得られた神経場とスパース音響データによる表面インピーダンスの推定"
    },
    {
      "id": "2602.11145",
      "arxivId": "2602.11145",
      "title": "SCRAPL: Scattering Transform with Random Paths for Machine Learning",
      "authors": [
        "Christopher Mitcheltree",
        "Vincent Lostanlen",
        "Emmanouil Benetos",
        "Mathieu Lagrange"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "The Euclidean distance between wavelet scattering transform coefficients (known as paths) provides informative gradients for perceptual quality assessment of deep inverse problems in computer vision, speech, and audio processing. However, these transforms are computationally expensive when employed as differentiable loss functions for stochastic gradient descent due to their numerous paths, which significantly limits their use in neural network training. Against this problem, we propose \"Scattering transform with Random Paths for machine Learning\" (SCRAPL): a stochastic optimization scheme for efficient evaluation of multivariable scattering transforms. We implement SCRAPL for the joint time-frequency scattering transform (JTFS) which demodulates spectrotemporal patterns at multiple scales and rates, allowing a fine characterization of intermittent auditory textures. We apply SCRAPL to differentiable digital signal processing (DDSP), specifically, unsupervised sound matching of a granular synthesizer and the Roland TR-808 drum machine. We also propose an initialization heuristic based on importance sampling, which adapts SCRAPL to the perceptual content of the dataset, improving neural network convergence and evaluation performance. We make our code and audio samples available and provide SCRAPL as a Python package.",
      "url": "https://arxiv.org/abs/2602.11145",
      "pdfUrl": "https://arxiv.org/pdf/2602.11145.pdf",
      "titleJa": "SCRAPL: 機械学習のためのランダムパスを用いた散乱変換"
    },
    {
      "id": "2602.11072",
      "arxivId": "2602.11072",
      "title": "Simultaneous Speech-to-Speech Translation Without Aligned Data",
      "authors": [
        "Tom Labiausse",
        "Romain Fabre",
        "Yannick Estève",
        "Alexandre Défossez",
        "Neil Zeghidour"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Simultaneous speech translation requires translating source speech into a target language in real-time while handling non-monotonic word dependencies. Traditional approaches rely on supervised training with word-level aligned data, which is difficult to collect at scale and thus depends on synthetic alignments using language-specific heuristics that are suboptimal. We propose Hibiki-Zero, which eliminates the need for word-level alignments entirely. This fundamentally simplifies the training pipeline and enables seamless scaling to diverse languages with varying grammatical structures, removing the bottleneck of designing language-specific alignment heuristics. We first train on sentence-level aligned data to learn speech translation at high latency, then apply a novel reinforcement learning strategy using GRPO to optimize latency while preserving translation quality. Hibiki-Zero achieves state-of-the-art performance in translation accuracy, latency, voice transfer, and naturalness across five X-to-English tasks. Moreover, we demonstrate that our model can be adapted to support a new input language with less than 1000h of speech. We provide examples, model weights, inference code and we release a benchmark containing 45h of multilingual data for speech translation evaluation.",
      "url": "https://arxiv.org/abs/2602.11072",
      "pdfUrl": "https://arxiv.org/pdf/2602.11072.pdf",
      "titleJa": "整合データなしの音声同時翻訳"
    },
    {
      "id": "2602.10934",
      "arxivId": "2602.10934",
      "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
      "authors": [
        "Yitian Gong",
        "Kuangwei Chen",
        "Zhaoye Fei",
        "Xiaogui Yang",
        "Ke Chen",
        "Yang Wang",
        "Kexin Huang",
        "Mingshu Chen",
        "Ruixiao Li",
        "Qingyuan Cheng",
        "Shimin Li",
        "Xipeng Qiu"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.",
      "url": "https://arxiv.org/abs/2602.10934",
      "pdfUrl": "https://arxiv.org/pdf/2602.10934.pdf",
      "titleJa": "MOSS-Audio-Tokenizer: 将来のオーディオ基盤モデルに向けたオーディオトークナイザーのスケーリング"
    },
    {
      "id": "2602.10829",
      "arxivId": "2602.10829",
      "title": "Self-Supervised Learning for Speaker Recognition: A study and review",
      "authors": [
        "Theo Lepage",
        "Reda Dehak"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Deep learning models trained in a supervised setting have revolutionized audio and speech processing. However, their performance inherently depends on the quantity of human-annotated data, making them costly to scale and prone to poor generalization under unseen conditions. To address these challenges, Self-Supervised Learning (SSL) has emerged as a promising paradigm, leveraging vast amounts of unlabeled data to learn relevant representations. The application of SSL for Automatic Speech Recognition (ASR) has been extensively studied, but research on other downstream tasks, notably Speaker Recognition (SR), remains in its early stages. This work describes major SSL instance-invariance frameworks (e.g., SimCLR, MoCo, and DINO), initially developed for computer vision, along with their adaptation to SR. Various SSL methods for SR, proposed in the literature and built upon these frameworks, are also presented. An extensive review of these approaches is then conducted: (1) the effect of the main hyperparameters of SSL frameworks is investigated; (2) the role of SSL components is studied (e.g., data-augmentation, projector, positive sampling); and (3) SSL frameworks are evaluated on SR with in-domain and out-of-domain data, using a consistent experimental setup, and a comprehensive comparison of SSL methods from the literature is provided. Specifically, DINO achieves the best downstream performance and effectively models intra-speaker variability, although it is highly sensitive to hyperparameters and training conditions, while SimCLR and MoCo provide robust alternatives that effectively capture inter-speaker variability and are less prone to collapse. This work aims to highlight recent trends and advancements, identifying current challenges in the field.",
      "url": "https://arxiv.org/abs/2602.10829",
      "pdfUrl": "https://arxiv.org/pdf/2602.10829.pdf",
      "titleJa": "話者認識のための自己教師学習：研究とレビュー"
    },
    {
      "id": "2602.10735",
      "arxivId": "2602.10735",
      "title": "Calliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity",
      "authors": [
        "Hugo L. Hammer",
        "Vajira Thambawita",
        "Pål Halvorsen"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "A narrated e-book combines synchronized audio with digital text, highlighting the currently spoken word or sentence during playback. This format supports early literacy and assists individuals with reading challenges, while also allowing general readers to seamlessly switch between reading and listening. With the emergence of natural-sounding neural Text-to-Speech (TTS) technology, several commercial services have been developed to leverage these technology for converting standard text e-books into high-quality narrated e-books. However, no open-source solutions currently exist to perform this task. In this paper, we present Calliope, an open-source framework designed to fill this gap. Our method leverages state-of-the-art open-source TTS to convert a text e-book into a narrated e-book in the EPUB 3 Media Overlay format. The method offers several innovative steps: audio timestamps are captured directly during TTS, ensuring exact synchronization between narration and text highlighting; the publisher's original typography, styling, and embedded media are strictly preserved; and the entire pipeline operates offline. This offline capability eliminates recurring API costs, mitigates privacy concerns, and avoids copyright compliance issues associated with cloud-based services. The framework currently supports the state-of-the-art open-source TTS systems XTTS-v2 and Chatterbox. A potential alternative approach involves first generating narration via TTS and subsequently synchronizing it with the text using forced alignment. However, while our method ensures exact synchronization, our experiments show that forced alignment introduces drift between the audio and text highlighting significant enough to degrade the reading experience. Source code and usage instructions are available at https://github.com/hugohammer/TTS-Narrated-Ebook-Creator.git.",
      "url": "https://arxiv.org/abs/2602.10735",
      "pdfUrl": "https://arxiv.org/pdf/2602.10735.pdf",
      "titleJa": "Calliope: 正確な同期、プライバシー、レイアウトの忠実性を保証する TTS ベースのナレーション付き電子書籍作成ツール"
    },
    {
      "id": "2602.10716",
      "arxivId": "2602.10716",
      "title": "RE-LLM: Refining Empathetic Speech-LLM Responses by Integrating Emotion Nuance",
      "authors": [
        "Jing-Han Chen",
        "Bo-Hao Su",
        "Ya-Tse Wu",
        "Chi-Chun Lee"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "With generative AI advancing, empathy in human-AI interaction is essential. While prior work focuses on emotional reflection, emotional exploration, key to deeper engagement, remains overlooked. Existing LLMs rely on text which captures limited emotion nuances. To address this, we propose RE-LLM, a speech-LLM integrating dimensional emotion embeddings and auxiliary learning. Experiments show statistically significant gains in empathy metrics across three datasets. RE-LLM relatively improves the Emotional Reaction score by 14.79% and 6.76% compared to text-only and speech-LLM baselines on ESD. Notably, it raises the Exploration score by 35.42% and 3.91% on IEMOCAP, 139.28% and 9.83% on ESD, and 60.95% and 22.64% on MSP-PODCAST. It also boosts unweighted accuracy by 5.4% on IEMOCAP, 2.3% on ESD, and 6.9% on MSP-PODCAST in speech emotion recognition. These results highlight the enriched emotional understanding and improved empathetic response generation of RE-LLM.",
      "url": "https://arxiv.org/abs/2602.10716",
      "pdfUrl": "https://arxiv.org/pdf/2602.10716.pdf",
      "titleJa": "RE-LLM: 感情のニュアンスを統合することで共感的なスピーチLLM応答を洗練させる"
    },
    {
      "id": "2602.10666",
      "arxivId": "2602.10666",
      "title": "From Diet to Free Lunch: Estimating Auxiliary Signal Properties using Dynamic Pruning Masks in Speech Enhancement Networks",
      "authors": [
        "Riccardo Miccini",
        "Clément Laroche",
        "Tobias Piechowiak",
        "Xenofon Fafoutis",
        "Luca Pezzarossa"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Speech Enhancement (SE) in audio devices is often supported by auxiliary modules for Voice Activity Detection (VAD), SNR estimation, or Acoustic Scene Classification to ensure robust context-aware behavior and seamless user experience. Just like SE, these tasks often employ deep learning; however, deploying additional models on-device is computationally impractical, whereas cloud-based inference would introduce additional latency and compromise privacy. Prior work on SE employed Dynamic Channel Pruning (DynCP) to reduce computation by adaptively disabling specific channels based on the current input. In this work, we investigate whether useful signal properties can be estimated from these internal pruning masks, thus removing the need for separate models. We show that simple, interpretable predictors achieve up to 93% accuracy on VAD, 84% on noise classification, and an R2 of 0.86 on F0 estimation. With binary masks, predictions reduce to weighted sums, inducing negligible overhead. Our contribution is twofold: on one hand, we examine the emergent behavior of DynCP models through the lens of downstream prediction tasks, to reveal what they are learning; on the other, we repurpose and re-propose DynCP as a holistic solution for efficient SE and simultaneous estimation of signal properties.",
      "url": "https://arxiv.org/abs/2602.10666",
      "pdfUrl": "https://arxiv.org/pdf/2602.10666.pdf",
      "titleJa": "ダイエットから無料ランチへ：音声強調ネットワークにおける動的プルーニングマスクを用いた補助信号特性の推定"
    },
    {
      "id": "2602.10656",
      "arxivId": "2602.10656",
      "title": "AudioRAG: A Challenging Benchmark for Audio Reasoning and Information Retrieval",
      "authors": [
        "Jingru Lin",
        "Chen Zhang",
        "Tianrui Wang",
        "Haizhou Li"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Due to recent advancements in Large Audio-Language Models (LALMs) that demonstrate remarkable performance across a range of sound-, speech- and music-related tasks, there is a growing interest in proposing benchmarks to assess these models. Existing benchmarks generally focus only on reasoning with internal knowledge, neglecting real-world scenarios that require external information grounding. To bridge this gap, we introduce AudioRAG, a novel benchmark designed to evaluate audio-based reasoning augmented by information retrieval in realistic web environments. This benchmark comprises both LLM-generated and manually curated question-answer pairs. Our evaluations reveal that even the state-of-the-art LALMs struggle to answer these questions. We therefore propose an agentic pipeline that integrates audio reasoning with retrieval-augmented generation, providing a stronger baseline for future research.",
      "url": "https://arxiv.org/abs/2602.10656",
      "pdfUrl": "https://arxiv.org/pdf/2602.10656.pdf",
      "titleJa": "AudioRAG: オーディオ推論と情報検索のための挑戦的なベンチマーク"
    },
    {
      "id": "2602.10439",
      "arxivId": "2602.10439",
      "title": "AudioRouter: Data Efficient Audio Understanding via RL based Dual Reasoning",
      "authors": [
        "Liyang Chen",
        "Hongkai Chen",
        "Yujun Cai",
        "Sifan Li",
        "Qingwen Ye",
        "Yiwei Wang"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Large Audio Language Models (LALMs) have demonstrated strong capabilities in audio understanding and reasoning. However, their performance on fine grained auditory perception remains unreliable, and existing approaches largely rely on data intensive training to internalize perceptual abilities. We propose AudioRouter, a reinforcement learning framework that enables LALMs to improve audio understanding by learning when and how to use external audio tools. Rather than tightly coupling tool usage with audio reasoning, AudioRouter formulates tool use as an explicit decision making problem and optimizes a lightweight routing policy while keeping the underlying reasoning model frozen. Experimental results show that AudioRouter achieves substantial improvements on standard audio understanding benchmarks while requiring up to 600x less training data to learn tool usage compared with conventional training paradigms. These findings suggest that learning effective tool usage offers a data efficient and scalable alternative to internalizing perceptual abilities in LALMs.",
      "url": "https://arxiv.org/abs/2602.10439",
      "pdfUrl": "https://arxiv.org/pdf/2602.10439.pdf",
      "titleJa": "AudioRouter: RLベースの二重推論によるデータ効率の高いオーディオ理解"
    },
    {
      "id": "2602.10230",
      "arxivId": "2602.10230",
      "title": "Frame-Level Internal Tool Use for Temporal Grounding in Audio LMs",
      "authors": [
        "Joesph An",
        "Phillip Keung",
        "Jiaqi Wang",
        "Orevaoghene Ahia",
        "Noah A. Smith"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Large audio language models are increasingly used for complex audio understanding tasks, but they struggle with temporal tasks that require precise temporal grounding, such as word alignment and speaker diarization. The standard approach, where we generate timestamps as sequences of text tokens, is computationally expensive and prone to hallucination, especially when processing audio lengths outside the model's training distribution. In this work, we propose frame-level internal tool use, a method that trains audio LMs to use their own internal audio representations to perform temporal grounding directly. We introduce a lightweight prediction mechanism trained via two objectives: a binary frame classifier and a novel inhomogeneous Poisson process (IHP) loss that models temporal event intensity. Across word localization, speaker diarization, and event localization tasks, our approach outperforms token-based baselines. Most notably, it achieves a >50x inference speedup and demonstrates robust length generalization, maintaining high accuracy on out-of-distribution audio durations where standard token-based models collapse completely.",
      "url": "https://arxiv.org/abs/2602.10230",
      "pdfUrl": "https://arxiv.org/pdf/2602.10230.pdf",
      "titleJa": "オーディオLMにおける時間的グラウンディングのためのフレームレベル内部ツールの使用"
    },
    {
      "id": "2602.10058",
      "arxivId": "2602.10058",
      "title": "Evaluating Disentangled Representations for Controllable Music Generation",
      "authors": [
        "Laura Ibáñez-Martínez",
        "Chukwuemeka Nkama",
        "Andrea Poltronieri",
        "Xavier Serra",
        "Martín Rocamora"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Recent approaches in music generation rely on disentangled representations, often labeled as structure and timbre or local and global, to enable controllable synthesis. Yet the underlying properties of these embeddings remain underexplored. In this work, we evaluate such disentangled representations in a set of music audio models for controllable generation using a probing-based framework that goes beyond standard downstream tasks. The selected models reflect diverse unsupervised disentanglement strategies, including inductive biases, data augmentations, adversarial objectives, and staged training procedures. We further isolate specific strategies to analyze their effect. Our analysis spans four key axes: informativeness, equivariance, invariance, and disentanglement, which are assessed across datasets, tasks, and controlled transformations. Our findings reveal inconsistencies between intended and actual semantics of the embeddings, suggesting that current strategies fall short of producing truly disentangled representations, and prompting a re-examination of how controllability is approached in music generation.",
      "url": "https://arxiv.org/abs/2602.10058",
      "pdfUrl": "https://arxiv.org/pdf/2602.10058.pdf",
      "titleJa": "制御可能な音楽生成のための分離表現の評価"
    },
    {
      "id": "2602.09970",
      "arxivId": "2602.09970",
      "title": "BioME: A Resource-Efficient Bioacoustic Foundational Model for IoT Applications",
      "authors": [
        "Heitor R. Guimarães",
        "Abhishek Tiwari",
        "Mahsa Abdollahi",
        "Anderson R. Avila",
        "Tiago H. Falk"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Passive acoustic monitoring has become a key strategy in biodiversity assessment, conservation, and behavioral ecology, especially as Internet-of-Things (IoT) devices enable continuous in situ audio collection at scale. While recent self-supervised learning (SSL)-based audio encoders, such as BEATs and AVES, have shown strong performance in bioacoustic tasks, their computational cost and limited robustness to unseen environments hinder deployment on resource-constrained platforms. In this work, we introduce BioME, a resource-efficient audio encoder designed for bioacoustic applications. BioME is trained via layer-to-layer distillation from a high-capacity teacher model, enabling strong representational transfer while reducing the parameter count by 75%. To further improve ecological generalization, the model is pretrained on multi-domain data spanning speech, environmental sounds, and animal vocalizations. A key contribution is the integration of modulation-aware acoustic features via FiLM conditioning, injecting a DSP-inspired inductive bias that enhances feature disentanglement in low-capacity regimes. Across multiple bioacoustic tasks, BioME matches or surpasses the performance of larger models, including its teacher, while being suitable for resource-constrained IoT deployments. For reproducibility, code and pretrained checkpoints are publicly available.",
      "url": "https://arxiv.org/abs/2602.09970",
      "pdfUrl": "https://arxiv.org/pdf/2602.09970.pdf",
      "titleJa": "BioME: IoTアプリケーションのためのリソース効率の高いバイオ音響基礎モデル"
    },
    {
      "id": "2602.09891",
      "arxivId": "2602.09891",
      "title": "Stemphonic: All-at-once Flexible Multi-stem Music Generation",
      "authors": [
        "Shih-Lun Wu",
        "Ge Zhu",
        "Juan-Pablo Caceres",
        "Cheng-Zhi Anna Huang",
        "Nicholas J. Bryan"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM"
      ],
      "abstract": "Music stem generation, the task of producing musically-synchronized and isolated instrument audio clips, offers the potential of greater user control and better alignment with musician workflows compared to conventional text-to-music models. Existing stem generation approaches, however, either rely on fixed architectures that output a predefined set of stems in parallel, or generate only one stem at a time, resulting in slow inference despite flexibility in stem combination. We propose Stemphonic, a diffusion-/flow-based framework that overcomes this trade-off and generates a variable set of synchronized stems in one inference pass. During training, we treat each stem as a batch element, group synchronized stems in a batch, and apply a shared noise latent to each group. At inference-time, we use a shared initial noise latent and stem-specific text inputs to generate synchronized multi-stem outputs in one pass. We further expand our approach to enable one-pass conditional multi-stem generation and stem-wise activity controls to empower users to iteratively generate and orchestrate the temporal layering of a mix. We benchmark our results on multiple open-source stem evaluation sets and show that Stemphonic produces higher-quality outputs while accelerating the full mix generation process by 25 to 50%. Demos at: https://stemphonic-demo.vercel.app.",
      "url": "https://arxiv.org/abs/2602.09891",
      "pdfUrl": "https://arxiv.org/pdf/2602.09891.pdf",
      "titleJa": "Stemphonic: 一度に柔軟なマルチステム音楽生成"
    },
    {
      "id": "2602.09823",
      "arxivId": "2602.09823",
      "title": "Covo-Audio Technical Report",
      "authors": [
        "Wenfu Wang",
        "Chenxing Li",
        "Liqiang Zhang",
        "Yiyang Zhao",
        "Yuxiang Zou",
        "Hanzhao Li",
        "Mingyu Cui",
        "Hao Zhang",
        "Kun Wei",
        "Le Xu",
        "Zikang Huang",
        "Jiajun Xu",
        "Jiliang Hu",
        "Xiang He",
        "Zeyu Xie",
        "Jiawen Kang",
        "Youjun Chen",
        "Meng Yu",
        "Dong Yu",
        "Rilin Chen",
        "Linlin Di",
        "Shulin Feng",
        "Na Hu",
        "Yang Liu",
        "Bang Wang",
        "Shan Yang"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "In this work, we present Covo-Audio, a 7B-parameter end-to-end LALM that directly processes continuous audio inputs and generates audio outputs within a single unified architecture. Through large-scale curated pretraining and targeted post-training, Covo-Audio achieves state-of-the-art or competitive performance among models of comparable scale across a broad spectrum of tasks, including speech-text modeling, spoken dialogue, speech understanding, audio understanding, and full-duplex voice interaction. Extensive evaluations demonstrate that the pretrained foundation model exhibits strong speech-text comprehension and semantic reasoning capabilities on multiple benchmarks, outperforming representative open-source models of comparable scale. Furthermore, Covo-Audio-Chat, the dialogue-oriented variant, demonstrates strong spoken conversational abilities, including understanding, contextual reasoning, instruction following, and generating contextually appropriate and empathetic responses, validating its applicability to real-world conversational assistant scenarios. Covo-Audio-Chat-FD, the evolved full-duplex model, achieves substantially superior performance on both spoken dialogue capabilities and full-duplex interaction behaviors, demonstrating its competence in practical robustness. To mitigate the high cost of deploying end-to-end LALMs for natural conversational systems, we propose an intelligence-speaker decoupling strategy that separates dialogue intelligence from voice rendering, enabling flexible voice customization with minimal text-to-speech (TTS) data while preserving dialogue performance. Overall, our results highlight the strong potential of 7B-scale models to integrate sophisticated audio intelligence with high-level semantic reasoning, and suggest a scalable path toward more capable and versatile LALMs.",
      "url": "https://arxiv.org/abs/2602.09823",
      "pdfUrl": "https://arxiv.org/pdf/2602.09823.pdf",
      "titleJa": "Covo-Audio 技術レポート"
    },
    {
      "id": "2602.11670",
      "arxivId": "2602.11670",
      "title": "Exploring Frequency-Domain Feature Modeling for HRTF Magnitude Upsampling",
      "authors": [
        "Xingyu Chen",
        "Hanwen Bi",
        "Fei Ma",
        "Sipei Zhao",
        "Eva Cheng",
        "Ian S. Burnett"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Accurate upsampling of Head-Related Transfer Functions (HRTFs) from sparse measurements is crucial for personalized spatial audio rendering. Traditional interpolation methods, such as kernel-based weighting or basis function expansions, rely on measurements from a single subject and are limited by the spatial sampling theorem, resulting in significant performance degradation under sparse sampling. Recent learning-based methods alleviate this limitation by leveraging cross-subject information, yet most existing neural architectures primarily focus on modeling spatial relationships across directions, while spectral dependencies along the frequency dimension are often modeled implicitly or treated independently. However, HRTF magnitude responses exhibit strong local continuity and long-range structure in the frequency domain, which are not fully exploited. This work investigates frequency-domain feature modeling by examining how different architectural choices, ranging from per-frequency multilayer perceptrons to convolutional, dilated convolutional, and attention-based models, affect performance under varying sparsity levels, showing that explicit spectral modeling consistently improves reconstruction accuracy, particularly under severe sparsity. Motivated by this observation, a frequency-domain Conformer-based architecture is adopted to jointly capture local spectral continuity and long-range frequency correlations. Experimental results on the SONICOM and HUTUBS datasets demonstrate that the proposed method achieves state-of-the-art performance in terms of interaural level difference and log-spectral distortion.",
      "url": "https://arxiv.org/abs/2602.11670",
      "pdfUrl": "https://arxiv.org/pdf/2602.11670.pdf",
      "titleJa": "HRTF振幅アップサンプリングのための周波数領域特徴モデリングの検討"
    },
    {
      "id": "2602.11546",
      "arxivId": "2602.11546",
      "title": "TC-BiMamba: Trans-Chunk bidirectionally within BiMamba for unified streaming and non-streaming ASR",
      "authors": [
        "Qingshun She",
        "Jing Peng",
        "Yangui Fang",
        "Yu Xi",
        "Kai Yu"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This work investigates bidirectional Mamba (BiMamba) for unified streaming and non-streaming automatic speech recognition (ASR). Dynamic chunk size training enables a single model for offline decoding and streaming decoding with various latency settings. In contrast, existing BiMamba based streaming method is limited to fixed chunk size decoding. When dynamic chunk size training is applied, training overhead increases substantially. To tackle this issue, we propose the Trans-Chunk BiMamba (TC-BiMamba) for dynamic chunk size training. Trans-Chunk mechanism trains both bidirectional sequences in an offline style with dynamic chunk size. On the one hand, compared to traditional chunk-wise processing, TC-BiMamba simultaneously achieves 1.3 times training speedup, reduces training memory by 50%, and improves model performance since it can capture bidirectional context. On the other hand, experimental results show that TC-BiMamba outperforms U2++ and matches LC-BiMmaba with smaller model size.",
      "url": "https://arxiv.org/abs/2602.11546",
      "pdfUrl": "https://arxiv.org/pdf/2602.11546.pdf",
      "titleJa": "TC-BiMamba: BiMamba 内で双方向にチャンクを転送し、ストリーミングと非ストリーミングの ASR を統合"
    },
    {
      "id": "2602.11477",
      "arxivId": "2602.11477",
      "title": "SLD-L2S: Hierarchical Subspace Latent Diffusion for High-Fidelity Lip to Speech Synthesis",
      "authors": [
        "Yifan Liang",
        "Andong Li",
        "Kang Yang",
        "Guochen Yu",
        "Fangkun Liu",
        "Lingling Dai",
        "Xiaodong Li",
        "Chengshi Zheng"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "eess.AS",
        "cs.CE"
      ],
      "abstract": "Although lip-to-speech synthesis (L2S) has achieved significant progress in recent years, current state-of-the-art methods typically rely on intermediate representations such as mel-spectrograms or discrete self-supervised learning (SSL) tokens. The potential of latent diffusion models (LDMs) in this task remains largely unexplored. In this paper, we introduce SLD-L2S, a novel L2S framework built upon a hierarchical subspace latent diffusion model. Our method aims to directly map visual lip movements to the continuous latent space of a pre-trained neural audio codec, thereby avoiding the information loss inherent in traditional intermediate representations. The core of our method is a hierarchical architecture that processes visual representations through multiple parallel subspaces, initiated by a subspace decomposition module. To efficiently enhance interactions within and between these subspaces, we design the diffusion convolution block (DiCB) as our network backbone. Furthermore, we employ a reparameterized flow matching technique to directly generate the target latent vectors. This enables a principled inclusion of speech language model (SLM) and semantic losses during training, moving beyond conventional flow matching objectives and improving synthesized speech quality. Our experiments show that SLD-L2S achieves state-of-the-art generation quality on multiple benchmark datasets, surpassing existing methods in both objective and subjective evaluations.",
      "url": "https://arxiv.org/abs/2602.11477",
      "pdfUrl": "https://arxiv.org/pdf/2602.11477.pdf",
      "titleJa": "SLD-L2S: 階層的部分空間潜在拡散法による高忠実度口唇音声合成"
    },
    {
      "id": "2602.10166",
      "arxivId": "2602.10166",
      "title": "MerkleSpeech: Public-Key Verifiable, Chunk-Localised Speech Provenance via Perceptual Fingerprints and Merkle Commitments",
      "authors": [
        "Tatsunori Ono"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.CR",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Speech provenance goes beyond detecting whether a watermark is present. Real workflows involve splicing, quoting, trimming, and platform-level transforms that may preserve some regions while altering others. Neural watermarking systems have made strides in robustness and localised detection, but most deployments produce outputs with no third-party verifiable cryptographic proof tying a time segment to an issuer-signed original. Provenance standards like C2PA adopt signed manifests and Merkle-based fragment validation, yet their bindings target encoded assets and break under re-encoding or routine processing. We propose MerkleSpeech, a system for public-key verifiable, chunk-localised speech provenance offering two tiers of assurance. The first, a robust watermark attribution layer (WM-only), survives common distribution transforms and answers \"was this chunk issued by a known party?\". The second, a strict cryptographic integrity layer (MSv1), verifies Merkle inclusion of the chunk's fingerprint under an issuer signature. The system computes perceptual fingerprints over short speech chunks, commits them in a Merkle tree whose root is signed with an issuer key, and embeds a compact in-band watermark payload carrying a random content identifier and chunk metadata sufficient to retrieve Merkle inclusion proofs from a repository. Once the payload is extracted, all subsequent verification steps (signature check, fingerprint recomputation, Merkle inclusion) use only public information. The result is a splice-aware timeline indicating which regions pass each tier and why any given region fails. We describe the protocol, provide pseudocode, and present experiments targeting very low false positive rates under resampling, bandpass filtering, and additive noise, informed by recent audits identifying neural codecs as a major stressor for post-hoc audio watermarks.",
      "url": "https://arxiv.org/abs/2602.10166",
      "pdfUrl": "https://arxiv.org/pdf/2602.10166.pdf",
      "titleJa": "MerkleSpeech: 知覚指紋とMerkleコミットメントによる公開鍵検証可能、チャンク局所化音声来歴"
    },
    {
      "id": "2602.09594",
      "arxivId": "2602.09594",
      "title": "Evaluation of acoustic Green's function in rectangular rooms with general surface impedance walls",
      "authors": [
        "Matteo Calafà",
        "Yuanxin Xia",
        "Jonas Brunskog",
        "Cheol-Ho Jeong"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "eess.AS",
        "cs.CE",
        "cs.SD"
      ],
      "abstract": "Acoustic room modes and the Green's function mode expansion are well-known for rectangular rooms with perfectly reflecting walls. First-order approximations also exist for nearly rigid boundaries; however, current analytical methods fail to accommodate more general boundary conditions, e.g., when wall absorption is significant. In this work, we present a comprehensive analysis that extends previous studies by including additional first-order asymptotics that account for soft-wall boundaries. In addition, we introduce a semi-analytical, efficient, and reliable method for computing the Green's function in rectangular rooms, which is described and validated through numerical tests. With a sufficiently large truncation order, the resulting error becomes negligible, making the method suitable as a benchmark for numerical simulations. Additional aspects regarding the spectral basis orthogonality and completeness are also addressed, providing a general framework for the validity of the proposed approach.",
      "url": "https://arxiv.org/abs/2602.09594",
      "pdfUrl": "https://arxiv.org/pdf/2602.09594.pdf",
      "titleJa": "一般的な表面インピーダンス壁を備えた長方形の部屋における音響グリーン関数の評価"
    },
    {
      "id": "2602.10164",
      "arxivId": "2602.10164",
      "title": "Emotion-Coherent Speech Data Augmentation and Self-Supervised Contrastive Style Training for Enhancing Kids's Story Speech Synthesis",
      "authors": [
        "Raymond Chung"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Expressive speech synthesis requires vibrant prosody and well-timed pauses. We propose an effective strategy to augment a small dataset to train an expressive end-to-end Text-to-Speech model. We merge audios of emotionally congruent text using a text emotion recognizer, creating augmented expressive speech data. By training with two-sentence audio, our model learns natural breaks between lines. We further apply self-supervised contrastive training to improve the speaking style embedding extraction from speech. During inference, our model produces multi-sentence speech in one step, guided by the text-predicted speaking style. Evaluations showcase the effectiveness of our proposed approach when compared to a baseline model trained with consecutive two-sentence audio. Our synthesized speeches give a closer inter-sentence pause distribution to the ground truth speech. Subjective evaluations reveal our synthesized speech scored higher in naturalness and style suitability than the baseline.",
      "url": "https://arxiv.org/abs/2602.10164",
      "pdfUrl": "https://arxiv.org/pdf/2602.10164.pdf",
      "titleJa": "子供向けストーリー音声合成を強化するための感情整合音声データ拡張と自己教師対照スタイルトレーニング"
    },
    {
      "id": "2602.12281",
      "arxivId": "2602.12281",
      "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
      "authors": [
        "Jacky Kwok",
        "Xilun Zhang",
        "Mengdi Xu",
        "Yuejiang Liu",
        "Azalia Mirhoseini",
        "Chelsea Finn",
        "Marco Pavone"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "abstract": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.",
      "url": "https://arxiv.org/abs/2602.12281",
      "pdfUrl": "https://arxiv.org/pdf/2602.12281.pdf",
      "titleJa": "ビジョン・言語・行動の整合には、ポリシー学習のスケーリングよりも検証のスケーリングの方が効果的である可能性がある"
    },
    {
      "id": "2602.12279",
      "arxivId": "2602.12279",
      "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
      "authors": [
        "Leon Liangyu Chen",
        "Haoyu Ma",
        "Zhipeng Fan",
        "Ziqi Huang",
        "Animesh Sinha",
        "Xiaoliang Dai",
        "Jialiang Wang",
        "Zecheng He",
        "Jianwei Yang",
        "Chunyuan Li",
        "Junzhe Sun",
        "Chu Wang",
        "Serena Yeung-Levy",
        "Felix Juefei-Xu"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.",
      "url": "https://arxiv.org/abs/2602.12279",
      "pdfUrl": "https://arxiv.org/pdf/2602.12279.pdf",
      "titleJa": "UniT: 統合マルチモーダル思考連鎖テスト時間スケーリング"
    },
    {
      "id": "2602.12278",
      "arxivId": "2602.12278",
      "title": "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers",
      "authors": [
        "David Jiahao Fu",
        "Lam Thanh Do",
        "Jiayu Li",
        "Kevin Chen-Chuan Chang"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "abstract": "Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.",
      "url": "https://arxiv.org/abs/2602.12278",
      "pdfUrl": "https://arxiv.org/pdf/2602.12278.pdf",
      "titleJa": "AttentionRetriever: Attention Layers は実は長いドキュメントリトリーバーです"
    },
    {
      "id": "2602.12276",
      "arxivId": "2602.12276",
      "title": "Agentic Test-Time Scaling for WebAgents",
      "authors": [
        "Nicholas Lee",
        "Lutfi Eren Erdogan",
        "Chris Joseph John",
        "Surya Krishnapillai",
        "Michael W. Mahoney",
        "Kurt Keutzer",
        "Amir Gholami"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.",
      "url": "https://arxiv.org/abs/2602.12276",
      "pdfUrl": "https://arxiv.org/pdf/2602.12276.pdf",
      "titleJa": "WebAgents のエージェントテスト時間のスケーリング"
    },
    {
      "id": "2602.12270",
      "arxivId": "2602.12270",
      "title": "Creative Ownership in the Age of AI",
      "authors": [
        "Annie Liang",
        "Jay Lu"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "econ.TH",
        "cs.AI",
        "cs.GT"
      ],
      "abstract": "Copyright law focuses on whether a new work is \"substantially similar\" to an existing one, but generative AI can closely imitate style without copying content, a capability now central to ongoing litigation. We argue that existing definitions of infringement are ill-suited to this setting and propose a new criterion: a generative AI output infringes on an existing work if it could not have been generated without that work in its training corpus. To operationalize this definition, we model generative systems as closure operators mapping a corpus of existing works to an output of new works. AI generated outputs are \\emph{permissible} if they do not infringe on any existing work according to our criterion. Our results characterize structural properties of permissible generation and reveal a sharp asymptotic dichotomy: when the process of organic creations is light-tailed, dependence on individual works eventually vanishes, so that regulation imposes no limits on AI generation; with heavy-tailed creations, regulation can be persistently constraining.",
      "url": "https://arxiv.org/abs/2602.12270",
      "pdfUrl": "https://arxiv.org/pdf/2602.12270.pdf",
      "titleJa": "AI時代のクリエイティブ・オーナーシップ"
    },
    {
      "id": "2602.12268",
      "arxivId": "2602.12268",
      "title": "CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use",
      "authors": [
        "Zhen Zhang",
        "Kaiqiang Song",
        "Xun Wang",
        "Yebowen Hu",
        "Weixiang Yan",
        "Chenyang Zhao",
        "Henry Peng Zou",
        "Haoyun Deng",
        "Sathish Reddy Indurthi",
        "Shujian Liu",
        "Simin Ma",
        "Xiaoyang Wang",
        "Xin Eric Wang",
        "Song Wang"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.AI"
      ],
      "abstract": "AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn's intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.",
      "url": "https://arxiv.org/abs/2602.12268",
      "pdfUrl": "https://arxiv.org/pdf/2602.12268.pdf",
      "titleJa": "CM2: マルチターンおよびマルチステップのエージェントツール使用のためのチェックリスト報酬による強化学習"
    },
    {
      "id": "2602.12259",
      "arxivId": "2602.12259",
      "title": "Think like a Scientist: Physics-guided LLM Agent for Equation Discovery",
      "authors": [
        "Jianke Yang",
        "Ohm Venkatachalam",
        "Mohammad Kianezhad",
        "Sharvaree Vadgama",
        "Rose Yu"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.",
      "url": "https://arxiv.org/abs/2602.12259",
      "pdfUrl": "https://arxiv.org/pdf/2602.12259.pdf",
      "titleJa": "科学者のように考える：物理学に基づいた方程式発見のためのLLMエージェント"
    },
    {
      "id": "2602.12257",
      "arxivId": "2602.12257",
      "title": "On the implicit regularization of Langevin dynamics with projected noise",
      "authors": [
        "Govind Menon",
        "Austin J. Stromme",
        "Adrien Vacher"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "math.PR",
        "cs.AI"
      ],
      "abstract": "We study Langevin dynamics with noise projected onto the directions orthogonal to an isometric group action. This mathematical model is introduced to shed new light on the effects of symmetry on stochastic gradient descent for over-parametrized models. Our main result identifies a novel form of implicit regularization: when the initial and target density are both invariant under the group action, Langevin dynamics with projected noise is equivalent in law to Langevin dynamics with isotropic diffusion but with an additional drift term proportional to the negative log volume of the group orbit. We prove this result by constructing a coupling of the two processes via a third process on the group itself, and identify the additional drift as the mean curvature of the orbits.",
      "url": "https://arxiv.org/abs/2602.12257",
      "pdfUrl": "https://arxiv.org/pdf/2602.12257.pdf",
      "titleJa": "投影ノイズを含むランジュバン力学の暗黙的正則化について"
    },
    {
      "id": "2602.12251",
      "arxivId": "2602.12251",
      "title": "A technical curriculum on language-oriented artificial intelligence in translation and specialised communication",
      "authors": [
        "Ralph Krüger"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "abstract": "This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.",
      "url": "https://arxiv.org/abs/2602.12251",
      "pdfUrl": "https://arxiv.org/pdf/2602.12251.pdf",
      "titleJa": "翻訳と専門コミュニケーションにおける言語指向人工知能に関する技術カリキュラム"
    },
    {
      "id": "2602.12249",
      "arxivId": "2602.12249",
      "title": "\"Sorry, I Didn't Catch That\": How Speech Models Miss What Matters Most",
      "authors": [
        "Kaitlyn Zhou",
        "Martijn Bartelds",
        "Federico Bianchi",
        "James Zou"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "abstract": "Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.",
      "url": "https://arxiv.org/abs/2602.12249",
      "pdfUrl": "https://arxiv.org/pdf/2602.12249.pdf",
      "titleJa": "「すみません、聞き取れませんでした」：音声モデルはいかにして最も重要なことを見逃すのか"
    },
    {
      "id": "2602.12247",
      "arxivId": "2602.12247",
      "title": "ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction",
      "authors": [
        "Nick Ferguson",
        "Josh Pennington",
        "Narek Beghian",
        "Aravind Mohan",
        "Douwe Kiela",
        "Sheshansh Agrawal",
        "Thien Hang Nguyen"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.",
      "url": "https://arxiv.org/abs/2602.12247",
      "pdfUrl": "https://arxiv.org/pdf/2602.12247.pdf",
      "titleJa": "ExtractBench: 複雑な構造抽出のためのベンチマークと評価方法"
    },
    {
      "id": "2602.12245",
      "arxivId": "2602.12245",
      "title": "Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces",
      "authors": [
        "Anthony Kobanda",
        "Waris Radji"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.",
      "url": "https://arxiv.org/abs/2602.12245",
      "pdfUrl": "https://arxiv.org/pdf/2602.12245.pdf",
      "titleJa": "内在エネルギー共同埋め込み予測アーキテクチャは準距離空間を誘導する"
    },
    {
      "id": "2602.12237",
      "arxivId": "2602.12237",
      "title": "Olmix: A Framework for Data Mixing Throughout LM Development",
      "authors": [
        "Mayee F. Chen",
        "Tyler Murray",
        "David Heineman",
        "Matt Jordan",
        "Hannaneh Hajishirzi",
        "Christopher Ré",
        "Luca Soldaini",
        "Kyle Lo"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.",
      "url": "https://arxiv.org/abs/2602.12237",
      "pdfUrl": "https://arxiv.org/pdf/2602.12237.pdf",
      "titleJa": "Olmix: LM 開発全体にわたるデータ混合のためのフレームワーク"
    },
    {
      "id": "2602.12236",
      "arxivId": "2602.12236",
      "title": "Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision",
      "authors": [
        "Anika Tabassum Meem",
        "Muntasir Hossain Nadid",
        "Md Zesun Ahmed Mia"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV"
      ],
      "abstract": "Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47\\%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.",
      "url": "https://arxiv.org/abs/2602.12236",
      "pdfUrl": "https://arxiv.org/pdf/2602.12236.pdf",
      "titleJa": "ニューロモルフィックビジョンのためのスパイキングニューラルネットワークにおける継続学習のためのエネルギーを考慮したスパイクバジェッティング"
    },
    {
      "id": "2602.12224",
      "arxivId": "2602.12224",
      "title": "Bandit Learning in Matching Markets with Interviews",
      "authors": [
        "Amirmahdi Mirfakhar",
        "Xuchuang Wang",
        "Mengfan Xu",
        "Hedyeh Beyhaghi",
        "Mohammad Hajiesmaili"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.GT",
        "cs.AI",
        "econ.TH"
      ],
      "abstract": "Two-sided matching markets rely on preferences from both sides, yet it is often impractical to evaluate preferences. Participants, therefore, conduct a limited number of interviews, which provide early, noisy impressions and shape final decisions. We study bandit learning in matching markets with interviews, modeling interviews as \\textit{low-cost hints} that reveal partial preference information to both sides. Our framework departs from existing work by allowing firm-side uncertainty: firms, like agents, may be unsure of their own preferences and can make early hiring mistakes by hiring less preferred agents. To handle this, we extend the firm's action space to allow \\emph{strategic deferral} (choosing not to hire in a round), enabling recovery from suboptimal hires and supporting decentralized learning without coordination. We design novel algorithms for (i) a centralized setting with an omniscient interview allocator and (ii) decentralized settings with two types of firm-side feedback. Across all settings, our algorithms achieve time-independent regret, a substantial improvement over the $O(\\log T)$ regret bounds known for learning stable matchings without interviews. Also, under mild structured markets, decentralized performance matches the centralized counterpart up to polynomial factors in the number of agents and firms.",
      "url": "https://arxiv.org/abs/2602.12224",
      "pdfUrl": "https://arxiv.org/pdf/2602.12224.pdf",
      "titleJa": "市場とインタビューのマッチングにおけるバンディット学習"
    },
    {
      "id": "2602.12222",
      "arxivId": "2602.12222",
      "title": "Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training",
      "authors": [
        "Miaosen Zhang",
        "Yishan Liu",
        "Shuxia Lin",
        "Xu Yang",
        "Qi Dai",
        "Chong Luo",
        "Weihao Jiang",
        "Peng Hou",
        "Anxiang Zeng",
        "Xin Geng",
        "Baining Guo"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "abstract": "Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \\textbf{\\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \\textbf{\\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \\textbf{\\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT",
      "url": "https://arxiv.org/abs/2602.12222",
      "pdfUrl": "https://arxiv.org/pdf/2602.12222.pdf",
      "titleJa": "オンポリシーSFTに向けて：分布判別理論とLLMトレーニングへの応用"
    },
    {
      "id": "2602.12218",
      "arxivId": "2602.12218",
      "title": "The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics",
      "authors": [
        "Christian Internò",
        "Jumpei Yamaguchi",
        "Loren Amdahl-Culleton",
        "Markus Olhofer",
        "David Klindt",
        "Barbara Hammer"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $ρ> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($ρ\\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.",
      "url": "https://arxiv.org/abs/2602.12218",
      "pdfUrl": "https://arxiv.org/pdf/2602.12218.pdf",
      "titleJa": "世界モデルにおける観察者効果：侵襲的適応は潜在的な物理特性を歪める"
    },
    {
      "id": "2602.12207",
      "arxivId": "2602.12207",
      "title": "VIRENA: Virtual Arena for Research, Education, and Democratic Innovation",
      "authors": [
        "Emma Hoes",
        "K. Jonathan Klueser",
        "Fabrizio Gilardi"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SI"
      ],
      "abstract": "Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA's no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.",
      "url": "https://arxiv.org/abs/2602.12207",
      "pdfUrl": "https://arxiv.org/pdf/2602.12207.pdf",
      "titleJa": "VIRENA: 研究、教育、民主的イノベーションのための仮想アリーナ"
    },
    {
      "id": "2602.12205",
      "arxivId": "2602.12205",
      "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
      "authors": [
        "Dianyi Wang",
        "Ruihang Li",
        "Feng Han",
        "Chaofan Ma",
        "Wei Song",
        "Siyuan Wang",
        "Yibin Wang",
        "Yi Xin",
        "Hongjian Liu",
        "Zhixiong Zhang",
        "Shengyuan Ding",
        "Tianhang Wang",
        "Zhenglin Cheng",
        "Tao Lin",
        "Cheng Jin",
        "Kaicheng Yu",
        "Jingjing Chen",
        "Wenjie Wang",
        "Zhongyu Wei",
        "Jiaqi Wang"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.",
      "url": "https://arxiv.org/abs/2602.12205",
      "pdfUrl": "https://arxiv.org/pdf/2602.12205.pdf",
      "titleJa": "DeepGen 1.0: 画像生成と編集を進化させる軽量統合マルチモーダルモデル"
    },
    {
      "id": "2602.12196",
      "arxivId": "2602.12196",
      "title": "Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education",
      "authors": [
        "Mohamed Huti",
        "Alasdair Mackintosh",
        "Amy Waldock",
        "Dominic Andrews",
        "Maxime Lelièvre",
        "Moritz Boos",
        "Tobias Murray",
        "Paul Atherton",
        "Robin A. A. Ince",
        "Oliver G. B. Garrod"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.",
      "url": "https://arxiv.org/abs/2602.12196",
      "pdfUrl": "https://arxiv.org/pdf/2602.12196.pdf",
      "titleJa": "視覚的推論ベンチマーク：初等教育における教室で実際に行われる視覚的問題を用いたマルチモーダルLLMの評価"
    },
    {
      "id": "2602.08794",
      "arxivId": "2602.08794",
      "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
      "authors": [
        "SII-OpenMOSS Team",
        " :",
        "Donghua Yu",
        "Mingshu Chen",
        "Qi Chen",
        "Qi Luo",
        "Qianyi Wu",
        "Qinyuan Cheng",
        "Ruixiao Li",
        "Tianyi Liang",
        "Wenbo Zhang",
        "Wenming Tu",
        "Xiangyu Peng",
        "Yang Gao",
        "Yanru Huo",
        "Ying Zhu",
        "Yinze Luo",
        "Yiyang Zhang",
        "Yuerong Song",
        "Zhe Xu",
        "Zhiyu Zhang",
        "Chenchen Yang",
        "Cheng Chang",
        "Chushu Zhou",
        "Hanfu Chen",
        "Hongnan Ma",
        "Jiaxi Li",
        "Jingqi Tong",
        "Junxi Liu",
        "Ke Chen",
        "Shimin Li",
        "Shiqi Jiang",
        "Songlin Wang",
        "Wei Jiang",
        "Zhaoye Fei",
        "Zhiyuan Ning",
        "Chunguo Li",
        "Chenhui Li",
        "Ziwei He",
        "Zengfeng Huang",
        "Xie Chen",
        "Xipeng Qiu"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.CV",
        "cs.SD"
      ],
      "abstract": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
      "url": "https://arxiv.org/abs/2602.08794",
      "pdfUrl": "https://arxiv.org/pdf/2602.08794.pdf",
      "titleJa": "MOVA: スケーラブルで同期したビデオ・オーディオ生成に向けて"
    },
    {
      "id": "2602.08671",
      "arxivId": "2602.08671",
      "title": "Input-Adaptive Spectral Feature Compression by Sequence Modeling for Source Separation",
      "authors": [
        "Kohei Saijo",
        "Yoshiaki Bando"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Time-frequency domain dual-path models have demonstrated strong performance and are widely used in source separation. Because their computational cost grows with the number of frequency bins, these models often use the band-split (BS) module in high-sampling-rate tasks such as music source separation (MSS) and cinematic audio source separation (CASS). The BS encoder compresses frequency information by encoding features for each predefined subband. It achieves effective compression by introducing an inductive bias that places greater emphasis on low-frequency parts. Despite its success, the BS module has two inherent limitations: (i) it is not input-adaptive, preventing the use of input-dependent information, and (ii) the parameter count is large, since each subband requires a dedicated module. To address these issues, we propose Spectral Feature Compression (SFC). SFC compresses the input using a single sequence modeling module, making it both input-adaptive and parameter-efficient. We investigate two variants of SFC, one based on cross-attention and the other on Mamba, and introduce inductive biases inspired by the BS module to make them suitable for frequency information compression. Experiments on MSS and CASS tasks demonstrate that the SFC module consistently outperforms the BS module across different separator sizes and compression ratios. We also provide an analysis showing that SFC adaptively captures frequency patterns from the input.",
      "url": "https://arxiv.org/abs/2602.08671",
      "pdfUrl": "https://arxiv.org/pdf/2602.08671.pdf",
      "titleJa": "音源分離のためのシーケンスモデリングによる入力適応型スペクトル特徴圧縮"
    },
    {
      "id": "2602.09070",
      "arxivId": "2602.09070",
      "title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
      "authors": [
        "Yufan Wen",
        "Zhaocheng Liu",
        "YeGuo Hua",
        "Ziyi Guo",
        "Lihua Zhang",
        "Chun Yuan",
        "Jian Wu"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a \\textit{Global Semantic Anchor} ensures stylistic stability, while a surgical \\textit{Token-Level Affective Adapter} modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation.",
      "url": "https://arxiv.org/abs/2602.09070",
      "pdfUrl": "https://arxiv.org/pdf/2602.09070.pdf",
      "titleJa": "NarraScore: 階層的感情制御による視覚的物語と音楽的ダイナミクスの橋渡し"
    },
    {
      "id": "2602.08233",
      "arxivId": "2602.08233",
      "title": "Tutti: Expressive Multi-Singer Synthesis via Structure-Level Timbre Control and Vocal Texture Modeling",
      "authors": [
        "Jiatao Chen",
        "Xing Tang",
        "Xiaoyue Duan",
        "Yutang Feng",
        "Jinchao Zhang",
        "Jie Zhou"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "While existing Singing Voice Synthesis systems achieve high-fidelity solo performances, they are constrained by global timbre control, failing to address dynamic multi-singer arrangement and vocal texture within a single song. To address this, we propose Tutti, a unified framework designed for structured multi-singer generation. Specifically, we introduce a Structure-Aware Singer Prompt to enable flexible singer scheduling evolving with musical structure, and propose Complementary Texture Learning via Condition-Guided VAE to capture implicit acoustic textures (e.g., spatial reverberation and spectral fusion) that are complementary to explicit controls. Experiments demonstrate that Tutti excels in precise multi-singer scheduling and significantly enhances the acoustic realism of choral generation, offering a novel paradigm for complex multi-singer arrangement. Audio samples are available at https://annoauth123-ctrl.github.io/Tutii_Demo/.",
      "url": "https://arxiv.org/abs/2602.08233",
      "pdfUrl": "https://arxiv.org/pdf/2602.08233.pdf",
      "titleJa": "Tutti: 構造レベルの音色制御とボーカルテクスチャモデリングによる表現力豊かなマルチシンガー合成"
    },
    {
      "id": "2602.08148",
      "arxivId": "2602.08148",
      "title": "SNC: A Stem-Native Codec for Efficient Lossless Audio Storage with Adaptive Playback Capabilities",
      "authors": [
        "Shaad Sufi"
      ],
      "publishedDate": "2026-02-08",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Current audio formats present a fundamental trade-off between file size and functionality: lossless formats like FLAC preserve quality but lack adaptability, while lossy formats reduce size at the cost of fidelity and offer no stem-level access.We introduce the Stem-Native Codec (SNC), a novel audio container format that stores music as independently encoded stems plus a low-energy mastering residual. By exploiting the lower information entropy of separated stems compared to mixed audio, SNC achieves a 38.2% file size reduction versus FLAC (7.76 MB vs. 12.55 MB for a 2:18 test track) while maintaining perceptual transparency (STOI = 0.996). Unlike existing formats, SNC enables context-aware adaptive playback, spatial audio rendering, and user-controlled remixing without requiring additional storage. Our experimental validation demonstrates that the stems-plus residual architecture successfully decouples the conflicting requirements of compression efficiency and feature richness, offering a practical path toward next-generation audio distribution systems.",
      "url": "https://arxiv.org/abs/2602.08148",
      "pdfUrl": "https://arxiv.org/pdf/2602.08148.pdf",
      "titleJa": "SNC: 適応型再生機能を備えた効率的なロスレスオーディオストレージのためのステムネイティブコーデック"
    },
    {
      "id": "2602.07803",
      "arxivId": "2602.07803",
      "title": "SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis",
      "authors": [
        "Jiale Qian",
        "Hao Meng",
        "Tian Zheng",
        "Pengcheng Zhu",
        "Haopeng Lin",
        "Yuhang Dai",
        "Hanke Xie",
        "Wenxiao Cao",
        "Ruixuan Shang",
        "Jun Wu",
        "Hongmei Liu",
        "Hanlin Wen",
        "Jian Zhao",
        "Zhonglin Jiang",
        "Yong Chen",
        "Shunshun Yin",
        "Ming Tao",
        "Jianguo Wei",
        "Lei Xie",
        "Xinsheng Wang"
      ],
      "publishedDate": "2026-02-08",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.",
      "url": "https://arxiv.org/abs/2602.07803",
      "pdfUrl": "https://arxiv.org/pdf/2602.07803.pdf",
      "titleJa": "SoulX-Singer: 高品質なゼロショット歌声合成に向けて"
    },
    {
      "id": "2602.06917",
      "arxivId": "2602.06917",
      "title": "Automatic Detection and Analysis of Singing Mistakes for Music Pedagogy",
      "authors": [
        "Sumit Kumar",
        "Suraj Jaiswal",
        "Parampreet Singh",
        "Vipul Arora"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "The advancement of machine learning in audio analysis has opened new possibilities for technology-enhanced music education. This paper introduces a framework for automatic singing mistake detection in the context of music pedagogy, supported by a newly curated dataset. The dataset comprises synchronized teacher learner vocal recordings, with annotations marking different types of mistakes made by learners. Using this dataset, we develop different deep learning models for mistake detection and benchmark them. To compare the efficacy of mistake detection systems, a new evaluation methodology is proposed. Experiments indicate that the proposed learning-based methods are superior to rule-based methods. A systematic study of errors and a cross-teacher study reveal insights into music pedagogy that can be utilised for various music applications. This work sets out new directions of research in music pedagogy. The codes and dataset are publicly available.",
      "url": "https://arxiv.org/abs/2602.06917",
      "pdfUrl": "https://arxiv.org/pdf/2602.06917.pdf",
      "titleJa": "音楽教育のための歌唱ミスの自動検出と分析"
    },
    {
      "id": "2602.06823",
      "arxivId": "2602.06823",
      "title": "AI-Generated Music Detection in Broadcast Monitoring",
      "authors": [
        "David Lopez-Ayala",
        "Asier Cabello",
        "Pablo Zinemanas",
        "Emilio Molina",
        "Martin Rocamora"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "AI music generators have advanced to the point where their outputs are often indistinguishable from human compositions. While detection methods have emerged, they are typically designed and validated in music streaming contexts with clean, full-length tracks. Broadcast audio, however, poses a different challenge: music appears as short excerpts, often masked by dominant speech, conditions under which existing detectors fail. In this work, we introduce AI-OpenBMAT, the first dataset tailored to broadcast-style AI-music detection. It contains 3,294 one-minute audio excerpts (54.9 hours) that follow the duration patterns and loudness relations of real television audio, combining human-made production music with stylistically matched continuations generated with Suno v3.5. We benchmark a CNN baseline and state-of-the-art SpectTTTra models to assess SNR and duration robustness, and evaluate on a full broadcast scenario. Across all settings, models that excel in streaming scenarios suffer substantial degradation, with F1-scores dropping below 60% when music is in the background or has a short duration. These results highlight speech masking and short music length as critical open challenges for AI music detection, and position AI-OpenBMAT as a benchmark for developing detectors capable of meeting industrial broadcast requirements.",
      "url": "https://arxiv.org/abs/2602.06823",
      "pdfUrl": "https://arxiv.org/pdf/2602.06823.pdf",
      "titleJa": "放送監視におけるAI生成音楽検出"
    },
    {
      "id": "2602.07063",
      "arxivId": "2602.07063",
      "title": "Video-based Music Generation",
      "authors": [
        "Serkan Sulun"
      ],
      "publishedDate": "2026-02-05",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called \"boundary offset encodings,\" aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.",
      "url": "https://arxiv.org/abs/2602.07063",
      "pdfUrl": "https://arxiv.org/pdf/2602.07063.pdf",
      "titleJa": "ビデオベースの音楽生成"
    },
    {
      "id": "2602.05220",
      "arxivId": "2602.05220",
      "title": "Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions",
      "authors": [
        "Jinchuan Tian",
        "Haoran Wang",
        "Bo-Hao Su",
        "Chien-yu Huang",
        "Qingzheng Wang",
        "Jiatong Shi",
        "William Chen",
        "Xun Gong",
        "Siddhant Arora",
        "Chin-Jou Li",
        "Masao Someki",
        "Takashi Maekaku",
        "Yusuke Shinohara",
        "Jin Sakuma",
        "Chao-Han Huck Yang",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-02-05",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.",
      "url": "https://arxiv.org/abs/2602.05220",
      "pdfUrl": "https://arxiv.org/pdf/2602.05220.pdf",
      "titleJa": "Bagpiper: 豊富なキャプションでオープンエンドの音声タスクを解決する"
    },
    {
      "id": "2602.04683",
      "arxivId": "2602.04683",
      "title": "UniAudio 2.0: A Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization",
      "authors": [
        "Dongchao Yang",
        "Yuanyuan Wang",
        "Dading Chong",
        "Songxiang Liu",
        "Xixin Wu",
        "Helen Meng"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.SD"
      ],
      "abstract": "We study two foundational problems in audio language models: (1) how to design an audio tokenizer that can serve as an intermediate representation for both understanding and generation; and (2) how to build an audio foundation model that generalizes in few-shot and zero-shot settings, analogous to large language models. To this end, we make the following two contributions. First, we propose ReasoningCodec, a discrete audio codec that factorizes audio into (i) reasoning tokens, which encode text-aligned, high-level analysis and planning representations for audio understanding and hierarchical generation, and (ii) reconstruction tokens, which encode semantic-rich acoustic cues for high-fidelity waveform reconstruction. This design achieves understanding performance comparable to strong continuous representations while improving generation quality and reconstruction fidelity over prior discrete tokenizers. Second, we introduce a unified autoregressive architecture for text and audio, together with multi-stage training and multi-task data construction. Using this framework, we train UniAudio 2.0 on 100B text tokens and 60B audio tokens. Across a wide range of speech, sound, and music tasks, UniAudio 2.0 performs competitively on in-domain evaluations and demonstrates strong few-shot and zero-shot generalization to unseen tasks. Demo, code, and checkpoints will be available at \\href{https://dongchaoyang.top/UniAudio2Demo/}{https://dongchaoyang.top/UniAudio2Demo/}.",
      "url": "https://arxiv.org/abs/2602.04683",
      "pdfUrl": "https://arxiv.org/pdf/2602.04683.pdf",
      "titleJa": "UniAudio 2.0: テキスト整合されたファクタライズされたオーディオトークン化を備えた統合オーディオ言語モデル"
    },
    {
      "id": "2602.09042",
      "arxivId": "2602.09042",
      "title": "The SJTU X-LANCE Lab System for MSR Challenge 2025",
      "authors": [
        "Jinxuan Zhu",
        "Hao Qiu",
        "Haina Zhu",
        "Jianwei Yu",
        "Kai Yu",
        "Xie Chen"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This report describes the system submitted to the music source restoration (MSR) Challenge 2025. Our approach is composed of sequential BS-RoFormers, each dealing with a single task including music source separation (MSS), denoise and dereverb. To support 8 instruments given in the task, we utilize pretrained checkpoints from MSS community and finetune the MSS model with several training schemes, including (1) mixing and cleaning of datasets; (2) random mixture of music pieces for data augmentation; (3) scale-up of audio length. Our system achieved the first rank in all three subjective and three objective evaluation metrics, including an MMSNR score of 4.4623 and an FAD score of 0.1988. We have open-sourced all the code and checkpoints at https://github.com/ModistAndrew/xlance-msr.",
      "url": "https://arxiv.org/abs/2602.09042",
      "pdfUrl": "https://arxiv.org/pdf/2602.09042.pdf",
      "titleJa": "MSRチャレンジ2025向けSJTU X-LANCEラボシステム"
    },
    {
      "id": "2602.04085",
      "arxivId": "2602.04085",
      "title": "BASS: Benchmarking Audio LMs for Musical Structure and Semantic Reasoning",
      "authors": [
        "Min Jang",
        "Orevaoghene Ahia",
        "Nazif Tamer",
        "Sachin Kumar",
        "Yulia Tsvetkov",
        "Noah A. Smith"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Music understanding is a complex task that often requires reasoning over both structural and semantic elements of audio. We introduce BASS, designed to evaluate music understanding and reasoning in audio language models across four broad categories: structural segmentation, lyric transcription, musicological analysis, and artist collaboration. BASS comprises 2658 questions spanning 12 tasks, 1993 unique songs and covering over 138 hours of music from a wide range of genres and tracks, crafted to assess musicological knowledge and reasoning in real-world scenarios. We evaluate 14 open-source and frontier multimodal LMs, finding that even state-of-the-art models struggle on higher-level reasoning tasks such as structural segmentation and artist collaboration, while performing best on lyric transcription. Our analysis reveals that current models leverage linguistic priors effectively but remain limited in reasoning over musical structure, vocal, and musicological attributes. BASS provides an evaluation framework with widespread applications in music recommendation and search and has the potential to guide the development of audio LMs.",
      "url": "https://arxiv.org/abs/2602.04085",
      "pdfUrl": "https://arxiv.org/pdf/2602.04085.pdf",
      "titleJa": "BASS: 音楽構造と意味論的推論のためのオーディオ LM のベンチマーク"
    },
    {
      "id": "2602.03549",
      "arxivId": "2602.03549",
      "title": "EarResp-ANS : Audio-Based On-Device Respiration Rate Estimation on Earphones with Adaptive Noise Suppression",
      "authors": [
        "Michael Küttner",
        "Valeria Zitz",
        "Supraja Ramesh",
        "Michael Beigl",
        "Tobias Röddiger"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.HC"
      ],
      "abstract": "Respiratory rate (RR) is a key vital sign for clinical assessment and mental well-being, yet it is rarely monitored in everyday life due to the lack of unobtrusive sensing technologies. In-ear audio sensing is promising due to its high social acceptance and the amplification of physiological sounds caused by the occlusion effect; however, existing approaches often fail under real-world noise or rely on computationally expensive models. We present EarResp-ANS, the first system enabling fully on-device, real-time RR estimation on commercial earphones. The system employs LMS-based adaptive noise suppression (ANS) to attenuate ambient noise while preserving respiration-related acoustic components, without requiring neural networks or audio streaming, thereby explicitly addressing the energy and privacy constraints of wearable devices. We evaluate EarResp-ANS in a study with 18 participants under realistic acoustic conditions, including music, cafeteria noise, and white noise up to 80 dB SPL. EarResp-ANS achieves robust performance with a global MAE of 0.84 CPM , reduced to 0.47 CPM via automatic outlier rejection, while operating with less than 2% processor load directly on the earphone.",
      "url": "https://arxiv.org/abs/2602.03549",
      "pdfUrl": "https://arxiv.org/pdf/2602.03549.pdf",
      "titleJa": "EarResp-ANS：適応型ノイズ抑制機能を備えたイヤホンにおける音声ベースのデバイス内呼吸数推定"
    },
    {
      "id": "2602.09321",
      "arxivId": "2602.09321",
      "title": "Performance Comparison of CNN and AST Models with Stacked Features for Environmental Sound Classification",
      "authors": [
        "Parinaz Binandeh Dehaghania",
        "Danilo Penab",
        "A. Pedro Aguiar"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Environmental sound classification (ESC) has gained significant attention due to its diverse applications in smart city monitoring, fault detection, acoustic surveillance, and manufacturing quality control. To enhance CNN performance, feature stacking techniques have been explored to aggregate complementary acoustic descriptors into richer input representations. In this paper, we investigate CNN-based models employing various stacked feature combinations, including Log-Mel Spectrogram (LM), Spectral Contrast (SPC), Chroma (CH), Tonnetz (TZ), Mel-Frequency Cepstral Coefficients (MFCCs), and Gammatone Cepstral Coefficients (GTCC). Experiments are conducted on the widely used ESC-50 and UrbanSound8K datasets under different training regimes, including pretraining on ESC-50, fine-tuning on UrbanSound8K, and comparison with Audio Spectrogram Transformer (AST) models pretrained on large-scale corpora such as AudioSet. This experimental design enables an analysis of how feature-stacked CNNs compare with transformer-based models under varying levels of training data and pretraining diversity. The results indicate that feature-stacked CNNs offer a more computationally and data-efficient alternative when large-scale pretraining or extensive training data are unavailable, making them particularly well suited for resource-constrained and edge-level sound classification scenarios.",
      "url": "https://arxiv.org/abs/2602.09321",
      "pdfUrl": "https://arxiv.org/pdf/2602.09321.pdf",
      "titleJa": "環境音分類におけるスタック特徴量を用いたCNNとASTモデルの性能比較"
    },
    {
      "id": "2602.09295",
      "arxivId": "2602.09295",
      "title": "Positive-Unlabelled Active Learning to Curate a Dataset for Orca Resident Interpretation",
      "authors": [
        "Bret Nestor",
        "Bohan Yao",
        "Jasmine Moore",
        "Jasper Kanes"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "This work presents the largest curation of Southern Resident Killer Whale (SRKW) acoustic data to date, also containing other marine mammals in their environment. We systematically search all available public archival hydrophone data within the SRKW habitat (over 30 years of audio data). The search consists of a weakly-supervised, positive-unlabelled, active learning strategy to identify all instances of marine mammals. The resulting transformer-based detectors outperform state-of-the-art detectors on the DEEPAL, DCLDE-2026, and two newly introduced expert-annotated datasets in terms of accuracy, energy efficiency, and speed. The detection model has a specificity of 0-28.8% at 95% sensitivity. Our multiclass species classifier obtains a top-1 accuracy of 42.1% (11 train classes, 4 test classes) and our ecotype classifier obtains a top-1 accuracy of 43.0% (4 train classes, 5 test classes) on the DCLDE-2026 dataset. We yield 919 hours of SRKW data, 230 hours of Bigg's orca data, 1374 hours of orca data from unlabelled ecotypes, 1501 hours of humpback data, 88 hours of sea lion data, 246 hours of pacific white-sided dolphin data, and over 784 hours of unspecified marine mammal data. This SRKW dataset is larger than DCLDE-2026, Ocean Networks Canada, and OrcaSound combined. The curated species labels are available under CC-BY 4.0 license, and the corresponding audio data are available under the licenses of the original owners. The comprehensive nature of this dataset makes it suitable for unsupervised machine translation, habitat usage surveys, and conservation endeavours for this critically endangered ecotype.",
      "url": "https://arxiv.org/abs/2602.09295",
      "pdfUrl": "https://arxiv.org/pdf/2602.09295.pdf",
      "titleJa": "シャチの生息環境解釈のためのデータセットをキュレーションするためのポジティブラベルなしアクティブラーニング"
    },
    {
      "id": "2602.09233",
      "arxivId": "2602.09233",
      "title": "Gencho: Room Impulse Response Generation from Reverberant Speech and Text via Diffusion Transformers",
      "authors": [
        "Jackie Lin",
        "Jiaqi Su",
        "Nishit Anand",
        "Zeyu Jin",
        "Minje Kim",
        "Paris Smaragdis"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Blind room impulse response (RIR) estimation is a core task for capturing and transferring acoustic properties; yet existing methods often suffer from limited modeling capability and degraded performance under unseen conditions. Moreover, emerging generative audio applications call for more flexible impulse response generation methods. We propose Gencho, a diffusion-transformer-based model that predicts complex spectrogram RIRs from reverberant speech. A structure-aware encoder leverages isolation between early and late reflections to encode the input audio into a robust representation for conditioning, while the diffusion decoder generates diverse and perceptually realistic impulse responses from it. Gencho integrates modularly with standard speech processing pipelines for acoustic matching. Results show richer generated RIRs than non-generative baselines while maintaining strong performance in standard RIR metrics. We further demonstrate its application to text-conditioned RIR generation, highlighting Gencho's versatility for controllable acoustic simulation and generative audio tasks.",
      "url": "https://arxiv.org/abs/2602.09233",
      "pdfUrl": "https://arxiv.org/pdf/2602.09233.pdf",
      "titleJa": "源長：拡散トランスフォーマーによる残響音声とテキストからの室内インパルス応答生成"
    },
    {
      "id": "2602.09210",
      "arxivId": "2602.09210",
      "title": "AI-Driven Cardiorespiratory Signal Processing: Separation, Clustering, and Anomaly Detection",
      "authors": [
        "Yasaman Torabi"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This research applies artificial intelligence (AI) to separate, cluster, and analyze cardiorespiratory sounds. We recorded a new dataset (HLS-CMDS) and developed several AI models, including generative AI methods based on large language models (LLMs) for guided separation, explainable AI (XAI) techniques to interpret latent representations, variational autoencoders (VAEs) for waveform separation, a chemistry-inspired non-negative matrix factorization (NMF) algorithm for clustering, and a quantum convolutional neural network (QCNN) designed to detect abnormal physiological patterns. The performance of these AI models depends on the quality of the recorded signals. Therefore, this thesis also reviews the biosensing technologies used to capture biomedical data. It summarizes developments in microelectromechanical systems (MEMS) acoustic sensors and quantum biosensors, such as quantum dots and nitrogen-vacancy centers. It further outlines the transition from electronic integrated circuits (EICs) to photonic integrated circuits (PICs) and early progress toward integrated quantum photonics (IQP) for chip-based biosensing. Together, these studies show how AI and next-generation sensors can support more intelligent diagnostic systems for future healthcare.",
      "url": "https://arxiv.org/abs/2602.09210",
      "pdfUrl": "https://arxiv.org/pdf/2602.09210.pdf",
      "titleJa": "AI駆動型心肺信号処理：分離、クラスタリング、異常検出"
    },
    {
      "id": "2602.08979",
      "arxivId": "2602.08979",
      "title": "Beyond Transcripts: A Renewed Perspective on Audio Chaptering",
      "authors": [
        "Fabian Retkowski",
        "Maike Züfle",
        "Thai Binh Nguyen",
        "Jan Niehues",
        "Alexander Waibel"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Audio chaptering, the task of automatically segmenting long-form audio into coherent sections, is increasingly important for navigating podcasts, lectures, and videos. Despite its relevance, research remains limited and text-based, leaving key questions unresolved about leveraging audio information, handling ASR errors, and transcript-free evaluation. We address these gaps through three contributions: (1) a systematic comparison between text-based models with acoustic features, a novel audio-only architecture (AudioSeg) operating on learned audio representations, and multimodal LLMs; (2) empirical analysis of factors affecting performance, including transcript quality, acoustic features, duration, and speaker composition; and (3) formalized evaluation protocols contrasting transcript-dependent text-space protocols with transcript-invariant time-space protocols. Our experiments on YTSeg reveal that AudioSeg substantially outperforms text-based approaches, pauses provide the largest acoustic gains, and MLLMs remain limited by context length and weak instruction following, yet MLLMs are promising on shorter audio.",
      "url": "https://arxiv.org/abs/2602.08979",
      "pdfUrl": "https://arxiv.org/pdf/2602.08979.pdf",
      "titleJa": "トランスクリプトを超えて：オーディオチャプターの新たな視点"
    },
    {
      "id": "2602.08293",
      "arxivId": "2602.08293",
      "title": "Cross-Modal Bottleneck Fusion For Noise Robust Audio-Visual Speech Recognition",
      "authors": [
        "Seaone Ok",
        "Min Jun Choi",
        "Eungbeom Kim",
        "Seungu Han",
        "Kyogu Lee"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Audio-Visual Speech Recognition (AVSR) leverages both acoustic and visual cues to improve speech recognition under noisy conditions. A central question is how to design a fusion mechanism that allows the model to effectively exploit visual information when the audio signal is degraded, while maintaining strong performance on clean speech. We propose CoBRA (Cross-modal Bottleneck for Robust AVSR), a bottleneck-based fusion framework that introduces a compact set of learnable tokens to mediate cross-modal exchange. By regulating information flow through these tokens, the audio stream can reliably access essential visual cues even under adverse or out-of-domain noise. Despite limited training data, our model surpasses comparable baselines and remains competitive with large-scale systems through noise-adaptive fusion, demonstrating both efficiency and robustness. Ablation studies highlight that the depth of fusion is the most critical factor, underscoring its importance in designing robust AVSR systems.",
      "url": "https://arxiv.org/abs/2602.08293",
      "pdfUrl": "https://arxiv.org/pdf/2602.08293.pdf",
      "titleJa": "ノイズ耐性のあるオーディオビジュアル音声認識のためのクロスモーダルボトルネック融合"
    },
    {
      "id": "2602.06937",
      "arxivId": "2602.06937",
      "title": "Reciprocal Latent Fields for Precomputed Sound Propagation",
      "authors": [
        "Hugo Seuté",
        "Pranai Vasudev",
        "Etienne Richan",
        "Louis-Xavier Buffoni"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Realistic sound propagation is essential for immersion in a virtual scene, yet physically accurate wave-based simulations remain computationally prohibitive for real-time applications. Wave coding methods address this limitation by precomputing and compressing impulse responses of a given scene into a set of scalar acoustic parameters, which can reach unmanageable sizes in large environments with many source-receiver pairs. We introduce Reciprocal Latent Fields (RLF), a memory-efficient framework for encoding and predicting these acoustic parameters. The RLF framework employs a volumetric grid of trainable latent embeddings decoded with a symmetric function, ensuring acoustic reciprocity. We study a variety of decoders and show that leveraging Riemannian metric learning leads to a better reproduction of acoustic phenomena in complex scenes. Experimental validation demonstrates that RLF maintains replication quality while reducing the memory footprint by several orders of magnitude. Furthermore, a MUSHRA-like subjective listening test indicates that sound rendered via RLF is perceptually indistinguishable from ground-truth simulations.",
      "url": "https://arxiv.org/abs/2602.06937",
      "pdfUrl": "https://arxiv.org/pdf/2602.06937.pdf",
      "titleJa": "事前計算による音の伝播のための逆潜在場"
    },
    {
      "id": "2602.06921",
      "arxivId": "2602.06921",
      "title": "The Combination of Several Decorrelation Methods to Improve Acoustic Feedback Cancellation",
      "authors": [
        "Klaus Linhard",
        "Philipp Bulling"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This paper extends an acoustic feedback cancellation system by incorporating multiple decorrelation methods. The baseline system is based on a frequency-domain Kalman filter implemented in a multi-delay structure. The proposed extensions include a variable time delay line, prediction, distortion compensation, and a simplified reverberation model. Each extension is analyzed, and a practical parameter range is defined. While existing literature often focuses on a single extension, such as prediction, to describe an optimal system, this work demonstrates that each individual extension contributes to performance improvements. Furthermore, the combination of all proposed extensions results in a superior system. The evaluation is conducted using publicly available datasets, with performance assessed through system distance metrics and the objective speech quality measure PSEQ.",
      "url": "https://arxiv.org/abs/2602.06921",
      "pdfUrl": "https://arxiv.org/pdf/2602.06921.pdf",
      "titleJa": "音響フィードバックキャンセルを改善するための複数の相関除去法の組み合わせ"
    },
    {
      "id": "2602.06846",
      "arxivId": "2602.06846",
      "title": "DynFOA: Generating First-Order Ambisonics with Conditional Diffusion for Dynamic and Acoustically Complex 360-Degree Videos",
      "authors": [
        "Ziyu Luo",
        "Lin Chen",
        "Qiang Qu",
        "Xiaoming Chen",
        "Yiran Shen"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Spatial audio is crucial for creating compelling immersive 360-degree video experiences. However, generating realistic spatial audio, such as first-order ambisonics (FOA), from 360-degree videos in complex acoustic scenes remains challenging. Existing methods often overlook the dynamic nature and acoustic complexity of 360-degree scenes, fail to fully account for dynamic sound sources, and neglect complex environmental effects such as occlusion, reflections, and reverberation, which are influenced by scene geometries and materials. We propose DynFOA, a framework based on dynamic acoustic perception and conditional diffusion, for generating high-fidelity FOA from 360-degree videos. DynFOA first performs visual processing via a video encoder, which detects and localizes multiple dynamic sound sources, estimates their depth and semantics, and reconstructs the scene geometry and materials using a 3D Gaussian Splatting. This reconstruction technique accurately models occlusion, reflections, and reverberation based on the geometries and materials of the reconstructed 3D scene and the listener's viewpoint. The audio encoder then captures the spatial motion and temporal 4D sound source trajectories to fine-tune the diffusion-based FOA generator. The fine-tuned FOA generator adjusts spatial cues in real time, ensuring consistent directional fidelity during listener head rotation and complex environmental changes. Extensive evaluations demonstrate that DynFOA consistently outperforms existing methods across metrics such as spatial accuracy, acoustic fidelity, and distribution matching, while also improving the user experience. Therefore, DynFOA provides a robust and scalable approach to rendering realistic dynamic spatial audio for VR and immersive media applications.",
      "url": "https://arxiv.org/abs/2602.06846",
      "pdfUrl": "https://arxiv.org/pdf/2602.06846.pdf",
      "titleJa": "DynFOA: 条件付き拡散を用いた一次アンビソニックスの生成による、動的かつ音響的に複雑な360度動画の制作"
    },
    {
      "id": "2602.06647",
      "arxivId": "2602.06647",
      "title": "Reading Between the Waves: Robust Topic Segmentation Using Inter-Sentence Audio Features",
      "authors": [
        "Steffen Freisinger",
        "Philipp Seeberger",
        "Tobias Bocklet",
        "Korbinian Riedhammer"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Spoken content, such as online videos and podcasts, often spans multiple topics, which makes automatic topic segmentation essential for user navigation and downstream applications. However, current methods do not fully leverage acoustic features, leaving room for improvement. We propose a multi-modal approach that fine-tunes both a text encoder and a Siamese audio encoder, capturing acoustic cues around sentence boundaries. Experiments on a large-scale dataset of YouTube videos show substantial gains over text-only and multi-modal baselines. Our model also proves more resilient to ASR noise and outperforms a larger text-only baseline on three additional datasets in Portuguese, German, and English, underscoring the value of learned acoustic features for robust topic segmentation.",
      "url": "https://arxiv.org/abs/2602.06647",
      "pdfUrl": "https://arxiv.org/pdf/2602.06647.pdf",
      "titleJa": "波間を読む：文間音声特徴を用いた堅牢なトピックセグメンテーション"
    },
    {
      "id": "2602.06602",
      "arxivId": "2602.06602",
      "title": "Scaling Speech Tokenizers with Diffusion Autoencoders",
      "authors": [
        "Yuancheng Wang",
        "Zhenyu Tang",
        "Yun Wang",
        "Arthur Hinsvark",
        "Yingru Liu",
        "Yinghao Li",
        "Kainan Peng",
        "Junyi Ao",
        "Mingbo Ma",
        "Mike Seltzer",
        "Qing He",
        "Xubo Liu"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Speech tokenizers are foundational to speech language models, yet existing approaches face two major challenges: (1) balancing trade-offs between encoding semantics for understanding and acoustics for reconstruction, and (2) achieving low bit rates and low token rates. We propose Speech Diffusion Tokenizer (SiTok), a diffusion autoencoder that jointly learns semantic-rich representations through supervised learning and enables high-fidelity audio reconstruction with diffusion. We scale SiTok to 1.6B parameters and train it on 2 million hours of speech. Experiments show that SiTok outperforms strong baselines on understanding, reconstruction and generation tasks, at an extremely low token rate of $12.5$ Hz and a bit-rate of 200 bits-per-second.",
      "url": "https://arxiv.org/abs/2602.06602",
      "pdfUrl": "https://arxiv.org/pdf/2602.06602.pdf",
      "titleJa": "拡散オートエンコーダを用いた音声トークナイザーのスケーリング"
    },
    {
      "id": "2602.06180",
      "arxivId": "2602.06180",
      "title": "STACodec: Semantic Token Assignment for Balancing Acoustic Fidelity and Semantic Information in Audio Codecs",
      "authors": [
        "Kaiyuan Zhang",
        "Mohan Shi",
        "Eray Eren",
        "Natarajan Balaji Shankar",
        "Zilai Wang",
        "Abeer Alwan"
      ],
      "publishedDate": "2026-02-05",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "abstract": "Neural audio codecs are widely used for audio compression and can be integrated into token-based language models. Traditional codecs preserve acoustic details well but lack semantic information. Recent hybrid codecs attempt to incorporate semantic information through distillation, but this often degrades reconstruction performance, making it difficult to achieve both. To address this limitation, we introduce STACodec, a unified codec that integrates semantic information from self-supervised learning (SSL) models into the first layer of residual vector quantization (RVQ-1) via semantic token assignment (STA). To further eliminate reliance on SSL-based semantic tokenizers and improve efficiency during inference, we propose a semantic pre-distillation (SPD) module, which predicts semantic tokens directly for assignment to the first RVQ layer during inference. Experimental results show that STACodec outperforms existing hybrid codecs in both audio reconstruction and downstream semantic tasks, demonstrating a better balance between acoustic fidelity and semantic capability.",
      "url": "https://arxiv.org/abs/2602.06180",
      "pdfUrl": "https://arxiv.org/pdf/2602.06180.pdf",
      "titleJa": "STACodec: オーディオコーデックにおける音響忠実度と意味情報のバランスをとるための意味トークン割り当て"
    },
    {
      "id": "2602.05770",
      "arxivId": "2602.05770",
      "title": "Zero-Shot TTS With Enhanced Audio Prompts: Bsc Submission For The 2026 Wildspoof Challenge TTS Track",
      "authors": [
        "Jose Giraldo",
        "Alex Peiró-Lilja",
        "Rodolfo Zevallos",
        "Cristina España-Bonet"
      ],
      "publishedDate": "2026-02-05",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We evaluate two non-autoregressive architectures, StyleTTS2 and F5-TTS, to address the spontaneous nature of in-the-wild speech. Our models utilize flexible duration modeling to improve prosodic naturalness. To handle acoustic noise, we implement a multi-stage enhancement pipeline using the Sidon model, which significantly outperforms standard Demucs in signal quality. Experimental results show that finetuning enhanced audios yields superior robustness, achieving up to 4.21 UTMOS and 3.47 DNSMOS. Furthermore, we analyze the impact of reference prompt quality and length on zero-shot synthesis performance, demonstrating the effectiveness of our approach for realistic speech generation.",
      "url": "https://arxiv.org/abs/2602.05770",
      "pdfUrl": "https://arxiv.org/pdf/2602.05770.pdf",
      "titleJa": "強化された音声プロンプトを備えたゼロショットTTS：2026年ワイルドスポフチャレンジTTSトラックへのBSC提出"
    },
    {
      "id": "2602.05373",
      "arxivId": "2602.05373",
      "title": "Speech-XL: Towards Long-Form Speech Understanding in Large Speech Language Models",
      "authors": [
        "Haoqin Sun",
        "Chenyang Lyu",
        "Shiwan Zhao",
        "Xuanfan Ni",
        "Xiangyu Kong",
        "Longyue Wang",
        "Weihua Luo",
        "Yong Qin"
      ],
      "publishedDate": "2026-02-05",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Despite the growing success of Large Speech Language Models (LSLMs) in processing short-term acoustic signals, their extension to long-form audio understanding is severely bottlenecked. This limitation stems from the limited context length and the exorbitant memory footprints required for long-form inference. In this work, we propose Speech-XL, a new model that capitalizes on the intrinsic key-value (KV) sparsification capacity of Large Language Models (LLMs) to achieve high-ratio speech input compression. Specifically, we introduce a novel special token, the Speech Summarization Token (SST), for each speech interval to encapsulate the intra-interval speech information into its associated KV pairs. The SST module is trained via instruction fine-tuning, employing a curriculum learning strategy where the SST learns to compress information in a progressive manner--advancing from low-ratio (simple) to high-ratio (challenging) compression. Despite utilizing significantly less training data than other baselines, our model achieves highly competitive performance on major benchmarks, including LongSpeech and AUDIOMARATHON. By addressing the long-standing bottlenecks in long-form audio modeling, our approach offers a novel perspective on the condensation of extensive acoustic sequences.",
      "url": "https://arxiv.org/abs/2602.05373",
      "pdfUrl": "https://arxiv.org/pdf/2602.05373.pdf",
      "titleJa": "Speech-XL: 大規模音声言語モデルにおける長文音声理解に向けて"
    }
  ],
  "lastUpdated": "2026-02-15T01:09:34.621081",
  "totalCount": 74
}