{
  "papers": [
    {
      "id": "2601.14227",
      "arxivId": "2601.14227",
      "title": "Transformer Architectures for Respiratory Sound Analysis and Multimodal Diagnosis",
      "authors": [
        "Theodore Aptekarev",
        "Vladimir Sokolovsky",
        "Gregory Furman"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Respiratory sound analysis is a crucial tool for screening asthma and other pulmonary pathologies, yet traditional auscultation remains subjective and experience-dependent. Our prior research established a CNN baseline using DenseNet201, which demonstrated high sensitivity in classifying respiratory sounds. In this work, we (i) adapt the Audio Spectrogram Transformer (AST) for respiratory sound analysis and (ii) evaluate a multimodal Vision-Language Model (VLM) that integrates spectrograms with structured patient metadata. AST is initialized from publicly available weights and fine-tuned on a medical dataset containing hundreds of recordings per diagnosis. The VLM experiment uses a compact Moondream-type model that processes spectrogram images alongside a structured text prompt (sex, age, recording site) to output a JSON-formatted diagnosis. Results indicate that AST achieves approximately 97% accuracy with an F1-score around 97% and ROC AUC of 0.98 for asthma detection, significantly outperforming both the internal CNN baseline and typical external benchmarks. The VLM reaches 86-87% accuracy, performing comparably to the CNN baseline while demonstrating the capability to integrate clinical context into the inference process. These results confirm the effectiveness of self-attention for acoustic screening and highlight the potential of multimodal architectures for holistic diagnostic tools.",
      "url": "https://arxiv.org/abs/2601.14227",
      "pdfUrl": "https://arxiv.org/pdf/2601.14227.pdf",
      "titleJa": "呼吸音解析とマルチモーダル診断のためのトランスフォーマーアーキテクチャ"
    },
    {
      "id": "2601.14157",
      "arxivId": "2601.14157",
      "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models",
      "authors": [
        "Bruno Sienkiewicz",
        "Łukasz Neumann",
        "Mateusz Modrzejewski"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.",
      "url": "https://arxiv.org/abs/2601.14157",
      "pdfUrl": "https://arxiv.org/pdf/2601.14157.pdf",
      "titleJa": "ConceptCaps - 音楽モデルの解釈可能性のための蒸留概念データセット"
    },
    {
      "id": "2601.14046",
      "arxivId": "2601.14046",
      "title": "PRiSM: Benchmarking Phone Realization in Speech Models",
      "authors": [
        "Shikhar Bharadwaj",
        "Chin-Jou Li",
        "Yoonjae Kim",
        "Kwanghee Choi",
        "Eunjung Yeo",
        "Ryan Soh-Eun Shim",
        "Hanyu Zhou",
        "Brendon Boldt",
        "Karen Rosero Jacome",
        "Kalvin Chang",
        "Darsh Agrawal",
        "Keer Xu",
        "Chao-Han Huck Yang",
        "Jian Zhu",
        "Shinji Watanabe",
        "David R. Mortensen"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.",
      "url": "https://arxiv.org/abs/2601.14046",
      "pdfUrl": "https://arxiv.org/pdf/2601.14046.pdf",
      "titleJa": "PRiSM: 音声モデルにおける音素実現のベンチマーク"
    },
    {
      "id": "2601.13931",
      "arxivId": "2601.13931",
      "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music",
      "authors": [
        "Yannis Vasilakis",
        "Rachel Bittner",
        "Johan Pauwels"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG"
      ],
      "abstract": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.",
      "url": "https://arxiv.org/abs/2601.13931",
      "pdfUrl": "https://arxiv.org/pdf/2601.13931.pdf",
      "titleJa": "音楽のための音声テキスト結合モデルにおける効果的な否定モデリングに向けて"
    },
    {
      "id": "2601.13847",
      "arxivId": "2601.13847",
      "title": "Emotion and Acoustics Should Agree: Cross-Level Inconsistency Analysis for Audio Deepfake Detection",
      "authors": [
        "Jinhua Zhang",
        "Zhenqi Jia",
        "Rui Liu"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio Deepfake Detection (ADD) aims to detect spoof speech from bonafide speech. Most prior studies assume that stronger correlations within or across acoustic and emotional features imply authenticity, and thus focus on enhancing or measuring such correlations. However, existing methods often treat acoustic and emotional features in isolation or rely on correlation metrics, which overlook subtle desynchronization between them and smooth out abrupt discontinuities. To address these issues, we propose EAI-ADD, which treats cross level emotion acoustic inconsistency as the primary detection signal. We first project emotional and acoustic representations into a comparable space. Then we progressively integrate frame level and utterance level emotion features with acoustic features to capture cross level emotion acoustic inconsistencies across different temporal granularities. Experimental results on the ASVspoof 2019LA and 2021LA datasets demonstrate that the proposed EAI-ADD outperforms baselines, providing a more effective solution for audio anti spoofing detection.",
      "url": "https://arxiv.org/abs/2601.13847",
      "pdfUrl": "https://arxiv.org/pdf/2601.13847.pdf",
      "titleJa": "感情と音響は一致するはず：オーディオディープフェイク検出のためのクロスレベル不整合分析"
    },
    {
      "id": "2601.13802",
      "arxivId": "2601.13802",
      "title": "Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis",
      "authors": [
        "Yushen Chen",
        "Junzhe Liu",
        "Yujie Tu",
        "Zhikang Niu",
        "Yuzhe Liang",
        "Kai Yu",
        "Chunyu Qiang",
        "Chen Zhang",
        "Xie Chen"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "A notable gap persists in speech synthesis research and development for Arabic dialects, particularly from a unified modeling perspective. Despite its high practical value, the inherent linguistic complexity of Arabic dialects, further compounded by a lack of standardized data, benchmarks, and evaluation guidelines, steers researchers toward safer ground. To bridge this divide, we present Habibi, a suite of specialized and unified text-to-speech models that harnesses existing open-source ASR corpora to support a wide range of high- to low-resource Arabic dialects through linguistically-informed curriculum learning. Our approach outperforms the leading commercial service in generation quality, while maintaining extensibility through effective in-context learning, without requiring text diacritization. We are committed to open-sourcing the model, along with creating the first systematic benchmark for multi-dialect Arabic speech synthesis. Furthermore, by identifying the key challenges in and establishing evaluation standards for the process, we aim to provide a solid groundwork for subsequent research. Resources at https://SWivid.github.io/Habibi/ .",
      "url": "https://arxiv.org/abs/2601.13802",
      "pdfUrl": "https://arxiv.org/pdf/2601.13802.pdf",
      "titleJa": "Habibi: 統一方言アラビア語音声合成のオープンソース基盤の構築"
    },
    {
      "id": "2601.13758",
      "arxivId": "2601.13758",
      "title": "GOMPSNR: Reflourish the Signal-to-Noise Ratio Metric for Audio Generation Tasks",
      "authors": [
        "Lingling Dai",
        "Andong Li",
        "Cheng Chi",
        "Yifan Liang",
        "Xiaodong Li",
        "Chengshi Zheng"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In the field of audio generation, signal-to-noise ratio (SNR) has long served as an objective metric for evaluating audio quality. Nevertheless, recent studies have shown that SNR and its variants are not always highly correlated with human perception, prompting us to raise the questions: Why does SNR fail in measuring audio quality? And how to improve its reliability as an objective metric? In this paper, we identify the inadequate measurement of phase distance as a pivotal factor and propose to reformulate SNR with specially designed phase-distance terms, yielding an improved metric named GOMPSNR. We further extend the newly proposed formulation to derive two novel categories of loss function, corresponding to magnitude-guided phase refinement and joint magnitude-phase optimization, respectively. Besides, extensive experiments are conducted for an optimal combination of different loss functions. Experimental results on advanced neural vocoders demonstrate that our proposed GOMPSNR exhibits more reliable error measurement than SNR. Meanwhile, our proposed loss functions yield substantial improvements in model performance, and our wellchosen combination of different loss functions further optimizes the overall model capability.",
      "url": "https://arxiv.org/abs/2601.13758",
      "pdfUrl": "https://arxiv.org/pdf/2601.13758.pdf",
      "titleJa": "GOMPSNR: オーディオ生成タスクの信号対雑音比メトリックを刷新"
    },
    {
      "id": "2601.13704",
      "arxivId": "2601.13704",
      "title": "Performance and Complexity Trade-off Optimization of Speech Models During Training",
      "authors": [
        "Esteban Gómez",
        "Tom Bäckström"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "In speech machine learning, neural network models are typically designed by choosing an architecture with fixed layer sizes and structure. These models are then trained to maximize performance on metrics aligned with the task's objective. While the overall architecture is usually guided by prior knowledge of the task, the sizes of individual layers are often chosen heuristically. However, this approach does not guarantee an optimal trade-off between performance and computational complexity; consequently, post hoc methods such as weight quantization or model pruning are typically employed to reduce computational cost. This occurs because stochastic gradient descent (SGD) methods can only optimize differentiable functions, while factors influencing computational complexity, such as layer sizes and floating-point operations per second (FLOP/s), are non-differentiable and require modifying the model structure during training. We propose a reparameterization technique based on feature noise injection that enables joint optimization of performance and computational complexity during training using SGD-based methods. Unlike traditional pruning methods, our approach allows the model size to be dynamically optimized for a target performance-complexity trade-off, without relying on heuristic criteria to select which weights or structures to remove. We demonstrate the effectiveness of our method through three case studies, including a synthetic example and two practical real-world applications: voice activity detection and audio anti-spoofing. The code related to our work is publicly available to encourage further research.",
      "url": "https://arxiv.org/abs/2601.13704",
      "pdfUrl": "https://arxiv.org/pdf/2601.13704.pdf",
      "titleJa": "学習中の音声モデルの性能と複雑さのトレードオフ最適化"
    },
    {
      "id": "2601.13700",
      "arxivId": "2601.13700",
      "title": "DistilMOS: Layer-Wise Self-Distillation For Self-Supervised Learning Model-Based MOS Prediction",
      "authors": [
        "Jianing Yang",
        "Wataru Nakata",
        "Yuki Saito",
        "Hiroshi Saruwatari"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "With the advancement of self-supervised learning (SSL), fine-tuning pretrained SSL models for mean opinion score (MOS) prediction has achieved state-of-the-art performance. However, during fine-tuning, these SSL-based MOS prediction models often suffer from catastrophic forgetting of the pretrained knowledge and tend to overfit the training set, resulting in poor generalization performance. In this study, we propose DistilMOS, a novel method that learns to predict not only MOS but also token IDs obtained by clustering the hidden representations of each layer in the pretrained SSL model. These layer-wise token targets serve as self-distillation signals that enables the MOS prediction model to extract rich internal knowledge from SSL models, enhancing both prediction accuracy and generalization capability. Experimental evaluations demonstrate that our method significantly outperforms standard SSL-based MOS prediction models on both in-domain and out-of-domain evaluations, verifying the effectiveness and practicality of the proposed method.",
      "url": "https://arxiv.org/abs/2601.13700",
      "pdfUrl": "https://arxiv.org/pdf/2601.13700.pdf",
      "titleJa": "DistilMOS: 自己教師あり学習モデルに基づく MOS 予測のための層単位の自己蒸留"
    },
    {
      "id": "2601.13679",
      "arxivId": "2601.13679",
      "title": "Ultra-Lightweight Network for Ship-Radiated Sound Classification on Embedded Deployment",
      "authors": [
        "Sangwon Park",
        "Dongjun Kim",
        "Sung-Hoon Byun",
        "Sangwook Park"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This letter presents ShuffleFAC, a lightweight acoustic model for ship-radiated sound classification in resource-constrained maritime monitoring systems. ShuffleFAC integrates Frequency-Aware convolution into an efficiency-oriented backbone using separable convolution, point-wise group convolution, and channel shuffle, enabling frequency-sensitive feature extraction with low computational cost. Experiments on the DeepShip dataset show that ShuffleFAC achieves competitive performance with substantially reduced complexity. In particular, ShuffleFAC ($γ=16$) attains a macro F1-score of 71.45 $\\pm$ 1.18% using 39K parameters and 3.06M MACs, and achieves an inference latency of 6.05 $\\pm$ 0.95ms on a Raspberry Pi. Compared with MicroNet0, it improves macro F1-score by 1.82 % while reducing model size by 9.7x and latency by 2.5x. These results indicate that ShuffleFAC is suitable for real-time embedded UATR.",
      "url": "https://arxiv.org/abs/2601.13679",
      "pdfUrl": "https://arxiv.org/pdf/2601.13679.pdf",
      "titleJa": "組み込み型船舶放射音分類用超軽量ネットワーク"
    },
    {
      "id": "2601.13647",
      "arxivId": "2601.13647",
      "title": "Fusion Segment Transformer: Bi-Directional Attention Guided Fusion Network for AI-Generated Music Detection",
      "authors": [
        "Yumin Kim",
        "Seonghyeon Go"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "With the rise of generative AI technology, anyone can now easily create and deploy AI-generated music, which has heightened the need for technical solutions to address copyright and ownership issues. While existing works mainly focused on short-audio, the challenge of full-audio detection, which requires modeling long-term structure and context, remains insufficiently explored. To address this, we propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. As in our previous work, we extract content embeddings from short music segments using diverse feature extractors. Furthermore, we enhance the architecture for full-audio AI-generated music detection by introducing a Gated Fusion Layer that effectively integrates content and structural information, enabling the capture of long-term context. Experiments on the SONICS and AIME datasets show that our approach outperforms the previous model and recent baselines, achieving state-of-the-art results in AI-generated music detection.",
      "url": "https://arxiv.org/abs/2601.13647",
      "pdfUrl": "https://arxiv.org/pdf/2601.13647.pdf",
      "titleJa": "Fusion Segment Transformer: AI生成音楽検出のための双方向アテンションガイド融合ネットワーク"
    },
    {
      "id": "2601.13589",
      "arxivId": "2601.13589",
      "title": "Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification",
      "authors": [
        "HyeYoung Lee"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.",
      "url": "https://arxiv.org/abs/2601.13589",
      "pdfUrl": "https://arxiv.org/pdf/2601.13589.pdf",
      "titleJa": "リアルタイム安全性検証を備えたマルチエージェントAIシステムによる動作応答コンテンツ生成"
    },
    {
      "id": "2601.13539",
      "arxivId": "2601.13539",
      "title": "LongSpeech: A Scalable Benchmark for Transcription, Translation and Understanding in Long Speech",
      "authors": [
        "Fei Yang",
        "Xuanfan Ni",
        "Renyi Yang",
        "Jiahui Geng",
        "Qing Li",
        "Chenyang Lyu",
        "Yichao Du",
        "Longyue Wang",
        "Weihua Luo",
        "Kaifu Zhang"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Recent advances in audio-language models have demonstrated remarkable success on short, segment-level speech tasks. However, real-world applications such as meeting transcription, spoken document understanding, and conversational analysis require robust models capable of processing and reasoning over long-form audio. In this work, we present LongSpeech, a large-scale and scalable benchmark specifically designed to evaluate and advance the capabilities of speech models on long-duration audio. LongSpeech comprises over 100,000 speech segments, each approximately 10 minutes long, with rich annotations for ASR, speech translation, summarization, language detection, speaker counting, content separation, and question answering. We introduce a reproducible pipeline for constructing long-form speech benchmarks from diverse sources, enabling future extensions. Our initial experiments with state-of-the-art models reveal significant performance gaps, with models often specializing in one task at the expense of others and struggling with higher-level reasoning. These findings underscore the challenging nature of our benchmark. Our benchmark will be made publicly available to the research community.",
      "url": "https://arxiv.org/abs/2601.13539",
      "pdfUrl": "https://arxiv.org/pdf/2601.13539.pdf",
      "titleJa": "LongSpeech: 長文音声の書き起こし、翻訳、理解のためのスケーラブルなベンチマーク"
    },
    {
      "id": "2601.13531",
      "arxivId": "2601.13531",
      "title": "ICASSP 2026 URGENT Speech Enhancement Challenge",
      "authors": [
        "Chenda Li",
        "Wei Wang",
        "Marvin Sach",
        "Wangyou Zhang",
        "Kohei Saijo",
        "Samuele Cornell",
        "Yihui Fu",
        "Zhaoheng Ni",
        "Tim Fingscheidt",
        "Shinji Watanabe",
        "Yanmin Qian"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "The ICASSP 2026 URGENT Challenge advances the series by focusing on universal speech enhancement (SE) systems that handle diverse distortions, domains, and input conditions. This overview paper details the challenge's motivation, task definitions, datasets, baseline systems, evaluation protocols, and results. The challenge is divided into two complementary tracks. Track 1 focuses on universal speech enhancement, while Track 2 introduces speech quality assessment for enhanced speech. The challenge attracted over 80 team registrations, with 29 submitting valid entries, demonstrating significant community interest in robust SE technologies.",
      "url": "https://arxiv.org/abs/2601.13531",
      "pdfUrl": "https://arxiv.org/pdf/2601.13531.pdf",
      "titleJa": "ICASSP 2026 緊急音声強調チャレンジ"
    },
    {
      "id": "2601.13513",
      "arxivId": "2601.13513",
      "title": "Event Classification by Physics-informed Inpainting for Distributed Multichannel Acoustic Sensor with Partially Degraded Channels",
      "authors": [
        "Noriyuki Tonami",
        "Wataru Kohno",
        "Yoshiyuki Yajima",
        "Sakiko Mishima",
        "Yumi Arai",
        "Reishi Kondo",
        "Tomoyuki Hino"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Distributed multichannel acoustic sensing (DMAS) enables large-scale sound event classification (SEC), but performance drops when many channels are degraded and when sensor layouts at test time differ from training layouts. We propose a learning-free, physics-informed inpainting frontend based on reverse time migration (RTM). In this approach, observed multichannel spectrograms are first back-propagated on a 3D grid using an analytic Green's function to form a scene-consistent image, and then forward-projected to reconstruct inpainted signals before log-mel feature extraction and Transformer-based classification. We evaluate the method on ESC-50 with 50 sensors and three layouts (circular, linear, right-angle), where per-channel SNRs are sampled from -30 to 0 dB. Compared with an AST baseline, scaling-sparsemax channel selection, and channel-swap augmentation, the proposed RTM frontend achieves the best or competitive accuracy across all layouts, improving accuracy by 13.1 points on the right-angle layout (from 9.7% to 22.8%). Correlation analyses show that spatial weights align more strongly with SNR than with channel--source distance, and that higher SNR--weight correlation corresponds to higher SEC accuracy. These results demonstrate that a reconstruct-then-project, physics-based preprocessing effectively complements learning-only methods for DMAS under layout-open configurations and severe channel degradation.",
      "url": "https://arxiv.org/abs/2601.13513",
      "pdfUrl": "https://arxiv.org/pdf/2601.13513.pdf",
      "titleJa": "部分的に劣化したチャネルを持つ分散型マルチチャネル音響センサーのための物理学に基づくインペインティングによるイベント分類"
    },
    {
      "id": "2601.13464",
      "arxivId": "2601.13464",
      "title": "Context and Transcripts Improve Detection of Deepfake Audios of Public Figures",
      "authors": [
        "Chongyang Gao",
        "Marco Postiglione",
        "Julian Baldwin",
        "Natalia Denisenko",
        "Isabel Gortner",
        "Luke Fosdick",
        "Chiara Pulice",
        "Sarit Kraus",
        "V. S. Subrahmanian"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P$^2$V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors. Performance (measured via F1 score, AUC, and EER) of multiple baseline audio deepfake detectors and traditional classifiers can be improved by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We additionally show that CADD, via its use of context and/or transcripts, is more robust to 5 adversarial evasion strategies, limiting performance degradation to an average of just -0.71% across all experiments. Code, models, and datasets are available at our project page: https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection (access restricted during review).",
      "url": "https://arxiv.org/abs/2601.13464",
      "pdfUrl": "https://arxiv.org/pdf/2601.13464.pdf",
      "titleJa": "文脈とトランスクリプトにより著名人のディープフェイク音声の検出が向上"
    },
    {
      "id": "2601.13198",
      "arxivId": "2601.13198",
      "title": "The Achilles' Heel of Angular Margins: A Chebyshev Polynomial Fix for Speaker Verification",
      "authors": [
        "Yang Wang",
        "Yiqi Liu",
        "Chenghao Xiao",
        "Chenghua Lin"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Angular margin losses, such as AAM-Softmax, have become the de facto in speaker and face verification. Their success hinges on directly manipulating the angle between features and class prototypes. However, this manipulation relies on the arccos function to recover the angle, introducing a significant yet overlooked source of training instability. The derivative of arccos explodes at its boundaries, causing gradient peaks during optimisation. Furthermore, the formulation fails to generate a sufficiently sharp gradient for hard-to-classify examples. We address these issues by proposing ChebyAAM, a loss that replaces the arccos operation with its Chebyshev polynomial approximation. This substitution eliminates gradient explosion and applies a stronger corrective signal to hard examples, leading to more effective optimisation. Experiments on three benchmarks (VoxCeleb, SITW, and CN-Celeb) demonstrate that our method resolves the instability and consistently improves performance. Our work suggests that approximating angular operations, rather than calculating them explicitly, offers a more robust path for designing future metric learning losses. Code is available at https://github.com/ExtraOrdinaryLab/vibe.",
      "url": "https://arxiv.org/abs/2601.13198",
      "pdfUrl": "https://arxiv.org/pdf/2601.13198.pdf",
      "titleJa": "角度マージンの弱点：話者検証のためのチェビシェフ多項式修正"
    },
    {
      "id": "2601.13107",
      "arxivId": "2601.13107",
      "title": "Content Leakage in LibriSpeech and Its Impact on the Privacy Evaluation of Speaker Anonymization",
      "authors": [
        "Carlos Franzreb",
        "Arnab Das",
        "Tim Polzehl",
        "Sebastian Möller"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Speaker anonymization aims to conceal a speaker's identity, without considering the linguistic content. In this study, we reveal a weakness of Librispeech, the dataset that is commonly used to evaluate anonymizers: the books read by the Librispeech speakers are so distinct, that speakers can be identified by their vocabularies. Even perfect anonymizers cannot prevent this identity leakage. The EdAcc dataset is better in this regard: only a few speakers can be identified through their vocabularies, encouraging the attacker to look elsewhere for the identities of the anonymized speakers. EdAcc also comprises spontaneous speech and more diverse speakers, complementing Librispeech and giving more insights into how anonymizers work.",
      "url": "https://arxiv.org/abs/2601.13107",
      "pdfUrl": "https://arxiv.org/pdf/2601.13107.pdf",
      "titleJa": "LibriSpeechにおけるコンテンツ漏洩と話者匿名化のプライバシー評価への影響"
    },
    {
      "id": "2601.12966",
      "arxivId": "2601.12966",
      "title": "Lombard Speech Synthesis for Any Voice with Controllable Style Embeddings",
      "authors": [
        "Seymanur Akti",
        "Alexander Waibel"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "The Lombard effect plays a key role in natural communication, particularly in noisy environments or when addressing hearing-impaired listeners. We present a controllable text-to-speech (TTS) system capable of synthesizing Lombard speech for any speaker without requiring explicit Lombard data during training. Our approach leverages style embeddings learned from a large, prosodically diverse dataset and analyzes their correlation with Lombard attributes using principal component analysis (PCA). By shifting the relevant PCA components, we manipulate the style embeddings and incorporate them into our TTS model to generate speech at desired Lombard levels. Evaluations demonstrate that our method preserves naturalness and speaker identity, enhances intelligibility under noise, and provides fine-grained control over prosody, offering a robust solution for controllable Lombard TTS for any speaker.",
      "url": "https://arxiv.org/abs/2601.12966",
      "pdfUrl": "https://arxiv.org/pdf/2601.12966.pdf",
      "titleJa": "制御可能なスタイル埋め込みを備えたあらゆる音声のためのロンバード音声合成"
    },
    {
      "id": "2601.12961",
      "arxivId": "2601.12961",
      "title": "Supervised Learning for Game Music Segmentation",
      "authors": [
        "Shangxuan Luo",
        "Joshua Reiss"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD"
      ],
      "abstract": "At present, neural network-based models, including transformers, struggle to generate memorable and readily comprehensible music from unified and repetitive musical material due to a lack of understanding of musical structure. Consequently, these models are rarely employed by the games industry. It is hypothesised by many scholars that the modelling of musical structure may inform models at a higher level, thereby enhancing the quality of music generation. The aim of this study is to explore the performance of supervised learning methods in the task of structural segmentation, which is the initial step in music structure modelling. An audio game music dataset with 309 structural annotations was created to train the proposed method, which combines convolutional neural networks and recurrent neural networks, achieving performance comparable to the state-of-the-art unsupervised learning methods with fewer training resources.",
      "url": "https://arxiv.org/abs/2601.12961",
      "pdfUrl": "https://arxiv.org/pdf/2601.12961.pdf",
      "titleJa": "ゲーム音楽セグメンテーションのための教師あり学習"
    },
    {
      "id": "2601.14012",
      "arxivId": "2601.14012",
      "title": "MATE: Matryoshka Audio-Text Embeddings for Open-Vocabulary Keyword Spotting",
      "authors": [
        "Youngmoon Jung",
        "Myunghun Jung",
        "Joon-Young Yang",
        "Yong-Hyeok Lee",
        "Jaeyoung Roh",
        "Hoon-Young Cho"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "Open-vocabulary keyword spotting (KWS) with text-based enrollment has emerged as a flexible alternative to fixed-phrase triggers. Prior utterance-level matching methods, from an embedding-learning standpoint, learn embeddings at a single fixed dimensionality. We depart from this design and propose Matryoshka Audio-Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings (\"prefixes\"). Specifically, we introduce a PCA-guided prefix alignment: PCA-compressed versions of the full text embedding for each prefix size serve as teacher targets to align both audio and text prefixes. This alignment concentrates salient keyword cues in lower-dimensional prefixes, while higher dimensions add detail. MATE is trained with standard deep metric learning objectives for audio-text KWS, and is loss-agnostic. To our knowledge, this is the first application of matryoshka-style embeddings to KWS, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead.",
      "url": "https://arxiv.org/abs/2601.14012",
      "pdfUrl": "https://arxiv.org/pdf/2601.14012.pdf",
      "titleJa": "MATE: オープン語彙キーワードスポッティングのためのマトリョーシカ音声テキスト埋め込み"
    },
    {
      "id": "2601.13999",
      "arxivId": "2601.13999",
      "title": "DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification",
      "authors": [
        "Youngmoon Jung",
        "Joon-Young Yang",
        "Ju-ho Kim",
        "Jaeyoung Roh",
        "Chang Woo Han",
        "Hoon-Young Cho"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. While existing methods focus on enhancing speaker encoders, the embedding learning strategy still forces a single fixed-dimensional representation reused for utterances of any length, leaving capacity misaligned with the information available at different durations. We propose Duration-Aware Matryoshka Embedding (DAME), a model-agnostic framework that builds a nested hierarchy of sub-embeddings aligned to utterance durations: lower-dimensional representations capture compact speaker traits from short utterances, while higher dimensions encode richer details from longer speech. DAME supports both training from scratch and fine-tuning, and serves as a direct alternative to conventional large-margin fine-tuning, consistently improving performance across durations. On the VoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently reduces the equal error rate on 1-s and other short-duration trials, while maintaining full-length performance with no additional inference cost. These gains generalize across various speaker encoder architectures under both general training and fine-tuning setups.",
      "url": "https://arxiv.org/abs/2601.13999",
      "pdfUrl": "https://arxiv.org/pdf/2601.13999.pdf",
      "titleJa": "DAME: 継続時間を考慮したマトリョーシカ埋め込みによる継続時間ロバストな話者検証"
    },
    {
      "id": "2601.13948",
      "arxivId": "2601.13948",
      "title": "Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models",
      "authors": [
        "Nikita Kuzmin",
        "Songting Liu",
        "Kong Aik Lee",
        "Eng Siong Chng"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.",
      "url": "https://arxiv.org/abs/2601.13948",
      "pdfUrl": "https://arxiv.org/pdf/2601.13948.pdf",
      "titleJa": "Stream-Voice-Anon: ニューラルオーディオコーデックと言語モデルによるリアルタイム話者匿名化の有用性向上"
    },
    {
      "id": "2601.13910",
      "arxivId": "2601.13910",
      "title": "Synthetic Singers: A Review of Deep-Learning-based Singing Voice Synthesis Approaches",
      "authors": [
        "Changhao Pan",
        "Dongyu Yao",
        "Yu Zhang",
        "Wenxiang Guo",
        "Jingyu Lu",
        "Zhiyuan Zhu",
        "Zhou Zhao"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Recent advances in singing voice synthesis (SVS) have attracted substantial attention from both academia and industry. With the advent of large language models and novel generative paradigms, producing controllable, high-fidelity singing voices has become an attainable goal. Yet the field still lacks a comprehensive survey that systematically analyzes deep-learning-based singing voice synthesis systems and their enabling technologies. To address the aforementioned issue, this survey first categorizes existing systems by task type and then organizes current architectures into two major paradigms: cascaded and end-to-end approaches. Moreover, we provide an in-depth analysis of core technologies, covering singing modeling and control techniques. Finally, we review relevant datasets, annotation tools, and evaluation benchmarks that support training and assessment. In appendix, we introduce training strategies and further discussion of SVS. This survey provides an up-to-date review of the literature on SVS models, which would be a useful reference for both researchers and engineers. Related materials are available at https://github.com/David-Pigeon/SyntheticSingers.",
      "url": "https://arxiv.org/abs/2601.13910",
      "pdfUrl": "https://arxiv.org/pdf/2601.13910.pdf",
      "titleJa": "合成歌手：ディープラーニングに基づく歌声合成アプローチのレビュー"
    },
    {
      "id": "2601.13849",
      "arxivId": "2601.13849",
      "title": "Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control",
      "authors": [
        "Ziyi Yang",
        "Li Rao",
        "Zhengding Luo",
        "Dongyuan Shi",
        "Qirui Huang",
        "Woon-Seng Gan"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.LG",
        "eess.SP"
      ],
      "abstract": "Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train.",
      "url": "https://arxiv.org/abs/2601.13849",
      "pdfUrl": "https://arxiv.org/pdf/2601.13849.pdf",
      "titleJa": "能動騒音制御のためのメタ学習による制御フィルタと二次パスの共初期化"
    },
    {
      "id": "2601.13629",
      "arxivId": "2601.13629",
      "title": "S$^2$Voice: Style-Aware Autoregressive Modeling with Enhanced Conditioning for Singing Style Conversion",
      "authors": [
        "Ziqian Wang",
        "Xianjun Xia",
        "Chuanzeng Huang",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "We present S$^2$Voice, the winning system of the Singing Voice Conversion Challenge (SVCC) 2025 for both the in-domain and zero-shot singing style conversion tracks. Built on the strong two-stage Vevo baseline, S$^2$Voice advances style control and robustness through several contributions. First, we integrate style embeddings into the autoregressive large language model (AR LLM) via a FiLM-style layer-norm conditioning and a style-aware cross-attention for enhanced fine-grained style modeling. Second, we introduce a global speaker embedding into the flow-matching transformer to improve timbre similarity. Third, we curate a large, high-quality singing corpus via an automated pipeline for web harvesting, vocal separation, and transcript refinement. Finally, we employ a multi-stage training strategy combining supervised fine-tuning (SFT) and direct preference optimization (DPO). Subjective listening tests confirm our system's superior performance: leading in style similarity and singer similarity for Task 1, and across naturalness, style similarity, and singer similarity for Task 2. Ablation studies demonstrate the effectiveness of our contributions in enhancing style fidelity, timbre preservation, and generalization. Audio samples are available~\\footnote{https://honee-w.github.io/SVC-Challenge-Demo/}.",
      "url": "https://arxiv.org/abs/2601.13629",
      "pdfUrl": "https://arxiv.org/pdf/2601.13629.pdf",
      "titleJa": "S$^2$Voice: 歌唱スタイル変換のための強化された条件付けによるスタイルを考慮した自己回帰モデリング"
    },
    {
      "id": "2601.13409",
      "arxivId": "2601.13409",
      "title": "RLBR: Reinforcement Learning with Biasing Rewards for Contextual Speech Large Language Models",
      "authors": [
        "Bo Ren",
        "Ruchao Fan",
        "Yelong Shen",
        "Weizhu Chen",
        "Jinyu Li"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Speech large language models (LLMs) have driven significant progress in end-to-end speech understanding and recognition, yet they continue to struggle with accurately recognizing rare words and domain-specific terminology. This paper presents a novel fine-tuning method, Reinforcement Learning with Biasing Rewards (RLBR), which employs a specialized biasing words preferred reward to explicitly emphasize biasing words in the reward calculation. In addition, we introduce reference-aware mechanisms that extend the reinforcement learning algorithm with reference transcription to strengthen the potential trajectory exploration space. Experiments on the LibriSpeech corpus across various biasing list sizes demonstrate that RLBR delivers substantial performance improvements over a strong supervised fine-tuning (SFT) baseline and consistently outperforms several recently published methods. The proposed approach achieves excellent performance on the LibriSpeech test-clean and test-other sets, reaching Biasing Word Error Rates (BWERs) of 0.59% / 2.11%, 1.09% / 3.24%, and 1.36% / 4.04% for biasing list sizes of 100, 500, and 1000, respectively, without compromising the overall WERs.",
      "url": "https://arxiv.org/abs/2601.13409",
      "pdfUrl": "https://arxiv.org/pdf/2601.13409.pdf",
      "titleJa": "RLBR: コンテキスト音声大規模言語モデルのためのバイアス報酬を用いた強化学習"
    },
    {
      "id": "2601.13357",
      "arxivId": "2601.13357",
      "title": "On the Relation of State Space Models and Hidden Markov Models",
      "authors": [
        "Aydin Ghojogh",
        "M. Hadi Sepanj",
        "Benyamin Ghojogh"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.LG",
        "cs.CL",
        "eess.AS",
        "eess.SY"
      ],
      "abstract": "State Space Models (SSMs) and Hidden Markov Models (HMMs) are foundational frameworks for modeling sequential data with latent variables and are widely used in signal processing, control theory, and machine learning. Despite their shared temporal structure, they differ fundamentally in the nature of their latent states, probabilistic assumptions, inference procedures, and training paradigms. Recently, deterministic state space models have re-emerged in natural language processing through architectures such as S4 and Mamba, raising new questions about the relationship between classical probabilistic SSMs, HMMs, and modern neural sequence models. In this paper, we present a unified and systematic comparison of HMMs, linear Gaussian state space models, Kalman filtering, and contemporary NLP state space models. We analyze their formulations through the lens of probabilistic graphical models, examine their inference algorithms -- including forward-backward inference and Kalman filtering -- and contrast their learning procedures via Expectation-Maximization and gradient-based optimization. By highlighting both structural similarities and semantic differences, we clarify when these models are equivalent, when they fundamentally diverge, and how modern NLP SSMs relate to classical probabilistic models. Our analysis bridges perspectives from control theory, probabilistic modeling, and modern deep learning.",
      "url": "https://arxiv.org/abs/2601.13357",
      "pdfUrl": "https://arxiv.org/pdf/2601.13357.pdf",
      "titleJa": "状態空間モデルと隠れマルコフモデルの関係について"
    },
    {
      "id": "2601.13140",
      "arxivId": "2601.13140",
      "title": "AMDM-SE: Attention-based Multichannel Diffusion Model for Speech Enhancement",
      "authors": [
        "Renana Opochinsky",
        "Sharon Gannot"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Diffusion models have recently achieved impressive results in reconstructing images from noisy inputs, and similar ideas have been applied to speech enhancement by treating time-frequency representations as images. With the ubiquity of multi-microphone devices, we extend state-of-the-art diffusion-based methods to exploit multichannel inputs for improved performance. Multichannel diffusion-based enhancement remains in its infancy, with prior work making limited use of advanced mechanisms such as attention for spatial modeling - a gap addressed in this paper. We propose AMDM-SE, an Attention-based Multichannel Diffusion Model for Speech Enhancement, designed specifically for noise reduction. AMDM-SE leverages spatial inter-channel information through a novel cross-channel time-frequency attention block, enabling faithful reconstruction of fine-grained signal details within a generative diffusion framework. On the CHiME-3 benchmark, AMDM-SE outperforms both a single-channel diffusion baseline and a multichannel model without attention, as well as a strong DNN-based predictive method. Simulated-data experiments further underscore the importance of the proposed multichannel attention mechanism. Overall, our results show that incorporating targeted multichannel attention into diffusion models substantially improves noise reduction. While multichannel diffusion-based speech enhancement is still an emerging field, our work contributes a new and complementary approach to the growing body of research in this direction.",
      "url": "https://arxiv.org/abs/2601.13140",
      "pdfUrl": "https://arxiv.org/pdf/2601.13140.pdf",
      "titleJa": "AMDM-SE: 音声強調のための注意ベースのマルチチャネル拡散モデル"
    },
    {
      "id": "2601.13055",
      "arxivId": "2601.13055",
      "title": "VoCodec: An Efficient Lightweight Low-Bitrate Speech Codec",
      "authors": [
        "Leyan Yang",
        "Ronghui Hu",
        "Yang Xu",
        "Jing Lu"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Recent advancements in end-to-end neural speech codecs enable compressing audio at extremely low bitrates while maintaining high-fidelity reconstruction. Meanwhile, low computational complexity and low latency are crucial for real-time communication. In this paper, we propose VoCodec, a speech codec model featuring a computational complexity of only 349.29M multiply-accumulate operations per second (MACs/s) and a latency of 30 ms. With the competitive vocoder Vocos as its backbone, the proposed model ranked fourth on Track 1 in the 2025 LRAC Challenge and achieved the highest subjective evaluation score (MUSHRA) on the clean speech test set. Additionally, we cascade a lightweight neural network at the front end to extend its capability of speech enhancement. Experimental results demonstrate that the two systems achieve competitive performance across multiple evaluation metrics. Speech samples can be found at https://acceleration123.github.io/.",
      "url": "https://arxiv.org/abs/2601.13055",
      "pdfUrl": "https://arxiv.org/pdf/2601.13055.pdf",
      "titleJa": "VoCodec: 効率的で軽量な低ビットレート音声コーデック"
    },
    {
      "id": "2601.12950",
      "arxivId": "2601.12950",
      "title": "ImmersiveFlow: Stereo-to-7.1.4 spatial audio generation with flow matching",
      "authors": [
        "Zining Liang",
        "Runbang Wang",
        "Xuzhou Ye",
        "Qiuqiang Kong"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Immersive spatial audio has become increasingly critical for applications ranging from AR/VR to home entertainment and automotive sound systems. However, existing generative methods remain constrained to low-dimensional formats such as binaural audio and First-Order Ambisonics (FOA). Binaural rendering is inherently limited to headphone playback, while FOA suffers from spatial aliasing and insufficient resolution for high-frequency. To overcome these limitations, we introduce ImmersiveFlow, the first end-to-end generative framework that directly synthesizes discrete 7.1.4 format spatial audio from stereo input. ImmersiveFlow leverages Flow Matching to learn trajectories from stereo inputs to multichannel spatial features within a pretrained VAE latent space. At inference, the Flow Matching model predicted latent features are decoded by the VAE and converted into the final 7.1.4 waveform. Comprehensive objective and subjective evaluations demonstrate that our method produces perceptually rich sound fields and enhanced externalization, significantly outperforming traditional upmixing techniques. Code implementations and audio samples are provided at: https://github.com/violet-audio/ImmersiveFlow.",
      "url": "https://arxiv.org/abs/2601.12950",
      "pdfUrl": "https://arxiv.org/pdf/2601.12950.pdf",
      "titleJa": "ImmersiveFlow: フローマッチングによるステレオから7.1.4への空間オーディオ生成"
    },
    {
      "id": "2601.12802",
      "arxivId": "2601.12802",
      "title": "UNMIXX: Untangling Highly Correlated Singing Voices Mixtures",
      "authors": [
        "Jihoo Jung",
        "Ji-Hoon Kim",
        "Doyeop Kwak",
        "Junwon Lee",
        "Juhan Nam",
        "Joon Son Chung"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We introduce UNMIXX, a novel framework for multiple singing voices separation (MSVS). While related to speech separation, MSVS faces unique challenges: data scarcity and the highly correlated nature of singing voices mixture. To address these issues, we propose UNMIXX with three key components: (1) musically informed mixing strategy to construct highly correlated, music-like mixtures, (2) cross-source attention that drives representations of two singers apart via reverse attention, and (3) magnitude penalty loss penalizing erroneously assigned interfering energy. UNMIXX not only addresses data scarcity by simulating realistic training data, but also excels at separating highly correlated mixtures through cross-source interactions at both the architectural and loss levels. Our extensive experiments demonstrate that UNMIXX greatly enhances performance, with SDRi gains exceeding 2.2 dB over prior work.",
      "url": "https://arxiv.org/abs/2601.12802",
      "pdfUrl": "https://arxiv.org/pdf/2601.12802.pdf",
      "titleJa": "UNMIXX: 相関性の高い歌声ミックスを解きほぐす"
    },
    {
      "id": "2601.12769",
      "arxivId": "2601.12769",
      "title": "Adaptive Speaker Embedding Self-Augmentation for Personal Voice Activity Detection with Short Enrollment Speech",
      "authors": [
        "Fuyuan Feng",
        "Wenbin Zhang",
        "Yu Gao",
        "Longting Xu",
        "Xiaofeng Mou",
        "Yi Xu"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Personal Voice Activity Detection (PVAD) is crucial for identifying target speaker segments in the mixture, yet its performance heavily depends on the quality of speaker embeddings. A key practical limitation is the short enrollment speech--such as a wake-up word--which provides limited cues. This paper proposes a novel adaptive speaker embedding self-augmentation strategy that enhances PVAD performance by augmenting the original enrollment embeddings through additive fusion of keyframe embeddings extracted from mixed speech. Furthermore, we introduce a long-term adaptation strategy to iteratively refine embeddings during detection, mitigating speaker temporal variability. Experiments show significant gains in recall, precision, and F1-score under short enrollment conditions, matching full-length enrollment performance after five iterative updates. The source code is available at https://anonymous.4open.science/r/ASE-PVAD-E5D6 .",
      "url": "https://arxiv.org/abs/2601.12769",
      "pdfUrl": "https://arxiv.org/pdf/2601.12769.pdf",
      "titleJa": "短い登録音声による個人の音声活動検出のための適応型話者埋め込み自己拡張"
    },
    {
      "id": "2601.12757",
      "arxivId": "2601.12757",
      "title": "CodeSep: Low-Bitrate Codec-Driven Speech Separation with Base-Token Disentanglement and Auxiliary-Token Serial Prediction",
      "authors": [
        "Hui-Peng Du",
        "Yang Ai",
        "Xiao-Hang Jiang",
        "Rui-Chen Zheng",
        "Zhen-Hua Ling"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This paper targets a new scenario that integrates speech separation with speech compression, aiming to disentangle multiple speakers while producing discrete representations for efficient transmission or storage, with applications in online meetings and dialogue archiving. To address this scenario, we propose CodeSep, a codec-driven model that jointly performs speech separation and low-bitrate compression. CodeSep comprises a residual vector quantizer (RVQ)-based plain neural speech codec, a base-token disentanglement (BTD) module, and parallel auxiliary-token serial prediction (ATSP) modules. The BTD module disentangles mixed-speech mel-spectrograms into base tokens for each speaker, which are then refined by ATSP modules to serially predict auxiliary tokens, and finally, all tokens are decoded to reconstruct separated waveforms through the codec decoder. During training, the codec's RVQ provides supervision with permutation-invariant and teacher-forcing-based cross-entropy losses. As only base tokens are transmitted or stored, CodeSep achieves low-bitrate compression. Experimental results show that CodeSep attains satisfactory separation performance at only 1 kbps compared with baseline methods.",
      "url": "https://arxiv.org/abs/2601.12757",
      "pdfUrl": "https://arxiv.org/pdf/2601.12757.pdf",
      "titleJa": "CodeSep: ベーストークン分離と補助トークンシリアル予測による低ビットレートコーデック駆動型音声分離"
    },
    {
      "id": "2601.12700",
      "arxivId": "2601.12700",
      "title": "Improving Audio Question Answering with Variational Inference",
      "authors": [
        "Haolin Chen"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Variational inference (VI) provides a principled framework for estimating posterior distributions over model parameters, enabling explicit modeling of weight uncertainty during optimization. By capturing this uncertainty, VI improves the reliability of predictions, yielding better calibrated outputs. In this work, we investigate the benefits of VI for challenging multimodal understanding and reasoning by applying the Improved Variational Online Newton (IVON), a recent VI optimizer, to fine-tuning a multimodal large language model on audio question answering tasks. Our results show that VI not only enhances predictive accuracy but also significantly improves calibration, reducing the model's overconfidence. These advances further support risk-sensitive applications such as selective prediction, where reliable confidence estimates are crucial.",
      "url": "https://arxiv.org/abs/2601.12700",
      "pdfUrl": "https://arxiv.org/pdf/2601.12700.pdf",
      "titleJa": "変分推論による音声質問応答の改善"
    },
    {
      "id": "2601.14255",
      "arxivId": "2601.14255",
      "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
      "authors": [
        "Sangbeom Lim",
        "Seoung Wug Oh",
        "Jiahui Huang",
        "Heeji Yoon",
        "Seungryong Kim",
        "Joon-Young Lee"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.",
      "url": "https://arxiv.org/abs/2601.14255",
      "pdfUrl": "https://arxiv.org/pdf/2601.14255.pdf",
      "titleJa": "VideoMaMa: 生成事前確率によるマスクガイドビデオマット処理"
    },
    {
      "id": "2601.14242",
      "arxivId": "2601.14242",
      "title": "APEX-Agents",
      "authors": [
        "Bertie Vidgen",
        "Austin Mann",
        "Abby Fennelly",
        "John Wright Stanly",
        "Lucas Rothman",
        "Marco Burstein",
        "Julien Benchek",
        "David Ostrofsky",
        "Anirudh Ravichandran",
        "Debnil Sur",
        "Neel Venugopal",
        "Alannah Hsia",
        "Isaac Robinson",
        "Calix Huang",
        "Olivia Varones",
        "Daniyal Khan",
        "Michael Haines",
        "Zach Richards",
        "Chirag Mahapatra",
        "Brendan Foody",
        "Osvald Nitski"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.",
      "url": "https://arxiv.org/abs/2601.14242",
      "pdfUrl": "https://arxiv.org/pdf/2601.14242.pdf",
      "titleJa": "APEXエージェント"
    },
    {
      "id": "2601.14235",
      "arxivId": "2601.14235",
      "title": "Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration",
      "authors": [
        " LSST Dark Energy Science Collaboration",
        "Eric Aubourg",
        "Camille Avestruz",
        "Matthew R. Becker",
        "Biswajit Biswas",
        "Rahul Biswas",
        "Boris Bolliet",
        "Adam S. Bolton",
        "Clecio R. Bom",
        "Raphaël Bonnet-Guerrini",
        "Alexandre Boucaud",
        "Jean-Eric Campagne",
        "Chihway Chang",
        "Aleksandra Ćiprijanović",
        "Johann Cohen-Tanugi",
        "Michael W. Coughlin",
        "John Franklin Crenshaw",
        "Juan C. Cuevas-Tello",
        "Juan de Vicente",
        "Seth W. Digel",
        "Steven Dillmann",
        "Mariano Javier de León Dominguez Romero",
        "Alex Drlica-Wagner",
        "Sydney Erickson",
        "Alexander T. Gagliano",
        "Christos Georgiou",
        "Aritra Ghosh",
        "Matthew Grayling",
        "Kirill A. Grishin",
        "Alan Heavens",
        "Lindsay R. House",
        "Mustapha Ishak",
        "Wassim Kabalan",
        "Arun Kannawadi",
        "François Lanusse",
        "C. Danielle Leonard",
        "Pierre-François Léget",
        "Michelle Lochner",
        "Yao-Yuan Mao",
        "Peter Melchior",
        "Grant Merz",
        "Martin Millon",
        "Anais Möller",
        "Gautham Narayan",
        "Yuuki Omori",
        "Hiranya Peiris",
        "Laurence Perreault-Levasseur",
        "Andrés A. Plazas Malagón",
        "Nesar Ramachandra",
        "Benjamin Remy",
        "Cécile Roucelle",
        "Jaime Ruiz-Zapatero",
        "Stefan Schuldt",
        "Ignacio Sevilla-Noarbe",
        "Ved G. Shah",
        "Tjitske Starkenburg",
        "Stephen Thorp",
        "Laura Toribio San Cipriano",
        "Tilman Tröster",
        "Roberto Trotta",
        "Padma Venkatraman",
        "Amanda Wasserman",
        "Tim White",
        "Justine Zeghal",
        "Tianqing Zhang",
        "Yuanyuan Zhang"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "abstract": "The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification to weak lensing inference and cosmological simulations. Yet their utility for precision cosmology hinges on trustworthy uncertainty quantification, robustness to covariate shift and model misspecification, and reproducible integration within scientific pipelines. This white paper surveys the current landscape of AI/ML across DESC's primary cosmological probes and cross-cutting analyses, revealing that the same core methodologies and fundamental challenges recur across disparate science cases. Since progress on these cross-cutting challenges would benefit multiple probes simultaneously, we identify key methodological research priorities, including Bayesian inference at scale, physics-informed methods, validation frameworks, and active learning for discovery. With an eye on emerging techniques, we also explore the potential of the latest foundation model methodologies and LLM-driven agentic AI systems to reshape DESC workflows, provided their deployment is coupled with rigorous evaluation and governance. Finally, we discuss critical software, computing, data infrastructure, and human capital requirements for the successful deployment of these new methodologies, and consider associated risks and opportunities for broader coordination with external actors.",
      "url": "https://arxiv.org/abs/2601.14235",
      "pdfUrl": "https://arxiv.org/pdf/2601.14235.pdf",
      "titleJa": "Rubin LSST ダークエネルギー科学コラボレーションにおける AI/ML の機会"
    },
    {
      "id": "2601.14234",
      "arxivId": "2601.14234",
      "title": "Q-learning with Adjoint Matching",
      "authors": [
        "Qiyang Li",
        "Sergey Levine"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "abstract": "We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.",
      "url": "https://arxiv.org/abs/2601.14234",
      "pdfUrl": "https://arxiv.org/pdf/2601.14234.pdf",
      "titleJa": "随伴マッチングによるQ学習"
    },
    {
      "id": "2601.14232",
      "arxivId": "2601.14232",
      "title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning",
      "authors": [
        "Egor Cherepanov",
        "Daniil Zelezetsky",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "abstract": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.",
      "url": "https://arxiv.org/abs/2601.14232",
      "pdfUrl": "https://arxiv.org/pdf/2601.14232.pdf",
      "titleJa": "KAGE-Bench: 強化学習のための高速な既知軸視覚一般化評価"
    },
    {
      "id": "2601.14230",
      "arxivId": "2601.14230",
      "title": "MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems",
      "authors": [
        "Yiyang Wang",
        "Yiqiao Jin",
        "Alex Cabral",
        "Josiah Hester"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "abstract": "Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.",
      "url": "https://arxiv.org/abs/2601.14230",
      "pdfUrl": "https://arxiv.org/pdf/2601.14230.pdf",
      "titleJa": "MASCOT: マルチエージェント社会協調コンパニオンシステムに向けて"
    },
    {
      "id": "2601.14209",
      "arxivId": "2601.14209",
      "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
      "authors": [
        "Matthew Y. R. Yang",
        "Hao Bai",
        "Ian Wu",
        "Gene Yang",
        "Amrith Setlur",
        "Aviral Kumar"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.",
      "url": "https://arxiv.org/abs/2601.14209",
      "pdfUrl": "https://arxiv.org/pdf/2601.14209.pdf",
      "titleJa": "InT: 自発的な介入により、LLM推論における単位の割り当てが可能になる"
    },
    {
      "id": "2601.14192",
      "arxivId": "2601.14192",
      "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
      "authors": [
        "Xiaofang Yang",
        "Lijun Li",
        "Heng Zhou",
        "Tong Zhu",
        "Xiaoye Qu",
        "Yuchen Fan",
        "Qianshan Wei",
        "Rui Ye",
        "Li Kang",
        "Yiran Qin",
        "Zhiqiang Kou",
        "Daizong Liu",
        "Qi Li",
        "Ning Ding",
        "Siheng Chen",
        "Jing Shao"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.",
      "url": "https://arxiv.org/abs/2601.14192",
      "pdfUrl": "https://arxiv.org/pdf/2601.14192.pdf",
      "titleJa": "効率的なエージェントに向けて：記憶、ツール学習、計画"
    },
    {
      "id": "2601.14175",
      "arxivId": "2601.14175",
      "title": "A model of errors in transformers",
      "authors": [
        "Suvrat Raju",
        "Praneeth Netrapalli"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "hep-th"
      ],
      "abstract": "We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.",
      "url": "https://arxiv.org/abs/2601.14175",
      "pdfUrl": "https://arxiv.org/pdf/2601.14175.pdf",
      "titleJa": "変圧器の誤差モデル"
    },
    {
      "id": "2601.14172",
      "arxivId": "2601.14172",
      "title": "Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum",
      "authors": [
        "Víctor Yeste",
        "Paolo Rosso"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task (\"does any value appear?\") and show that it is learnable from single sentences (positive-class F1 $\\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.",
      "url": "https://arxiv.org/abs/2601.14172",
      "pdfUrl": "https://arxiv.org/pdf/2601.14172.pdf",
      "titleJa": "ひと言で表す人間の価値：シュワルツ連続体における道徳的存在、階層構造、そして変容者集団"
    },
    {
      "id": "2601.14171",
      "arxivId": "2601.14171",
      "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
      "authors": [
        "Qianli Ma",
        "Chang Guo",
        "Zhiheng Tian",
        "Siyu Wang",
        "Jipeng Xiao",
        "Yuanhao Yue",
        "Zhipeng Zhang"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.",
      "url": "https://arxiv.org/abs/2601.14171",
      "pdfUrl": "https://arxiv.org/pdf/2601.14171.pdf",
      "titleJa": "Paper2Rebuttal: 透過的な著者応答支援のためのマルチエージェントフレームワーク"
    },
    {
      "id": "2601.14160",
      "arxivId": "2601.14160",
      "title": "Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law",
      "authors": [
        "Ali Hamza Bashir",
        "Muhammad Rehan Khalid",
        "Kostadin Cvejoski",
        "Jana Birr",
        "Jule Berghaus",
        "Armin Berger",
        "Sandra Halscheidt",
        "Christian Temath",
        "Rafet Sifa",
        "David Berghaus"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.",
      "url": "https://arxiv.org/abs/2601.14160",
      "pdfUrl": "https://arxiv.org/pdf/2601.14160.pdf",
      "titleJa": "合成データによるドメイン適応：ドイツ法のための大規模言語モデルの微調整"
    },
    {
      "id": "2601.14154",
      "arxivId": "2601.14154",
      "title": "LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery",
      "authors": [
        "Shubham Pandey",
        "Bhavin Jawade",
        "Srirangaraj Setlur",
        "Venu Govindaraju",
        "Kenneth Seastedt"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.",
      "url": "https://arxiv.org/abs/2601.14154",
      "pdfUrl": "https://arxiv.org/pdf/2601.14154.pdf",
      "titleJa": "肺癌手術における術後合併症予測のためのLLM拡張介入マルチモーダルアダプタ"
    },
    {
      "id": "2601.14152",
      "arxivId": "2601.14152",
      "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
      "authors": [
        "Hyunjong Ok",
        "Jaeho Lee"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.",
      "url": "https://arxiv.org/abs/2601.14152",
      "pdfUrl": "https://arxiv.org/pdf/2601.14152.pdf",
      "titleJa": "プロンプト順序の喪失：言語モデルにおける因果的注意の限界を明らかにする"
    },
    {
      "id": "2601.14124",
      "arxivId": "2601.14124",
      "title": "Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic",
      "authors": [
        "Saad Mankarious",
        "Aya Zirikly"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.",
      "url": "https://arxiv.org/abs/2601.14124",
      "pdfUrl": "https://arxiv.org/pdf/2601.14124.pdf",
      "titleJa": "バイアス緩和のためのスタイル転送：アラビア語向けメンタルヘルス合成テキストの拡散モデル"
    },
    {
      "id": "2601.14115",
      "arxivId": "2601.14115",
      "title": "Riemannian Liquid Spatio-Temporal Graph Network",
      "authors": [
        "Liangsi Lu",
        "Jingchao Wang",
        "Zhaorong Dai",
        "Hanqian Liu",
        "Yang Shi"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. Moreover, we provide rigorous theoretical guarantees for RLSTG, extending stability theorems of LTCs to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that, by combining advanced temporal dynamics with a Riemannian spatial representation, RLSTG achieves superior performance on graphs with complex structures. Project Page: https://rlstg.github.io",
      "url": "https://arxiv.org/abs/2601.14115",
      "pdfUrl": "https://arxiv.org/pdf/2601.14115.pdf",
      "titleJa": "リーマン液体時空間グラフネットワーク"
    },
    {
      "id": "2601.14099",
      "arxivId": "2601.14099",
      "title": "Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping",
      "authors": [
        "Shi-Shun Chen",
        "Xiao-Yang Li",
        "Enrico Zio"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.",
      "url": "https://arxiv.org/abs/2601.14099",
      "pdfUrl": "https://arxiv.org/pdf/2601.14099.pdf",
      "titleJa": "時間遅延クロスマッピングに基づく安定したソフトセンサーモデリングのための因果的特徴選択フレームワーク"
    },
    {
      "id": "2601.14096",
      "arxivId": "2601.14096",
      "title": "Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems",
      "authors": [
        "Benedikt Hartl",
        "Léo Pio-Lopez",
        "Chris Fields",
        "Michael Levin"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.AI"
      ],
      "abstract": "The emerging field of diverse intelligence seeks an integrated view of problem-solving in agents of very different provenance, composition, and substrates. From subcellular chemical networks to swarms of organisms, and across evolved, engineered, and chimeric systems, it is hypothesized that scale-invariant principles of decision-making can be discovered. We propose that cognition in both natural and synthetic systems can be characterized and understood by the interplay between two equally important invariants: (1) the remapping of embedding spaces, and (2) the navigation within these spaces. Biological collectives, from single cells to entire organisms (and beyond), remap transcriptional, morphological, physiological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating these spaces through distributed error correction. Modern Artificial Intelligence (AI) systems, including transformers, diffusion models, and neural cellular automata enact analogous processes by remapping data into latent embeddings and refining them iteratively through contextualization. We argue that this dual principle - remapping and navigation of embedding spaces via iterative error minimization - constitutes a substrate-independent invariant of cognition. Recognizing this shared mechanism not only illuminates deep parallels between living systems and artificial models, but also provides a unifying framework for engineering adaptive intelligence across scales.",
      "url": "https://arxiv.org/abs/2601.14096",
      "pdfUrl": "https://arxiv.org/pdf/2601.14096.pdf",
      "titleJa": "誤差最小化による埋め込み空間の再マッピングとナビゲーション：自然および人工システムにおける認知の基本的組織化原理"
    },
    {
      "id": "2601.14091",
      "arxivId": "2601.14091",
      "title": "Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems",
      "authors": [
        "Hossein Naderi",
        "Alireza Shojaei",
        "Lifu Huang",
        "Philip Agee",
        "Kereshmeh Afsari",
        "Abiola Akanmu"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "abstract": "Robots are expected to play a major role in the future construction industry but face challenges due to high costs and difficulty adapting to dynamic tasks. This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate the improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.",
      "url": "https://arxiv.org/abs/2601.14091",
      "pdfUrl": "https://arxiv.org/pdf/2601.14091.pdf",
      "titleJa": "自律建設ロボットのためのゼロショット適応型タスク計画：軽量単一および複数AIエージェントシステムの比較研究"
    },
    {
      "id": "2601.12314",
      "arxivId": "2601.12314",
      "title": "A Similarity Network for Correlating Musical Structure to Military Strategy",
      "authors": [
        "Yiwen Zhang",
        "Hui Zhang",
        "Fanqin Meng"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Music perception, a multi-sensory process based on the synesthesia effect, is an essential component of music aesthetic education. Understanding music structure helps both perception and aesthetic education. Music structure incorporates a range of information, the coordination of which forms the melody, just as different military actions cooperate to produce a military strategy. However, there are a few ways for assessing music perception from the perspectives of system operation and information management. In this paper, we explore the similarities between music structure and military strategy while creating the Music Clips Correlation Network (MCCN) based on Mel-frequency Cepstral Coefficients (MFCCs). The inspiration comes from the comparison between a concert conductor's musical score and a military war commander's sand table exercise. Specifically, we create MCCNs for various kinds of war movie soundtracks, then relate military tactics (Sun Tzu's Art of War, etc.) and political institutions to military operations networks. Our primary findings suggest a few similarities, implying that music perception and aesthetic education can be approached from a military strategy and management perspective through this interdisciplinary research. Similarly, we can discover similarities between the art of military scheming and the art of musical structure based on network analysis in order to facilitate the understanding of the relationship between technology and art.",
      "url": "https://arxiv.org/abs/2601.12314",
      "pdfUrl": "https://arxiv.org/pdf/2601.12314.pdf",
      "titleJa": "音楽構造と軍事戦略を相関させる類似性ネットワーク"
    },
    {
      "id": "2601.12245",
      "arxivId": "2601.12245",
      "title": "Sound2Hap: Learning Audio-to-Vibrotactile Haptic Generation from Human Ratings",
      "authors": [
        "Yinan Li",
        "Hasti Seifi"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Environmental sounds like footsteps, keyboard typing, or dog barking carry rich information and emotional context, making them valuable for designing haptics in user applications. Existing audio-to-vibration methods, however, rely on signal-processing rules tuned for music or games and often fail to generalize across diverse sounds. To address this, we first investigated user perception of four existing audio-to-haptic algorithms, then created a data-driven model for environmental sounds. In Study 1, 34 participants rated vibrations generated by the four algorithms for 1,000 sounds, revealing no consistent algorithm preferences. Using this dataset, we trained Sound2Hap, a CNN-based autoencoder, to generate perceptually meaningful vibrations from diverse sounds with low latency. In Study 2, 15 participants rated its output higher than signal-processing baselines on both audio-vibration match and Haptic Experience Index (HXI), finding it more harmonious with diverse sounds. This work demonstrates a perceptually validated approach to audio-haptic translation, broadening the reach of sound-driven haptics.",
      "url": "https://arxiv.org/abs/2601.12245",
      "pdfUrl": "https://arxiv.org/pdf/2601.12245.pdf",
      "titleJa": "Sound2Hap: 人間の評価から音声から振動触覚への触覚生成を学習する"
    },
    {
      "id": "2601.12222",
      "arxivId": "2601.12222",
      "title": "Song Aesthetics Evaluation with Multi-Stem Attention and Hierarchical Uncertainty Modeling",
      "authors": [
        "Yishan Lv",
        "Jing Luo",
        "Boyuan Ju",
        "Yang Zhang",
        "Xinda Wu",
        "Bo Yuan",
        "Xinyu Yang"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Music generative artificial intelligence (AI) is rapidly expanding music content, necessitating automated song aesthetics evaluation. However, existing studies largely focus on speech, audio or singing quality, leaving song aesthetics underexplored. Moreover, conventional approaches often predict a precise Mean Opinion Score (MOS) value directly, which struggles to capture the nuances of human perception in song aesthetics evaluation. This paper proposes a song-oriented aesthetics evaluation framework, featuring two novel modules: 1) Multi-Stem Attention Fusion (MSAF) builds bidirectional cross-attention between mixture-vocal and mixture-accompaniment pairs, fusing them to capture complex musical features; 2) Hierarchical Granularity-Aware Interval Aggregation (HiGIA) learns multi-granularity score probability distributions, aggregates them into a score interval, and applies a regression within the interval to produce the final score. We evaluated on two datasets of full-length songs: SongEval dataset (AI-generated) and an internal aesthetics dataset (human-created), and compared with two state-of-the-art (SOTA) models. Results show that the proposed method achieves stronger performance for multi-dimensional song aesthetics evaluation.",
      "url": "https://arxiv.org/abs/2601.12222",
      "pdfUrl": "https://arxiv.org/pdf/2601.12222.pdf",
      "titleJa": "マルチステムアテンションと階層的不確実性モデリングによる歌の美的評価"
    },
    {
      "id": "2601.12205",
      "arxivId": "2601.12205",
      "title": "Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks",
      "authors": [
        "Shih-Heng Wang",
        "Jiatong Shi",
        "Jinchuan Tian",
        "Haibin Wu",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "This paper investigates three crucial yet underexplored aspects of the generalization capabilities of neural audio codecs (NACs): (i) whether NACs can generalize to unseen languages during pre-training, (ii) whether speech-only pre-trained NACs can effectively generalize to non-speech applications such as environmental sounds, music, and animal vocalizations, and (iii) whether incorporating non-speech data during pre-training can improve performance on both speech and non-speech tasks. Existing studies typically rely on off-the-shelf NACs for comparison, which limits insight due to variations in implementation. In this work, we train NACs from scratch using strictly controlled configurations and carefully curated pre-training data to enable fair comparisons. We conduct a comprehensive evaluation of NAC performance on both signal reconstruction quality and downstream applications using 11 metrics. Our results show that NACs can generalize to unseen languages during pre-training, speech-only pre-trained NACs exhibit degraded performance on non-speech tasks, and incorporating non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks.",
      "url": "https://arxiv.org/abs/2601.12205",
      "pdfUrl": "https://arxiv.org/pdf/2601.12205.pdf",
      "titleJa": "ニューラルコーデックは一般化するか？未知の言語と非音声タスクを対象とした対照研究"
    },
    {
      "id": "2601.12180",
      "arxivId": "2601.12180",
      "title": "VidTune: Creating Video Soundtracks with Generative Music and Contextual Thumbnails",
      "authors": [
        "Mina Huh",
        "Ailie C. Fraser",
        "Dingzeyu Li",
        "Mira Dontcheva",
        "Bryan Wang"
      ],
      "publishedDate": "2026-01-17",
      "categories": [
        "cs.HC",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Music shapes the tone of videos, yet creators often struggle to find soundtracks that match their video's mood and narrative. Recent text-to-music models let creators generate music from text prompts, but our formative study (N=8) shows creators struggle to construct diverse prompts, quickly review and compare tracks, and understand their impact on the video. We present VidTune, a system that supports soundtrack creation by generating diverse music options from a creator's prompt and producing contextual thumbnails for rapid review. VidTune extracts representative video subjects to ground thumbnails in context, maps each track's valence and energy onto visual cues like color and brightness, and depicts prominent genres and instruments. Creators can refine tracks through natural language edits, which VidTune expands into new generations. In a controlled user study (N=12) and an exploratory case study (N=6), participants found VidTune helpful for efficiently reviewing and comparing music options and described the process as playful and enriching.",
      "url": "https://arxiv.org/abs/2601.12180",
      "pdfUrl": "https://arxiv.org/pdf/2601.12180.pdf",
      "titleJa": "VidTune: ジェネレーティブミュージックとコンテキストサムネイルを使ったビデオサウンドトラックの作成"
    },
    {
      "id": "2601.11968",
      "arxivId": "2601.11968",
      "title": "MuseAgent-1: Interactive Grounded Multimodal Understanding of Music Scores and Performance Audio",
      "authors": [
        "Qihao Zhao",
        "Yunqi Cao",
        "Yangyu Huang",
        "Hui Yi Leong",
        "Fan Zhang",
        "Kim-Hui Yap",
        "Wei Hu"
      ],
      "publishedDate": "2026-01-17",
      "categories": [
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Despite recent advances in multimodal large language models (MLLMs), their ability to understand and interact with music remains limited. Music understanding requires grounded reasoning over symbolic scores and expressive performance audio, which general-purpose MLLMs often fail to handle due to insufficient perceptual grounding. We introduce MuseAgent, a music-centric multimodal agent that augments language models with structured symbolic representations derived from sheet music images and performance audio. By integrating optical music recognition and automatic music transcription modules, MuseAgent enables multi-step reasoning and interaction over fine-grained musical content. To systematically evaluate music understanding capabilities, we further propose MuseBench, a benchmark covering music theory reasoning, score interpretation, and performance-level analysis across text, image, and audio modalities. Experiments show that existing MLLMs perform poorly on these tasks, while MuseAgent achieves substantial improvements, highlighting the importance of structured multimodal grounding for interactive music understanding.",
      "url": "https://arxiv.org/abs/2601.11968",
      "pdfUrl": "https://arxiv.org/pdf/2601.11968.pdf",
      "titleJa": "MuseAgent-1: 楽譜と演奏音声のインタラクティブなマルチモーダル理解"
    },
    {
      "id": "2601.11768",
      "arxivId": "2601.11768",
      "title": "Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music",
      "authors": [
        "Venkat Suprabath Bitra",
        "Homayoon Beigi"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "Reliable fundamental frequency (F 0) and voicing estimation is essential for neural synthesis, yet many pitch extractors depend on large labeled corpora and degrade under realistic recording artifacts. We propose a lightweight, fully self-supervised framework for joint F 0 estimation and voicing inference, designed for rapid single-instrument training from limited audio. Using transposition-equivariant learning on CQT features, we introduce an EM-style iterative reweighting scheme that uses Shift Cross-Entropy (SCE) consistency as a reliability signal to suppress uninformative noisy/unvoiced frames. The resulting weights provide confidence scores that enable pseudo-labeling for a separate lightweight voicing classifier without manual annotations. Trained on MedleyDB and evaluated on MDB-stem-synth ground truth, our method achieves competitive cross-corpus performance (RPA 95.84, RCA 96.24) and demonstrates cross-instrument generalization.",
      "url": "https://arxiv.org/abs/2601.11768",
      "pdfUrl": "https://arxiv.org/pdf/2601.11768.pdf",
      "titleJa": "モノフォニック音楽における基本周波数と正確な有声音確率の軽量自己教師検出"
    },
    {
      "id": "2601.11262",
      "arxivId": "2601.11262",
      "title": "Scalable Music Cover Retrieval Using Lyrics-Aligned Audio Embeddings",
      "authors": [
        "Joanne Affolter",
        "Benjamin Martin",
        "Elena V. Epure",
        "Gabriel Meseguer-Brocal",
        "Frédéric Kaplan"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG"
      ],
      "abstract": "Music Cover Retrieval, also known as Version Identification, aims to recognize distinct renditions of the same underlying musical work, a task central to catalog management, copyright enforcement, and music retrieval. State-of-the-art approaches have largely focused on harmonic and melodic features, employing increasingly complex audio pipelines designed to be invariant to musical attributes that often vary widely across covers. While effective, these methods demand substantial training time and computational resources. By contrast, lyrics constitute a strong invariant across covers, though their use has been limited by the difficulty of extracting them accurately and efficiently from polyphonic audio. Early methods relied on simple frameworks that limited downstream performance, while more recent systems deliver stronger results but require large models integrated within complex multimodal architectures. We introduce LIVI (Lyrics-Informed Version Identification), an approach that seeks to balance retrieval accuracy with computational efficiency. First, LIVI leverages supervision from state-of-the-art transcription and text embedding models during training to achieve retrieval accuracy on par with--or superior to--harmonic-based systems. Second, LIVI remains lightweight and efficient by removing the transcription step at inference, challenging the dominance of complexity-heavy pipelines.",
      "url": "https://arxiv.org/abs/2601.11262",
      "pdfUrl": "https://arxiv.org/pdf/2601.11262.pdf",
      "titleJa": "歌詞に合わせたオーディオ埋め込みを用いたスケーラブルな音楽カバー検索"
    },
    {
      "id": "2601.10547",
      "arxivId": "2601.10547",
      "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
      "authors": [
        "Dongchao Yang",
        "Yuxin Xie",
        "Yuguo Yin",
        "Zheyu Wang",
        "Xiaoyu Yi",
        "Gongxi Zhu",
        "Xiaolong Weng",
        "Zihan Xiong",
        "Yingzhe Ma",
        "Dading Cong",
        "Jingliang Liu",
        "Zihang Huang",
        "Jinghan Ru",
        "Rongjie Huang",
        "Haoran Wan",
        "Peixu Wang",
        "Kuoxi Yu",
        "Helin Wang",
        "Liming Liang",
        "Xianwei Zhuang",
        "Yuanyuan Wang",
        "Haohan Guo",
        "Junjie Cao",
        "Zeqian Ju",
        "Songxiang Liu",
        "Yuewen Cao",
        "Heming Weng",
        "Yuexian Zou"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD"
      ],
      "abstract": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio-text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "url": "https://arxiv.org/abs/2601.10547",
      "pdfUrl": "https://arxiv.org/pdf/2601.10547.pdf",
      "titleJa": "HeartMuLa: オープンソースの音楽基盤モデルファミリー"
    },
    {
      "id": "2601.09603",
      "arxivId": "2601.09603",
      "title": "Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer",
      "authors": [
        "Petros Vavaroutsos",
        "Theodoros Palamas",
        "Pantelis Vikatos"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "In recent years, foundation models have become very popular due to their exceptional performance, mainly in natural language (NLP) tasks where they were first introduced. These models usually consist of hundreds of millions, or even billions, of parameters, making them resource-intensive during training and in production systems, leading to increased costs. This paper focuses on the reduction of a foundation's model size when applied to music information retrieval (MIR) tasks. Our research combines the Branchformer architecture with SummaryMixing, which were first applied in speech recognition, along with a random quantization process. To facilitate reproducibility, we conduct pre-training on publicly available datasets, complemented by a proprietary dataset comparable in scale to other private datasets reported in the literature. We ensure robust evaluation by using a framework consisting of a variety of downstream MIR tasks. Our results show that our architecture achieves competitive performance when compared with other state-of-the-art models that use multi-head self-attention, while reducing the model size from 8.5% up to 12.3%.",
      "url": "https://arxiv.org/abs/2601.09603",
      "pdfUrl": "https://arxiv.org/pdf/2601.09603.pdf",
      "titleJa": "ランダム量子化器を用いた音楽理解のための線形複雑度自己教師学習"
    },
    {
      "id": "2601.09461",
      "arxivId": "2601.09461",
      "title": "Analysis of the Maximum Prediction Gain of Short-Term Prediction on Sustained Speech",
      "authors": [
        "Reemt Hinrichs",
        "Muhamad Fadli Damara",
        "Stephan Preihs",
        "Jörn Ostermann"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Signal prediction is widely used in, e.g., economic forecasting, echo cancellation and in data compression, particularly in predictive coding of speech and music. Predictive coding algorithms reduce the bit-rate required for data transmission or storage by signal prediction. The prediction gain is a classic measure in applied signal coding of the quality of a predictor, as it links the mean-squared prediction error to the signal-to-quantization-noise of predictive coders. To evaluate predictor models, knowledge about the maximum achievable prediction gain independent of a predictor model is desirable. In this manuscript, Nadaraya-Watson kernel-regression (NWKR) and an information theoretic upper bound are applied to analyze the upper bound of the prediction gain on a newly recorded dataset of sustained speech/phonemes. It was found that for unvoiced speech a linear predictor always achieves the maximum prediction gain within at most 0.3 dB. On voiced speech, the optimum one-tap predictor was found to be linear but starting with two taps, the maximum achievable prediction gain was found to be about 2 dB to 6 dB above the prediction gain of the linear predictor. Significant differences between speakers/subjects were observed. The created dataset as well as the code can be obtained for research purpose upon request.",
      "url": "https://arxiv.org/abs/2601.09461",
      "pdfUrl": "https://arxiv.org/pdf/2601.09461.pdf",
      "titleJa": "持続音声における短期予測の最大予測利得の分析"
    },
    {
      "id": "2601.09385",
      "arxivId": "2601.09385",
      "title": "SLAM-LLM: A Modular, Open-Source Multimodal Large Language Model Framework and Best Practice for Speech, Language, Audio and Music Processing",
      "authors": [
        "Ziyang Ma",
        "Guanrou Yang",
        "Wenxi Chen",
        "Zhifu Gao",
        "Yexing Du",
        "Xiquan Li",
        "Zhisheng Zheng",
        "Haina Zhu",
        "Jianheng Zhuo",
        "Zheshu Song",
        "Ruiyang Xu",
        "Tiranrui Wang",
        "Yifan Yang",
        "Yanqiao Zhu",
        "Zhikang Niu",
        "Liumeng Xue",
        "Yinghao Ma",
        "Ruibin Yuan",
        "Shiliang Zhang",
        "Kai Yu",
        "Eng Siong Chng",
        "Xie Chen"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.MM"
      ],
      "abstract": "The recent surge in open-source Multimodal Large Language Models (MLLM) frameworks, such as LLaVA, provides a convenient kickoff for artificial intelligence developers and researchers. However, most of the MLLM frameworks take vision as the main input modality, and provide limited in-depth support for the modality of speech, audio, and music. This situation hinders the development of audio-language models, and forces researchers to spend a lot of effort on code writing and hyperparameter tuning. We present SLAM-LLM, an open-source deep learning framework designed to train customized MLLMs, focused on speech, language, audio, and music processing. SLAM-LLM provides a modular configuration of different encoders, projectors, LLMs, and parameter-efficient fine-tuning plugins. SLAM-LLM also includes detailed training and inference recipes for mainstream tasks, along with high-performance checkpoints like LLM-based Automatic Speech Recognition (ASR), Automated Audio Captioning (AAC), and Music Captioning (MC). Some of these recipes have already reached or are nearing state-of-the-art performance, and some relevant techniques have also been accepted by academic papers. We hope SLAM-LLM will accelerate iteration, development, data engineering, and model training for researchers. We are committed to continually pushing forward audio-based MLLMs through this open-source framework, and call on the community to contribute to the LLM-based speech, audio and music processing.",
      "url": "https://arxiv.org/abs/2601.09385",
      "pdfUrl": "https://arxiv.org/pdf/2601.09385.pdf",
      "titleJa": "SLAM-LLM: 音声、言語、オーディオ、音楽処理のためのモジュール式オープンソースマルチモーダル大規模言語モデルフレームワークとベストプラクティス"
    },
    {
      "id": "2601.09333",
      "arxivId": "2601.09333",
      "title": "Research on Piano Timbre Transformation System Based on Diffusion Model",
      "authors": [
        "Chun-Chieh Hsu",
        "Tsai-Ling Hsu",
        "Chen-Chen Yeh",
        "Shao-Chien Lu",
        "Cheng-Han Wu",
        "Bing-Ze Liu",
        "Timothy K. Shih",
        "Yu-Cheng Lin"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.MM"
      ],
      "abstract": "We propose a timbre conversion model based on the Diffusion architecture de-signed to precisely translate music played by various instruments into piano ver-sions. The model employs a Pitch Encoder and Loudness Encoder to extract pitch and loudness features of the music, which serve as conditional inputs to the Dif-fusion Model's decoder, generating high-quality piano timbres. Case analysis re-sults show that the model performs excellently in terms of pitch accuracy and timbral similarity, maintaining stable conversion across different musical styles (classical, jazz, pop) and lengths (from short clips to full pieces). Particularly, the model maintains high sound quality and accuracy even when dealing with rapidly changing notes and complex musical structures, demonstrating good generaliza-tion capability. Additionally, the model has the potential for real-time musical conversion and is suitable for live performances and digital music creation tools. Future research will focus on enhancing the handling of loudness dynamics and incorporating additional musical features (such as timbral variations and rhythmic complexity) to improve the model's adaptability and expressiveness. We plan to explore the model's application potential in other timbre conversion tasks, such as converting vocals to instrumental sounds or integration with MIDI digital pianos, further expanding the application scope of the Diffusion-based timbre conversion model in the field of music generation.",
      "url": "https://arxiv.org/abs/2601.09333",
      "pdfUrl": "https://arxiv.org/pdf/2601.09333.pdf",
      "titleJa": "拡散モデルに基づくピアノ音色変換システムの研究"
    },
    {
      "id": "2601.08764",
      "arxivId": "2601.08764",
      "title": "FusID: Modality-Fused Semantic IDs for Generative Music Recommendation",
      "authors": [
        "Haven Kim",
        "Yupeng Hou",
        "Julian McAuley"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.IR",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Generative recommendation systems have achieved significant advances by leveraging semantic IDs to represent items. However, existing approaches that tokenize each modality independently face two critical limitations: (1) redundancy across modalities that reduces efficiency, and (2) failure to capture inter-modal interactions that limits item representation. We introduce FusID, a modality-fused semantic ID framework that addresses these limitations through three key components: (i) multimodal fusion that learns unified representations by jointly encoding information across modalities, (ii) representation learning that brings frequently co-occurring item embeddings closer while maintaining distinctiveness and preventing feature redundancy, and (iii) product quantization that converts the fused continuous embeddings into multiple discrete tokens to mitigate ID conflict. Evaluated on a multimodal next-song recommendation (i.e., playlist continuation) benchmark, FusID achieves zero ID conflicts, ensuring that each token sequence maps to exactly one song, mitigates codebook underutilization, and outperforms baselines in terms of MRR and Recall@k (k = 1, 5, 10, 20).",
      "url": "https://arxiv.org/abs/2601.08764",
      "pdfUrl": "https://arxiv.org/pdf/2601.08764.pdf",
      "titleJa": "FusID: 生成的音楽推薦のためのモダリティ融合セマンティックID"
    },
    {
      "id": "2601.08537",
      "arxivId": "2601.08537",
      "title": "Weakly Supervised Tabla Stroke Transcription via TI-SDRM: A Rhythm-Aware Lattice Rescoring Framework",
      "authors": [
        "Rahul Bapusaheb Kodag",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Tabla Stroke Transcription (TST) is central to the analysis of rhythmic structure in Hindustani classical music, yet remains challenging due to complex rhythmic organization and the scarcity of strongly annotated data. Existing approaches largely rely on fully supervised learning with onset-level annotations, which are costly and impractical at scale. This work addresses TST in a weakly supervised setting, using only symbolic stroke sequences without temporal alignment. We propose a framework that combines a CTC-based acoustic model with sequence-level rhythmic rescoring. The acoustic model produces a decoding lattice, which is refined using a \\textbf{$T\\bar{a}la$}-Independent Static--Dynamic Rhythmic Model (TI-SDRM) that integrates long-term rhythmic structure with short-term adaptive dynamics through an adaptive interpolation mechanism. We curate a new real-world tabla solo dataset and a complementary synthetic dataset, establishing the first benchmark for weakly supervised TST in Hindustani classical music. Experiments demonstrate consistent and substantial reductions in stroke error rate over acoustic-only decoding, confirming the importance of explicit rhythmic structure for accurate transcription.",
      "url": "https://arxiv.org/abs/2601.08537",
      "pdfUrl": "https://arxiv.org/pdf/2601.08537.pdf",
      "titleJa": "TI-SDRMによる弱教師付きタブラストローク転写：リズムを考慮したラティス再採点フレームワーク"
    },
    {
      "id": "2601.12752",
      "arxivId": "2601.12752",
      "title": "SoundPlot: An Open-Source Framework for Birdsong Acoustic Analysis and Neural Synthesis with Interactive 3D Visualization",
      "authors": [
        "Naqcho Ali Mehdi",
        "Mohammad Adeel",
        "Aizaz Ali Larik"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "We present SoundPlot, an open-source framework for analyzing avian vocalizations through acoustic feature extraction, dimensionality reduction, and neural audio synthesis. The system transforms audio signals into a multi-dimensional acoustic feature space, enabling real-time visualization of temporal dynamics in 3D using web-based interactive graphics. Our framework implements a complete analysis-synthesis pipeline that extracts spectral features (centroid, bandwidth, contrast), pitch contours via probabilistic YIN (pYIN), and mel-frequency cepstral coefficients (MFCCs), mapping them to a unified timbre space for visualization. Audio reconstruction employs the Griffin-Lim phase estimation algorithm applied to mel spectrograms. The accompanying Three.js-based interface provides dual-viewport visualization comparing original and synthesized audio trajectories with independent playback controls. We demonstrate the framework's capabilities through comprehensive waveform analysis, spectrogram comparisons, and feature space evaluation using Principal Component Analysis (PCA). Quantitative evaluation shows mel spectrogram correlation scores exceeding 0.92, indicating high-fidelity preservation of perceptual acoustic structure. SoundPlot is released under the MIT License to facilitate research in bioacoustics, audio signal processing, and computational ethology.",
      "url": "https://arxiv.org/abs/2601.12752",
      "pdfUrl": "https://arxiv.org/pdf/2601.12752.pdf",
      "titleJa": "SoundPlot: インタラクティブな3D可視化による鳥のさえずりの音響分析とニューラルネットワーク合成のためのオープンソースフレームワーク"
    },
    {
      "id": "2601.12660",
      "arxivId": "2601.12660",
      "title": "Toward Faithful Explanations in Acoustic Anomaly Detection",
      "authors": [
        "Maab Elrashid",
        "Anthony Deschênes",
        "Cem Subakan",
        "Mirco Ravanelli",
        "Rémi Georges",
        "Michael Morin"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Interpretability is essential for user trust in real-world anomaly detection applications. However, deep learning models, despite their strong performance, often lack transparency. In this work, we study the interpretability of autoencoder-based models for audio anomaly detection, by comparing a standard autoencoder (AE) with a mask autoencoder (MAE) in terms of detection performance and interpretability. We applied several attribution methods, including error maps, saliency maps, SmoothGrad, Integrated Gradients, GradSHAP, and Grad-CAM. Although MAE shows a slightly lower detection, it consistently provides more faithful and temporally precise explanations, suggesting a better alignment with true anomalies. To assess the relevance of the regions highlighted by the explanation method, we propose a perturbation-based faithfulness metric that replaces them with their reconstructions to simulate normal input. Our findings, based on experiments in a real industrial scenario, highlight the importance of incorporating interpretability into anomaly detection pipelines and show that masked training improves explanation quality without compromising performance.",
      "url": "https://arxiv.org/abs/2601.12660",
      "pdfUrl": "https://arxiv.org/pdf/2601.12660.pdf",
      "titleJa": "音響異常検知における忠実な説明に向けて"
    },
    {
      "id": "2601.12600",
      "arxivId": "2601.12600",
      "title": "SSVD-O: Parameter-Efficient Fine-Tuning with Structured SVD for Speech Recognition",
      "authors": [
        "Pu Wang",
        "Shinji Watanabe",
        "Hugo Van hamme"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) is a scalable approach for adapting large speech foundation models to new domains. While methods such as LoRA and its state-of-the-art variants reduce adaptation costs, they typically allocate parameters uniformly across model subspaces, which limits their efficiency and scalability in speech applications. Building on our prior work, this paper introduces SSVD-Outer (SSVD-O), an extension of the structured SVD-guided (SSVD) fine-tuning method. SSVD-O combines input acoustic feature space-associated inner transformations with output semantic feature space-associated outer transformations to enable scalable and balanced adaptation. We conduct the first systematic analysis of parameter budget allocation across model subspaces in PEFT for automatic speech recognition (ASR), and investigate the trade-off between learning and forgetting under constrained resources. SSVD-O is benchmarked against LoRA, DoRA, PiSSA, and SSVD on domain-shifted ASR tasks, including child speech and regional accents, across model scales from 0.1B to 2B within the ESPnet framework. Experimental results show that SSVD-O consistently narrows the performance gap to full fine-tuning while improving generalization and mitigating catastrophic forgetting.",
      "url": "https://arxiv.org/abs/2601.12600",
      "pdfUrl": "https://arxiv.org/pdf/2601.12600.pdf",
      "titleJa": "SSVD-O: 音声認識のための構造化SVDによるパラメータ効率の高い微調整"
    },
    {
      "id": "2601.12494",
      "arxivId": "2601.12494",
      "title": "Harmonizing the Arabic Audio Space with Data Scheduling",
      "authors": [
        "Hunzalah Hassan Bhatti",
        "Firoj Alam",
        "Shammur Absar Chowdhury"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments.",
      "url": "https://arxiv.org/abs/2601.12494",
      "pdfUrl": "https://arxiv.org/pdf/2601.12494.pdf",
      "titleJa": "データスケジューリングによるアラビア語オーディオ空間の調和"
    },
    {
      "id": "2601.12480",
      "arxivId": "2601.12480",
      "title": "A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation",
      "authors": [
        "Hanchen Pei",
        "Shujie Liu",
        "Yanqing Liu",
        "Jianwei Yu",
        "Yuanhang Qian",
        "Gongping Huang",
        "Sheng Zhao",
        "Yan Lu"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Neural codec language models achieve impressive zero-shot Text-to-Speech (TTS) by fully imitating the acoustic characteristics of a short speech prompt, including timbre, prosody, and paralinguistic information. However, such holistic imitation limits their ability to isolate and control individual attributes. In this paper, we present a unified codec language model SpeechEdit that extends zero-shot TTS with a selective control mechanism. By default, SpeechEdit reproduces the complete acoustic profile inferred from the speech prompt, but it selectively overrides only the attributes specified by explicit control instructions. To enable controllable modeling, SpeechEdit is trained on our newly constructed LibriEdit dataset, which provides delta (difference-aware) training pairs derived from LibriHeavy. Experimental results show that our approach maintains naturalness and robustness while offering flexible and localized control over desired attributes. Audio samples are available at https://speech-editing.github.io/speech-editing/.",
      "url": "https://arxiv.org/abs/2601.12480",
      "pdfUrl": "https://arxiv.org/pdf/2601.12480.pdf",
      "titleJa": "選択的に編集可能なテキスト音声生成のための統合ニューラルコーデック言語モデル"
    },
    {
      "id": "2601.12354",
      "arxivId": "2601.12354",
      "title": "Bone-conduction Guided Multimodal Speech Enhancement with Conditional Diffusion Models",
      "authors": [
        "Sina Khanagha",
        "Bunlong Lay",
        "Timo Gerkmann"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Single-channel speech enhancement models face significant performance degradation in extremely noisy environments. While prior work has shown that complementary bone-conducted speech can guide enhancement, effective integration of this noise-immune modality remains a challenge. This paper introduces a novel multimodal speech enhancement framework that integrates bone-conduction sensors with air-conducted microphones using a conditional diffusion model. Our proposed model significantly outperforms previously established multimodal techniques and a powerful diffusion-based single-modal baseline across a wide range of acoustic conditions.",
      "url": "https://arxiv.org/abs/2601.12354",
      "pdfUrl": "https://arxiv.org/pdf/2601.12354.pdf",
      "titleJa": "条件付き拡散モデルを用いた骨伝導誘導マルチモーダル音声強調"
    },
    {
      "id": "2601.12345",
      "arxivId": "2601.12345",
      "title": "Adaptive Rotary Steering with Joint Autoregression for Robust Extraction of Closely Moving Speakers in Dynamic Scenarios",
      "authors": [
        "Jakob Kienegger",
        "Timo Gerkmann"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Latest advances in deep spatial filtering for Ambisonics demonstrate strong performance in stationary multi-speaker scenarios by rotating the sound field toward a target speaker prior to multi-channel enhancement. For applicability in dynamic acoustic conditions with moving speakers, we propose to automate this rotary steering using an interleaved tracking algorithm conditioned on the target's initial direction. However, for nearby or crossing speakers, robust tracking becomes difficult and spatial cues less effective for enhancement. By incorporating the processed recording as additional guide into both algorithms, our novel joint autoregressive framework leverages temporal-spectral correlations of speech to resolve spatially challenging speaker constellations. Consequently, our proposed method significantly improves tracking and enhancement of closely spaced speakers, consistently outperforming comparable non-autoregressive methods on a synthetic dataset. Real-world recordings complement these findings in complex scenarios with multiple speaker crossings and varying speaker-to-array distances.",
      "url": "https://arxiv.org/abs/2601.12345",
      "pdfUrl": "https://arxiv.org/pdf/2601.12345.pdf",
      "titleJa": "動的シナリオにおける近接して移動する話者のロバストな抽出のためのジョイント自己回帰を用いた適応型ロータリーステアリング"
    },
    {
      "id": "2601.12203",
      "arxivId": "2601.12203",
      "title": "Embryonic Exposure to VPA Influences Chick Vocalisations: A Computational Study",
      "authors": [
        "Antonella M. C. Torrisi",
        "Inês Nolasco",
        "Paola Sgadò",
        "Elisabetta Versace",
        "Emmanouil Benetos"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In young animals like poultry chicks (Gallus gallus), vocalisations convey information about affective and behavioural states. Traditional approaches to vocalisation analysis, relying on manual annotation and predefined categories, introduce biases, limit scalability, and fail to capture the full complexity of vocal repertoires. We introduce a computational framework for the automated detection, acoustic feature extraction, and unsupervised learning of chick vocalisations. Applying this framework to a dataset of newly hatched chicks, we identified two primary vocal clusters. We then tested our computational framework on an independent dataset of chicks exposed during embryonic development to vehicle or Valproic Acid (VPA), a compound that disrupts neural development and is linked to autistic-like symptoms. Clustering analysis on the experimental dataset confirmed two primary vocal clusters and revealed systematic differences between groups. VPA-exposed chicks showed an altered repertoire, with a relative increase in softer calls. VPA differentially affected call clusters, modulating temporal, frequency, and energy domain features. Overall, VPA-exposed chicks produced vocalisations with shorter duration, reduced pitch variability, and modified energy profiles, with the strongest alterations observed in louder calls. This study provides a computational framework for analysing animal vocalisations, advancing knowledge of early-life communication in typical and atypical vocal development.",
      "url": "https://arxiv.org/abs/2601.12203",
      "pdfUrl": "https://arxiv.org/pdf/2601.12203.pdf",
      "titleJa": "胎児期のVPA曝露がニワトリの発声に影響を与える：計算論的研究"
    },
    {
      "id": "2601.12153",
      "arxivId": "2601.12153",
      "title": "A Survey on 30+ Years of Automatic Singing Assessment and Singing Information Processing",
      "authors": [
        "Arthur N. dos Santos",
        "Bruno S. Masiero"
      ],
      "publishedDate": "2026-01-17",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Automatic Singing Assessment and Singing Information Processing have evolved over the past three decades to support singing pedagogy, performance analysis, and vocal training. While the first approach objectively evaluates a singer's performance through computational metrics ranging from real-time visual feedback and acoustical biofeedback to sophisticated pitch tracking and spectral analysis, the latter method compares a predictor vocal signal with a target reference to capture nuanced data embedded in the singing voice. Notable advancements include the development of interactive systems that have significantly improved real-time visual feedback, and the integration of machine learning and deep neural network architectures that enhance the precision of vocal signal processing. This survey critically examines the literature to map the historical evolution of these technologies, while identifying and discussing key gaps. The analysis reveals persistent challenges, such as the lack of standardized evaluation frameworks, difficulties in reliably separating vocal signals from various noise sources, and the underutilization of advanced digital signal processing and artificial intelligence methodologies for capturing artistic expressivity. By detailing these limitations and the corresponding technological advances, this review demonstrates how addressing these issues can bridge the gap between objective computational assessments and subjective human-like evaluations of singing performance, ultimately enhancing both the technical accuracy and pedagogical relevance of automated singing evaluation systems.",
      "url": "https://arxiv.org/abs/2601.12153",
      "pdfUrl": "https://arxiv.org/pdf/2601.12153.pdf",
      "titleJa": "30年以上にわたる自動歌唱評価と歌唱情報処理に関する調査"
    },
    {
      "id": "2601.10384",
      "arxivId": "2601.10384",
      "title": "RSA-Bench: Benchmarking Audio Large Models in Real-World Acoustic Scenarios",
      "authors": [
        "Yibo Zhang",
        "Liang Lin",
        "Kaiwen Luo",
        "Shilinlu Yan",
        "Jin Wang",
        "Yaoqi Guo",
        "Yitian Chen",
        "Yalan Qin",
        "Zhenhong Zhou",
        "Kun Wang",
        "Li Sun"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD"
      ],
      "abstract": "While Audio Large Models (ALMs) have achieved remarkable proficiency, their robustness remains brittle in real-world deployment. Existing evaluations largely rely on synthetic Gaussian noise or simplistic single-source interference, failing to capture the intricate, multi-layered acoustic dynamics -- or ``Acoustic Ecology'' -- that characterize authentic physical environments. To bridge this ecological gap, we introduce \\textbf{RSA-Bench}, a comprehensive robustness benchmark designed to stress-test ALLMs through high-fidelity auditory scene simulations. Unlike traditional methods, we construct evaluation samples by naturally superimposing diverse environmental soundscapes -- spanning \\textit{Pasture}, \\textit{Extreme Weather}, \\textit{Classroom}, and \\textit{Outdoors} -- onto clean speech signals across a spectrum of interference intensities. By evaluating models on six core tasks ranging from fundamental perception to complex reasoning, our study unveils three macro-level insights: \\textbf{(I) The Perception-Cognition Gap:} Models maintain relative resilience in low-level recognition but suffer a \\textbf{functional collapse} in high-order reasoning tasks under stress; \\textbf{(II) Scenario Sensitivity:} ``Vocal-like'' interference (e.g., background laughter) proves significantly more destructive than mechanical noise, challenging the model's auditory attention mechanisms; and \\textbf{(III) The Denoising Paradox:} Standard speech enhancement often exacerbates performance degradation, as ALLMs prove highly sensitive to the semantic distortions introduced by denoising artifacts.",
      "url": "https://arxiv.org/abs/2601.10384",
      "pdfUrl": "https://arxiv.org/pdf/2601.10384.pdf",
      "titleJa": "RSA-Bench: 実世界の音響シナリオにおける大規模オーディオモデルのベンチマーク"
    },
    {
      "id": "2601.10345",
      "arxivId": "2601.10345",
      "title": "Self-supervised restoration of singing voice degraded by pitch shifting using shallow diffusion",
      "authors": [
        "Yunyi Liu",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Pitch shifting has been an essential feature in singing voice production. However, conventional signal processing approaches exhibit well known trade offs such as formant shifts and robotic coloration that becomes more severe at larger transposition jumps. This paper targets high quality pitch shifting for singing by reframing it as a restoration problem: given an audio track that has been pitch shifted (and thus contaminated by artifacts), we recover a natural sounding performance while preserving its melody and timing. Specifically, we use a lightweight, mel space diffusion model driven by frame level acoustic features such as f0, volume, and content features. We construct training pairs in a self supervised manner by applying pitch shifts and reversing them to simulate realistic artifacts while retaining ground truth. On a curated singing set, the proposed approach substantially reduces pitch shift artifacts compared to representative classical baselines, as measured by both statistical metrics and pairwise acoustic measures. The results suggest that restoration based pitch shifting could be a viable approach towards artifact resistant transposition in vocal production workflows.",
      "url": "https://arxiv.org/abs/2601.10345",
      "pdfUrl": "https://arxiv.org/pdf/2601.10345.pdf",
      "titleJa": "浅い拡散を用いたピッチシフトによって劣化した歌声の自己教師あり復元"
    },
    {
      "id": "2601.09931",
      "arxivId": "2601.09931",
      "title": "Diffusion-based Frameworks for Unsupervised Speech Enhancement",
      "authors": [
        "Jean-Eudes Ayilo",
        "Mostafa Sadeghi",
        "Romain Serizel",
        "Xavier Alameda-Pineda"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This paper addresses $\\textit{unsupervised}$ diffusion-based single-channel speech enhancement (SE). Prior work in this direction combines a score-based diffusion model trained on clean speech with a Gaussian noise model whose covariance is structured by non-negative matrix factorization (NMF). This combination is used within an iterative expectation-maximization (EM) scheme, in which a diffusion-based posterior-sampling E-step estimates the clean speech. We first revisit this framework and propose to explicitly model both speech and acoustic noise as latent variables, jointly sampling them in the E-step instead of sampling speech alone as in previous approaches. We then introduce a new unsupervised SE framework that replaces the NMF noise prior with a diffusion-based noise model, learned jointly with the speech prior in a single conditional score model. Within this framework, we derive two variants: one that implicitly accounts for noise and one that explicitly treats noise as a latent variable. Experiments on WSJ0-QUT and VoiceBank-DEMAND show that explicit noise modeling systematically improves SE performance for both NMF-based and diffusion-based noise priors. Under matched conditions, the diffusion-based noise model attains the best overall quality and intelligibility among unsupervised methods, while under mismatched conditions the proposed NMF-based explicit-noise framework is more robust and suffers less degradation than several supervised baselines. Our code will be publicly available on this $\\href{https://github.com/jeaneudesAyilo/enudiffuse}{URL}$.",
      "url": "https://arxiv.org/abs/2601.09931",
      "pdfUrl": "https://arxiv.org/pdf/2601.09931.pdf",
      "titleJa": "教師なし音声強調のための拡散ベースフレームワーク"
    },
    {
      "id": "2601.09520",
      "arxivId": "2601.09520",
      "title": "Towards Realistic Synthetic Data for Automatic Drum Transcription",
      "authors": [
        "Pierfrancesco Melucci",
        "Paolo Merialdo",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Deep learning models define the state-of-the-art in Automatic Drum Transcription (ADT), yet their performance is contingent upon large-scale, paired audio-MIDI datasets, which are scarce. Existing workarounds that use synthetic data often introduce a significant domain gap, as they typically rely on low-fidelity SoundFont libraries that lack acoustic diversity. While high-quality one-shot samples offer a better alternative, they are not available in a standardized, large-scale format suitable for training. This paper introduces a new paradigm for ADT that circumvents the need for paired audio-MIDI training data. Our primary contribution is a semi-supervised method to automatically curate a large and diverse corpus of one-shot drum samples from unlabeled audio sources. We then use this corpus to synthesize a high-quality dataset from MIDI files alone, which we use to train a sequence-to-sequence transcription model. We evaluate our model on the ENST and MDB test sets, where it achieves new state-of-the-art results, significantly outperforming both fully supervised methods and previous synthetic-data approaches. The code for reproducing our experiments is publicly available at https://github.com/pier-maker92/ADT_STR",
      "url": "https://arxiv.org/abs/2601.09520",
      "pdfUrl": "https://arxiv.org/pdf/2601.09520.pdf",
      "titleJa": "自動ドラム転写のための現実的な合成データに向けて"
    }
  ],
  "lastUpdated": "2026-01-22T00:54:06.090954",
  "totalCount": 82
}