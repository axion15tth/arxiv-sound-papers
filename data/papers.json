{
  "papers": [
    {
      "id": "2602.02413",
      "arxivId": "2602.02413",
      "title": "Masked Autoencoders as Universal Speech Enhancer",
      "authors": [
        "Rajalaxmi Rajagopalan",
        "Ritwik Giri",
        "Zhiqiang Tang",
        "Kyu Han"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Supervised speech enhancement methods have been very successful. However, in practical scenarios, there is a lack of clean speech, and self-supervised learning-based (SSL) speech enhancement methods that offer comparable enhancement performance and can be applied to other speech-related downstream applications are desired. In this work, we develop a masked autoencoder based universal speech enhancer that is agnostic to the type of distortion affecting speech, can handle multiple distortions simultaneously, and is trained in a self-supervised manner. An augmentation stack adds further distortions to the noisy input data. The masked autoencoder model learns to remove the added distortions along with reconstructing the masked regions of the spectrogram during pre-training. The pre-trained embeddings are then used by fine-tuning models trained on a small amount of paired data for specific downstream tasks. We evaluate the pre-trained features for denoising and dereverberation downstream tasks. We explore different augmentations (like single or multi-speaker) in the pre-training augmentation stack and the effect of different noisy input feature representations (like $log1p$ compression) on pre-trained embeddings and downstream fine-tuning enhancement performance. We show that the proposed method not only outperforms the baseline but also achieves state-of-the-art performance for both in-domain and out-of-domain evaluation datasets.",
      "url": "https://arxiv.org/abs/2602.02413",
      "pdfUrl": "https://arxiv.org/pdf/2602.02413.pdf",
      "titleJa": "ユニバーサルスピーチエンハンサーとしてのマスクされたオートエンコーダー"
    },
    {
      "id": "2602.02286",
      "arxivId": "2602.02286",
      "title": "DFKI-Speech System for WildSpoof Challenge: A robust framework for SASV In-the-Wild",
      "authors": [
        "Arnab Das",
        "Yassine El Kheir",
        "Enes Erdem Erdogan",
        "Feidi Kallel",
        "Tim Polzehl",
        "Sebastian Moeller"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "This paper presents the DFKI-Speech system developed for the WildSpoof Challenge under the Spoofing aware Automatic Speaker Verification (SASV) track. We propose a robust SASV framework in which a spoofing detector and a speaker verification (SV) network operate in tandem. The spoofing detector employs a self-supervised speech embedding extractor as the frontend, combined with a state-of-the-art graph neural network backend. In addition, a top-3 layer based mixture-of-experts (MoE) is used to fuse high-level and low-level features for effective spoofed utterance detection. For speaker verification, we adapt a low-complexity convolutional neural network that fuses 2D and 1D features at multiple scales, trained with the SphereFace loss. Additionally, contrastive circle loss is applied to adaptively weight positive and negative pairs within each training batch, enabling the network to better distinguish between hard and easy sample pairs. Finally, fixed imposter cohort based AS Norm score normalization and model ensembling are used to further enhance the discriminative capability of the speaker verification system.",
      "url": "https://arxiv.org/abs/2602.02286",
      "pdfUrl": "https://arxiv.org/pdf/2602.02286.pdf",
      "titleJa": "WildSpoofチャレンジのためのDFKI音声システム：SASV In-the-Wildのための堅牢なフレームワーク"
    },
    {
      "id": "2602.02249",
      "arxivId": "2602.02249",
      "title": "Evaluating Acoustic Data Transmission Schemes for Ad-Hoc Communication Between Nearby Smart Devices",
      "authors": [
        "Florentin Putz",
        "Philipp Fortmann",
        "Jan Frank",
        "Christoph Haugwitz",
        "Mario Kupnik",
        "Matthias Hollick"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.NI",
        "cs.SD"
      ],
      "abstract": "Acoustic data transmission offers a compelling alternative to Bluetooth and NFC by leveraging the ubiquitous speakers and microphones in smartphones and IoT devices. However, most research in this field relies on simulations or limited on-device testing, which makes the real-world reliability of proposed schemes difficult to assess. We systematically reviewed 31 acoustic communication studies for commodity devices and found that none provided accessible source code. After contacting authors and re-implementing three promising schemes, we assembled a testbed of eight representative acoustic communication systems. Using over 11000 smartphone transmissions in both realistic indoor environments and an anechoic chamber, we provide a systematic and repeatable methodology for evaluating the reliability and generalizability of these schemes under real-world conditions. Our results show that many existing schemes face challenges in practical usage, largely due to severe multipath propagation indoors and varying audio characteristics across device models. To support future research and foster more robust evaluations, we release our re-implementations alongside the first comprehensive dataset of real-world acoustic transmissions. Overall, our findings highlight the importance of rigorous on-device testing and underscore the need for robust design strategies to bridge the gap between simulation results and reliable IoT deployments.",
      "url": "https://arxiv.org/abs/2602.02249",
      "pdfUrl": "https://arxiv.org/pdf/2602.02249.pdf",
      "titleJa": "近くのスマートデバイス間のアドホック通信のための音響データ伝送方式の評価"
    },
    {
      "id": "2602.01908",
      "arxivId": "2602.01908",
      "title": "LipSody: Lip-to-Speech Synthesis with Enhanced Prosody Consistency",
      "authors": [
        "Jaejun Lee",
        "Yoori Oh",
        "Kyogu Lee"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Lip-to-speech synthesis aims to generate speech audio directly from silent facial video by reconstructing linguistic content from lip movements, providing valuable applications in situations where audio signals are unavailable or degraded. While recent diffusion-based models such as LipVoicer have demonstrated impressive performance in reconstructing linguistic content, they often lack prosodic consistency. In this work, we propose LipSody, a lip-to-speech framework enhanced for prosody consistency. LipSody introduces a prosody-guiding strategy that leverages three complementary cues: speaker identity extracted from facial images, linguistic content derived from lip movements, and emotional context inferred from face video. Experimental results demonstrate that LipSody substantially improves prosody-related metrics, including global and local pitch deviations, energy consistency, and speaker similarity, compared to prior approaches.",
      "url": "https://arxiv.org/abs/2602.01908",
      "pdfUrl": "https://arxiv.org/pdf/2602.01908.pdf",
      "titleJa": "LipSody: 韻律の一貫性を強化した唇音声合成"
    },
    {
      "id": "2602.01879",
      "arxivId": "2602.01879",
      "title": "Speaking Without Sound: Multi-speaker Silent Speech Voicing with Facial Inputs Only",
      "authors": [
        "Jaejun Lee",
        "Yoori Oh",
        "Kyogu Lee"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In this paper, we introduce a novel framework for generating multi-speaker speech without relying on any audible inputs. Our approach leverages silent electromyography (EMG) signals to capture linguistic content, while facial images are used to match with the vocal identity of the target speaker. Notably, we present a pitch-disentangled content embedding that enhances the extraction of linguistic content from EMG signals. Extensive analysis demonstrates that our method can generate multi-speaker speech without any audible inputs and confirms the effectiveness of the proposed pitch-disentanglement approach.",
      "url": "https://arxiv.org/abs/2602.01879",
      "pdfUrl": "https://arxiv.org/pdf/2602.01879.pdf",
      "titleJa": "音なしで話す：顔入力のみによるマルチスピーカーサイレントスピーチ音声化"
    },
    {
      "id": "2602.01793",
      "arxivId": "2602.01793",
      "title": "ParaGSE: Parallel Generative Speech Enhancement with Group-Vector-Quantization-based Neural Speech Codec",
      "authors": [
        "Fei Liu",
        "Yang Ai"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Recently, generative speech enhancement has garnered considerable interest; however, existing approaches are hindered by excessive complexity, limited efficiency, and suboptimal speech quality. To overcome these challenges, this paper proposes a novel parallel generative speech enhancement (ParaGSE) framework that leverages a group vector quantization (GVQ)-based neural speech codec. The GVQ-based codec adopts separate VQs to produce mutually independent tokens, enabling efficient parallel token prediction in ParaGSE. Specifically, ParaGSE leverages the GVQ-based codec to encode degraded speech into distinct tokens, predicts the corresponding clean tokens through parallel branches conditioned on degraded spectral features, and ultimately reconstructs clean speech via the codec decoder. Experimental results demonstrate that ParaGSE consistently produces superior enhanced speech compared to both discriminative and generative baselines, under a wide range of distortions including noise, reverberation, band-limiting, and their mixtures. Furthermore, empowered by parallel computation in token prediction, ParaGSE attains about a 1.5-fold improvement in generation efficiency on CPU compared with serial generative speech enhancement approaches.",
      "url": "https://arxiv.org/abs/2602.01793",
      "pdfUrl": "https://arxiv.org/pdf/2602.01793.pdf",
      "titleJa": "ParaGSE: グループベクトル量子化ベースのニューラル音声コーデックによる並列生成音声拡張"
    },
    {
      "id": "2602.01727",
      "arxivId": "2602.01727",
      "title": "Voting-based Pitch Estimation with Temporal and Frequential Alignment and Correlation Aware Selection",
      "authors": [
        "Junya Koguchi",
        "Tomoki Koriyama"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD"
      ],
      "abstract": "The voting method, an ensemble approach for fundamental frequency estimation, is empirically known for its robustness but lacks thorough investigation. This paper provides a principled analysis and improvement of this technique. First, we offer a theoretical basis for its effectiveness, explaining the error variance reduction for fundamental frequency estimation and invoking Condorcet's jury theorem for voiced/unvoiced detection accuracy. To address its practical limitations, we propose two key improvements: 1) a pre-voting alignment procedure to correct temporal and frequential biases among estimators, and 2) a greedy algorithm to select a compact yet effective subset of estimators based on error correlation. Experiments on a diverse dataset of speech, singing, and music show that our proposed method with alignment outperforms individual state-of-the-art estimators in clean conditions and maintains robust voiced/unvoiced detection in noisy environments.",
      "url": "https://arxiv.org/abs/2602.01727",
      "pdfUrl": "https://arxiv.org/pdf/2602.01727.pdf",
      "titleJa": "時間的・頻度的アライメントと相関を考慮した選択による投票ベースのピッチ推定"
    },
    {
      "id": "2602.01645",
      "arxivId": "2602.01645",
      "title": "Membership Inference Attack Against Music Diffusion Models via Generative Manifold Perturbation",
      "authors": [
        "Yuxuan Liu",
        "Peihong Zhang",
        "Rui Sang",
        "Zhixin Li",
        "Yizhou Tan",
        "Yiqiang Cai",
        "Shengchen Li"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Membership inference attacks (MIAs) test whether a specific audio clip was used to train a model, making them a key tool for auditing generative music models for copyright compliance. However, loss-based signals (e.g., reconstruction error) are weakly aligned with human perception in practice, yielding poor separability at the low false-positive rates (FPRs) required for forensics. We propose the Latent Stability Adversarial Probe (LSA-Probe), a white-box method that measures a geometric property of the reverse diffusion: the minimal time-normalized perturbation budget needed to cross a fixed perceptual degradation threshold at an intermediate diffusion state. We show that training members, residing in more stable regions, exhibit a significantly higher degradation cost.",
      "url": "https://arxiv.org/abs/2602.01645",
      "pdfUrl": "https://arxiv.org/pdf/2602.01645.pdf",
      "titleJa": "生成多様体摂動法による音楽拡散モデルに対するメンバーシップ推論攻撃"
    },
    {
      "id": "2602.01547",
      "arxivId": "2602.01547",
      "title": "Attention-weighted Centered Kernel Alignment for Knowledge Distillation in Large Audio-Language Models Applied to Speech Emotion Recognition",
      "authors": [
        "Qingran Yang",
        "Botao Zhao",
        "Zuheng Kang",
        "Xue Li",
        "Yayun He",
        "Chuhang Liu",
        "Xulong Zhang",
        "Xiaoyang Qu",
        "Junqing Peng",
        "Jianzong Wang"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The emergence of Large Audio-Language Models (LALMs) has advanced Speech Emotion Recognition (SER), but their size limits deployment in resource-constrained environments. While Knowledge Distillation is effective for LALM compression, existing methods remain underexplored in distilling the cross-modal projection module (Projector), and often struggle with alignment due to differences in feature dimensions. We propose PL-Distill, a KD framework that combines Projector-Level Distillation (PDist) to align audio embeddings and Logits-Level Distillation (LDist) to align output logits. PDist introduces Attention-weighted Centered Kernel Alignment, a novel approach we propose to highlight important time steps and address dimension mismatches. Meanwhile, LDist minimizes the Kullback-Leibler divergence between teacher and student logits from audio and text modalities. On IEMOCAP, RAVDESS, and SAVEE, PL-Distill compresses an 8.4B-parameter teacher to a compact 1.1B-parameter student, consistently outperforming the teacher, state-of-the-art pretrained models, and other KD baselines across all metrics.",
      "url": "https://arxiv.org/abs/2602.01547",
      "pdfUrl": "https://arxiv.org/pdf/2602.01547.pdf",
      "titleJa": "音声感情認識に適用される大規模音声言語モデルにおける知識蒸留のための注目度重み付け中心カーネルアライメント"
    },
    {
      "id": "2602.01394",
      "arxivId": "2602.01394",
      "title": "SSNAPS: Audio-Visual Separation of Speech and Background Noise with Diffusion Inverse Sampling",
      "authors": [
        "Yochai Yemini",
        "Yoav Ellinson",
        "Rami Ben-Ari",
        "Sharon Gannot",
        "Ethan Fetaya"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "This paper addresses the challenge of audio-visual single-microphone speech separation and enhancement in the presence of real-world environmental noise. Our approach is based on generative inverse sampling, where we model clean speech and ambient noise with dedicated diffusion priors and jointly leverage them to recover all underlying sources. To achieve this, we reformulate a recent inverse sampler to match our setting. We evaluate on mixtures of 1, 2, and 3 speakers with noise and show that, despite being entirely unsupervised, our method consistently outperforms leading supervised baselines in \\ac{WER} across all conditions. We further extend our framework to handle off-screen speaker separation. Moreover, the high fidelity of the separated noise component makes it suitable for downstream acoustic scene detection. Demo page: https://ssnapsicml.github.io/ssnapsicml2026/",
      "url": "https://arxiv.org/abs/2602.01394",
      "pdfUrl": "https://arxiv.org/pdf/2602.01394.pdf",
      "titleJa": "SSNAPS: 拡散逆サンプリングによる音声と背景雑音のオーディオビジュアル分離"
    },
    {
      "id": "2602.01363",
      "arxivId": "2602.01363",
      "title": "Causally Disentangled Contrastive Learning for Multilingual Speaker Embeddings",
      "authors": [
        "Mariëtte Olijslager",
        "Seyed Sahand Mohammadi Ziabari",
        "Ali Mohammed Mansoor Alsahag"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Self-supervised speaker embeddings are widely used in speaker verification systems, but prior work has shown that they often encode sensitive demographic attributes, raising fairness and privacy concerns. This paper investigates the extent to which demographic information, specifically gender, age, and accent, is present in SimCLR-trained speaker embeddings and whether such leakage can be mitigated without severely degrading speaker verification performance. We study two debiasing strategies: adversarial training through gradient reversal and a causal bottleneck architecture that explicitly separates demographic and residual information. Demographic leakage is quantified using both linear and nonlinear probing classifiers, while speaker verification performance is evaluated using ROC-AUC and EER. Our results show that gender information is strongly and linearly encoded in baseline embeddings, whereas age and accent are weaker and primarily nonlinearly represented. Adversarial debiasing reduces gender leakage but has limited effect on age and accent and introduces a clear trade-off with verification accuracy. The causal bottleneck further suppresses demographic information, particularly in the residual representation, but incurs substantial performance degradation. These findings highlight fundamental limitations in mitigating demographic leakage in self-supervised speaker embeddings and clarify the trade-offs inherent in current debiasing approaches.",
      "url": "https://arxiv.org/abs/2602.01363",
      "pdfUrl": "https://arxiv.org/pdf/2602.01363.pdf",
      "titleJa": "多言語話者埋め込みのための因果的に分離した対照学習"
    },
    {
      "id": "2602.01060",
      "arxivId": "2602.01060",
      "title": "TLDiffGAN: A Latent Diffusion-GAN Framework with Temporal Information Fusion for Anomalous Sound Detection",
      "authors": [
        "Chengyuan Ma",
        "Peng Jia",
        "Hongyue Guo",
        "Wenming Yang"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Existing generative models for unsupervised anomalous sound detection are limited by their inability to fully capture the complex feature distribution of normal sounds, while the potential of powerful diffusion models in this domain remains largely unexplored. To address this challenge, we propose a novel framework, TLDiffGAN, which consists of two complementary branches. One branch incorporates a latent diffusion model into the GAN generator for adversarial training, thereby making the discriminator's task more challenging and improving the quality of generated samples. The other branch leverages pretrained audio model encoders to extract features directly from raw audio waveforms for auxiliary discrimination. This framework effectively captures feature representations of normal sounds from both raw audio and Mel spectrograms. Moreover, we introduce a TMixup spectrogram augmentation technique to enhance sensitivity to subtle and localized temporal patterns that are often overlooked. Extensive experiments on the DCASE 2020 Challenge Task 2 dataset demonstrate the superior detection performance of TLDiffGAN, as well as its strong capability in anomalous time-frequency localization.",
      "url": "https://arxiv.org/abs/2602.01060",
      "pdfUrl": "https://arxiv.org/pdf/2602.01060.pdf",
      "titleJa": "TLDiffGAN: 異常音検出のための時間情報融合機能を備えた潜在拡散GANフレームワーク"
    },
    {
      "id": "2602.01032",
      "arxivId": "2602.01032",
      "title": "HierCon: Hierarchical Contrastive Attention for Audio Deepfake Detection",
      "authors": [
        "Zhili Nicholas Liang",
        "Soyeon Caren Han",
        "Qizhou Wang",
        "Christopher Leckie"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Audio deepfakes generated by modern TTS and voice conversion systems are increasingly difficult to distinguish from real speech, raising serious risks for security and online trust. While state-of-the-art self-supervised models provide rich multi-layer representations, existing detectors treat layers independently and overlook temporal and hierarchical dependencies critical for identifying synthetic artefacts. We propose HierCon, a hierarchical layer attention framework combined with margin-based contrastive learning that models dependencies across temporal frames, neighbouring layers, and layer groups, while encouraging domain-invariant embeddings. Evaluated on ASVspoof 2021 DF and In-the-Wild datasets, our method achieves state-of-the-art performance (1.93% and 6.87% EER), improving over independent layer weighting by 36.6% and 22.5% respectively. The results and attention visualisations confirm that hierarchical modelling enhances generalisation to cross-domain generation techniques and recording conditions.",
      "url": "https://arxiv.org/abs/2602.01032",
      "pdfUrl": "https://arxiv.org/pdf/2602.01032.pdf",
      "titleJa": "HierCon: オーディオディープフェイク検出のための階層的コントラストアテンション"
    },
    {
      "id": "2602.01030",
      "arxivId": "2602.01030",
      "title": "Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations",
      "authors": [
        "Sheng-Lun Wei",
        "Yu-Ling Liao",
        "Yen-Hua Chang",
        "Hen-Hsen Huang",
        "Hsin-Hsi Chen"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This work presents the first systematic investigation of speech bias in multilingual MLLMs. We construct and release the BiasInEar dataset, a speech-augmented benchmark based on Global MMLU Lite, spanning English, Chinese, and Korean, balanced by gender and accent, and totaling 70.8 hours ($\\approx$4,249 minutes) of speech with 11,200 questions. Using four complementary metrics (accuracy, entropy, APES, and Fleiss' $κ$), we evaluate nine representative models under linguistic (language and accent), demographic (gender), and structural (option order) perturbations. Our findings reveal that MLLMs are relatively robust to demographic factors but highly sensitive to language and option order, suggesting that speech can amplify existing structural biases. Moreover, architectural design and reasoning strategy substantially affect robustness across languages. Overall, this study establishes a unified framework for assessing fairness and robustness in speech-integrated LLMs, bridging the gap between text- and speech-based evaluation. The resources can be found at https://github.com/ntunlplab/BiasInEar.",
      "url": "https://arxiv.org/abs/2602.01030",
      "pdfUrl": "https://arxiv.org/pdf/2602.01030.pdf",
      "titleJa": "聞き手の耳のバイアス：言語的、人口統計学的、および位置的差異における音声言語モデルの感度評価"
    },
    {
      "id": "2602.01008",
      "arxivId": "2602.01008",
      "title": "Adapting Where It Matters: Depth-Aware Adaptation for Efficient Multilingual Speech Recognition in Low-Resource Languages",
      "authors": [
        "Yang Xiao",
        "Eun-Jung Holden",
        "Ting Dang"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "Recent speech foundation models excel at multilingual automatic speech recognition (ASR) for high-resource languages, but adapting them to low-resource languages remains challenging due to data scarcity and efficiency constraints. Full-model fine-tuning is computationally expensive and prone to overfitting, while parameter-efficient methods like LoRA apply adaptation uniformly across layers, overlooking internal representations thus compromising effectiveness and efficiency. We analyze multilingual ASR models and reveal a U-shaped adaptability pattern: early and late layers are language-specific and require more adaptation, while intermediate layers retain shared semantics and need less. Building on this observation, we propose DAMA, a Depth-Aware Model Adaptation framework that allocates adaptation capacity according to each layer's role. DAMA also introduces Singular Value Decomposition (SVD)-based initialization to constrain adaptation and preserve the U-shaped pattern, as well as a frozen middle-layer basis for further efficiency. Evaluated on 18 low-resource languages across two benchmark datasets, DAMA matches or surpasses state-of-the-art accuracy with 80% fewer trainable parameters, achieves a 29% error reduction under extreme data scarcity, and significantly improves memory, training time, and computational efficiency over baselines. These results highlight the benefits of structure-aware adaptation for efficient, scalable multilingual ASR.",
      "url": "https://arxiv.org/abs/2602.01008",
      "pdfUrl": "https://arxiv.org/pdf/2602.01008.pdf",
      "titleJa": "重要な箇所への適応：低リソース言語における効率的な多言語音声認識のための深度を考慮した適応"
    },
    {
      "id": "2602.00914",
      "arxivId": "2602.00914",
      "title": "A Baseline Multimodal Approach to Emotion Recognition in Conversations",
      "authors": [
        "Víctor Yeste",
        "Rodrigo Rivas-Arévalo"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (i) a transformer-based text classifier and (ii) a self-supervised speech representation model, with a simple late-fusion ensemble. We report the baseline setup and empirical results obtained under a limited training protocol, highlighting when multimodal fusion improves over unimodal models. This preprint is provided for transparency and to support future, more rigorous comparisons.",
      "url": "https://arxiv.org/abs/2602.00914",
      "pdfUrl": "https://arxiv.org/pdf/2602.00914.pdf",
      "titleJa": "会話における感情認識のためのベースラインマルチモーダルアプローチ"
    },
    {
      "id": "2602.00744",
      "arxivId": "2602.00744",
      "title": "ACE-Step 1.5: Pushing the Boundaries of Open-Source Music Generation",
      "authors": [
        "Junmin Gong",
        "Yulin Song",
        "Wenxiao Zhao",
        "Sen Wang",
        "Shengyuan Xu",
        "Jing Guo"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.SD"
      ],
      "abstract": "We present ACE-Step v1.5, a highly efficient open-source music foundation model that brings commercial-grade generation to consumer hardware. On commonly used evaluation metrics, ACE-Step v1.5 achieves quality beyond most commercial music models while remaining extremely fast -- under 2 seconds per full song on an A100 and under 10 seconds on an RTX 3090. The model runs locally with less than 4GB of VRAM, and supports lightweight personalization: users can train a LoRA from just a few songs to capture their own style. At its core lies a novel hybrid architecture where the Language Model (LM) functions as an omni-capable planner: it transforms simple user queries into comprehensive song blueprints -- scaling from short loops to 10-minute compositions -- while synthesizing metadata, lyrics, and captions via Chain-of-Thought to guide the Diffusion Transformer (DiT). Uniquely, this alignment is achieved through intrinsic reinforcement learning relying solely on the model's internal mechanisms, thereby eliminating the biases inherent in external reward models or human preferences. Beyond standard synthesis, ACE-Step v1.5 unifies precise stylistic control with versatile editing capabilities -- such as cover generation, repainting, and vocal-to-BGM conversion -- while maintaining strict adherence to prompts across 50+ languages. This paves the way for powerful tools that seamlessly integrate into the creative workflows of music artists, producers, and content creators. The code, the model weights and the demo are available at: https://ace-step.github.io/ace-step-v1.5.github.io/",
      "url": "https://arxiv.org/abs/2602.00744",
      "pdfUrl": "https://arxiv.org/pdf/2602.00744.pdf",
      "titleJa": "ACE-ステップ1.5: オープンソース音楽生成の限界を押し広げる"
    },
    {
      "id": "2602.00701",
      "arxivId": "2602.00701",
      "title": "Cross-Modal Binary Attention: An Energy-Efficient Fusion Framework for Audio-Visual Learning",
      "authors": [
        "Mohamed Saleh",
        "Zahra Ahmadi"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.MM",
        "cs.CV",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Effective multimodal fusion requires mechanisms that can capture complex cross-modal dependencies while remaining computationally scalable for real-world deployment. Existing audio-visual fusion approaches face a fundamental trade-off: attention-based methods effectively model cross-modal relationships but incur quadratic computational complexity that prevents hierarchical, multi-scale architectures, while efficient fusion strategies rely on simplistic concatenation that fails to extract complementary cross-modal information. We introduce CMQKA, a novel cross-modal fusion mechanism that achieves linear O(N) complexity through efficient binary operations, enabling scalable hierarchical fusion previously infeasible with conventional attention. CMQKA employs bidirectional cross-modal Query-Key attention to extract complementary spatiotemporal features and uses learnable residual fusion to preserve modality-specific characteristics while enriching representations with cross-modal information. Building upon CMQKA, we present SNNergy, an energy-efficient multimodal fusion framework with a hierarchical architecture that processes inputs through progressively decreasing spatial resolutions and increasing semantic abstraction. This multi-scale fusion capability allows the framework to capture both local patterns and global context across modalities. Implemented with event-driven binary spike operations, SNNergy achieves remarkable energy efficiency while maintaining fusion effectiveness and establishing new state-of-the-art results on challenging audio-visual benchmarks, including CREMA-D, AVE, and UrbanSound8K-AV, significantly outperforming existing multimodal fusion baselines. Our framework advances multimodal fusion by introducing a scalable fusion mechanism that enables hierarchical cross-modal integration with practical energy efficiency for real-world audio-visual intelligence systems.",
      "url": "https://arxiv.org/abs/2602.00701",
      "pdfUrl": "https://arxiv.org/pdf/2602.00701.pdf",
      "titleJa": "クロスモーダルバイナリアテンション：オーディオビジュアル学習のためのエネルギー効率の高い融合フレームワーク"
    },
    {
      "id": "2602.00681",
      "arxivId": "2602.00681",
      "title": "Audio-to-Image Bird Species Retrieval without Audio-Image Pairs via Text Distillation",
      "authors": [
        "Ilyass Moummad",
        "Marius Miron",
        "Lukas Rauch",
        "David Robinson",
        "Alexis Joly",
        "Olivier Pietquin",
        "Emmanuel Chemla",
        "Matthieu Geist"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG"
      ],
      "abstract": "Audio-to-image retrieval offers an interpretable alternative to audio-only classification for bioacoustic species recognition, but learning aligned audio-image representations is challenging due to the scarcity of paired audio-image data. We propose a simple and data-efficient approach that enables audio-to-image retrieval without any audio-image supervision. Our proposed method uses text as a semantic intermediary: we distill the text embedding space of a pretrained image-text model (BioCLIP-2), which encodes rich visual and taxonomic structure, into a pretrained audio-text model (BioLingual) by fine-tuning its audio encoder with a contrastive objective. This distillation transfers visually grounded semantics into the audio representation, inducing emergent alignment between audio and image embeddings without using images during training. We evaluate the resulting model on multiple bioacoustic benchmarks. The distilled audio encoder preserves audio discriminative power while substantially improving audio-text alignment on focal recordings and soundscape datasets. Most importantly, on the SSW60 benchmark, the proposed approach achieves strong audio-to-image retrieval performance exceeding baselines based on zero-shot model combinations or learned mappings between text embeddings, despite not training on paired audio-image data. These results demonstrate that indirect semantic transfer through text is sufficient to induce meaningful audio-image alignment, providing a practical solution for visually grounded species recognition in data-scarce bioacoustic settings.",
      "url": "https://arxiv.org/abs/2602.00681",
      "pdfUrl": "https://arxiv.org/pdf/2602.00681.pdf",
      "titleJa": "テキスト蒸留による音声・画像ペアなしの鳥類音声画像検索"
    },
    {
      "id": "2602.00648",
      "arxivId": "2602.00648",
      "title": "High-Fidelity Generative Audio Compression at 0.275kbps",
      "authors": [
        "Hao Ma",
        "Ruihao Jing",
        "Shansong Liu",
        "Cheng Gong",
        "Chi Zhang",
        "Xiao-Lei Zhang",
        "Xuelong Li"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "High-fidelity general audio compression at ultra-low bitrates is crucial for applications ranging from low-bandwidth communication to generative audio-language modeling. Traditional audio compression methods and contemporary neural codecs are fundamentally designed for waveform reconstruction. As a result, when operating at ultra-low bitrates, these methods degrade rapidly and often fail to preserve essential information, leading to severe acoustic artifacts and pronounced semantic distortion. To overcome these limitations, we introduce Generative Audio Compression (GAC), a novel paradigm shift from signal fidelity to task-oriented effectiveness. Implemented within the AI Flow framework, GAC is theoretically grounded in the Law of Information Capacity. These foundations posit that abundant computational power can be leveraged at the receiver to offset extreme communication bottlenecks--exemplifying the More Computation, Less Bandwidth philosophy. By integrating semantic understanding at the transmitter with scalable generative synthesis at the receiver, GAC offloads the information burden to powerful model priors. Our 1.8B-parameter model achieves high-fidelity reconstruction of 32kHz general audio at an unprecedented bitrate of 0.275kbps. Even at 0.175kbps, it still preserves a strong intelligible audio transmission capability, which represents an about 3000x compression ratio, significantly outperforming current state-of-the-art neural codecs in maintaining both perceptual quality and semantic consistency.",
      "url": "https://arxiv.org/abs/2602.00648",
      "pdfUrl": "https://arxiv.org/pdf/2602.00648.pdf",
      "titleJa": "0.275kbpsの高忠実度生成オーディオ圧縮"
    },
    {
      "id": "2602.02198",
      "arxivId": "2602.02198",
      "title": "QuietPrint: Protecting 3D Printers Against Acoustic Side-Channel Attacks",
      "authors": [
        "Seyed Ali Ghazi Asgar",
        "Narasimha Reddy"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.CR",
        "eess.AS"
      ],
      "abstract": "The 3D printing market has experienced significant growth in recent years, with an estimated revenue of 15 billion USD for 2025. Cyber-attacks targeting the 3D printing process whether through the machine itself, the supply chain, or the fabricated components are becoming increasingly common. One major concern is intellectual property (IP) theft, where a malicious attacker gains access to the design file. One method for carrying out such theft is through side-channel attacks. In this work, we investigate the possibility of IP theft via acoustic side channels and propose a novel method to protect 3D printers against such attacks. The primary advantage of our approach is that it requires no additional hardware, such as large speakers or noise-canceling devices. Instead, it secures printed parts by minimal modifications to the G-code.",
      "url": "https://arxiv.org/abs/2602.02198",
      "pdfUrl": "https://arxiv.org/pdf/2602.02198.pdf",
      "titleJa": "QuietPrint: 音響サイドチャネル攻撃から3Dプリンターを保護する"
    },
    {
      "id": "2602.01861",
      "arxivId": "2602.01861",
      "title": "RIR-Former: Coordinate-Guided Transformer for Continuous Reconstruction of Room Impulse Responses",
      "authors": [
        "Shaoheng Xu",
        "Chunyi Sun",
        " Jihui",
        " Zhang",
        "Prasanga N. Samarasinghe",
        "Thushara D. Abhayapala"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Room impulse responses (RIRs) are essential for many acoustic signal processing tasks, yet measuring them densely across space is often impractical. In this work, we propose RIR-Former, a grid-free, one-step feed-forward model for RIR reconstruction. By introducing a sinusoidal encoding module into a transformer backbone, our method effectively incorporates microphone position information, enabling interpolation at arbitrary array locations. Furthermore, a segmented multi-branch decoder is designed to separately handle early reflections and late reverberation, improving reconstruction across the entire RIR. Experiments on diverse simulated acoustic environments demonstrate that RIR-Former consistently outperforms state-of-the-art baselines in terms of normalized mean square error (NMSE) and cosine distance (CD), under varying missing rates and array configurations. These results highlight the potential of our approach for practical deployment and motivate future work on scaling from randomly spaced linear arrays to complex array geometries, dynamic acoustic scenes, and real-world environments.",
      "url": "https://arxiv.org/abs/2602.01861",
      "pdfUrl": "https://arxiv.org/pdf/2602.01861.pdf",
      "titleJa": "RIR-Former: 室内インパルス応答の連続再構成のための座標誘導型変換器"
    },
    {
      "id": "2602.01758",
      "arxivId": "2602.01758",
      "title": "Short-wave admittance correction for a time-domain cochlear transmission line model",
      "authors": [
        "François Deloche",
        "Morgan Thienpont",
        "Sarah Verhulst"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS",
        "physics.bio-ph"
      ],
      "abstract": "Transmission line (TL) models implemented in the time domain can efficiently simulate basilar-membrane (BM) displacement in response to transient or non-stationary sounds. By design, a TL model is well-suited for an one-dimensional (1-D) characterization of the traveling wave, but the real configuration of the cochlea also introduces higher-dimensional effects. Such effects include the focusing of the pressure around the BM and transverse viscous damping, both of which are magnified in the short-wave region. The two effects depend on the wavelength and are more readily expressed in the frequency domain. In this paper, we introduce a numerical correction for the BM admittance to account for 2-D effects in the time domain using autoregressive filtering and regression techniques. The correction was required for the implementation of a TL model tailored to the gerbil cochlear physiology. The model, which includes instantaneous nonlinearities in the form of variable damping, initially presented insufficient compression with increasing sound levels. This limitation was explained by the strong coupling between gain and frequency selectivity assumed in the 1-D nonlinear TL model, whereas cochlear frequency selectivity shows only a moderate dependence on sound level in small mammals. The correction factor was implemented in the gerbil model and made level-dependent using a feedback loop. The updated model achieved some decoupling between frequency selectivity and gain, providing 5 dB of additional gain and extending the range of sound levels of the compressive regime by 10 dB. We discuss the relevance of this work through two key features: the integration of both analytical and regression methods for characterizing BM admittance, and the combination of instantaneous and non-instantaneous nonlinearities.",
      "url": "https://arxiv.org/abs/2602.01758",
      "pdfUrl": "https://arxiv.org/pdf/2602.01758.pdf",
      "titleJa": "時間領域蝸牛伝送線路モデルの短波アドミタンス補正"
    },
    {
      "id": "2602.01722",
      "arxivId": "2602.01722",
      "title": "Joint Optimization of ASV and CM tasks: BTUEF Team's Submission for WildSpoof Challenge",
      "authors": [
        "Oguzhan Kurnaz",
        "Jagabandhu Mishra",
        "Tomi Kinnunen",
        "Cemal Hanilci"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Spoofing-aware speaker verification (SASV) jointly addresses automatic speaker verification and spoofing countermeasures to improve robustness against adversarial attacks. In this paper, we investigate our recently proposed modular SASV framework that enables effective reuse of publicly available ASV and CM systems through non-linear fusion, explicitly modeling their interaction, and optimization with an operating-condition-dependent trainable a-DCF loss. The framework is evaluated using ECAPA-TDNN and ReDimNet as ASV embedding extractors and SSL-AASIST as the CM model, with experiments conducted both with and without fine-tuning on the WildSpoof SASV training data. Results show that the best performance is achieved by combining ReDimNet-based ASV embeddings with fine-tuned SSL-AASIST representations, yielding an a-DCF of 0.0515 on the progress evaluation set and 0.2163 on the final evaluation set.",
      "url": "https://arxiv.org/abs/2602.01722",
      "pdfUrl": "https://arxiv.org/pdf/2602.01722.pdf",
      "titleJa": "ASVとCMタスクの共同最適化：BTUEFチームのWildSpoofチャレンジへの応募"
    },
    {
      "id": "2602.01634",
      "arxivId": "2602.01634",
      "title": "HuPER: A Human-Inspired Framework for Phonetic Perception",
      "authors": [
        "Chenxu Guo",
        "Jiachen Lian",
        "Yisi Liu",
        "Baihe Huang",
        "Shriyaa Narayanan",
        "Cheol Jun Cho",
        "Gopala Anumanchipalli"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "We propose HuPER, a human-inspired framework that models phonetic perception as adaptive inference over acoustic-phonetics evidence and linguistic knowledge. With only 100 hours of training data, HuPER achieves state-of-the-art phonetic error rates on five English benchmarks and strong zero-shot transfer to 95 unseen languages. HuPER is also the first framework to enable adaptive, multi-path phonetic perception under diverse acoustic conditions. All training data, models, and code are open-sourced. Code and demo avaliable at https://github.com/HuPER29/HuPER.",
      "url": "https://arxiv.org/abs/2602.01634",
      "pdfUrl": "https://arxiv.org/pdf/2602.01634.pdf",
      "titleJa": "HuPER: 人間に着想を得た音声知覚フレームワーク"
    },
    {
      "id": "2602.01249",
      "arxivId": "2602.01249",
      "title": "Generative AI in Signal Processing Education: An Audio Foundation Model Based Approach",
      "authors": [
        "Muhammad Salman Khan",
        "Ahmad Ullah",
        "Siddique Latif",
        "Junaid Qadir"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "eess.SP",
        "eess.AS"
      ],
      "abstract": "Audio Foundation Models (AFMs), a specialized category of Generative AI (GenAI), have the potential to transform signal processing (SP) education by integrating core applications such as speech and audio enhancement, denoising, source separation, feature extraction, automatic classification, and real-time signal analysis into learning and research. This paper introduces SPEduAFM, a conceptual AFM tailored for SP education, bridging traditional SP principles with GenAI-driven innovations. Through an envisioned case study, we outline how AFMs can enable a range of applications, including automated lecture transcription, interactive demonstrations, and inclusive learning tools, showcasing their potential to transform abstract concepts into engaging, practical experiences. This paper also addresses challenges such as ethics, explainability, and customization by highlighting dynamic, real-time auditory interactions that foster experiential and authentic learning. By presenting SPEduAFM as a forward-looking vision, we aim to inspire broader adoption of GenAI in engineering education, enhancing accessibility, engagement, and innovation in the classroom and beyond.",
      "url": "https://arxiv.org/abs/2602.01249",
      "pdfUrl": "https://arxiv.org/pdf/2602.01249.pdf",
      "titleJa": "信号処理教育における生成AI：オーディオ基盤モデルに基づくアプローチ"
    },
    {
      "id": "2602.00652",
      "arxivId": "2602.00652",
      "title": "Solving Room Impulse Response Inverse Problems Using Flow Matching with Analytic Wiener Denoiser",
      "authors": [
        "Kyung Yun Lee",
        "Nils Meyer-Kahlen",
        "Vesa Välimäki",
        "Sebastian J. Schlecht"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Room impulse response (RIR) estimation naturally arises as a class of inverse problems, including denoising and deconvolution. While recent approaches often rely on supervised learning or learned generative priors, such methods require large amounts of training data and may generalize poorly outside the training distribution. In this work, we present RIRFlow, a training-free Bayesian framework for RIR inverse problems using flow matching. We derive a flow-consistent analytic prior from the statistical structure of RIRs, eliminating the need for data-driven priors. Specifically, we model RIR as a Gaussian process with exponentially decaying variance, which yields a closed-form minimum mean squared error (MMSE) Wiener denoiser. This analytic denoiser is integrated as a prior in an existing flow-based inverse solver, where inverse problems are solved via guided posterior sampling. Furthermore, we extend the solver to nonlinear and non-Gaussian inverse problems via a local Gaussian approximation of the guided posterior, and empirically demonstrate that this approximation remains effective in practice. Experiments on real RIRs across different inverse problems demonstrate robust performance, highlighting the effectiveness of combining a classic RIR model with the recent flow-based generative inference.",
      "url": "https://arxiv.org/abs/2602.00652",
      "pdfUrl": "https://arxiv.org/pdf/2602.00652.pdf",
      "titleJa": "解析的ウィーナーデノイザーを用いたフローマッチングによる室内インパルス応答逆問題の解法"
    },
    {
      "id": "2602.00604",
      "arxivId": "2602.00604",
      "title": "The TMU System for the XACLE Challenge: Training Large Audio Language Models with CLAP Pseudo-Labels",
      "authors": [
        "Ayuto Tsutsumi",
        "Kohei Tanaka",
        "Sayaka Shiota"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "In this paper, we propose a submission to the x-to-audio alignment (XACLE) challenge. The goal is to predict semantic alignment of a given general audio and text pair. The proposed system is based on a large audio language model (LALM) architecture. We employ a three-stage training pipeline: automated audio captioning pretraining, pretraining with CLAP pseudo-labels, and fine-tuning on the XACLE dataset. Our experiments show that pretraining with CLAP pseudo-labels is the primary performance driver. On the XACLE test set, our system reaches an SRCC of 0.632, significantly outperforming the baseline system (0.334) and securing third place in the challenge team ranking. Code and models can be found at https://github.com/shiotalab-tmu/tmu-xacle2026",
      "url": "https://arxiv.org/abs/2602.00604",
      "pdfUrl": "https://arxiv.org/pdf/2602.00604.pdf",
      "titleJa": "XACLEチャレンジのためのTMUシステム：CLAP疑似ラベルを用いた大規模音声言語モデルのトレーニング"
    },
    {
      "id": "2602.00594",
      "arxivId": "2602.00594",
      "title": "Kanade: A Simple Disentangled Tokenizer for Spoken Language Modeling",
      "authors": [
        "Zhijie Huang",
        "Stephen McIntosh",
        "Daisuke Saito",
        "Nobuaki Minematsu"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "A good language model starts with a good tokenizer. Tokenization is especially important for speech modeling, which must handle continuous signals that mix linguistic and non-linguistic information. A speech tokenizer should extract phonetics and prosody, suppress linguistically irrelevant information like speaker identity, and enable high-quality synthesis. We present Kanade, a single-layer disentangled speech tokenizer that realizes this ideal. Kanade separates out acoustic constants to create a single stream of tokens that captures rich phonetics and prosody. It does so without the need for auxiliary methods that existing disentangled codecs often rely on. Experiments show that Kanade achieves state-of-the-art speaker disentanglement and lexical availability, while maintaining excellent reconstruction quality.",
      "url": "https://arxiv.org/abs/2602.00594",
      "pdfUrl": "https://arxiv.org/pdf/2602.00594.pdf",
      "titleJa": "Kanade: 音声言語モデルのためのシンプルな分離トークナイザー"
    },
    {
      "id": "2602.00568",
      "arxivId": "2602.00568",
      "title": "Dual-View Predictive Diffusion: Lightweight Speech Enhancement via Spectrogram-Image Synergy",
      "authors": [
        "Ke Xue",
        "Rongfei Fan",
        "Kai Li",
        "Shanping Yu",
        "Puning Zhao",
        "Jianping An"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Diffusion models have recently set new benchmarks in Speech Enhancement (SE). However, most existing score-based models treat speech spectrograms merely as generic 2D images, applying uniform processing that ignores the intrinsic structural sparsity of audio, which results in inefficient spectral representation and prohibitive computational complexity. To bridge this gap, we propose DVPD, an extremely lightweight Dual-View Predictive Diffusion model, which uniquely exploits the dual nature of spectrograms as both visual textures and physical frequency-domain representations across both training and inference stages. Specifically, during training, we optimize spectral utilization via the Frequency-Adaptive Non-uniform Compression (FANC) encoder, which preserves critical low-frequency harmonics while pruning high-frequency redundancies. Simultaneously, we introduce a Lightweight Image-based Spectro-Awareness (LISA) module to capture features from a visual perspective with minimal overhead. During inference, we propose a Training-free Lossless Boost (TLB) strategy that leverages the same dual-view priors to refine generation quality without any additional fine-tuning. Extensive experiments across various benchmarks demonstrate that DVPD achieves state-of-the-art performance while requiring only 35% of the parameters and 40% of the inference MACs compared to SOTA lightweight model, PGUSE. These results highlight DVPD's superior ability to balance high-fidelity speech quality with extreme architectural efficiency. Code and audio samples are available at the anonymous website: {https://anonymous.4open.science/r/dvpd_demo-E630}",
      "url": "https://arxiv.org/abs/2602.00568",
      "pdfUrl": "https://arxiv.org/pdf/2602.00568.pdf",
      "titleJa": "デュアルビュー予測拡散：スペクトログラムと画像の相乗効果による軽量音声強調"
    },
    {
      "id": "2602.00560",
      "arxivId": "2602.00560",
      "title": "Edit Content, Preserve Acoustics: Imperceptible Text-Based Speech Editing via Self-Consistency Rewards",
      "authors": [
        "Yong Ren",
        "Jiangyan Yi",
        "Jianhua Tao",
        "Zhengqi Wen",
        "Tao Wang"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Imperceptible text-based speech editing allows users to modify spoken content by altering the transcript. It demands that modified segments fuse seamlessly with the surrounding context. Prevalent methods operating in the acoustic space suffer from inherent content-style entanglement, leading to generation instability and boundary artifacts. In this paper, we propose a novel framework grounded in the principle of \"Edit Content, Preserve Acoustics\". Our approach relies on two core components: (1) Structural Foundations, which decouples editing into a stable semantic space while delegating acoustic reconstruction to a Flow Matching decoder; and (2) Perceptual Alignment, which employs a novel Self-Consistency Rewards Group Relative Policy Optimization. By leveraging a pre-trained Text-to-Speech model as an implicit critic -- complemented by strict intelligibility and duration constraints -- we effectively align the edited semantic token sequence with the original context. Empirical evaluations demonstrate that our method significantly outperforms state-of-the-art autoregressive and non-autoregressive baselines, achieving superior intelligibility, robustness, and perceptual quality.",
      "url": "https://arxiv.org/abs/2602.00560",
      "pdfUrl": "https://arxiv.org/pdf/2602.00560.pdf",
      "titleJa": "コンテンツを編集し、音響を維持する：自己一貫性報酬による知覚できないテキストベースの音声編集"
    },
    {
      "id": "2602.02495",
      "arxivId": "2602.02495",
      "title": "Reward-free Alignment for Conflicting Objectives",
      "authors": [
        "Peter Chen",
        "Xiaopeng Li",
        "Xi Chen",
        "Tianyi Lin"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.",
      "url": "https://arxiv.org/abs/2602.02495",
      "pdfUrl": "https://arxiv.org/pdf/2602.02495.pdf",
      "titleJa": "相反する目的に対する報酬なしの調整"
    },
    {
      "id": "2602.02493",
      "arxivId": "2602.02493",
      "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
      "authors": [
        "Zehong Ma",
        "Ruihan Xu",
        "Shiliang Zhang"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.",
      "url": "https://arxiv.org/abs/2602.02493",
      "pdfUrl": "https://arxiv.org/pdf/2602.02493.pdf",
      "titleJa": "PixelGen: ピクセル拡散は知覚損失を伴う潜在拡散に勝る"
    },
    {
      "id": "2602.02486",
      "arxivId": "2602.02486",
      "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
      "authors": [
        "Jialiang Zhu",
        "Gongrui Zhang",
        "Xiaolong Ma",
        "Lin Xu",
        "Miaosen Zhang",
        "Ruiqi Yang",
        "Song Wang",
        "Kai Qiu",
        "Zhirong Wu",
        "Qi Dai",
        "Ruichun Ma",
        "Bei Liu",
        "Yifan Yang",
        "Chong Luo",
        "Zhengyuan Yang",
        "Linjie Li",
        "Lijuan Wang",
        "Weizhu Chen",
        "Xin Geng",
        "Baining Guo"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.",
      "url": "https://arxiv.org/abs/2602.02486",
      "pdfUrl": "https://arxiv.org/pdf/2602.02486.pdf",
      "titleJa": "RE-TRAC: 深層探索エージェントのための再帰的軌跡圧縮"
    },
    {
      "id": "2602.02481",
      "arxivId": "2602.02481",
      "title": "Flow Policy Gradients for Robot Control",
      "authors": [
        "Brent Yi",
        "Hongsuk Choi",
        "Himanshu Gaurav Singh",
        "Xiaoyu Huang",
        "Takara E. Truong",
        "Carmelo Sferrazza",
        "Yi Ma",
        "Rocky Duan",
        "Pieter Abbeel",
        "Guanya Shi",
        "Karen Liu",
        "Angjoo Kanazawa"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "abstract": "Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.",
      "url": "https://arxiv.org/abs/2602.02481",
      "pdfUrl": "https://arxiv.org/pdf/2602.02481.pdf",
      "titleJa": "ロボット制御のためのフローポリシー勾配"
    },
    {
      "id": "2602.02475",
      "arxivId": "2602.02475",
      "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories",
      "authors": [
        "Shraddha Barke",
        "Arnav Goyal",
        "Alind Khare",
        "Avaljot Singh",
        "Suman Nath",
        "Chetan Bansal"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.AI"
      ],
      "abstract": "AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.",
      "url": "https://arxiv.org/abs/2602.02475",
      "pdfUrl": "https://arxiv.org/pdf/2602.02475.pdf",
      "titleJa": "AgentRx: 実行軌跡から AI エージェントの障害を診断する"
    },
    {
      "id": "2602.02474",
      "arxivId": "2602.02474",
      "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
      "authors": [
        "Haozhen Zhang",
        "Quanyu Long",
        "Jianzhu Bao",
        "Tao Feng",
        "Weizhi Zhang",
        "Haodong Yue",
        "Wenya Wang"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \\emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \\emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \\emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.",
      "url": "https://arxiv.org/abs/2602.02474",
      "pdfUrl": "https://arxiv.org/pdf/2602.02474.pdf",
      "titleJa": "MemSkill: 自己進化エージェントのための記憶スキルの学習と進化"
    },
    {
      "id": "2602.02471",
      "arxivId": "2602.02471",
      "title": "Multi-head automated segmentation by incorporating detection head into the contextual layer neural network",
      "authors": [
        "Edwin Kys",
        "Febian Febian"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.AI",
        "physics.med-ph"
      ],
      "abstract": "Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \\pm 0.036$ versus $0.732 \\pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.",
      "url": "https://arxiv.org/abs/2602.02471",
      "pdfUrl": "https://arxiv.org/pdf/2602.02471.pdf",
      "titleJa": "コンテキスト層ニューラルネットワークに検出ヘッドを組み込むことによるマルチヘッド自動セグメンテーション"
    },
    {
      "id": "2602.02470",
      "arxivId": "2602.02470",
      "title": "Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge",
      "authors": [
        "Xutao Ma",
        "Yixiao Huang",
        "Hanlin Zhu",
        "Somayeh Sojoudi"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the \"reversal curse\" -- when trained on forward knowledge data of the form \"$A \\rightarrow B$\" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge \"$B \\leftarrow A$\" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form \"$A \\to A$\" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.",
      "url": "https://arxiv.org/abs/2602.02470",
      "pdfUrl": "https://arxiv.org/pdf/2602.02470.pdf",
      "titleJa": "アイデンティティブリッジによる自己回帰言語モデルの反転の呪いの打破"
    },
    {
      "id": "2602.02468",
      "arxivId": "2602.02468",
      "title": "Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts",
      "authors": [
        "Aiden Yiliu Li",
        "Xinyue Hao",
        "Shilong Liu",
        "Mengdi Wang"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.",
      "url": "https://arxiv.org/abs/2602.02468",
      "pdfUrl": "https://arxiv.org/pdf/2602.02468.pdf",
      "titleJa": "Avenir-Web: グラウンディング専門家の混合による人間の経験を模倣したマルチモーダルWebエージェント"
    },
    {
      "id": "2602.02465",
      "arxivId": "2602.02465",
      "title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
      "authors": [
        "Jana Zeller",
        "Thaddäus Wiedemer",
        "Fanfei Li",
        "Thomas Klein",
        "Prasanna Mayilvahanan",
        "Matthias Bethge",
        "Felix Wichmann",
        "Ryan Cotterell",
        "Wieland Brendel"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "abstract": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.",
      "url": "https://arxiv.org/abs/2602.02465",
      "pdfUrl": "https://arxiv.org/pdf/2602.02465.pdf",
      "titleJa": "MentisOculi：心的イメージで推論の限界を明らかにする"
    },
    {
      "id": "2602.02462",
      "arxivId": "2602.02462",
      "title": "Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models",
      "authors": [
        "Gabriele Maraia",
        "Marco Valentino",
        "Fabio Massimo Zanzotto",
        "Leonardo Ranaldi"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.",
      "url": "https://arxiv.org/abs/2602.02462",
      "pdfUrl": "https://arxiv.org/pdf/2602.02462.pdf",
      "titleJa": "大規模言語モデルにおける内容不変推論のための抽象活性化空間"
    },
    {
      "id": "2602.02455",
      "arxivId": "2602.02455",
      "title": "Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction",
      "authors": [
        "Han Bao",
        "Zheyuan Zhang",
        "Pengcheng Jing",
        "Zhengqing Yuan",
        "Kaiwen Shi",
        "Yanfang Ye"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "abstract": "As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \\textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \\textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \\textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \\MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.",
      "url": "https://arxiv.org/abs/2602.02455",
      "pdfUrl": "https://arxiv.org/pdf/2602.02455.pdf",
      "titleJa": "ドリフトベンチ: マルチターンインタラクションによる入力障害下におけるLLMエージェントの協調動作の崩壊の診断"
    },
    {
      "id": "2602.02454",
      "arxivId": "2602.02454",
      "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
      "authors": [
        "Ansh Kumar Sharma",
        "Yixiang Sun",
        "Ninghao Lu",
        "Yunzhe Zhang",
        "Jiarao Liu",
        "Sherry Yang"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "abstract": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.",
      "url": "https://arxiv.org/abs/2602.02454",
      "pdfUrl": "https://arxiv.org/pdf/2602.02454.pdf",
      "titleJa": "World-Gymnast: 世界モデルにおける強化学習によるロボットのトレーニング"
    },
    {
      "id": "2602.02453",
      "arxivId": "2602.02453",
      "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling",
      "authors": [
        "Andong Chen",
        "Wenxin Zhu",
        "Qiuyu Ding",
        "Yuchen Song",
        "Muyun Yang",
        "Tiejun Zhao"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.",
      "url": "https://arxiv.org/abs/2602.02453",
      "pdfUrl": "https://arxiv.org/pdf/2602.02453.pdf",
      "titleJa": "漫画で考える：構造化されたビジュアルストーリーテリングによるマルチモーダル推論の強化"
    },
    {
      "id": "2602.02451",
      "arxivId": "2602.02451",
      "title": "Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization",
      "authors": [
        "Patrick Cooper",
        "Alvaro Velasquez"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.",
      "url": "https://arxiv.org/abs/2602.02451",
      "pdfUrl": "https://arxiv.org/pdf/2602.02451.pdf",
      "titleJa": "能動的因果実験主義者（ACE）：直接的な選好最適化による介入戦略の学習"
    },
    {
      "id": "2602.02437",
      "arxivId": "2602.02437",
      "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
      "authors": [
        "Dianyi Wang",
        "Chaofan Ma",
        "Feng Han",
        "Size Wu",
        "Wei Song",
        "Yibin Wang",
        "Zhixiong Zhang",
        "Tianhang Wang",
        "Siyuan Wang",
        "Zhongyu Wei",
        "Jiaqi Wang"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.",
      "url": "https://arxiv.org/abs/2602.02437",
      "pdfUrl": "https://arxiv.org/pdf/2602.02437.pdf",
      "titleJa": "UniReason 1.0: 世界知識に基づいた画像生成と編集のための統合推論フレームワーク"
    },
    {
      "id": "2602.02422",
      "arxivId": "2602.02422",
      "title": "Poly-attention: a general scheme for higher-order self-attention",
      "authors": [
        "Sayak Chakrabarti",
        "Toniann Pitassi",
        "Josh Alman"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times. In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time. Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.",
      "url": "https://arxiv.org/abs/2602.02422",
      "pdfUrl": "https://arxiv.org/pdf/2602.02422.pdf",
      "titleJa": "ポリアテンション：高階自己注意のための一般的なスキーム"
    },
    {
      "id": "2602.02419",
      "arxivId": "2602.02419",
      "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration",
      "authors": [
        "Qingni Wang",
        "Yue Fan",
        "Xin Eric Wang"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "abstract": "Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\\% percentage points over Gemini-only inference.",
      "url": "https://arxiv.org/abs/2602.02419",
      "pdfUrl": "https://arxiv.org/pdf/2602.02419.pdf",
      "titleJa": "SafeGround: 不確かさのキャリブレーションで GUI 接地モデルを信頼すべきタイミングを知る"
    },
    {
      "id": "2602.02416",
      "arxivId": "2602.02416",
      "title": "Structure Enables Effective Self-Localization of Errors in LLMs",
      "authors": [
        "Ankur Samanta",
        "Akshayaa Magesh",
        "Ayush Jain",
        "Kavosh Asadi",
        "Youliang Yu",
        "Daniel Jiang",
        "Boris Vidolov",
        "Kaveh Hassani",
        "Paul Sajda",
        "Jalaj Bhandari",
        "Yonathan Efroni"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.",
      "url": "https://arxiv.org/abs/2602.02416",
      "pdfUrl": "https://arxiv.org/pdf/2602.02416.pdf",
      "titleJa": "構造はLLMにおけるエラーの効果的な自己局在化を可能にする"
    },
    {
      "id": "2602.02408",
      "arxivId": "2602.02408",
      "title": "ReasonEdit: Editing Vision-Language Models using Human Reasoning",
      "authors": [
        "Jiaxing Qiu",
        "Kaihua Hou",
        "Roxana Daneshjou",
        "Ahmed Alaa",
        "Thomas Hartvigsen"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.",
      "url": "https://arxiv.org/abs/2602.02408",
      "pdfUrl": "https://arxiv.org/pdf/2602.02408.pdf",
      "titleJa": "ReasonEdit: 人間の推論を用いた視覚言語モデルの編集"
    },
    {
      "id": "2601.22764",
      "arxivId": "2601.22764",
      "title": "How Far Can Pretrained LLMs Go in Symbolic Music? Controlled Comparisons of Supervised and Preference-based Adaptation",
      "authors": [
        "Deepak Kumar",
        "Emmanouil Karystinaios",
        "Gerhard Widmer",
        "Markus Schedl"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Music often shares notable parallels with language, motivating the use of pretrained large language models (LLMs) for symbolic music understanding and generation. Despite growing interest, the practical effectiveness of adapting instruction-tuned LLMs to symbolic music remains insufficiently characterized. We present a controlled comparative study of finetuning strategies for ABC-based generation and understanding, comparing an off-the-shelf instruction-tuned backbone to domain-adapted variants and a music-specialized LLM baseline. Across multiple symbolic music corpora and evaluation signals, we provide some insights into adaptation choices for symbolic music applications. We highlight the domain adaptation vs.~preserving prior information tradeoff as well as the distinct behaviour of metrics used to measure the domain adaptation for symbolic music.",
      "url": "https://arxiv.org/abs/2601.22764",
      "pdfUrl": "https://arxiv.org/pdf/2601.22764.pdf",
      "titleJa": "事前学習済みLLMは記号音楽においてどこまで到達できるか？教師あり学習と選好に基づく適応の制御比較"
    },
    {
      "id": "2601.21740",
      "arxivId": "2601.21740",
      "title": "MIDI-LLaMA: An Instruction-Following Multimodal LLM for Symbolic Music Understanding",
      "authors": [
        "Meng Yang",
        "Jon McCormack",
        "Maria Teresa Llano",
        "Wanchao Su",
        "Chao Lei"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLM) for audio music have demonstrated strong capabilities in music understanding, yet symbolic music, a fundamental representation of musical structure, remains unexplored. In this work, we introduce MIDI-LLaMA, the first instruction-following MLLM for symbolic music understanding. Our approach aligns the MIDI encoder MusicBERT and Llama-3-8B via a two-stage pipeline comprising feature alignment and instruction tuning. To support training, we design a scalable annotation pipeline that annotates GiantMIDI-Piano with fine-grained metadata, resulting in a MIDI-text dataset. Compared with the baseline trained on converting MIDI into ABC notation under the same instruction-tuning procedure, MIDI-LLaMA substantially outperforms in captioning and semantic alignment in question answering. Human evaluation further confirms the advantages of MIDI-LLaMA in music understanding, emotion recognition, creativity, and overall preference. These findings demonstrate that incorporating symbolic music into large language models enhances their capacity for musical understanding.",
      "url": "https://arxiv.org/abs/2601.21740",
      "pdfUrl": "https://arxiv.org/pdf/2601.21740.pdf",
      "titleJa": "MIDI-LLaMA: 記号的音楽理解のための指示追従型マルチモーダルLLM"
    },
    {
      "id": "2601.21260",
      "arxivId": "2601.21260",
      "title": "Music Plagiarism Detection: Problem Formulation and a Segment-based Solution",
      "authors": [
        "Seonghyeon Go",
        "Yumin Kim"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Recently, the problem of music plagiarism has emerged as an even more pressing social issue. As music information retrieval research advances, there is a growing effort to address issues related to music plagiarism. However, many studies, including our previous work, have conducted research without clearly defining what the music plagiarism detection task actually involves. This lack of a clear definition has slowed research progress and made it hard to apply results to real-world scenarios. To fix this situation, we defined how Music Plagiarism Detection is different from other MIR tasks and explained what problems need to be solved. We introduce the Similar Music Pair dataset to support this newly defined task. In addition, we propose a method based on segment transcription as one way to solve the task. Our demo and dataset are available at https://github.com/Mippia/ICASSP2026-MPD.",
      "url": "https://arxiv.org/abs/2601.21260",
      "pdfUrl": "https://arxiv.org/pdf/2601.21260.pdf",
      "titleJa": "音楽盗作検出：問題の定式化とセグメントベースのソリューション"
    },
    {
      "id": "2601.20478",
      "arxivId": "2601.20478",
      "title": "On Every Note a Griff: Looking for a Useful Representation of Basso Continuo Performance Style",
      "authors": [
        "Adam Štefunko",
        "Carlos Eduardo Cancino-Chacón",
        "Jan Hajič"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.SD",
        "cs.IR"
      ],
      "abstract": "Basso continuo is a baroque improvisatory accompaniment style which involves improvising multiple parts above a given bass line in a musical score on a harpsichord or organ. Basso continuo is not merely a matter of history; moreover, it is a historically inspired living practice, and The Aligned Continuo Dataset (ACoRD) records the first sample of modern-day basso continuo playing in the symbolic domain. This dataset, containing 175 MIDI recordings of 5 basso continuo scores performed by 7 players, allows us to start observing and analyzing the variety that basso continuo improvisation brings. A recently proposed basso continuo performance-to-score alignment system provides a way of mapping improvised performance notes to score notes. In order to study aligned basso continuo performances, we need an appropriate feature representation. We propose griff, a representation inspired by historical basso continuo treatises. It enables us to encode both pitch content and structure of a basso continuo realization in a transposition-invariant way. Griffs are directly extracted from aligned basso continuo performances by grouping together performance notes aligned to the same score note in a onset-time ordered way, and they provide meaningful tokens that form a feature space in which we can analyze basso continuo performance styles. We statistically describe griffs extracted from the ACoRD dataset recordings, and show in two experiments how griffs can be used for statistical analysis of individuality of different players' basso continuo performance styles. We finally present an argument why it is desirable to preserve the structure of a basso continuo improvisation in order to conduct a refined analysis of personal performance styles of individual basso continuo practitioners, and why griffs can provide a meaningful historically informed feature space worthy of a more robust empirical validation.",
      "url": "https://arxiv.org/abs/2601.20478",
      "pdfUrl": "https://arxiv.org/pdf/2601.20478.pdf",
      "titleJa": "すべての音符にグリフ：通奏低音演奏スタイルの有用な表現を求めて"
    },
    {
      "id": "2601.20883",
      "arxivId": "2601.20883",
      "title": "VoxMorph: Scalable Zero-shot Voice Identity Morphing via Disentangled Embeddings",
      "authors": [
        "Bharath Krishnamurthy",
        "Ajita Rattani"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.CR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Morphing techniques generate artificial biometric samples that combine features from multiple individuals, allowing each contributor to be verified against a single enrolled template. While extensively studied in face recognition, this vulnerability remains largely unexplored in voice biometrics. Prior work on voice morphing is computationally expensive, non-scalable, and limited to acoustically similar identity pairs, constraining practical deployment. Moreover, existing sound-morphing methods target audio textures, music, or environmental sounds and are not transferable to voice identity manipulation. We propose VoxMorph, a zero-shot framework that produces high-fidelity voice morphs from as little as five seconds of audio per subject without model retraining. Our method disentangles vocal traits into prosody and timbre embeddings, enabling fine-grained interpolation of speaking style and identity. These embeddings are fused via Spherical Linear Interpolation (Slerp) and synthesized using an autoregressive language model coupled with a Conditional Flow Matching network. VoxMorph achieves state-of-the-art performance, delivering a 2.6x gain in audio quality, a 73% reduction in intelligibility errors, and a 67.8% morphing attack success rate on automated speaker verification systems under strict security thresholds. This work establishes a practical and scalable paradigm for voice morphing with significant implications for biometric security. The code and dataset are available on our project page: https://vcbsl.github.io/VoxMorph/",
      "url": "https://arxiv.org/abs/2601.20883",
      "pdfUrl": "https://arxiv.org/pdf/2601.20883.pdf",
      "titleJa": "VoxMorph: 分離埋め込みによるスケーラブルなゼロショット音声アイデンティティモーフィング"
    },
    {
      "id": "2601.19702",
      "arxivId": "2601.19702",
      "title": "SAM Audio Judge: A Unified Multimodal Framework for Perceptual Evaluation of Audio Separation",
      "authors": [
        "Helin Wang",
        "Bowen Shi",
        "Andros Tjandra",
        "John Hoffman",
        "Yi-Chiao Wu",
        "Apoorv Vyas",
        "Najim Dehak",
        "Ann Lee",
        "Wei-Ning Hsu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "The performance evaluation remains a complex challenge in audio separation, and existing evaluation metrics are often misaligned with human perception, course-grained, relying on ground truth signals. On the other hand, subjective listening tests remain the gold standard for real-world evaluation, but they are expensive, time-consuming, and difficult to scale. This paper addresses the growing need for automated systems capable of evaluating audio separation without human intervention. The proposed evaluation metric, SAM Audio Judge (SAJ), is a multimodal fine-grained reference-free objective metric, which shows highly alignment with human perceptions. SAJ supports three audio domains (speech, music and general sound events) and three prompt inputs (text, visual and span), covering four different dimensions of evaluation (recall, percision, faithfulness, and overall). SAM Audio Judge also shows potential applications in data filtering, pseudo-labeling large datasets and reranking in audio separation models. We release our code and pre-trained models at: https://github.com/facebookresearch/sam-audio.",
      "url": "https://arxiv.org/abs/2601.19702",
      "pdfUrl": "https://arxiv.org/pdf/2601.19702.pdf",
      "titleJa": "SAM Audio Judge: 音声分離の知覚評価のための統合マルチモーダルフレームワーク"
    },
    {
      "id": "2601.19109",
      "arxivId": "2601.19109",
      "title": "Interpretable and Perceptually-Aligned Music Similarity with Pretrained Embeddings",
      "authors": [
        "Arhan Vohra",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Perceptual similarity representations enable music retrieval systems to determine which songs sound most similar to listeners. State-of-the-art approaches based on task-specific training via self-supervised metric learning show promising alignment with human judgment, but are difficult to interpret or generalize due to limited dataset availability. We show that pretrained text-audio embeddings (CLAP and MuQ-MuLan) offer comparable perceptual alignment on similarity tasks without any additional fine-tuning. To surpass this baseline, we introduce a novel method to perceptually align pretrained embeddings with source separation and linear optimization on ABX preference data from listening tests. Our model provides interpretable and controllable instrument-wise weights, allowing music producers to retrieve stem-level loops and samples based on mixed reference songs.",
      "url": "https://arxiv.org/abs/2601.19109",
      "pdfUrl": "https://arxiv.org/pdf/2601.19109.pdf",
      "titleJa": "事前学習済みの埋め込みによる解釈可能かつ知覚的に整合された音楽類似性"
    },
    {
      "id": "2601.18766",
      "arxivId": "2601.18766",
      "title": "Learning to Discover: A Generalized Framework for Raga Identification without Forgetting",
      "authors": [
        "Parampreet Singh",
        "Somya Kumar",
        "Chaitanya Shailendra Nitawe",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.",
      "url": "https://arxiv.org/abs/2601.18766",
      "pdfUrl": "https://arxiv.org/pdf/2601.18766.pdf",
      "titleJa": "発見することを学ぶ：忘れずにラーガを識別するための一般化された枠組み"
    },
    {
      "id": "2601.18339",
      "arxivId": "2601.18339",
      "title": "A Dataset for Automatic Vocal Mode Classification",
      "authors": [
        "Reemt Hinrichs",
        "Sonja Stephan",
        "Alexander Lange",
        "Jörn Ostermann"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\\,\\% was achieved with a ResNet18. The dataset can be downloaded under https://zenodo.org/records/14276415.",
      "url": "https://arxiv.org/abs/2601.18339",
      "pdfUrl": "https://arxiv.org/pdf/2601.18339.pdf",
      "titleJa": "自動音声モード分類のためのデータセット"
    },
    {
      "id": "2601.19951",
      "arxivId": "2601.19951",
      "title": "Pianoroll-Event: A Novel Score Representation for Symbolic Music",
      "authors": [
        "Lekai Qian",
        "Haoyu Gu",
        "Dehan Li",
        "Boyu Cao",
        "Qi Liu"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Symbolic music representation is a fundamental challenge in computational musicology. While grid-based representations effectively preserve pitch-time spatial correspondence, their inherent data sparsity leads to low encoding efficiency. Discrete-event representations achieve compact encoding but fail to adequately capture structural invariance and spatial locality. To address these complementary limitations, we propose Pianoroll-Event, a novel encoding scheme that describes pianoroll representations through events, combining structural properties with encoding efficiency while maintaining temporal dependencies and local spatial patterns. Specifically, we design four complementary event types: Frame Events for temporal boundaries, Gap Events for sparse regions, Pattern Events for note patterns, and Musical Structure Events for musical metadata. Pianoroll-Event strikes an effective balance between sequence length and vocabulary size, improving encoding efficiency by 1.36\\times to 7.16\\times over representative discrete sequence methods. Experiments across multiple autoregressive architectures show models using our representation consistently outperform baselines in both quantitative and human evaluations.",
      "url": "https://arxiv.org/abs/2601.19951",
      "pdfUrl": "https://arxiv.org/pdf/2601.19951.pdf",
      "titleJa": "ピアノロールイベント：象徴音楽のための新しい楽譜表現"
    },
    {
      "id": "2601.17645",
      "arxivId": "2601.17645",
      "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
      "authors": [
        "Xilin Jiang",
        "Qiaolin Wang",
        "Junkai Wu",
        "Xiaomin He",
        "Zhongweiyang Xu",
        "Yinghao Ma",
        "Minshuo Piao",
        "Kaiyi Yang",
        "Xiuwen Zheng",
        "Riki Shimizu",
        "Yicong Chen",
        "Arsalan Firoozi",
        "Gavin Mischler",
        "Sukru Samet Dindar",
        "Richard Antonello",
        "Linyang He",
        "Tsun-An Hsieh",
        "Xulin Fan",
        "Yulun Wu",
        "Yuesheng Ma",
        "Chaitanya Amballa",
        "Weixiong Chen",
        "Jiarui Hai",
        "Ruisi Li",
        "Vishal Choudhari",
        "Cong Han",
        "Yinghao Aaron Li",
        "Adeen Flinker",
        "Mounya Elhilali",
        "Emmanouil Benetos",
        "Mark Hasegawa-Johnson",
        "Romit Roy Choudhury",
        "Nima Mesgarani"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public",
      "url": "https://arxiv.org/abs/2601.17645",
      "pdfUrl": "https://arxiv.org/pdf/2601.17645.pdf",
      "titleJa": "AVMeme試験：法学修士（LLM）の文脈的・文化的知識と思考力を評価するマルチモーダル・多言語・多文化ベンチマーク"
    },
    {
      "id": "2601.17517",
      "arxivId": "2601.17517",
      "title": "EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding",
      "authors": [
        "Luca Cerovaz",
        "Michele Mancusi",
        "Emanuele Rodolà"
      ],
      "publishedDate": "2026-01-24",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Audio codecs power discrete music generative modelling, music streaming and immersive media by shrinking PCM audio to bandwidth-friendly bit-rates. Recent works have gravitated towards processing in the spectral domain; however, spectrogram-domains typically struggle with phase modeling which is naturally complex-valued. Most frequency-domain neural codecs either disregard phase information or encode it as two separate real-valued channels, limiting spatial fidelity. This entails the need to introduce adversarial discriminators at the expense of convergence speed and training stability to compensate for the inadequate representation power of the audio signal. In this work we introduce an end-to-end complex-valued RVQ-VAE audio codec that preserves magnitude-phase coupling across the entire analysis-quantization-synthesis pipeline and removes adversarial discriminators and diffusion post-filters. Without GANs or diffusion we match or surpass much longer-trained baselines in-domain and reach SOTA out-of-domain performance. Compared to standard baselines that train for hundreds of thousands of steps, our model reducing training budget by an order of magnitude is markedly more compute-efficient while preserving high perceptual quality.",
      "url": "https://arxiv.org/abs/2601.17517",
      "pdfUrl": "https://arxiv.org/pdf/2601.17517.pdf",
      "titleJa": "EuleroDec: 効率的かつ堅牢なオーディオコーディングのための複素値RVQ-VAE"
    },
    {
      "id": "2601.16675",
      "arxivId": "2601.16675",
      "title": "I Guess That's Why They Call it the Blues: Causal Analysis for Audio Classifiers",
      "authors": [
        "David A. Kelly",
        "Hana Chockler"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "It is well-known that audio classifiers often rely on non-musically relevant features and spurious correlations to classify audio. Hence audio classifiers are easy to manipulate or confuse, resulting in wrong classifications. While inducing a misclassification is not hard, until now the set of features that the classifiers rely on was not well understood. In this paper we introduce a new method that uses causal reasoning to discover features of the frequency space that are sufficient and necessary for a given classification. We describe an implementation of this algorithm in the tool FreqReX and provide experimental results on a number of standard benchmark datasets. Our experiments show that causally sufficient and necessary subsets allow us to manipulate the outputs of the models in a variety of ways by changing the input very slightly. Namely, a change to one out of 240,000 frequencies results in a change in classification 58% of the time, and the change can be so small that it is practically inaudible. These results show that causal analysis is useful for understanding the reasoning process of audio classifiers and can be used to successfully manipulate their outputs.",
      "url": "https://arxiv.org/abs/2601.16675",
      "pdfUrl": "https://arxiv.org/pdf/2601.16675.pdf",
      "titleJa": "ブルースと呼ばれる理由：音声分類器の因果分析"
    },
    {
      "id": "2601.16273",
      "arxivId": "2601.16273",
      "title": "The CMU-AIST submission for the ICME 2025 Audio Encoder Challenge",
      "authors": [
        "Shikhar Bharadwaj",
        "Samuele Cornell",
        "Kwanghee Choi",
        "Hye-jin Shim",
        "Soham Deshmukh",
        "Satoru Fukayama",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This technical report describes our submission to the ICME 2025 audio encoder challenge. Our submitted system is built on BEATs, a masked speech token prediction based audio encoder. We extend the BEATs model using 74,000 hours of data derived from various speech, music, and sound corpora and scale its architecture upto 300 million parameters. We experiment with speech-heavy and balanced pre-training mixtures to study the impact of different domains on final performance. Our submitted system consists of an ensemble of the Dasheng 1.2 billion model with two custom scaled-up BEATs models trained on the aforementioned pre-training data mixtures. We also propose a simple ensembling technique that retains the best capabilities of constituent models and surpasses both the baseline and Dasheng 1.2B. For open science, we publicly release our trained checkpoints via huggingface at https://huggingface.co/shikhar7ssu/OpenBEATs-ICME-SOUND and https://huggingface.co/shikhar7ssu/OpenBEATs-ICME.",
      "url": "https://arxiv.org/abs/2601.16273",
      "pdfUrl": "https://arxiv.org/pdf/2601.16273.pdf",
      "titleJa": "ICME 2025 オーディオエンコーダチャレンジへの CMU-AIST の応募"
    },
    {
      "id": "2601.16150",
      "arxivId": "2601.16150",
      "title": "Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization",
      "authors": [
        "Maximos Kaliakatsos-Papakostas",
        "Dimos Makris",
        "Konstantinos Soiledis",
        "Konstantinos-Theodoros Tsamis",
        "Vassilis Katsouros",
        "Emilios Cambouropoulos"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody-harmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.",
      "url": "https://arxiv.org/abs/2601.16150",
      "pdfUrl": "https://arxiv.org/pdf/2601.16150.pdf",
      "titleJa": "メロディーに注意を払う（交差させる）：単一エンコーダーによるメロディーハーモニーのためのカリキュラムマスキング"
    },
    {
      "id": "2601.15872",
      "arxivId": "2601.15872",
      "title": "PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation",
      "authors": [
        "Jaekwon Im",
        "Natalia Polouliakh",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.",
      "url": "https://arxiv.org/abs/2601.15872",
      "pdfUrl": "https://arxiv.org/pdf/2601.15872.pdf",
      "titleJa": "PF-D2M: ユニバーサルなダンス・トゥ・ミュージック生成のためのポーズフリー拡散モデル"
    },
    {
      "id": "2601.15083",
      "arxivId": "2601.15083",
      "title": "Bangla Music Genre Classification Using Bidirectional LSTMS",
      "authors": [
        "Muntakimur Rahaman",
        "Md Mahmudul Hoque",
        "Md Mehedi Hassain"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Bangla music is enrich in its own music cultures. Now a days music genre classification is very significant because of the exponential increase in available music, both in digital and physical formats. It is necessary to index them accordingly to facilitate improved retrieval. Automatically classifying Bangla music by genre is essential for efficiently locating specific pieces within a vast and diverse music library. Prevailing methods for genre classification predominantly employ conventional machine learning or deep learning approaches. This work introduces a novel music dataset comprising ten distinct genres of Bangla music. For the task of audio classification, we utilize a recurrent neural network (RNN) architecture. Specifically, a Long Short-Term Memory (LSTM) network is implemented to train the model and perform the classification. Feature extraction represents a foundational stage in audio data processing. This study utilizes Mel-Frequency Cepstral Coefficients (MFCCs) to transform raw audio waveforms into a compact and representative set of features. The proposed framework facilitates music genre classification by leveraging these extracted features. Experimental results demonstrate a classification accuracy of 78%, indicating the system's strong potential to enhance and streamline the organization of Bangla music genres.",
      "url": "https://arxiv.org/abs/2601.15083",
      "pdfUrl": "https://arxiv.org/pdf/2601.15083.pdf",
      "titleJa": "双方向LSTMSを用いたバングラ音楽のジャンル分類"
    },
    {
      "id": "2601.23161",
      "arxivId": "2601.23161",
      "title": "DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding",
      "authors": [
        "Jiaming Zhou",
        "Xuxin Cheng",
        "Shiwan Zhao",
        "Yuhang Jia",
        "Cao Liu",
        "Ke Zeng",
        "Xunliang Cai",
        "Yong Qin"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.",
      "url": "https://arxiv.org/abs/2601.23161",
      "pdfUrl": "https://arxiv.org/pdf/2601.23161.pdf",
      "titleJa": "DIFFA-2: 一般的な音声理解のための実用的な拡散大規模言語モデル"
    },
    {
      "id": "2601.23149",
      "arxivId": "2601.23149",
      "title": "Hearing is Believing? Evaluating and Analyzing Audio Language Model Sycophancy with SYAUDIO",
      "authors": [
        "Junchi Yao",
        "Lokranjan Lakshmikanthan",
        "Annie Zhao",
        "Danielle Zhao",
        "Shu Yang",
        "Zikang Ding",
        "Di Wang",
        "Lijie Hu"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio Language Models (ALMs) have recently shown strong capabilities in unified reasoning over speech, sound, and natural language; yet they inherit behavioral issues observed in Large Language Models, including sycophancy--the tendency to agree with user assertions even when they contradict objective evidence. While sycophancy has been extensively studied in text and vision-language models, its manifestation in audio-conditioned reasoning remains largely unexplored, despite the need for ALMs to rely on auditory cues such as acoustic events, speaker characteristics, and speech rate. To address this gap, we introduce SYAUDIO, the first benchmark dedicated to evaluating sycophancy in ALMs, consisting of 4,319 audio questions spanning Audio Perception, Audio Reasoning, Audio Math, and Audio Ethics. Built upon established audio benchmarks and augmented with TTS-generated arithmetic and moral reasoning tasks, SYAUDIO enables systematic evaluation across multiple domains and sycophancy types with carefully verified data quality. Furthermore, we analyze audio-specific sycophancy under realistic conditions involving noise and rate, and demonstrate that supervised fine-tuning with chain-of-thought data is an effective mitigation strategy for reducing sycophantic behavior in ALMs.",
      "url": "https://arxiv.org/abs/2601.23149",
      "pdfUrl": "https://arxiv.org/pdf/2601.23149.pdf",
      "titleJa": "聞くことは信じること？SYAUDIOによる音声言語モデルの追従性の評価と分析"
    },
    {
      "id": "2601.23066",
      "arxivId": "2601.23066",
      "title": "Towards Explicit Acoustic Evidence Perception in Audio LLMs for Speech Deepfake Detection",
      "authors": [
        "Xiaoxuan Guo",
        "Yuankun Xie",
        "Haonan Cheng",
        "Jiayi Zhou",
        "Jian Liu",
        "Hengyan Huang",
        "Long Ye",
        "Qin Zhang"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Speech deepfake detection (SDD) focuses on identifying whether a given speech signal is genuine or has been synthetically generated. Existing audio large language model (LLM)-based methods excel in content understanding; however, their predictions are often biased toward semantically correlated cues, which results in fine-grained acoustic artifacts being overlooked during the decisionmaking process. Consequently, fake speech with natural semantics can bypass detectors despite harboring subtle acoustic anomalies; this suggests that the challenge stems not from the absence of acoustic data, but from its inadequate accessibility when semantic-dominant reasoning prevails. To address this issue, we investigate SDD within the audio LLM paradigm and introduce SDD with Auditory Perception-enhanced Audio Large Language Model (SDD-APALLM), an acoustically enhanced framework designed to explicitly expose fine-grained time-frequency evidence as accessible acoustic cues. By combining raw audio with structured spectrograms, the proposed framework empowers audio LLMs to more effectively capture subtle acoustic inconsistencies without compromising their semantic understanding. Experimental results indicate consistent gains in detection accuracy and robustness, especially in cases where semantic cues are misleading. Further analysis reveals that these improvements stem from a coordinated utilization of semantic and acoustic information, as opposed to simple modality aggregation.",
      "url": "https://arxiv.org/abs/2601.23066",
      "pdfUrl": "https://arxiv.org/pdf/2601.23066.pdf",
      "titleJa": "音声ディープフェイク検出のためのオーディオLLMにおける明示的な音響証拠認識に向けて"
    },
    {
      "id": "2601.23004",
      "arxivId": "2601.23004",
      "title": "Layer-Aware Early Fusion of Acoustic and Linguistic Embeddings for Cognitive Status Classification",
      "authors": [
        "Krystof Novotny",
        "Laureano Moro-Velázquez",
        "Jiri Mekyska"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Speech contains both acoustic and linguistic patterns that reflect cognitive decline, and therefore models describing only one domain cannot fully capture such complexity. This study investigates how early fusion (EF) of speech and its corresponding transcription text embeddings, with attention to encoder layer depth, can improve cognitive status classification. Using a DementiaBank-derived collection of recordings (1,629 speakers; cognitively normal controls$\\unicode{x2013}$CN, Mild Cognitive Impairment$\\unicode{x2013}$MCI, and Alzheimer's Disease and Related Dementias$\\unicode{x2013}$ADRD), we extracted frame-aligned embeddings from different internal layers of wav2vec 2.0 or Whisper combined with DistilBERT or RoBERTa. Unimodal, EF and late fusion (LF) models were trained with a transformer classifier, optimized, and then evaluated across 10 seeds. Performance consistently peaked in mid encoder layers ($\\sim$8$\\unicode{x2013}$10), with the single best F1 at Whisper + RoBERTa layer 9 and the best log loss at Whisper + DistilBERT layer 10. Acoustic-only models consistently outperformed text-only variants. EF boosts discrimination for genuinely acoustic embeddings, whereas LF improves probability calibration. Layer choice critically shapes clinical multimodal synergy.",
      "url": "https://arxiv.org/abs/2601.23004",
      "pdfUrl": "https://arxiv.org/pdf/2601.23004.pdf",
      "titleJa": "認知状態分類のための音響と言語埋め込みのレイヤー認識型早期融合"
    },
    {
      "id": "2601.22792",
      "arxivId": "2601.22792",
      "title": "CALM: Joint Contextual Acoustic-Linguistic Modeling for Personalization of Multi-Speaker ASR",
      "authors": [
        "Muhammad Shakeel",
        "Yosuke Fukumoto",
        "Chikara Maeda",
        "Chyi-Jiunn Lin",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "We present CALM, a joint Contextual Acoustic-Linguistic Modeling framework for multi-speaker automatic speech recognition (ASR). In personalized AI scenarios, the joint availability of acoustic and linguistic cues naturally motivates the integration of target-speaker conditioning with contextual biasing in overlapping conversations. CALM implements this integration in an end-to-end framework through speaker embedding-driven target-speaker extraction and dynamic vocabulary-based contextual biasing. We evaluate CALM on simulated English (LibriSpeechMix) and Japanese (Corpus of Spontaneous Japanese mixtures, CSJMix). On two-speaker mixtures, CALM reduces biased word error rate (B-WER) from 12.7 to 4.7 on LibriSpeech2Mix and biased character error rate (B-CER) from 16.6 to 8.4 on CSJMix2 (eval3), demonstrating the effectiveness of joint acoustic-linguistic modeling across languages. We additionally report results on the AMI corpus (IHM-mix condition) to validate performance on standardized speech mixtures.",
      "url": "https://arxiv.org/abs/2601.22792",
      "pdfUrl": "https://arxiv.org/pdf/2601.22792.pdf",
      "titleJa": "CALM: 複数話者ASRのパーソナライゼーションのためのコンテキスト音響言語モデリング"
    },
    {
      "id": "2601.22783",
      "arxivId": "2601.22783",
      "title": "Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval",
      "authors": [
        "Ilyass Moummad",
        "Marius Miron",
        "David Robinson",
        "Kawtar Zaher",
        "Hervé Goëau",
        "Olivier Pietquin",
        "Pierre Bonnet",
        "Emmanuel Chemla",
        "Matthieu Geist",
        "Alexis Joly"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.IR",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.",
      "url": "https://arxiv.org/abs/2601.22783",
      "pdfUrl": "https://arxiv.org/pdf/2601.22783.pdf",
      "titleJa": "高速テキストベース野生生物観察検索のためのコンパクトハイパーキューブ埋め込み"
    },
    {
      "id": "2601.22599",
      "arxivId": "2601.22599",
      "title": "A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation",
      "authors": [
        "Kai Li",
        "Jintao Cheng",
        "Chang Zeng",
        "Zijun Yan",
        "Helin Wang",
        "Zixiong Su",
        "Bo Zheng",
        "Xiaolin Hu"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.HC"
      ],
      "abstract": "Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from a data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events. These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via a semantically consistent synthesis protocol. Utilizing this pipeline, we constructed Hive, a high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on a huge dataset $\\sim$500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-of-distribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency, offering a new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive.",
      "url": "https://arxiv.org/abs/2601.22599",
      "pdfUrl": "https://arxiv.org/pdf/2601.22599.pdf",
      "titleJa": "データ効率の高いクエリベースのユニバーサルサウンド分離のための意味的に一貫性のあるデータセット"
    },
    {
      "id": "2601.22504",
      "arxivId": "2601.22504",
      "title": "Class-Aware Permutation-Invariant Signal-to-Distortion Ratio for Semantic Segmentation of Sound Scene with Same-Class Sources",
      "authors": [
        "Binh Thien Nguyen",
        "Masahiro Yasuda",
        "Daiki Takeuchi",
        "Daisuke Niizumi",
        "Noboru Harada"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "eess.AS"
      ],
      "abstract": "To advance immersive communication, the Detection and Classification of Acoustic Scenes and Events (DCASE) 2025 Challenge recently introduced Task 4 on Spatial Semantic Segmentation of Sound Scenes (S5). An S5 system takes a multi-channel audio mixture as input and outputs single-channel dry sources along with their corresponding class labels. Although the DCASE 2025 Challenge simplifies the task by constraining class labels in each mixture to be mutually exclusive, real-world mixtures frequently contain multiple sources from the same class. The presence of duplicated labels can significantly degrade the performance of the label-queried source separation (LQSS) model, which is the key component of many existing S5 systems, and can also limit the validity of the official evaluation metric of DCASE 2025 Task 4. To address these issues, we propose a class-aware permutation-invariant loss function that enables the LQSS model to handle queries involving duplicated labels. In addition, we redesign the S5 evaluation metric to eliminate ambiguities caused by these same-class sources. To evaluate the proposed method within the S5 system, we extend the label prediction model to support same-class labels. Experimental results demonstrate the effectiveness of the proposed methods and the robustness of the new metric on mixtures both with and without same-class sources.",
      "url": "https://arxiv.org/abs/2601.22504",
      "pdfUrl": "https://arxiv.org/pdf/2601.22504.pdf",
      "titleJa": "同一クラス音源によるサウンドシーンの意味的セグメンテーションのためのクラスを考慮した順列不変な信号対歪み比"
    },
    {
      "id": "2601.22480",
      "arxivId": "2601.22480",
      "title": "Rethinking Speech Representation Aggregation in Speech Enhancement: A Phonetic Mutual Information Perspective",
      "authors": [
        "Seungu Han",
        "Sungho Lee",
        "Kyogu Lee"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Recent speech enhancement (SE) models increasingly leverage self-supervised learning (SSL) representations for their rich semantic information. Typically, intermediate features are aggregated into a single representation via a lightweight adaptation module. However, most SSL models are not trained for noise robustness, which can lead to corrupted semantic representations. Moreover, the adaptation module is trained jointly with the SE model, potentially prioritizing acoustic details over semantic information, contradicting the original purpose. To address this issue, we first analyze the behavior of SSL models on noisy speech from an information-theoretic perspective. Specifically, we measure the mutual information (MI) between the corrupted SSL representations and the corresponding phoneme labels, focusing on preservation of linguistic contents. Building upon this analysis, we introduce the linguistic aggregation layer, which is pre-trained to maximize MI with phoneme labels (with optional dynamic aggregation) and then frozen during SE training. Experiments show that this decoupled approach improves Word Error Rate (WER) over jointly optimized baselines, demonstrating the benefit of explicitly aligning the adaptation module with linguistic contents.",
      "url": "https://arxiv.org/abs/2601.22480",
      "pdfUrl": "https://arxiv.org/pdf/2601.22480.pdf",
      "titleJa": "音声強調における音声表現集約の再考：音声相互情報量の観点"
    },
    {
      "id": "2601.22306",
      "arxivId": "2601.22306",
      "title": "Sylber 2.0: A Universal Syllable Embedding",
      "authors": [
        "Cheol Jun Cho",
        "Nicholas Lee",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "abstract": "Scaling spoken language modeling requires speech tokens that are both efficient and universal. Recent work has proposed syllables as promising speech tokens at low temporal resolution, but existing models are constrained to English and fail to capture sufficient acoustic detail. To address this gap, we present Sylber 2.0, a self-supervised framework for coding speech at the syllable level that enables efficient temporal compression and high-fidelity reconstruction. Sylber 2.0 achieves a very low token frequency around 5 Hz, while retaining both linguistic and acoustic detail across multiple languages and expressive styles. Experiments show that it performs on par with previous models operating on high-frequency baselines. Furthermore, Sylber 2.0 enables efficient TTS modeling which can generate speech with competitive intelligibility and quality with SOTA models using only 72M parameters. Moreover, the universality of Sylber 2.0 provides more effective features for low resource ASR than previous speech coding frameworks. In sum, we establish an effective syllable-level abstraction for general spoken language.",
      "url": "https://arxiv.org/abs/2601.22306",
      "pdfUrl": "https://arxiv.org/pdf/2601.22306.pdf",
      "titleJa": "Sylber 2.0: ユニバーサル音節埋め込み"
    },
    {
      "id": "2601.21463",
      "arxivId": "2601.21463",
      "title": "Unifying Speech Editing Detection and Content Localization via Prior-Enhanced Audio LLMs",
      "authors": [
        "Jun Xue",
        "Yi Chai",
        "Yanzhen Ren",
        "Jinshen He",
        "Zhiqiang Tang",
        "Zhuolin Yi",
        "Yihuan Huang",
        "Yuankun Xie",
        "Yujie Chen"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Speech editing achieves semantic inversion by performing fine-grained segment-level manipulation on original utterances, while preserving global perceptual naturalness. Existing detection studies mainly focus on manually edited speech with explicit splicing artifacts, and therefore struggle to cope with emerging end-to-end neural speech editing techniques that generate seamless acoustic transitions. To address this challenge, we first construct a large-scale bilingual dataset, AiEdit, which leverages large language models to drive precise semantic tampering logic and employs multiple advanced neural speech editing methods for data synthesis, thereby filling the gap of high-quality speech editing datasets. Building upon this foundation, we propose PELM (Prior-Enhanced Audio Large Language Model), the first large-model framework that unifies speech editing detection and content localization by formulating them as an audio question answering task. To mitigate the inherent forgery bias and semantic-priority bias observed in existing audio large models, PELM incorporates word-level probability priors to provide explicit acoustic cues, and further designs a centroid-aggregation-based acoustic consistency perception loss to explicitly enforce the modeling of subtle local distribution anomalies. Extensive experimental results demonstrate that PELM significantly outperforms state-of-the-art methods on both the HumanEdit and AiEdit datasets, achieving equal error rates (EER) of 0.57\\% and 9.28\\% (localization), respectively.",
      "url": "https://arxiv.org/abs/2601.21463",
      "pdfUrl": "https://arxiv.org/pdf/2601.21463.pdf",
      "titleJa": "事前強化オーディオ LLM による音声編集検出とコンテンツ ローカリゼーションの統合"
    },
    {
      "id": "2601.21402",
      "arxivId": "2601.21402",
      "title": "SemanticAudio: Audio Generation and Editing in Semantic Space",
      "authors": [
        "Zheqi Dai",
        "Guangyan Zhang",
        "Haolin He",
        "Xiquan Li",
        "Jingyu Li",
        "Chunyat Wu",
        "Yiwen Guo",
        "Qiuqiang Kong"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "In recent years, Text-to-Audio Generation has achieved remarkable progress, offering sound creators powerful tools to transform textual inspirations into vivid audio. However, existing models predominantly operate directly in the acoustic latent space of a Variational Autoencoder (VAE), often leading to suboptimal alignment between generated audio and textual descriptions. In this paper, we introduce SemanticAudio, a novel framework that conducts both audio generation and editing directly in a high-level semantic space. We define this semantic space as a compact representation capturing the global identity and temporal sequence of sound events, distinct from fine-grained acoustic details. SemanticAudio employs a two-stage Flow Matching architecture: the Semantic Planner first generates these compact semantic features to sketch the global semantic layout, and the Acoustic Synthesizer subsequently produces high-fidelity acoustic latents conditioned on this semantic plan. Leveraging this decoupled design, we further introduce a training-free text-guided editing mechanism that enables precise attribute-level modifications on general audio without retraining. Specifically, this is achieved by steering the semantic generation trajectory via the difference of velocity fields derived from source and target text prompts. Extensive experiments demonstrate that SemanticAudio surpasses existing mainstream approaches in semantic alignment. Demo available at: https://semanticaudio1.github.io/",
      "url": "https://arxiv.org/abs/2601.21402",
      "pdfUrl": "https://arxiv.org/pdf/2601.21402.pdf",
      "titleJa": "SemanticAudio: セマンティック空間におけるオーディオ生成と編集"
    }
  ],
  "lastUpdated": "2026-02-04T01:02:17.908196",
  "totalCount": 80
}