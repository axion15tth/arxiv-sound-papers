{
  "papers": [
    {
      "id": "2601.07367",
      "arxivId": "2601.07367",
      "title": "FOCAL: A Novel Benchmarking Technique for Multi-modal Agents",
      "authors": [
        "Aditya Choudhary",
        "Anupam Purwar"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD"
      ],
      "abstract": "With the recent advancements in reasoning capa- bilities, tool calling using MCP servers and Audio Language Models (ALMs), development and integration of multi-modal agents (with voice and text support) has come to the industry forefront. Cascading pipelines for voice agents still play a central role in the industry owing to their superior reasoning capabilities facilitated by LLMs. Although, cascading pipelines often present error propagation through the pipeline. We propose a framework, FOCAL to benchmark end-to-end reasoning, component-wise error propagation and error analysis for automated as well as human-assisted testing of multi-modal agents (voice to voice + text input). We also share two novel metrics viz. Reasoning and Semantic scores to evaluate efficacy of the agent in having meaningful conversations in voice mode.",
      "url": "https://arxiv.org/abs/2601.07367",
      "pdfUrl": "https://arxiv.org/pdf/2601.07367.pdf",
      "titleJa": "FOCAL: マルチモーダルエージェントのための新しいベンチマーク手法"
    },
    {
      "id": "2601.07331",
      "arxivId": "2601.07331",
      "title": "SEE: Signal Embedding Energy for Quantifying Noise Interference in Large Audio Language Models",
      "authors": [
        "Yuanhe Zhang",
        "Jiayu Tian",
        "Yibo Zhang",
        "Shilinlu Yan",
        "Liang Lin",
        "Zhenhong Zhou",
        "Li Sun",
        "Sen Su"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Large Audio Language Models (LALMs) have been widely applied in real-time scenarios, such as in-car assistants and online meeting comprehension. In practice, audio inputs are often corrupted by device and environmental noise, leading to performance degradation. However, existing LALM studies on noise lack quantitative analysis and rely mainly on intuition and empirical observation, thus failing to understand practical robustness. To address this issue, we introduce Signal Embedding Energy (SEE), a method for quantifying the impact of noise intensity on LALM inputs, enabling the differentiation of LALM robustness in real-world deployments. SEE introduces a perspective based on structured activation subspaces derived from the model's internal representations, which more accurately captures its perception of noise than raw audio features. Across experiments, SEE exhibits a strong correlation with LALM performance, achieving a correlation of 0.98. Surprisingly, traditional audio denoising methods are only marginally effective for LALMs, and, in some cases, even increase SEE and impair performance. This suggests a mismatch between speech-centric denoising objectives and the noise sensitivity of modern LALMs. Therefore, we propose a mitigation strategy derived from SEE to denoise LALM inputs, outperforming existing denoising methods. This paper introduces a novel metric for noise quantification in LALMs, providing guidance for robustness improvements in real-world deployments.",
      "url": "https://arxiv.org/abs/2601.07331",
      "pdfUrl": "https://arxiv.org/pdf/2601.07331.pdf",
      "titleJa": "参照: 大規模音声言語モデルにおけるノイズ干渉の定量化のための信号埋め込みエネルギー"
    },
    {
      "id": "2601.07303",
      "arxivId": "2601.07303",
      "title": "ESDD2: Environment-Aware Speech and Sound Deepfake Detection Challenge Evaluation Plan",
      "authors": [
        "Xueping Zhang",
        "Han Yin",
        "Yang Xiao",
        "Lin Zhang",
        "Ting Dang"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio recorded in real-world environments often contains a mixture of foreground speech and background environmental sounds. With rapid advances in text-to-speech, voice conversion, and other generation models, either component can now be modified independently. Such component-level manipulations are harder to detect, as the remaining unaltered component can mislead the systems designed for whole deepfake audio, and they often sound more natural to human listeners. To address this gap, we have proposed CompSpoofV2 dataset and a separation-enhanced joint learning framework. CompSpoofV2 is a large-scale curated dataset designed for component-level audio anti-spoofing, which contains over 250k audio samples, with a total duration of approximately 283 hours. Based on the CompSpoofV2 and the separation-enhanced joint learning framework, we launch the Environment-Aware Speech and Sound Deepfake Detection Challenge (ESDD2), focusing on component-level spoofing, where both speech and environmental sounds may be manipulated or synthesized, creating a more challenging and realistic detection scenario. The challenge will be held in conjunction with the IEEE International Conference on Multimedia and Expo 2026 (ICME 2026).",
      "url": "https://arxiv.org/abs/2601.07303",
      "pdfUrl": "https://arxiv.org/pdf/2601.07303.pdf",
      "titleJa": "ESDD2: 環境認識型音声ディープフェイク検出チャレンジ評価計画"
    },
    {
      "id": "2601.07237",
      "arxivId": "2601.07237",
      "title": "The ICASSP 2026 Automatic Song Aesthetics Evaluation Challenge",
      "authors": [
        "Guobin Ma",
        "Yuxuan Xia",
        "Jixun Yao",
        "Huixin Xue",
        "Hexin Liu",
        "Shuai Wang",
        "Hao Liu",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "This paper summarizes the ICASSP 2026 Automatic Song Aesthetics Evaluation (ASAE) Challenge, which focuses on predicting the subjective aesthetic scores of AI-generated songs. The challenge consists of two tracks: Track 1 targets the prediction of the overall musicality score, while Track 2 focuses on predicting five fine-grained aesthetic scores. The challenge attracted strong interest from the research community and received numerous submissions from both academia and industry. Top-performing systems significantly surpassed the official baseline, demonstrating substantial progress in aligning objective metrics with human aesthetic preferences. The outcomes establish a standardized benchmark and advance human-aligned evaluation methodologies for modern music generation systems.",
      "url": "https://arxiv.org/abs/2601.07237",
      "pdfUrl": "https://arxiv.org/pdf/2601.07237.pdf",
      "titleJa": "ICASSP 2026 自動歌曲美学評価チャレンジ"
    },
    {
      "id": "2601.06981",
      "arxivId": "2601.06981",
      "title": "Directional Selective Fixed-Filter Active Noise Control Based on a Convolutional Neural Network in Reverberant Environments",
      "authors": [
        "Boxiang Wang",
        "Zhengding Luo",
        "Haowen Li",
        "Dongyuan Shi",
        "Junwei Ji",
        "Ziyi Yang",
        "Woon-Seng Gan"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "Selective fixed-filter active noise control (SFANC) is a novel approach capable of mitigating noise with varying frequency characteristics. It offers faster response and greater computational efficiency compared to traditional adaptive algorithms. However, spatial factors, particularly the influence of the noise source location, are often overlooked. Some existing studies have explored the impact of the direction-of-arrival (DoA) of the noise source on ANC performance, but they are mostly limited to free-field conditions and do not consider the more complex indoor reverberant environments. To address this gap, this paper proposes a learning-based directional SFANC method that incorporates the DoA of the noise source in reverberant environments. In this framework, multiple reference signals are processed by a convolutional neural network (CNN) to estimate the azimuth and elevation angles of the noise source, as well as to identify the most appropriate control filter for effective noise cancellation. Compared to traditional adaptive algorithms, the proposed approach achieves superior noise reduction with shorter response times, even in the presence of reverberations.",
      "url": "https://arxiv.org/abs/2601.06981",
      "pdfUrl": "https://arxiv.org/pdf/2601.06981.pdf",
      "titleJa": "残響環境における畳み込みニューラルネットワークに基づく方向選択固定フィルタアクティブノイズコントロール"
    },
    {
      "id": "2601.06829",
      "arxivId": "2601.06829",
      "title": "MoEScore: Mixture-of-Experts-Based Text-Audio Relevance Score Prediction for Text-to-Audio System Evaluation",
      "authors": [
        "Bochao Sun",
        "Yang Xiao",
        "Han Yin"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Recent advances in generative models have enabled modern Text-to-Audio (TTA) systems to synthesize audio with high perceptual quality. However, TTA systems often struggle to maintain semantic consistency with the input text, leading to mismatches in sound events, temporal tructures, or contextual relationships. Evaluating semantic fidelity in TTA remains a significant challenge. Traditional methods primarily rely on subjective human listening tests, which is time-consuming. To solve this, we propose an objective evaluator based on a Mixture of Experts (MoE) architecture with Sequential Cross-Attention (SeqCoAttn). Our model achieves the first rank in the XACLE Challenge, with an SRCC of 0.6402 (an improvement of 30.6% over the challenge baseline) on the test dataset. Code is available at: https://github.com/S-Orion/MOESCORE.",
      "url": "https://arxiv.org/abs/2601.06829",
      "pdfUrl": "https://arxiv.org/pdf/2601.06829.pdf",
      "titleJa": "MoEScore: テキスト音声変換システム評価のための専門家混合ベースのテキスト音声関連度スコア予測"
    },
    {
      "id": "2601.06662",
      "arxivId": "2601.06662",
      "title": "Dereverberation Filter by Deconvolution with Frequency Bin Specific Faded Impulse Response",
      "authors": [
        "Stefan Ciba"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "This work introduces a robust single-channel inverse filter for dereverberation of non-ideal recordings, validated on real audio. The developed method focuses on the calculation and modification of a discrete impulse response in order to filter the characteristics from a known digital single channel recording setup and room characteristics such as early reflections and reverberations. The aim is a dryer and clearer signal reconstruction, which ideally would be the direct-path signal. The time domain impulse response is calculated from the cepstral domain and faded by means of frequency bin specific exponential decay in the spectrum. The decay rates are obtained by using the blind estimates of reverberation time ratio between recorded output and test signals for each frequency bin. The modified impulse response does filter a recorded audio-signal by deconvolution. The blind estimation is well known and stands out for its robustness to noise and non-idealities. Estimation of a direct path signal is key to many applications.",
      "url": "https://arxiv.org/abs/2601.06662",
      "pdfUrl": "https://arxiv.org/pdf/2601.06662.pdf",
      "titleJa": "周波数ビン特定フェードインパルス応答によるデコンボリューションによる残響除去フィルタ"
    },
    {
      "id": "2601.06621",
      "arxivId": "2601.06621",
      "title": "Stereo Audio Rendering for Personal Sound Zones Using a Binaural Spatially Adaptive Neural Network (BSANN)",
      "authors": [
        "Hao Jiang",
        "Edgar Choueiri"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "A binaural rendering framework for personal sound zones (PSZs) is proposed to enable multiple head-tracked listeners to receive fully independent stereo audio programs. Current PSZ systems typically rely on monophonic rendering and therefore cannot control the left and right ears separately, which limits the quality and accuracy of spatial imaging. The proposed method employs a Binaural Spatially Adaptive Neural Network (BSANN) to generate ear-optimized loudspeaker filters that reconstruct the desired acoustic field at each ear of multiple listeners. The framework integrates anechoically measured loudspeaker frequency responses, analytically modeled transducer directivity, and rigid-sphere head-related transfer functions (HRTFs) to enhance acoustic accuracy and spatial rendering fidelity. An explicit active crosstalk cancellation (XTC) stage further improves three-dimensional spatial perception. Experiments show significant gains in measured objective performance metrics, including inter-zone isolation (IZI), inter-program isolation (IPI), and crosstalk cancellation (XTC), with log-frequency-weighted values of 10.23/10.03 dB (IZI), 11.11/9.16 dB (IPI), and 10.55/11.13 dB (XTC), respectively, over 100-20,000 Hz. The combined use of ear-wise control, accurate acoustic modeling, and integrated active XTC produces a unified rendering method that delivers greater isolation performance, increased robustness to room asymmetry, and more faithful spatial reproduction in real acoustic environments.",
      "url": "https://arxiv.org/abs/2601.06621",
      "pdfUrl": "https://arxiv.org/pdf/2601.06621.pdf",
      "titleJa": "バイノーラル空間適応型ニューラルネットワーク（BSANN）を用いたパーソナルサウンドゾーン向けステレオオーディオレンダリング"
    },
    {
      "id": "2601.06560",
      "arxivId": "2601.06560",
      "title": "Lightweight Resolution-Aware Audio Deepfake Detection via Cross-Scale Attention and Consistency Learning",
      "authors": [
        "K. A. Shahriar"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Audio deepfake detection has become increasingly challenging due to rapid advances in speech synthesis and voice conversion technologies, particularly under channel distortions, replay attacks, and real-world recording conditions. This paper proposes a resolution-aware audio deepfake detection framework that explicitly models and aligns multi-resolution spectral representations through cross-scale attention and consistency learning. Unlike conventional single-resolution or implicit feature-fusion approaches, the proposed method enforces agreement across complementary time--frequency scales. The proposed framework is evaluated on three representative benchmarks: ASVspoof 2019 (LA and PA), the Fake-or-Real (FoR) dataset, and the In-the-Wild Audio Deepfake dataset under a speaker-disjoint protocol. The method achieves near-perfect performance on ASVspoof LA (EER 0.16%), strong robustness on ASVspoof PA (EER 5.09%), FoR rerecorded audio (EER 4.54%), and in-the-wild deepfakes (AUC 0.98, EER 4.81%), significantly outperforming single-resolution and non-attention baselines under challenging conditions. The proposed model remains lightweight and efficient, requiring only 159k parameters and less than 1~GFLOP per inference, making it suitable for practical deployment. Comprehensive ablation studies confirm the critical contributions of cross-scale attention and consistency learning, while gradient-based interpretability analysis reveals that the model learns resolution-consistent and semantically meaningful spectral cues across diverse spoofing conditions. These results demonstrate that explicit cross-resolution modeling provides a principled, robust, and scalable foundation for next-generation audio deepfake detection systems.",
      "url": "https://arxiv.org/abs/2601.06560",
      "pdfUrl": "https://arxiv.org/pdf/2601.06560.pdf",
      "titleJa": "クロススケールアテンションと一貫性学習による軽量解像度認識オーディオディープフェイク検出"
    },
    {
      "id": "2601.06406",
      "arxivId": "2601.06406",
      "title": "Representing Sounds as Neural Amplitude Fields: A Benchmark of Coordinate-MLPs and A Fourier Kolmogorov-Arnold Framework",
      "authors": [
        "Linfei Li",
        "Lin Zhang",
        "Zhong Wang",
        "Fengyi Zhang",
        "Zelin Li",
        "Ying Shen"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Although Coordinate-MLP-based implicit neural representations have excelled in representing radiance fields, 3D shapes, and images, their application to audio signals remains underexplored. To fill this gap, we investigate existing implicit neural representations, from which we extract 3 types of positional encoding and 16 commonly used activation functions. Through combinatorial design, we establish the first benchmark for Coordinate-MLPs in audio signal representations. Our benchmark reveals that Coordinate-MLPs require complex hyperparameter tuning and frequency-dependent initialization, limiting their robustness. To address these issues, we propose Fourier-ASR, a novel framework based on the Fourier series theorem and the Kolmogorov-Arnold representation theorem. Fourier-ASR introduces Fourier Kolmogorov-Arnold Networks (Fourier-KAN), which leverage periodicity and strong nonlinearity to represent audio signals, eliminating the need for additional positional encoding. Furthermore, a Frequency-adaptive Learning Strategy (FaLS) is proposed to enhance the convergence of Fourier-KAN by capturing high-frequency components and preventing overfitting of low-frequency signals. Extensive experiments conducted on natural speech and music datasets reveal that: (1) well-designed positional encoding and activation functions in Coordinate-MLPs can effectively improve audio representation quality; and (2) Fourier-ASR can robustly represent complex audio signals without extensive hyperparameter tuning. Looking ahead, the continuity and infinite resolution of implicit audio representations make our research highly promising for tasks such as audio compression, synthesis, and generation. The source code will be released publicly to ensure reproducibility. The code is available at https://github.com/lif314/Fourier-ASR.",
      "url": "https://arxiv.org/abs/2601.06406",
      "pdfUrl": "https://arxiv.org/pdf/2601.06406.pdf",
      "titleJa": "音を神経振幅場として表現する：座標MLPのベンチマークとフーリエ・コルモゴロフ・アーノルド枠組み"
    },
    {
      "id": "2601.06006",
      "arxivId": "2601.06006",
      "title": "Discriminative-Generative Target Speaker Extraction with Decoder-Only Language Models",
      "authors": [
        "Bang Zeng",
        "Beilong Tang",
        "Wang Xiang",
        "Ming Li"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Target speaker extraction (TSE) aims to recover the speech signal of a desired speaker from a mixed audio recording, given a short enrollment utterance. Most existing TSE approaches are based on discriminative modeling paradigms. Although effective at suppressing interfering speakers, these methods often struggle to produce speech with high perceptual quality and naturalness. To address this limitation, we first propose LauraTSE, a generative TSE model built upon an auto-regressive decoder-only language model. However, purely generative approaches may suffer from hallucinations, content drift, and limited controllability, which may undermine their reliability in complex acoustic scenarios. To overcome these challenges, we further introduce a discriminative-generative TSE framework. In this framework, a discriminative front-end is employed to robustly extract the target speaker's speech, yielding stable and controllable intermediate representations. A generative back-end then operates in the neural audio codec representation space to reconstruct fine-grained speech details and enhance perceptual quality. This two-stage design effectively combines the robustness and controllability of discriminative models with the superior naturalness and quality enhancement capabilities of generative models. Moreover, we systematically investigate collaborative training strategies for the proposed framework, including freezing or fine-tuning the front-end, incorporating an auxiliary SI-SDR loss, and exploring both auto-regressive and non-auto-regressive inference mechanisms. Experimental results demonstrate that the proposed framework achieves a more favorable trade-off among speech quality, intelligibility, and speaker consistency.",
      "url": "https://arxiv.org/abs/2601.06006",
      "pdfUrl": "https://arxiv.org/pdf/2601.06006.pdf",
      "titleJa": "デコーダのみの言語モデルを用いた識別的・生成的ターゲット話者抽出"
    },
    {
      "id": "2601.06235",
      "arxivId": "2601.06235",
      "title": "An Intelligent AI glasses System with Multi-Agent Architecture for Real-Time Voice Processing and Task Execution",
      "authors": [
        "Sheng-Kai Chen",
        "Jyh-Horng Wu",
        "Ching-Yao Lin",
        "Yen-Ting Lin"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.HC"
      ],
      "abstract": "This paper presents an AI glasses system that integrates real-time voice processing, artificial intelligence(AI) agents, and cross-network streaming capabilities. The system employs dual-agent architecture where Agent 01 handles Automatic Speech Recognition (ASR) and Agent 02 manages AI processing through local Large Language Models (LLMs), Model Context Protocol (MCP) tools, and Retrieval-Augmented Generation (RAG). The system supports real-time RTSP streaming for voice and video data transmission, eye tracking data collection, and remote task execution through RabbitMQ messaging. Implementation demonstrates successful voice command processing with multilingual support and cross-platform task execution capabilities.",
      "url": "https://arxiv.org/abs/2601.06235",
      "pdfUrl": "https://arxiv.org/pdf/2601.06235.pdf",
      "titleJa": "リアルタイム音声処理とタスク実行のためのマルチエージェントアーキテクチャを備えたインテリジェントAIグラスシステム"
    },
    {
      "id": "2601.05564",
      "arxivId": "2601.05564",
      "title": "The ICASSP 2026 HumDial Challenge: Benchmarking Human-like Spoken Dialogue Systems in the LLM Era",
      "authors": [
        "Zhixian Zhao",
        "Shuiyuan Wang",
        "Guojian Li",
        "Hongfei Xue",
        "Chengyou Wang",
        "Shuai Wang",
        "Longshuai Xiao",
        "Zihan Zhang",
        "Hui Bu",
        "Xin Xu",
        "Xinsheng Wang",
        "Hexin Liu",
        "Eng Siong Chng",
        "Hung-yi Lee",
        "Haizhou Li",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.HC",
        "eess.AS"
      ],
      "abstract": "Driven by the rapid advancement of Large Language Models (LLMs), particularly Audio-LLMs and Omni-models, spoken dialogue systems have evolved significantly, progressively narrowing the gap between human-machine and human-human interactions. Achieving truly ``human-like'' communication necessitates a dual capability: emotional intelligence to perceive and resonate with users' emotional states, and robust interaction mechanisms to navigate the dynamic, natural flow of conversation, such as real-time turn-taking. Therefore, we launched the first Human-like Spoken Dialogue Systems Challenge (HumDial) at ICASSP 2026 to benchmark these dual capabilities. Anchored by a sizable dataset derived from authentic human conversations, this initiative establishes a fair evaluation platform across two tracks: (1) Emotional Intelligence, targeting long-term emotion understanding and empathetic generation; and (2) Full-Duplex Interaction, systematically evaluating real-time decision-making under `` listening-while-speaking'' conditions. This paper summarizes the dataset, track configurations, and the final results.",
      "url": "https://arxiv.org/abs/2601.05564",
      "pdfUrl": "https://arxiv.org/pdf/2601.05564.pdf",
      "titleJa": "ICASSP 2026 HumDialチャレンジ：LLM時代における人間のような音声対話システムのベンチマーク"
    },
    {
      "id": "2601.05554",
      "arxivId": "2601.05554",
      "title": "SPAM: Style Prompt Adherence Metric for Prompt-based TTS",
      "authors": [
        "Chanhee Cho",
        "Nayeon Kim",
        "Bugeun Kim"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Prompt-based text-to-speech (TTS) aims to generate speech that adheres to fine-grained style cues provided in a text prompt. However, most prior works depend on neither plausible nor faithful measures to evaluate prompt adherence. That is, they cannot ensure whether the evaluation is grounded on the prompt and is similar to a human. Thus, we present a new automatic metric, the Style Prompt Adherence Metric, which explicitly satisfies both plausibility and faithfulness. Inspired by the CLAP, our approach factorizes speech into acoustic attributes and aligns them with the style prompt. Also, we trained the scorer with a supervised contrastive loss, which could provide a clearer distinction between different semantics. We conducted two experiments on two perspectives. The plausibility experiment showed that SPAM achieved a strong correlation with the mean opinion score (MOS). Also, the faithfulness experiment demonstrated that SPAM is successfully grounded to the given style prompt, as it can discriminate different semantics of the prompt. We believe that SPAM can provide a viable automatic solution for evaluating style prompt adherence of synthesized speech.",
      "url": "https://arxiv.org/abs/2601.05554",
      "pdfUrl": "https://arxiv.org/pdf/2601.05554.pdf",
      "titleJa": "SPAM: プロンプトベースの TTS におけるスタイルプロンプト遵守指標"
    },
    {
      "id": "2601.05543",
      "arxivId": "2601.05543",
      "title": "Closing the Modality Reasoning Gap for Speech Large Language Models",
      "authors": [
        "Chaoren Wang",
        "Heng Lu",
        "Xueyao Zhang",
        "Shujie Liu",
        "Yan Lu",
        "Jinyu Li",
        "Zhizheng Wu"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Although speech large language models have achieved notable progress, a substantial modality reasoning gap remains: their reasoning performance on speech inputs is markedly weaker than on text. This gap could be associated with representational drift across Transformer layers and behavior deviations in long-chain reasoning. To address this issue, we introduce TARS, a reinforcement-learning framework that aligns text-conditioned and speech-conditioned trajectories through an asymmetric reward design. The framework employs two dense and complementary signals: representation alignment, which measures layer-wise hidden-state similarity between speech- and text-conditioned trajectories, and behavior alignment, which evaluates semantic consistency between generated outputs and reference text completions. Experiments on challenging reasoning benchmarks, including MMSU and OBQA, show that our approach significantly narrows the modality reasoning gap and achieves state-of-the-art performance among 7B-scale Speech LLMs.",
      "url": "https://arxiv.org/abs/2601.05543",
      "pdfUrl": "https://arxiv.org/pdf/2601.05543.pdf",
      "titleJa": "音声大規模言語モデルのモダリティ推論ギャップを埋める"
    },
    {
      "id": "2601.05329",
      "arxivId": "2601.05329",
      "title": "CosyEdit: Unlocking End-to-End Speech Editing Capability from Zero-Shot Text-to-Speech Models",
      "authors": [
        "Junyang Chen",
        "Yuhang Jia",
        "Hui Wang",
        "Jiaming Zhou",
        "Yaxin Han",
        "Mengying Feng",
        "Yong Qin"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Automatic speech editing aims to modify spoken content based on textual instructions, yet traditional cascade systems suffer from complex preprocessing pipelines and a reliance on explicit external temporal alignment. Addressing these limitations, we propose CosyEdit, an end-to-end speech editing model adapted from CosyVoice through task-specific fine-tuning and an optimized inference procedure, which internalizes speech-text alignment while ensuring high consistency between the speech before and after editing. By fine-tuning on only 250 hours of supervised data from our curated GigaEdit dataset, our 400M-parameter model achieves reliable speech editing performance. Experiments on the RealEdit benchmark indicate that CosyEdit not only outperforms several billion-parameter language model baselines but also matches the performance of state-of-the-art cascade approaches. These results demonstrate that, with task-specific fine-tuning and inference optimization, robust and efficient speech editing capabilities can be unlocked from a zero-shot TTS model, yielding a novel and cost-effective end-to-end solution for high-quality speech editing.",
      "url": "https://arxiv.org/abs/2601.05329",
      "pdfUrl": "https://arxiv.org/pdf/2601.05329.pdf",
      "titleJa": "CosyEdit: ゼロショット音声合成モデルからエンドツーエンドの音声編集機能を解放"
    },
    {
      "id": "2601.05011",
      "arxivId": "2601.05011",
      "title": "Leveraging Prediction Entropy for Automatic Prompt Weighting in Zero-Shot Audio-Language Classification",
      "authors": [
        "Karim El Khoury",
        "Maxime Zanella",
        "Tiffanie Godelaine",
        "Christophe De Vleeschouwer",
        "Benoit Macq"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Audio-language models have recently demonstrated strong zero-shot capabilities by leveraging natural-language supervision to classify audio events without labeled training data. Yet, their performance is highly sensitive to the wording of text prompts, with small variations leading to large fluctuations in accuracy. Prior work has mitigated this issue through prompt learning or prompt ensembling. However, these strategies either require annotated data or fail to account for the fact that some prompts may negatively impact performance. In this work, we present an entropy-guided prompt weighting approach that aims to find a robust combination of prompt contributions to maximize prediction confidence. To this end, we formulate a tailored objective function that minimizes prediction entropy to yield new prompt weights, utilizing low-entropy as a proxy for high confidence. Our approach can be applied to individual samples or a batch of audio samples, requiring no additional labels and incurring negligible computational overhead. Experiments on five audio classification datasets covering environmental, urban, and vocal sounds, demonstrate consistent gains compared to classical prompt ensembling methods in a zero-shot setting, with accuracy improvements 5-times larger across the whole benchmark.",
      "url": "https://arxiv.org/abs/2601.05011",
      "pdfUrl": "https://arxiv.org/pdf/2601.05011.pdf",
      "titleJa": "ゼロショット音声言語分類における自動プロンプト重み付けのための予測エントロピーの活用"
    },
    {
      "id": "2601.04960",
      "arxivId": "2601.04960",
      "title": "A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction",
      "authors": [
        "Qing Wang",
        "Zehan Li",
        "Yaodong Song",
        "Hongjie Chen",
        "Jian Kang",
        "Jie Lian",
        "Jie Li",
        "Yongxiang Li",
        "Xuelong Li"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.",
      "url": "https://arxiv.org/abs/2601.04960",
      "pdfUrl": "https://arxiv.org/pdf/2601.04960.pdf",
      "titleJa": "人間のようなインタラクションを実現する感情帰属思考を組み込んだ統合音声言語モデル"
    },
    {
      "id": "2601.04876",
      "arxivId": "2601.04876",
      "title": "ChronosAudio: A Comprehensive Long-Audio Benchmark for Evaluating Audio-Large Language Models",
      "authors": [
        "Kaiwen Luo",
        "Liang Lin",
        "Yibo Zhang",
        "Moayad Aloqaily",
        "Dexian Wang",
        "Zhenhong Zhou",
        "Junwei Zhang",
        "Kun Wang",
        "Li Sun",
        "Qingsong Wen"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Although Audio Large Language Models (ALLMs) have witnessed substantial advancements, their long audio understanding capabilities remain unexplored. A plethora of benchmarks have been proposed for general audio tasks, they predominantly focus on short-form clips, leaving without a consensus on evaluating ALLMs over extended durations. This paper proposes ChronosAudio, the first multi-task benchmark tailored for long-audio understanding in ALLMs. It encompasses six major task categories and comprises 36,000 test instances totaling over 200 hours audio, stratified into short, middle, and long-form categories to comprehensively evaluate length generalization. Extensive experiments on 16 state-of-the-art models using ChronosAudio yield three critical findings: 1.Precipitous Long-Context Collapse: ALLMs exhibit a severe inability to sustain performance, with the transition from short to long contexts triggering a staggering performance degradation of over 90% in specific tasks. 2.Structural Attention Dilution: Performance degradation stems from a fundamental failure in maintaining temporal locality; attention mechanisms suffer from significant diffusion in later sequences. 3.Restorative Ceiling of Mitigation: Current strategies only offer 50% recovery. These findings reveal significant challenges in long-audio, underscoring the urgent need for approaches to achieve robust, document-level audio reasoning.",
      "url": "https://arxiv.org/abs/2601.04876",
      "pdfUrl": "https://arxiv.org/pdf/2601.04876.pdf",
      "titleJa": "ChronosAudio: 大規模音声言語モデルを評価するための包括的な長時間音声ベンチマーク"
    },
    {
      "id": "2601.04867",
      "arxivId": "2601.04867",
      "title": "Gradient-based Optimisation of Modulation Effects",
      "authors": [
        "Alistair Carson",
        "Alec Wright",
        "Stefan Bilbao"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Modulation effects such as phasers, flangers and chorus effects are heavily used in conjunction with the electric guitar. Machine learning based emulation of analog modulation units has been investigated in recent years, but most methods have either been limited to one class of effect or suffer from a high computational cost or latency compared to canonical digital implementations. Here, we build on previous work and present a framework for modelling flanger, chorus and phaser effects based on differentiable digital signal processing. The model is trained in the time-frequency domain, but at inference operates in the time-domain, requiring zero latency. We investigate the challenges associated with gradient-based optimisation of such effects, and show that low-frequency weighting of loss functions avoids convergence to local minima when learning delay times. We show that when trained against analog effects units, sound output from the model is in some cases perceptually indistinguishable from the reference, but challenges still remain for effects with long delay times and feedback.",
      "url": "https://arxiv.org/abs/2601.04867",
      "pdfUrl": "https://arxiv.org/pdf/2601.04867.pdf",
      "titleJa": "変調効果の勾配ベースの最適化"
    },
    {
      "id": "2601.07481",
      "arxivId": "2601.07481",
      "title": "Directional reflection modeling via wavenumber-domain reflection coefficient for 3D acoustic field simulation",
      "authors": [
        "Satoshi Hoshika",
        "Takahiro Iwami",
        "Akira Omoto"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This study proposes a framework for incorporating wavenumber-domain acoustic reflection coefficients into sound field analysis to characterize direction-dependent material reflection and scattering phenomena. The reflection coefficient is defined as the amplitude ratio between incident and reflected waves for each propagation direction and is estimated from spatial Fourier transforms of the incident and reflected sound fields. The resulting wavenumber-domain reflection coefficients are converted into an acoustic admittance representation that is directly compatible with numerical methods such as the Boundary Element Method (BEM), enabling simulation of reflections beyond simple specular components. Unlike conventional extended reaction models, the proposed approach avoids explicit modeling of the material interior. This significantly reduces computational cost while allowing direct use of measured data, empirical models, or user-defined directional reflection characteristics. The validity of the proposed formulation was previously demonstrated by the authors through two-dimensional sound field simulations, in which accurate reproduction of direction-dependent reflection behavior was confirmed. In the present work, the framework is extended to three-dimensional analysis, demonstrating its applicability to more realistic and complex acoustic environments. The proposed approach provides a practical and flexible tool for simulating direction-dependent acoustic reflections and scattering, with potential applications in architectural acoustics, material characterization, and noise control.",
      "url": "https://arxiv.org/abs/2601.07481",
      "pdfUrl": "https://arxiv.org/pdf/2601.07481.pdf",
      "titleJa": "3次元音響場シミュレーションのための波数領域反射係数による方向反射モデリング"
    },
    {
      "id": "2601.07064",
      "arxivId": "2601.07064",
      "title": "Bridging Attribution and Open-Set Detection using Graph-Augmented Instance Learning in Synthetic Speech",
      "authors": [
        "Mohd Mujtaba Akhtar",
        " Girish",
        "Farhan Sheth",
        "Muskaan Singh"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We propose a unified framework for not only attributing synthetic speech to its source but also for detecting speech generated by synthesizers that were not encountered during training. This requires methods that move beyond simple detection to support both detailed forensic analysis and open-set generalization. To address this, we introduce SIGNAL, a hybrid framework that combines speech foundation models (SFMs) with graph-based modeling and open-set-aware inference. Our framework integrates Graph Neural Networks (GNNs) and a k-Nearest Neighbor (KNN) classifier, allowing it to capture meaningful relationships between utterances and recognize speech that doesn`t belong to any known generator. It constructs a query-conditioned graph over generator class prototypes, enabling the GNN to reason over relationships among candidate generators, while the KNN branch supports open-set detection via confidence-based thresholding. We evaluate SIGNAL using the DiffSSD dataset, which offers a diverse mix of real speech and synthetic audio from both open-source and commercial diffusion-based TTS systems. To further assess generalization, we also test on the SingFake benchmark. Our results show that SIGNAL consistently improves performance across both tasks, with Mamba-based embeddings delivering especially strong results. To the best of our knowledge, this is the first study to unify graph-based learning and open-set detection for tracing synthetic speech back to its origin.",
      "url": "https://arxiv.org/abs/2601.07064",
      "pdfUrl": "https://arxiv.org/pdf/2601.07064.pdf",
      "titleJa": "合成音声におけるグラフ拡張インスタンス学習を用いた帰属とオープンセット検出の橋渡し"
    },
    {
      "id": "2601.07014",
      "arxivId": "2601.07014",
      "title": "DIVINE: Coordinating Multimodal Disentangled Representations for Oro-Facial Neurological Disorder Assessment",
      "authors": [
        "Mohd Mujtaba Akhtar",
        " Girish",
        "Muskaan Singh"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "eess.AS"
      ],
      "abstract": "In this study, we present a multimodal framework for predicting neuro-facial disorders by capturing both vocal and facial cues. We hypothesize that explicitly disentangling shared and modality-specific representations within multimodal foundation model embeddings can enhance clinical interpretability and generalization. To validate this hypothesis, we propose DIVINE a fully disentangled multimodal framework that operates on representations extracted from state-of-the-art (SOTA) audio and video foundation models, incorporating hierarchical variational bottlenecks, sparse gated fusion, and learnable symptom tokens. DIVINE operates in a multitask learning setup to jointly predict diagnostic categories (Healthy Control,ALS, Stroke) and severity levels (Mild, Moderate, Severe). The model is trained using synchronized audio and video inputs and evaluated on the Toronto NeuroFace dataset under full (audio-video) as well as single-modality (audio- only and video-only) test conditions. Our proposed approach, DIVINE achieves SOTA result, with the DeepSeek-VL2 and TRILLsson combination reaching 98.26% accuracy and 97.51% F1-score. Under modality-constrained scenarios, the framework performs well, showing strong generalization when tested with video-only or audio-only inputs. It consistently yields superior performance compared to unimodal models and baseline fusion techniques. To the best of our knowledge, DIVINE is the first framework that combines cross-modal disentanglement, adaptive fusion, and multitask learning to comprehensively assess neurological disorders using synchronized speech and facial video.",
      "url": "https://arxiv.org/abs/2601.07014",
      "pdfUrl": "https://arxiv.org/pdf/2601.07014.pdf",
      "titleJa": "DIVINE: 口腔顔面神経疾患評価のためのマルチモーダル分離表現の調整"
    },
    {
      "id": "2601.06896",
      "arxivId": "2601.06896",
      "title": "TagSpeech: End-to-End Multi-Speaker ASR and Diarization with Fine-Grained Temporal Grounding",
      "authors": [
        "Mingyue Huo",
        "Yiwen Shao",
        "Yuheng Zhang"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "abstract": "We present TagSpeech, a unified LLM-based framework that utilizes Temporal Anchor Grounding for joint multi-speaker ASR and diarization. The framework is built on two key designs: (1) decoupled semantic and speaker streams fine-tuned via Serialized Output Training (SOT) to learn turn-taking dynamics; and (2) an interleaved time anchor mechanism that not only supports fine-grained timestamp prediction but also acts as a synchronization signal between semantic understanding and speaker tracking. Compared to previous works that primarily focus on speaker-attributed ASR or implicit diarization, TagSpeech addresses the challenge of fine-grained speaker-content alignment and explicitly models \"who spoke what and when\" in an end-to-end manner. Experiments on AMI and AliMeeting benchmarks demonstrate that our method achieves consistent improvements in Diarization Error Rate (DER) over strong end-to-end baselines, including Qwen-Omni and Gemini, particularly in handling complex speech overlaps. Moreover, TagSpeech employs a parameter-efficient training paradigm in which the LLM backbone is frozen and only lightweight projectors are trained, resulting in strong performance with low computational cost.",
      "url": "https://arxiv.org/abs/2601.06896",
      "pdfUrl": "https://arxiv.org/pdf/2601.06896.pdf",
      "titleJa": "TagSpeech: 細粒度時間グラウンディングによるエンドツーエンドのマルチスピーカーASRとダイアライゼーション"
    },
    {
      "id": "2601.06844",
      "arxivId": "2601.06844",
      "title": "Variational decomposition autoencoding improves disentanglement of latent representations",
      "authors": [
        "Ioannis Ziogas",
        "Aamna Al Shehhi",
        "Ahsan H. Khandoker",
        "Leontios J. Hadjileontiadis"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.AS",
        "eess.SP",
        "stat.ML"
      ],
      "abstract": "Understanding the structure of complex, nonstationary, high-dimensional time-evolving signals is a central challenge in scientific data analysis. In many domains, such as speech and biomedical signal processing, the ability to learn disentangled and interpretable representations is critical for uncovering latent generative mechanisms. Traditional approaches to unsupervised representation learning, including variational autoencoders (VAEs), often struggle to capture the temporal and spectral diversity inherent in such data. Here we introduce variational decomposition autoencoding (VDA), a framework that extends VAEs by incorporating a strong structural bias toward signal decomposition. VDA is instantiated through variational decomposition autoencoders (DecVAEs), i.e., encoder-only neural networks that combine a signal decomposition model, a contrastive self-supervised task, and variational prior approximation to learn multiple latent subspaces aligned with time-frequency characteristics. We demonstrate the effectiveness of DecVAEs on simulated data and three publicly available scientific datasets, spanning speech recognition, dysarthria severity evaluation, and emotional speech classification. Our results demonstrate that DecVAEs surpass state-of-the-art VAE-based methods in terms of disentanglement quality, generalization across tasks, and the interpretability of latent encodings. These findings suggest that decomposition-aware architectures can serve as robust tools for extracting structured representations from dynamic signals, with potential applications in clinical diagnostics, human-computer interaction, and adaptive neurotechnologies.",
      "url": "https://arxiv.org/abs/2601.06844",
      "pdfUrl": "https://arxiv.org/pdf/2601.06844.pdf",
      "titleJa": "変分分解オートエンコーディングは潜在表現の分離を改善する"
    },
    {
      "id": "2601.06199",
      "arxivId": "2601.06199",
      "title": "FastSLM: Hierarchical Frame Q-Former for Effective Speech Modality Adaptation",
      "authors": [
        "Junseok Lee",
        "Sangyong Lee",
        "Chang-Jae Chun"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated human-expert-level capabilities, driving significant interest in their potential for achieving artificial general intelligence (AGI). In particular, there is growing momentum in adapting LLMs to various modalities, including vision, video, and speech, through the development of multimodal LLMs (MLLMs). However, existing speech-language model (SLM) research has largely overlooked cost-effective adaptation strategies for leveraging LLMs in the speech domain. In this paper, we propose FastSLM, a lightweight yet efficient SLM designed for effective understanding and reasoning over long-form speech. To address the challenge of aligning high-frame-rate speech features with LLMs, we introduce the Hierarchical Frame Querying Transformer (HFQ-Former), which compresses frame-level speech features while capturing both local and global context. Furthermore, we present a novel three-stage training strategy that enhances generalization across a wide range of speech-related tasks. Experimental results demonstrate that FastSLM achieves competitive performance compared to existing state-of-the-art models, despite operating with significantly lower FLOPs and parameter counts, while representing speech with only 1.67 tokens per second. The source code and model checkpoints are available at https://huggingface.co/okestro-ai-lab/FastSLM.",
      "url": "https://arxiv.org/abs/2601.06199",
      "pdfUrl": "https://arxiv.org/pdf/2601.06199.pdf",
      "titleJa": "FastSLM: 効果的な音声モダリティ適応のための階層的フレームQフォーマー"
    },
    {
      "id": "2601.04654",
      "arxivId": "2601.04654",
      "title": "LLMs-Integrated Automatic Hate Speech Recognition Using Controllable Text Generation Models",
      "authors": [
        "Ryutaro Oshima",
        "Yuya Hosoda",
        "Youji Iiguni"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "This paper proposes an automatic speech recognition (ASR) model for hate speech using large language models (LLMs). The proposed method integrates the encoder of the ASR model with the decoder of the LLMs, enabling simultaneous transcription and censorship tasks to prevent the exposure of harmful content. Instruction tuning of the LLM to mask hate-related words with specific tokens requires an annotated hate speech dataset, which is limited. We generate text samples using an LLM with the Chain-of-Thought (CoT) prompting technique guided by cultural context and examples and then convert them into speech samples using a text-to-speech (TTS) system. However, some of them contain non-hate speech samples with hate-related words, which degrades the censorship performance. This paper filters the samples which text classification models correctly label as hate content. By adjusting the threshold for the number of correct answer models, we can control the level of hate in the generated dataset, allowing us to train the LLMs through curriculum learning in a gradual manner. Experimental results show that the proposed method achieves a masking accuracy of 58.6\\% for hate-related words, surpassing previous baselines. We also confirm that the curriculum training contributes to the efficiency of both transcription and censorship tasks.",
      "url": "https://arxiv.org/abs/2601.04654",
      "pdfUrl": "https://arxiv.org/pdf/2601.04654.pdf",
      "titleJa": "制御可能なテキスト生成モデルを用いたLLM統合型自動ヘイトスピーチ認識"
    },
    {
      "id": "2601.04459",
      "arxivId": "2601.04459",
      "title": "Latent-Level Enhancement with Flow Matching for Robust Automatic Speech Recognition",
      "authors": [
        "Da-Hee Yang",
        "Joon-Hyuk Chang"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Noise-robust automatic speech recognition (ASR) has been commonly addressed by applying speech enhancement (SE) at the waveform level before recognition. However, speech-level enhancement does not always translate into consistent recognition improvements due to residual distortions and mismatches with the latent space of the ASR encoder. In this letter, we introduce a complementary strategy termed latent-level enhancement, where distorted representations are refined during ASR inference. Specifically, we propose a plug-and-play Flow Matching Refinement module (FM-Refiner) that operates on the output latents of a pretrained CTC-based ASR encoder. Trained to map imperfect latents-either directly from noisy inputs or from enhanced-but-imperfect speech-toward their clean counterparts, the FM-Refiner is applied only at inference, without fine-tuning ASR parameters. Experiments show that FM-Refiner consistently reduces word error rate, both when directly applied to noisy inputs and when combined with conventional SE front-ends. These results demonstrate that latent-level refinement via flow matching provides a lightweight and effective complement to existing SE approaches for robust ASR.",
      "url": "https://arxiv.org/abs/2601.04459",
      "pdfUrl": "https://arxiv.org/pdf/2601.04459.pdf",
      "titleJa": "フローマッチングによる潜在レベル強化による堅牢な自動音声認識"
    },
    {
      "id": "2601.04343",
      "arxivId": "2601.04343",
      "title": "Summary of The Inaugural Music Source Restoration Challenge",
      "authors": [
        "Yongyi Zang",
        "Jiarui Hai",
        "Wanying Ge",
        "Qiuqiang Kong",
        "Zheqi Dai",
        "Helin Wang",
        "Yuki Mitsufuji",
        "Mark D. Plumbley"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Music Source Restoration (MSR) aims to recover original, unprocessed instrument stems from professionally mixed and degraded audio, requiring the reversal of both production effects and real-world degradations. We present the inaugural MSR Challenge, which features objective evaluation on studio-produced mixtures using Multi-Mel-SNR, Zimtohrli, and FAD-CLAP, alongside subjective evaluation on real-world degraded recordings. Five teams participated in the challenge. The winning system achieved 4.46 dB Multi-Mel-SNR and 3.47 MOS-Overall, corresponding to relative improvements of 91% and 18% over the second-place system, respectively. Per-stem analysis reveals substantial variation in restoration difficulty across instruments, with bass averaging 4.59 dB across all teams, while percussion averages only 0.29 dB. The dataset, evaluation protocols, and baselines are available at https://msrchallenge.com/.",
      "url": "https://arxiv.org/abs/2601.04343",
      "pdfUrl": "https://arxiv.org/pdf/2601.04343.pdf",
      "titleJa": "第1回音楽ソース修復チャレンジの概要"
    },
    {
      "id": "2601.07832",
      "arxivId": "2601.07832",
      "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
      "authors": [
        "Kewei Zhang",
        "Ye Huang",
        "Yufan Deng",
        "Jincheng Yu",
        "Junsong Chen",
        "Huan Ling",
        "Enze Xie",
        "Daquan Zhou"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\% improvement on ImageNet classification, a 6.3\\% gain on NLP, a 12.6\\% improvement on image generation, and a 41\\% enhancement on video generation under the same time complexity.",
      "url": "https://arxiv.org/abs/2601.07832",
      "pdfUrl": "https://arxiv.org/pdf/2601.07832.pdf",
      "titleJa": "MHLA: トークンレベルマルチヘッドによる線形注意の表現力の回復"
    },
    {
      "id": "2601.07821",
      "arxivId": "2601.07821",
      "title": "Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation",
      "authors": [
        "Huanyu Li",
        "Kun Lei",
        "Sheng Zang",
        "Kaizhe Hu",
        "Yongyuan Liang",
        "Bo An",
        "Xiaoli Li",
        "Huazhe Xu"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.",
      "url": "https://arxiv.org/abs/2601.07821",
      "pdfUrl": "https://arxiv.org/pdf/2601.07821.pdf",
      "titleJa": "障害認識強化学習：実世界操作のための自己回復機能を備えた信頼性の高いオフラインからオンラインへの強化学習"
    },
    {
      "id": "2601.07794",
      "arxivId": "2601.07794",
      "title": "Kinship Data Benchmark for Multi-hop Reasoning",
      "authors": [
        "Tianda Sun",
        "Dimitar Kazakov"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Large language models (LLMs) are increasingly evaluated on their ability to perform multi-hop reasoning, i.e., to combine multiple pieces of information into a coherent inference. We introduce KinshipQA, a benchmark designed to probe this capability through reasoning over kinship relations. The central contribution of our work is a generative pipeline that produces, on demand, large-scale, realistic, and culture-specific genealogical data: collections of interconnected family trees that satisfy explicit marriage constraints associated with different kinship systems. This allows task difficulty, cultural assumptions, and relational depth to be systematically controlled and varied. From these genealogies, we derive textual inference tasks that require reasoning over implicit relational chains. We evaluate the resulting benchmark using six state-of-the-art LLMs, spanning both open-source and closed-source models, under a uniform zero-shot protocol with deterministic decoding. Performance is measured using exact-match and set-based metrics. Our results demonstrate that KinshipQA yields a wide spread of outcomes and exposes systematic differences in multi-hop reasoning across models and cultural settings.",
      "url": "https://arxiv.org/abs/2601.07794",
      "pdfUrl": "https://arxiv.org/pdf/2601.07794.pdf",
      "titleJa": "マルチホップ推論のための親族データベンチマーク"
    },
    {
      "id": "2601.07790",
      "arxivId": "2601.07790",
      "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
      "authors": [
        "Yahya Masri",
        "Emily Ma",
        "Zifu Wang",
        "Joseph Rogers",
        "Chaowei Yang"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.AI"
      ],
      "abstract": "System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.",
      "url": "https://arxiv.org/abs/2601.07790",
      "pdfUrl": "https://arxiv.org/pdf/2601.07790.pdf",
      "titleJa": "システムログ重大度分類における小規模言語モデルと小規模推論言語モデルのベンチマーク"
    },
    {
      "id": "2601.07782",
      "arxivId": "2601.07782",
      "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning",
      "authors": [
        "Wei Fang",
        "James Glass"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "abstract": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.",
      "url": "https://arxiv.org/abs/2601.07782",
      "pdfUrl": "https://arxiv.org/pdf/2601.07782.pdf",
      "titleJa": "シングルショットを超えて: クエリプランニングによるマルチステップツール取得"
    },
    {
      "id": "2601.07779",
      "arxivId": "2601.07779",
      "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
      "authors": [
        "Bowen Yang",
        "Kaiming Jin",
        "Zhenyu Wu",
        "Zhaoyang Liu",
        "Qiushi Sun",
        "Zehao Li",
        "JingJing Xie",
        "Zhoumianze Liu",
        "Fangzhi Xu",
        "Kanzhi Cheng",
        "Qingyun Li",
        "Yian Wang",
        "Yu Qiao",
        "Zun Wang",
        "Zichen Ding"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.HC"
      ],
      "abstract": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.",
      "url": "https://arxiv.org/abs/2601.07779",
      "pdfUrl": "https://arxiv.org/pdf/2601.07779.pdf",
      "titleJa": "OS-Symphony: 堅牢かつ汎用的なコンピュータ利用エージェントのための総合的フレームワーク"
    },
    {
      "id": "2601.07778",
      "arxivId": "2601.07778",
      "title": "DT-ICU: Towards Explainable Digital Twins for ICU Patient Monitoring via Multi-Modal and Multi-Task Iterative Inference",
      "authors": [
        "Wen Guo"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "We introduce DT-ICU, a multimodal digital twin framework for continuous risk estimation in intensive care. DT-ICU integrates variable-length clinical time series with static patient information in a unified multitask architecture, enabling predictions to be updated as new observations accumulate over the ICU stay. We evaluate DT-ICU on the large, publicly available MIMIC-IV dataset, where it consistently outperforms established baseline models under different evaluation settings. Our test-length analysis shows that meaningful discrimination is achieved shortly after admission, while longer observation windows further improve the ranking of high-risk patients in highly imbalanced cohorts. To examine how the model leverages heterogeneous data sources, we perform systematic modality ablations, revealing that the model learnt a reasonable structured reliance on interventions, physiological response observations, and contextual information. These analyses provide interpretable insights into how multimodal signals are combined and how trade-offs between sensitivity and precision emerge. Together, these results demonstrate that DT-ICU delivers accurate, temporally robust, and interpretable predictions, supporting its potential as a practical digital twin framework for continuous patient monitoring in critical care. The source code and trained model weights for DT-ICU are publicly available at https://github.com/GUO-W/DT-ICU-release.",
      "url": "https://arxiv.org/abs/2601.07778",
      "pdfUrl": "https://arxiv.org/pdf/2601.07778.pdf",
      "titleJa": "DT-ICU: マルチモーダル・マルチタスク反復推論によるICU患者モニタリングのための説明可能なデジタルツインの構築"
    },
    {
      "id": "2601.07748",
      "arxivId": "2601.07748",
      "title": "Improving Domain Generalization in Contrastive Learning using Adaptive Temperature Control",
      "authors": [
        "Robert Lewis",
        "Katie Matton",
        "Rosalind W. Picard",
        "John Guttag"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Self-supervised pre-training with contrastive learning is a powerful method for learning from sparsely labeled data. However, performance can drop considerably when there is a shift in the distribution of data from training to test time. We study this phenomenon in a setting in which the training data come from multiple domains, and the test data come from a domain not seen at training that is subject to significant covariate shift. We present a new method for contrastive learning that incorporates domain labels to increase the domain invariance of learned representations, leading to improved out-of-distribution generalization. Our method adjusts the temperature parameter in the InfoNCE loss -- which controls the relative weighting of negative pairs -- using the probability that a negative sample comes from the same domain as the anchor. This upweights pairs from more similar domains, encouraging the model to discriminate samples based on domain-invariant attributes. Through experiments on a variant of the MNIST dataset, we demonstrate that our method yields better out-of-distribution performance than domain generalization baselines. Furthermore, our method maintains strong in-distribution task performance, substantially outperforming baselines on this measure.",
      "url": "https://arxiv.org/abs/2601.07748",
      "pdfUrl": "https://arxiv.org/pdf/2601.07748.pdf",
      "titleJa": "適応温度制御を用いた対照学習におけるドメイン汎化の改善"
    },
    {
      "id": "2601.07737",
      "arxivId": "2601.07737",
      "title": "Evaluating the encoding competence of visual language models using uncommon actions",
      "authors": [
        "Chen Ling",
        "Nai Ding"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "We propose UAIT (Uncommon-sense Action Image-Text) dataset, a new evaluation benchmark designed to test the semantic understanding ability of visual language models (VLMs) in uncommon-sense action scenes. Unlike previous datasets that focus on common visual scenes with statistical frequency advantages, UAIT challenges models with grammatically reasonable but semantically counter-common sense image-text pairs. Such tasks require models to go beyond superficial pattern recognition and demonstrate a deep understanding of agent-patient relationships and physical feasibility. To build UAIT, we designed a semi-automated process to synthesize high-quality uncommon-sense image-text samples using large language models, few-shot prompt engineering, and text-to-image generation. Each sample is accompanied by a carefully designed multiple-choice question to test the model's competence in fine-grained reasoning. We evaluate multiple state-of-the-art visual language models and compare them with models based on contrastive learning. Experiments show that all models perform significantly worse than humans in semantic judgment, especially in distinguishing grammatical correctness from semantic rationality. Further experiments show that even the lightweight model can improve its accuracy after fine-tuning, demonstrating the great potential of directional adaptation. This study not only reveals the key weaknesses of VLMs, but also provides diagnostic tools and research directions for the development of robust models with real visual semantic reasoning capabilities.",
      "url": "https://arxiv.org/abs/2601.07737",
      "pdfUrl": "https://arxiv.org/pdf/2601.07737.pdf",
      "titleJa": "非日常的な行動を用いた視覚言語モデルの符号化能力の評価"
    },
    {
      "id": "2601.07718",
      "arxivId": "2601.07718",
      "title": "Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids",
      "authors": [
        "Shaoting Zhu",
        "Ziwen Zhuang",
        "Mengjie Zhao",
        "Kun-Ying Lee",
        "Hang Zhao"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "abstract": "Achieving robust humanoid hiking in complex, unstructured environments requires transitioning from reactive proprioception to proactive perception. However, integrating exteroception remains a significant challenge: mapping-based methods suffer from state estimation drift; for instance, LiDAR-based methods do not handle torso jitter well. Existing end-to-end approaches often struggle with scalability and training complexity; specifically, some previous works using virtual obstacles are implemented case-by-case. In this work, we present \\textit{Hiking in the Wild}, a scalable, end-to-end parkour perceptive framework designed for robust humanoid hiking. To ensure safety and training stability, we introduce two key mechanisms: a foothold safety mechanism combining scalable \\textit{Terrain Edge Detection} with \\textit{Foot Volume Points} to prevent catastrophic slippage on edges, and a \\textit{Flat Patch Sampling} strategy that mitigates reward hacking by generating feasible navigation targets. Our approach utilizes a single-stage reinforcement learning scheme, mapping raw depth inputs and proprioception directly to joint actions, without relying on external state estimation. Extensive field experiments on a full-size humanoid demonstrate that our policy enables robust traversal of complex terrains at speeds up to 2.5 m/s. The training and deployment code is open-sourced to facilitate reproducible research and deployment on real robots with minimal hardware modifications.",
      "url": "https://arxiv.org/abs/2601.07718",
      "pdfUrl": "https://arxiv.org/pdf/2601.07718.pdf",
      "titleJa": "野生でのハイキング：ヒューマノイドのためのスケーラブルな知覚パルクールフレームワーク"
    },
    {
      "id": "2601.07701",
      "arxivId": "2601.07701",
      "title": "Deep Whole-body Parkour",
      "authors": [
        "Ziwen Zhuang",
        "Shaoting Zhu",
        "Mengjie Zhao",
        "Hang Zhao"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "abstract": "Current approaches to humanoid control generally fall into two paradigms: perceptive locomotion, which handles terrain well but is limited to pedal gaits, and general motion tracking, which reproduces complex skills but ignores environmental capabilities. This work unites these paradigms to achieve perceptive general motion control. We present a framework where exteroceptive sensing is integrated into whole-body motion tracking, permitting a humanoid to perform highly dynamic, non-locomotion tasks on uneven terrain. By training a single policy to perform multiple distinct motions across varied terrestrial features, we demonstrate the non-trivial benefit of integrating perception into the control loop. Our results show that this framework enables robust, highly dynamic multi-contact motions, such as vaulting and dive-rolling, on unstructured terrain, significantly expanding the robot's traversability beyond simple walking or running. https://project-instinct.github.io/deep-whole-body-parkour",
      "url": "https://arxiv.org/abs/2601.07701",
      "pdfUrl": "https://arxiv.org/pdf/2601.07701.pdf",
      "titleJa": "ディープ全身パルクール"
    },
    {
      "id": "2601.07685",
      "arxivId": "2601.07685",
      "title": "Predictive Analytics for Dementia: Machine Learning on Healthcare Data",
      "authors": [
        "Shafiul Ajam Opee",
        "Nafiz Fahad",
        "Anik Sen",
        "Rasel Ahmed",
        "Fariha Jahan",
        "Md. Kishor Morol",
        "Md Rashedul Islam"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Dementia is a complex syndrome impacting cognitive and emotional functions, with Alzheimer's disease being the most common form. This study focuses on enhancing dementia prediction using machine learning (ML) techniques on patient health data. Supervised learning algorithms are applied in this study, including K-Nearest Neighbors (KNN), Quadratic Discriminant Analysis (QDA), Linear Discriminant Analysis (LDA), and Gaussian Process Classifiers. To address class imbalance and improve model performance, techniques such as Synthetic Minority Over-sampling Technique (SMOTE) and Term Frequency-Inverse Document Frequency (TF-IDF) vectorization were employed. Among the models, LDA achieved the highest testing accuracy of 98%. This study highlights the importance of model interpretability and the correlation of dementia with features such as the presence of the APOE-epsilon4 allele and chronic conditions like diabetes. This research advocates for future ML innovations, particularly in integrating explainable AI approaches, to further improve predictive capabilities in dementia care.",
      "url": "https://arxiv.org/abs/2601.07685",
      "pdfUrl": "https://arxiv.org/pdf/2601.07685.pdf",
      "titleJa": "認知症の予測分析：医療データに基づく機械学習"
    },
    {
      "id": "2601.07667",
      "arxivId": "2601.07667",
      "title": "Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference",
      "authors": [
        "Rei Taniguchi",
        "Yuyang Dong",
        "Makoto Onizuka",
        "Chuan Xiao"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Due to the prevalence of large language models (LLMs), key-value (KV) cache reduction for LLM inference has received remarkable attention. Among numerous works that have been proposed in recent years, layer-wise token pruning approaches, which select a subset of tokens at particular layers to retain in KV cache and prune others, are one of the most popular schemes. They primarily adopt a set of pre-defined layers, at which tokens are selected. Such design is inflexible in the sense that the accuracy significantly varies across tasks and deteriorates in harder tasks such as KV retrieval. In this paper, we propose ASL, a training-free method that adaptively chooses the selection layer for KV cache reduction, exploiting the variance of token ranks ordered by attention score. The proposed method balances the performance across different tasks while meeting the user-specified KV budget requirement. ASL operates during the prefilling stage and can be jointly used with existing KV cache reduction methods such as SnapKV to optimize the decoding stage. By evaluations on the InfiniteBench, RULER, and NIAH benchmarks, we show that equipped with one-shot token selection, where tokens are selected at a layer and propagated to deeper layers, ASL outperforms state-of-the-art layer-wise token selection methods in accuracy while maintaining decoding speed and KV cache reduction.",
      "url": "https://arxiv.org/abs/2601.07667",
      "pdfUrl": "https://arxiv.org/pdf/2601.07667.pdf",
      "titleJa": "LLM推論における層単位のトークンプルーニングのための適応型層選択"
    },
    {
      "id": "2601.07666",
      "arxivId": "2601.07666",
      "title": "Variational Contrastive Learning for Skeleton-based Action Recognition",
      "authors": [
        "Dang Dinh Nguyen",
        "Decky Aspandi Latif",
        "Titus Zaharia"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "In recent years, self-supervised representation learning for skeleton-based action recognition has advanced with the development of contrastive learning methods. However, most of contrastive paradigms are inherently discriminative and often struggle to capture the variability and uncertainty intrinsic to human motion. To address this issue, we propose a variational contrastive learning framework that integrates probabilistic latent modeling with contrastive self-supervised learning. This formulation enables the learning of structured and semantically meaningful representations that generalize across different datasets and supervision levels. Extensive experiments on three widely used skeleton-based action recognition benchmarks show that our proposed method consistently outperforms existing approaches, particularly in low-label regimes. Moreover, qualitative analyses show that the features provided by our method are more relevant given the motion and sample characteristics, with more focus on important skeleton joints, when compared to the other methods.",
      "url": "https://arxiv.org/abs/2601.07666",
      "pdfUrl": "https://arxiv.org/pdf/2601.07666.pdf",
      "titleJa": "スケルトンベースの動作認識のための変分対照学習"
    },
    {
      "id": "2601.07663",
      "arxivId": "2601.07663",
      "title": "Reasoning Models Will Blatantly Lie About Their Reasoning",
      "authors": [
        "William Walden"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "It has been shown that Large Reasoning Models (LRMs) may not *say what they think*: they do not always volunteer information about how certain parts of the input influence their reasoning. But it is one thing for a model to *omit* such information and another, worse thing to *lie* about it. Here, we extend the work of Chen et al. (2025) to show that LRMs will do just this: they will flatly deny relying on hints provided in the prompt in answering multiple choice questions -- even when directly asked to reflect on unusual (i.e. hinted) prompt content, even when allowed to use hints, and even though experiments *show* them to be using the hints. Our results thus have discouraging implications for CoT monitoring and interpretability.",
      "url": "https://arxiv.org/abs/2601.07663",
      "pdfUrl": "https://arxiv.org/pdf/2601.07663.pdf",
      "titleJa": "推論モデルは推論について露骨に嘘をつく"
    },
    {
      "id": "2601.07654",
      "arxivId": "2601.07654",
      "title": "Towards Automating Blockchain Consensus Verification with IsabeLLM",
      "authors": [
        "Elliot Jones",
        "William Knottenbelt"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "abstract": "Consensus protocols are crucial for a blockchain system as they are what allow agreement between the system's nodes in a potentially adversarial environment. For this reason, it is paramount to ensure their correct design and implementation to prevent such adversaries from carrying out malicious behaviour. Formal verification allows us to ensure the correctness of such protocols, but requires high levels of effort and expertise to carry out and thus is often omitted in the development process. In this paper, we present IsabeLLM, a tool that integrates the proof assistant Isabelle with a Large Language Model to assist and automate proofs. We demonstrate the effectiveness of IsabeLLM by using it to develop a novel model of Bitcoin's Proof of Work consensus protocol and verify its correctness. We use the DeepSeek R1 API for this demonstration and found that we were able to generate correct proofs for each of the non-trivial lemmas present in the verification.",
      "url": "https://arxiv.org/abs/2601.07654",
      "pdfUrl": "https://arxiv.org/pdf/2601.07654.pdf",
      "titleJa": "IsabeLLMによるブロックチェーンコンセンサス検証の自動化に向けて"
    },
    {
      "id": "2601.07651",
      "arxivId": "2601.07651",
      "title": "Active Evaluation of General Agents: Problem Definition and Comparison of Baseline Algorithms",
      "authors": [
        "Marc Lanctot",
        "Kate Larson",
        "Ian Gemp",
        "Michael Kaisers"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.LG",
        "cs.MA"
      ],
      "abstract": "As intelligent agents become more generally-capable, i.e. able to master a wide variety of tasks, the complexity and cost of properly evaluating them rises significantly. Tasks that assess specific capabilities of the agents can be correlated and stochastic, requiring many samples for accurate comparisons, leading to added costs. In this paper, we propose a formal definition and a conceptual framework for active evaluation of agents across multiple tasks, which assesses the performance of ranking algorithms as a function of number of evaluation data samples. Rather than curating, filtering, or compressing existing data sets as a preprocessing step, we propose an online framing: on every iteration, the ranking algorithm chooses the task and agents to sample scores from. Then, evaluation algorithms report a ranking of agents on each iteration and their performance is assessed with respect to the ground truth ranking over time. Several baselines are compared under different experimental contexts, with synthetic generated data and simulated online access to real evaluation data from Atari game-playing agents. We find that the classical Elo rating system -- while it suffers from well-known failure modes, in theory -- is a consistently reliable choice for efficient reduction of ranking error in practice. A recently-proposed method, Soft Condorcet Optimization, shows comparable performance to Elo on synthetic data and significantly outperforms Elo on real Atari agent evaluation. When task variation from the ground truth is high, selecting tasks based on proportional representation leads to higher rate of ranking error reduction.",
      "url": "https://arxiv.org/abs/2601.07651",
      "pdfUrl": "https://arxiv.org/pdf/2601.07651.pdf",
      "titleJa": "汎用エージェントの能動評価：問題定義とベースラインアルゴリズムの比較"
    },
    {
      "id": "2601.07641",
      "arxivId": "2601.07641",
      "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
      "authors": [
        "Jiaxuan Lu",
        "Ziyu Kong",
        "Yemin Wang",
        "Rong Fu",
        "Haiyuan Wan",
        "Cheng Yang",
        "Wenjie Lou",
        "Haoran Sun",
        "Lilong Wang",
        "Yankai Jiang",
        "Xiaosong Wang",
        "Xiao Sun",
        "Dongzhan Zhou"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "abstract": "The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.",
      "url": "https://arxiv.org/abs/2601.07641",
      "pdfUrl": "https://arxiv.org/pdf/2601.07641.pdf",
      "titleJa": "静的ツールを超えて：科学的推論のためのテスト時ツールの進化"
    },
    {
      "id": "2601.07638",
      "arxivId": "2601.07638",
      "title": "SALT-KG: A Benchmark for Semantics-Aware Learning on Enterprise Tables",
      "authors": [
        "Isaiah Onando Mulang",
        "Felix Sasaki",
        "Tassilo Klein",
        "Jonas Kolk",
        "Nikolay Grechanov",
        "Johannes Hoffart"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Building upon the SALT benchmark for relational prediction (Klein et al., 2024), we introduce SALT-KG, a benchmark for semantics-aware learning on enterprise tables. SALT-KG extends SALT by linking its multi-table transactional data with a structured Operational Business Knowledge represented in a Metadata Knowledge Graph (OBKG) that captures field-level descriptions, relational dependencies, and business object types. This extension enables evaluation of models that jointly reason over tabular evidence and contextual semantics, an increasingly critical capability for foundation models on structured data. Empirical analysis reveals that while metadata-derived features yield modest improvements in classical prediction metrics, these metadata features consistently highlight gaps in the ability of models to leverage semantics in relational context. By reframing tabular prediction as semantics-conditioned reasoning, SALT-KG establishes a benchmark to advance tabular foundation models grounded in declarative knowledge, providing the first empirical step toward semantically linked tables in structured data at enterprise scale.",
      "url": "https://arxiv.org/abs/2601.07638",
      "pdfUrl": "https://arxiv.org/pdf/2601.07638.pdf",
      "titleJa": "SALT-KG: エンタープライズテーブルにおけるセマンティクスを考慮した学習のベンチマーク"
    },
    {
      "id": "2601.07635",
      "arxivId": "2601.07635",
      "title": "Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence",
      "authors": [
        "Denis D. Caprioti",
        "Matheus Haas",
        "Constantino F. Vasconcelos",
        "Mauricio Girardi-Schappo"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cond-mat.dis-nn",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph",
        "physics.ed-ph"
      ],
      "abstract": "The Hopfield model, originally inspired by spin-glass physics, occupies a central place at the intersection of statistical mechanics, neural networks, and modern artificial intelligence. Despite its conceptual simplicity and broad applicability -- from associative memory to near-optimal solutions of combinatorial optimization problems -- it is rarely integrated into standard undergraduate physics curricula. In this paper, we present the Hopfield model as a pedagogically rich framework that naturally unifies core topics from undergraduate statistical physics, dynamical systems, linear algebra, and computational methods. We provide a concise and illustrated theoretical introduction grounded in familiar physics concepts, analyze the model's energy function, dynamics, and pattern stability, and discuss practical aspects of simulation, including a freely available simulation code. To support instruction, we conclude with classroom-ready example problems designed to mirror research practice. By explicitly connecting fundamental physics to contemporary AI applications, this work aims to help prepare physics students to understand, apply, and critically engage with the computational tools increasingly central to research, industry, and society.",
      "url": "https://arxiv.org/abs/2601.07635",
      "pdfUrl": "https://arxiv.org/pdf/2601.07635.pdf",
      "titleJa": "学習について学ぶ：スピングラスから人工知能への物理学の道"
    },
    {
      "id": "2601.04592",
      "arxivId": "2601.04592",
      "title": "Density Matrix RNN (DM-RNN): A Quantum Information Theoretic Framework for Modeling Musical Context and Polyphony",
      "authors": [
        "Joonwon Seo",
        "Mariana Montiel"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.LG",
        "cs.SD",
        "math-ph"
      ],
      "abstract": "Classical Recurrent Neural Networks (RNNs) summarize musical context into a deterministic hidden state vector, imposing an information bottleneck that fails to capture the inherent ambiguity in music. We propose the Density Matrix RNN (DM-RNN), a novel theoretical architecture utilizing the Density Matrix. This allows the model to maintain a statistical ensemble of musical interpretations (a mixed state), capturing both classical probabilities and quantum coherences. We rigorously define the temporal dynamics using Quantum Channels (CPTP maps). Crucially, we detail a parameterization strategy based on the Choi-Jamiolkowski isomorphism, ensuring the learned dynamics remain physically valid (CPTP) by construction. We introduce an analytical framework using Von Neumann Entropy to quantify musical uncertainty and Quantum Mutual Information (QMI) to measure entanglement between voices. The DM-RNN provides a mathematically rigorous framework for modeling complex, ambiguous musical structures.",
      "url": "https://arxiv.org/abs/2601.04592",
      "pdfUrl": "https://arxiv.org/pdf/2601.04592.pdf",
      "titleJa": "密度行列RNN（DM-RNN）：音楽的文脈とポリフォニーをモデル化する量子情報理論的枠組み"
    },
    {
      "id": "2601.03973",
      "arxivId": "2601.03973",
      "title": "Muse: Towards Reproducible Long-Form Song Generation with Fine-Grained Style Control",
      "authors": [
        "Changhao Jiang",
        "Jiahao Chen",
        "Zhenghao Xiang",
        "Zhixiong Yang",
        "Hanchen Wang",
        "Jiabao Zhuang",
        "Xinmeng Che",
        "Jiajun Sun",
        "Hui Li",
        "Yifei Cao",
        "Shihan Dou",
        "Ming Zhang",
        "Junjie Ye",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Recent commercial systems such as Suno demonstrate strong capabilities in long-form song generation, while academic research remains largely non-reproducible due to the lack of publicly available training data, hindering fair comparison and progress. To this end, we release a fully open-source system for long-form song generation with fine-grained style conditioning, including a licensed synthetic dataset, training and evaluation pipelines, and Muse, an easy-to-deploy song generation model. The dataset consists of 116k fully licensed synthetic songs with automatically generated lyrics and style descriptions paired with audio synthesized by SunoV5. We train Muse via single-stage supervised finetuning of a Qwen-based language model extended with discrete audio tokens using MuCodec, without task-specific losses, auxiliary objectives, or additional architectural components. Our evaluations find that although Muse is trained with a modest data scale and model size, it achieves competitive performance on phoneme error rate, text--music style similarity, and audio aesthetic quality, while enabling controllable segment-level generation across different musical structures. All data, model weights, and training and evaluation pipelines will be publicly released, paving the way for continued progress in controllable long-form song generation research. The project repository is available at https://github.com/yuhui1038/Muse.",
      "url": "https://arxiv.org/abs/2601.03973",
      "pdfUrl": "https://arxiv.org/pdf/2601.03973.pdf",
      "titleJa": "Muse: きめ細かなスタイル制御による再現性の高い長編楽曲生成に向けて"
    },
    {
      "id": "2601.03626",
      "arxivId": "2601.03626",
      "title": "Learning from Limited Labels: Transductive Graph Label Propagation for Indian Music Analysis",
      "authors": [
        "Parampreet Singh",
        "Akshay Raina",
        "Sayeedul Islam Sheikh",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Supervised machine learning frameworks rely on extensive labeled datasets for robust performance on real-world tasks. However, there is a lack of large annotated datasets in audio and music domains, as annotating such recordings is resource-intensive, laborious, and often require expert domain knowledge. In this work, we explore the use of label propagation (LP), a graph-based semi-supervised learning technique, for automatically labeling the unlabeled set in an unsupervised manner. By constructing a similarity graph over audio embeddings, we propagate limited label information from a small annotated subset to a larger unlabeled corpus in a transductive, semi-supervised setting. We apply this method to two tasks in Indian Art Music (IAM): Raga identification and Instrument classification. For both these tasks, we integrate multiple public datasets along with additional recordings we acquire from Prasar Bharati Archives to perform LP. Our experiments demonstrate that LP significantly reduces labeling overhead and produces higher-quality annotations compared to conventional baseline methods, including those based on pretrained inductive models. These results highlight the potential of graph-based semi-supervised learning to democratize data annotation and accelerate progress in music information retrieval.",
      "url": "https://arxiv.org/abs/2601.03626",
      "pdfUrl": "https://arxiv.org/pdf/2601.03626.pdf",
      "titleJa": "限定ラベルからの学習：インド音楽分析のためのトランスダクティブグラフラベル伝播"
    },
    {
      "id": "2601.03612",
      "arxivId": "2601.03612",
      "title": "Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias",
      "authors": [
        "Joonwon Seo"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This monograph introduces a novel approach to polyphonic music generation by addressing the \"Missing Middle\" problem through structural inductive bias. Focusing on Beethoven's piano sonatas as a case study, we empirically verify the independence of pitch and hand attributes using normalized mutual information (NMI=0.167) and propose the Smart Embedding architecture, achieving a 48.30% reduction in parameters. We provide rigorous mathematical proofs using information theory (negligible loss bounded at 0.153 bits), Rademacher complexity (28.09% tighter generalization bound), and category theory to demonstrate improved stability and generalization. Empirical results show a 9.47% reduction in validation loss, confirmed by SVD analysis and an expert listening study (N=53). This dual theoretical and applied framework bridges gaps in AI music generation, offering verifiable insights for mathematically grounded deep learning.",
      "url": "https://arxiv.org/abs/2601.03612",
      "pdfUrl": "https://arxiv.org/pdf/2601.03612.pdf",
      "titleJa": "構造的帰納的バイアスによるポリフォニック音楽生成の数学的基礎"
    },
    {
      "id": "2601.03443",
      "arxivId": "2601.03443",
      "title": "Discriminating real and synthetic super-resolved audio samples using embedding-based classifiers",
      "authors": [
        "Mikhail Silaev",
        "Konstantinos Drossos",
        "Tuomas Virtanen"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "Generative adversarial networks (GANs) and diffusion models have recently achieved state-of-the-art performance in audio super-resolution (ADSR), producing perceptually convincing wideband audio from narrowband inputs. However, existing evaluations primarily rely on signal-level or perceptual metrics, leaving open the question of how closely the distributions of synthetic super-resolved and real wideband audio match. Here we address this problem by analyzing the separability of real and super-resolved audio in various embedding spaces. We consider both middle-band ($4\\to 16$~kHz) and full-band ($16\\to 48$~kHz) upsampling tasks for speech and music, training linear classifiers to distinguish real from synthetic samples based on multiple types of audio embeddings. Comparisons with objective metrics and subjective listening tests reveal that embedding-based classifiers achieve near-perfect separation, even when the generated audio attains high perceptual quality and state-of-the-art metric scores. This behavior is consistent across datasets and models, including recent diffusion-based approaches, highlighting a persistent gap between perceptual quality and true distributional fidelity in ADSR models.",
      "url": "https://arxiv.org/abs/2601.03443",
      "pdfUrl": "https://arxiv.org/pdf/2601.03443.pdf",
      "titleJa": "埋め込みベースの分類器を用いた実在および合成の超解像オーディオサンプルの識別"
    },
    {
      "id": "2601.02983",
      "arxivId": "2601.02983",
      "title": "Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning",
      "authors": [
        "Yuankun Xie",
        "Xiaoxuan Guo",
        "Jiayi Zhou",
        "Tao Wang",
        "Jian Liu",
        "Ruibo Fu",
        "Xiaopeng Wang",
        "Haonan Cheng",
        "Long Ye"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in audio large language models (ALLMs) have made high-quality synthetic audio widely accessible, increasing the risk of malicious audio deepfakes across speech, environmental sounds, singing voice, and music. Real-world audio deepfake detection (ADD) therefore requires all-type detectors that generalize across heterogeneous audio and provide interpretable decisions. Given the strong multi-task generalization ability of ALLMs, we first investigate their performance on all-type ADD under both supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). However, SFT using only binary real/fake labels tends to reduce the model to a black-box classifier, sacrificing interpretability. Meanwhile, vanilla RFT under sparse supervision is prone to reward hacking and can produce hallucinated, ungrounded rationales. To address this, we propose an automatic annotation and polishing pipeline that constructs Frequency-Time structured chain-of-thought (CoT) rationales, producing ~340K cold-start demonstrations. Building on CoT data, we propose Frequency Time-Group Relative Policy Optimization (FT-GRPO), a two-stage training paradigm that cold-starts ALLMs with SFT and then applies GRPO under rule-based frequency-time constraints. Experiments demonstrate that FT-GRPO achieves state-of-the-art performance on all-type ADD while producing interpretable, FT-grounded rationales. The data and code are available online.",
      "url": "https://arxiv.org/abs/2601.02983",
      "pdfUrl": "https://arxiv.org/pdf/2601.02983.pdf",
      "titleJa": "周波数時間強化学習によるオーディオLLMを用いた解釈可能な全タイプオーディオディープフェイク検出"
    },
    {
      "id": "2601.02967",
      "arxivId": "2601.02967",
      "title": "MoE Adapter for Large Audio Language Models: Sparsity, Disentanglement, and Gradient-Conflict-Free",
      "authors": [
        "Yishu Lei",
        "Shuwei He",
        "Jing Hu",
        "Dan Zhang",
        "Xianlong Luo",
        "Danxiang Zhu",
        "Shikun Feng",
        "Rui Liu",
        "Jingzhou He",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Extending the input modality of Large Language Models~(LLMs) to the audio domain is essential for achieving comprehensive multimodal perception. However, it is well-known that acoustic information is intrinsically \\textit{heterogeneous}, entangling attributes such as speech, music, and environmental context. Existing research is limited to a dense, parameter-shared adapter to model these diverse patterns, which induces \\textit{gradient conflict} during optimization, as parameter updates required for distinct attributes contradict each other. To address this limitation, we introduce the \\textit{\\textbf{MoE-Adapter}}, a sparse Mixture-of-Experts~(MoE) architecture designed to decouple acoustic information. Specifically, it employs a dynamic gating mechanism that routes audio tokens to specialized experts capturing complementary feature subspaces while retaining shared experts for global context, thereby mitigating gradient conflicts and enabling fine-grained feature learning. Comprehensive experiments show that the MoE-Adapter achieves superior performance on both audio semantic and paralinguistic tasks, consistently outperforming dense linear baselines with comparable computational costs. Furthermore, we will release the related code and models to facilitate future research.",
      "url": "https://arxiv.org/abs/2601.02967",
      "pdfUrl": "https://arxiv.org/pdf/2601.02967.pdf",
      "titleJa": "大規模音声言語モデルのためのMoEアダプタ：スパース性、分離、勾配衝突フリー"
    },
    {
      "id": "2601.02591",
      "arxivId": "2601.02591",
      "title": "A Music Information Retrieval Approach to Classify Sub-Genres in Role Playing Games",
      "authors": [
        "Daeun Hwang",
        "Xuyuan Cai",
        "Edward F. Melcer",
        "Elin Carstensdottir"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.IR"
      ],
      "abstract": "Video game music (VGM) is often studied under the same lens as film music, which largely focuses on its theoretical functionality with relation to the identified genres of the media. However, till date, we are unaware of any systematic approach that analyzes the quantifiable musical features in VGM across several identified game genres. Therefore, we extracted musical features from VGM in games from three sub-genres of Role-Playing Games (RPG), and then hypothesized how different musical features are correlated to the perceptions and portrayals of each genre. This observed correlation may be used to further suggest such features are relevant to the expected storytelling elements or play mechanics associated with the sub-genre.",
      "url": "https://arxiv.org/abs/2601.02591",
      "pdfUrl": "https://arxiv.org/pdf/2601.02591.pdf",
      "titleJa": "ロールプレイングゲームのサブジャンルを分類するための音楽情報検索アプローチ"
    },
    {
      "id": "2601.02586",
      "arxivId": "2601.02586",
      "title": "Understanding Human Perception of Music Plagiarism Through a Computational Approach",
      "authors": [
        "Daeun Hwang",
        "Hyeonbin Hwang"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.IR"
      ],
      "abstract": "There is a wide variety of music similarity detection algorithms, while discussions about music plagiarism in the real world are often based on audience perceptions. Therefore, we aim to conduct a study to examine the key criteria of human perception of music plagiarism, focusing on the three commonly used musical features in similarity analysis: melody, rhythm, and chord progression. After identifying the key features and levels of variation humans use in perceiving musical similarity, we propose a LLM-as-a-judge framework that applies a systematic, step-by-step approach, drawing on modules that extract such high-level attributes.",
      "url": "https://arxiv.org/abs/2601.02586",
      "pdfUrl": "https://arxiv.org/pdf/2601.02586.pdf",
      "titleJa": "計算論的アプローチによる音楽盗作に対する人間の認識の理解"
    },
    {
      "id": "2601.02357",
      "arxivId": "2601.02357",
      "title": "DARC: Drum accompaniment generation with fine-grained rhythm control",
      "authors": [
        "Trey Brosnan"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.",
      "url": "https://arxiv.org/abs/2601.02357",
      "pdfUrl": "https://arxiv.org/pdf/2601.02357.pdf",
      "titleJa": "DARC: きめ細かなリズムコントロールによるドラム伴奏生成"
    },
    {
      "id": "2601.02101",
      "arxivId": "2601.02101",
      "title": "A Mamba-Based Model for Automatic Chord Recognition",
      "authors": [
        "Chunyu Yuan",
        "Johanna Devaney"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In this work, we propose a new efficient solution, which is a Mamba-based model named BMACE (Bidirectional Mamba-based network, for Automatic Chord Estimation), which utilizes selective structured state-space models in a bidirectional Mamba layer to effectively model temporal dependencies. Our model achieves high prediction performance comparable to state-of-the-art models, with the advantage of requiring fewer parameters and lower computational resources",
      "url": "https://arxiv.org/abs/2601.02101",
      "pdfUrl": "https://arxiv.org/pdf/2601.02101.pdf",
      "titleJa": "自動コード認識のためのMambaベースのモデル"
    },
    {
      "id": "2601.02099",
      "arxivId": "2601.02099",
      "title": "BeatlesFC: Harmonic function annotations of Isophonics' The Beatles dataset",
      "authors": [
        "Ji Yeoung Sim",
        "Rebecca Moranis",
        "Johanna Devaney"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This paper presents BeatlesFC, a set of harmonic function annotations for Isophonics' The Beatles dataset. Harmonic function annotations characterize chord labels as stable (tonic) or unstable (predominant, dominant). They operate at the level of musical phrases, serving as a link between chord labels and higher-level formal structures.",
      "url": "https://arxiv.org/abs/2601.02099",
      "pdfUrl": "https://arxiv.org/pdf/2601.02099.pdf",
      "titleJa": "BeatlesFC: Isophonics の The Beatles データセットの調和関数注釈"
    },
    {
      "id": "2601.01294",
      "arxivId": "2601.01294",
      "title": "Diffusion Timbre Transfer Via Mutual Information Guided Inpainting",
      "authors": [
        "Ching Ho Lee",
        "Javier Nistal",
        "Stefan Lattner",
        "Marco Pasini",
        "George Fazekas"
      ],
      "publishedDate": "2026-01-03",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "We study timbre transfer as an inference-time editing problem for music audio. Starting from a strong pre-trained latent diffusion model, we introduce a lightweight procedure that requires no additional training: (i) a dimension-wise noise injection that targets latent channels most informative of instrument identity, and (ii) an early-step clamping mechanism that re-imposes the input's melodic and rhythmic structure during reverse diffusion. The method operates directly on audio latents and is compatible with text/audio conditioning (e.g., CLAP). We discuss design choices,analyze trade-offs between timbral change and structural preservation, and show that simple inference-time controls can meaningfully steer pre-trained models for style-transfer use cases.",
      "url": "https://arxiv.org/abs/2601.01294",
      "pdfUrl": "https://arxiv.org/pdf/2601.01294.pdf",
      "titleJa": "相互情報誘導による音色拡散転写"
    },
    {
      "id": "2601.00326",
      "arxivId": "2601.00326",
      "title": "MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality",
      "authors": [
        "Torin Hopkins",
        "Shih-Yu Ma",
        "Suibi Che-Chuan Weng",
        "Ming-Yuan Pai",
        "Ellen Yi-Luen Do",
        "Luca Turchet"
      ],
      "publishedDate": "2026-01-01",
      "categories": [
        "cs.HC",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Digital Audio Workstations (DAWs) are central to modern music production but often encumber the musician's workflow, tethering them to a desk and hindering natural interaction with their instrument. Furthermore, effective remote collaboration remains a significant challenge, with existing solutions hampered by network latency and asynchronous file sharing. This paper investigates the potential of Mixed Reality (MR) to overcome these barriers, creating an intuitive environment for real-time, remote musical collaboration. We employ qualitative and speculative design techniques to better understand: 1) how players currently use DAWs, and 2) to imagine a speculative future of collaborative MR-DAWs. To facilitate this discussion, we developed and evaluated the usability of a design probe, MR-DAW. An MR system enabling multiple, geographically dispersed users to control a single, shared DAW instance while moving freely in their local spaces. Our networked system enables each remote musician to use a physical foot pedal for collaborative looping, merging a familiar, hands-free interaction with a shared virtual session. Based on interviews and system evaluations with 20 musicians, we analyze current practices, report on the user experience with our MR system, and speculate on the future of musical collaboration in MR. Our results highlight the affordances of MR for unencumbered musical interaction and provide a speculative outlook on the future of remote collaborative DAWs in the Musical Metaverse.",
      "url": "https://arxiv.org/abs/2601.00326",
      "pdfUrl": "https://arxiv.org/pdf/2601.00326.pdf",
      "titleJa": "MR-DAW: 複合現実における協調型デジタルオーディオワークステーションに向けて"
    },
    {
      "id": "2601.00299",
      "arxivId": "2601.00299",
      "title": "Timed text extraction from Taiwanese Kua-á-hì TV series",
      "authors": [
        "Tzu-Hung Huang",
        "Yun-En Tsai",
        "Yun-Ning Hung",
        "Chih-Wei Wu",
        "I-Chieh Wei",
        "Li Su"
      ],
      "publishedDate": "2026-01-01",
      "categories": [
        "cs.SD",
        "cs.MM"
      ],
      "abstract": "Taiwanese opera (Kua-á-hì), a major form of local theatrical tradition, underwent extensive television adaptation notably by pioneers like Iûnn Lē-hua. These videos, while potentially valuable for in-depth studies of Taiwanese opera, often have low quality and require substantial manual effort during data preparation. To streamline this process, we developed an interactive system for real-time OCR correction and a two-step approach integrating OCR-driven segmentation with Speech and Music Activity Detection (SMAD) to efficiently identify vocal segments from archival episodes with high precision. The resulting dataset, consisting of vocal segments and corresponding lyrics, can potentially supports various MIR tasks such as lyrics identification and tune retrieval. Code is available at https://github.com/z-huang/ocr-subtitle-editor .",
      "url": "https://arxiv.org/abs/2601.00299",
      "pdfUrl": "https://arxiv.org/pdf/2601.00299.pdf",
      "titleJa": "台湾のテレビシリーズ「Kua-á-hì」からのタイムスタンプ付きテキスト抽出"
    },
    {
      "id": "2512.23994",
      "arxivId": "2512.23994",
      "title": "PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark for Physically Grounded Text-to-Audio-Video Generation",
      "authors": [
        "Tianxin Xie",
        "Wentao Lei",
        "Guanjie Huang",
        "Pengfei Zhang",
        "Kai Jiang",
        "Chunhui Zhang",
        "Fengji Ma",
        "Haoyu He",
        "Han Zhang",
        "Jiangshan He",
        "Jinting Wang",
        "Linghan Fang",
        "Lufei Gao",
        "Orkesh Ablet",
        "Peihua Zhang",
        "Ruolin Hu",
        "Shengyu Li",
        "Weilin Lin",
        "Xiaoyang Feng",
        "Xinyue Yang",
        "Yan Rong",
        "Yanyun Wang",
        "Zihang Shao",
        "Zelin Zhao",
        "Chenxing Li",
        "Shan Yang",
        "Wenfu Wang",
        "Meng Yu",
        "Dong Yu",
        "Li Liu"
      ],
      "publishedDate": "2025-12-30",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Text-to-audio-video (T2AV) generation underpins a wide range of applications demanding realistic audio-visual content, including virtual reality, world modeling, gaming, and filmmaking. However, existing T2AV models remain incapable of generating physically plausible sounds, primarily due to their limited understanding of physical principles. To situate current research progress, we present PhyAVBench, a challenging audio physics-sensitivity benchmark designed to systematically evaluate the audio physics grounding capabilities of existing T2AV models. PhyAVBench comprises 1,000 groups of paired text prompts with controlled physical variables that implicitly induce sound variations, enabling a fine-grained assessment of models' sensitivity to changes in underlying acoustic conditions. We term this evaluation paradigm the Audio-Physics Sensitivity Test (APST). Unlike prior benchmarks that primarily focus on audio-video synchronization, PhyAVBench explicitly evaluates models' understanding of the physical mechanisms underlying sound generation, covering 6 major audio physics dimensions, 4 daily scenarios (music, sound effects, speech, and their mix), and 50 fine-grained test points, ranging from fundamental aspects such as sound diffraction to more complex phenomena, e.g., Helmholtz resonance. Each test point consists of multiple groups of paired prompts, where each prompt is grounded by at least 20 newly recorded or collected real-world videos, thereby minimizing the risk of data leakage during model pre-training. Both prompts and videos are iteratively refined through rigorous human-involved error correction and quality control to ensure high quality. We argue that only models with a genuine grasp of audio-related physical principles can generate physically consistent audio-visual content. We hope PhyAVBench will stimulate future progress in this critical yet largely unexplored domain.",
      "url": "https://arxiv.org/abs/2512.23994",
      "pdfUrl": "https://arxiv.org/pdf/2512.23994.pdf",
      "titleJa": "PhyAVBench: 物理的に根拠のあるテキストからオーディオ・ビデオを生成するための挑戦的なオーディオ物理感度ベンチマーク"
    },
    {
      "id": "2601.04222",
      "arxivId": "2601.04222",
      "title": "From Imitation to Innovation: The Divergent Paths of Techno in Germany and the USA",
      "authors": [
        "Tim Ziemer",
        "Simon Linke"
      ],
      "publishedDate": "2025-12-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Many documentaries on early house and techno music exist. Here, protagonists from the scenes describe key elements and events that affected the evolution of the music. In the research community, there is consensus that such descriptions have to be examined critically. Yet, there have not been attempts to validate such statements on the basis of audio analyses. In this study, over 9,000 early house and techno tracks from Germany and the United States of America are analyzed using recording studio features, machine learning and inferential statistics. Three observations can be made: 1.) German and US house/techno music are distinct, 2.) US styles are much more alike, and 3.) scarcely evolved over time compared to German house/techno regarding the recording studio features. These findings are in agreement with documented statements and thus provide an audio-based perspective on why techno became a mass phenomenon in Germany but remained a fringe phenomenon in the USA. Observations like these can help the music industry estimate whether new trends will experience a breakthrough or disappear.",
      "url": "https://arxiv.org/abs/2601.04222",
      "pdfUrl": "https://arxiv.org/pdf/2601.04222.pdf",
      "titleJa": "模倣から革新へ：ドイツとアメリカにおけるテクノの異なる道"
    },
    {
      "id": "2601.04744",
      "arxivId": "2601.04744",
      "title": "Semi-Supervised Diseased Detection from Speech Dialogues with Multi-Level Data Modeling",
      "authors": [
        "Xingyuan Li",
        "Mengyue Wu"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Detecting medical conditions from speech acoustics is fundamentally a weakly-supervised learning problem: a single, often noisy, session-level label must be linked to nuanced patterns within a long, complex audio recording. This task is further hampered by severe data scarcity and the subjective nature of clinical annotations. While semi-supervised learning (SSL) offers a viable path to leverage unlabeled data, existing audio methods often fail to address the core challenge that pathological traits are not uniformly expressed in a patient's speech. We propose a novel, audio-only SSL framework that explicitly models this hierarchy by jointly learning from frame-level, segment-level, and session-level representations within unsegmented clinical dialogues. Our end-to-end approach dynamically aggregates these multi-granularity features and generates high-quality pseudo-labels to efficiently utilize unlabeled data. Extensive experiments show the framework is model-agnostic, robust across languages and conditions, and highly data-efficient-achieving, for instance, 90\\% of fully-supervised performance using only 11 labeled samples. This work provides a principled approach to learning from weak, far-end supervision in medical speech analysis.",
      "url": "https://arxiv.org/abs/2601.04744",
      "pdfUrl": "https://arxiv.org/pdf/2601.04744.pdf",
      "titleJa": "多層データモデリングを用いた音声対話からの半教師付き疾患検出"
    },
    {
      "id": "2601.04564",
      "arxivId": "2601.04564",
      "title": "When Tone and Words Disagree: Towards Robust Speech Emotion Recognition under Acoustic-Semantic Conflict",
      "authors": [
        "Dawei Huang",
        "Yongjie Lv",
        "Ruijie Xiong",
        "Chunxiang Jin",
        "Xiaojiang Peng"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Speech Emotion Recognition (SER) systems often assume congruence between vocal emotion and lexical semantics. However, in real-world interactions, acoustic-semantic conflict is common yet overlooked, where the emotion conveyed by tone contradicts the literal meaning of spoken words. We show that state-of-the-art SER models, including ASR-based, self-supervised learning (SSL) approaches and Audio Language Models (ALMs), suffer performance degradation under such conflicts due to semantic bias or entangled acoustic-semantic representations. To address this, we propose the Fusion Acoustic-Semantic (FAS) framework, which explicitly disentangles acoustic and semantic pathways and bridges them through a lightweight, query-based attention module. To enable systematic evaluation, we introduce the Conflict in Acoustic-Semantic Emotion (CASE), the first dataset dominated by clear and interpretable acoustic-semantic conflicts in varied scenarios. Extensive experiments demonstrate that FAS consistently outperforms existing methods in both in-domain and zero-shot settings. Notably, on the CASE benchmark, conventional SER models fail dramatically, while FAS sets a new SOTA with 59.38% accuracy. Our code and datasets is available at https://github.com/24DavidHuang/FAS.",
      "url": "https://arxiv.org/abs/2601.04564",
      "pdfUrl": "https://arxiv.org/pdf/2601.04564.pdf",
      "titleJa": "音調と言葉が一致しないとき：音響的・意味的矛盾下におけるロバストな音声感情認識に向けて"
    },
    {
      "id": "2601.03712",
      "arxivId": "2601.03712",
      "title": "TellWhisper: Tell Whisper Who Speaks When",
      "authors": [
        "Yifan Hu",
        "Peiji Yang",
        "Zhisheng Wang",
        "Yicheng Zhong",
        "Rui Liu"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Multi-speaker automatic speech recognition (MASR) aims to predict ''who spoke when and what'' from multi-speaker speech, a key technology for multi-party dialogue understanding. However, most existing approaches decouple temporal modeling and speaker modeling when addressing ''when'' and ''who'': some inject speaker cues before encoding (e.g., speaker masking), which can cause irreversible information loss; others fuse identity by mixing speaker posteriors after encoding, which may entangle acoustic content with speaker identity. This separation is brittle under rapid turn-taking and overlapping speech, often leading to degraded performance. To address these limitations, we propose TellWhisper, a unified framework that jointly models speaker identity and temporal within the speech encoder. Specifically, we design TS-RoPE, a time-speaker rotary positional encoding: time coordinates are derived from frame indices, while speaker coordinates are derived from speaker activity and pause cues. By applying region-specific rotation angles, the model explicitly captures per-speaker continuity, speaker-turn transitions, and state dynamics, enabling the attention mechanism to simultaneously attend to ''when'' and ''who''. Moreover, to estimate frame-level speaker activity, we develop Hyper-SD, which casts speaker classification in hyperbolic space to enhance inter-class separation and refine speaker-activity estimates. Extensive experiments demonstrate the effectiveness of the proposed approach.",
      "url": "https://arxiv.org/abs/2601.03712",
      "pdfUrl": "https://arxiv.org/pdf/2601.03712.pdf",
      "titleJa": "TellWhisper: 誰がいつ話すかを知らせる"
    },
    {
      "id": "2601.03615",
      "arxivId": "2601.03615",
      "title": "Analyzing Reasoning Shifts in Audio Deepfake Detection under Adversarial Attacks: The Reasoning Tax versus Shield Bifurcation",
      "authors": [
        "Binh Nguyen",
        "Thai Le"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Audio Language Models (ALMs) offer a promising shift towards explainable audio deepfake detections (ADDs), moving beyond \\textit{black-box} classifiers by providing some level of transparency into their predictions via reasoning traces. This necessitates a new class of model robustness analysis: robustness of the predictive reasoning under adversarial attacks, which goes beyond existing paradigm that mainly focuses on the shifts of the final predictions (e.g., fake v.s. real). To analyze such reasoning shifts, we introduce a forensic auditing framework to evaluate the robustness of ALMs' reasoning under adversarial attacks in three inter-connected dimensions: acoustic perception, cognitive coherence, and cognitive dissonance. Our systematic analysis reveals that explicit reasoning does not universally enhance robustness. Instead, we observe a bifurcation: for models exhibiting robust acoustic perception, reasoning acts as a defensive \\textit{``shield''}, protecting them from adversarial attacks. However, for others, it imposes a performance \\textit{``tax''}, particularly under linguistic attacks which reduce cognitive coherence and increase attack success rate. Crucially, even when classification fails, high cognitive dissonance can serve as a \\textit{silent alarm}, flagging potential manipulation. Overall, this work provides a critical evaluation of the role of reasoning in forensic audio deepfake analysis and its vulnerabilities.",
      "url": "https://arxiv.org/abs/2601.03615",
      "pdfUrl": "https://arxiv.org/pdf/2601.03615.pdf",
      "titleJa": "敵対的攻撃下における音声ディープフェイク検出の推論シフトの分析：推論税とシールド分岐"
    },
    {
      "id": "2601.03610",
      "arxivId": "2601.03610",
      "title": "Investigation into respiratory sound classification for an imbalanced data set using hybrid LSTM-KAN architectures",
      "authors": [
        "Nithinkumar K.",
        "Anand R"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Respiratory sounds captured via auscultation contain critical clues for diagnosing pulmonary conditions. Automated classification of these sounds faces challenges due to subtle acoustic differences and severe class imbalance in clinical datasets. This study investigates respiratory sound classification with a focus on mitigating pronounced class imbalance. We propose a hybrid deep learning model that combines a Long Short-Term Memory (LSTM) network for sequential feature encoding with a Kolmogorov-Arnold Network (KAN) for classification. The model is integrated with a comprehensive feature extraction pipeline and targeted imbalance mitigation strategies. Experiments were conducted on a public respiratory sound database comprising six classes with a highly skewed distribution. Techniques such as focal loss, class-specific data augmentation, and Synthetic Minority Over-sampling Technique (SMOTE) were employed to enhance minority class recognition. The proposed Hybrid LSTM-KAN model achieves an overall accuracy of 94.6 percent and a macro-averaged F1 score of 0.703, despite the dominant COPD class accounting for over 86 percent of the data. Improved detection performance is observed for minority classes compared to baseline approaches, demonstrating the effectiveness of the proposed architecture for imbalanced respiratory sound classification.",
      "url": "https://arxiv.org/abs/2601.03610",
      "pdfUrl": "https://arxiv.org/pdf/2601.03610.pdf",
      "titleJa": "ハイブリッドLSTM-KANアーキテクチャを用いた不均衡なデータセットの呼吸音分類の調査"
    },
    {
      "id": "2601.02954",
      "arxivId": "2601.02954",
      "title": "The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models",
      "authors": [
        "Yuhuan You",
        "Lai Wei",
        "Xihong Wu",
        "Tianshu Qu"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Existing large audio-language models perceive the world as \"mono\" -- a single stream of audio that ignores the critical spatial dimension (\"where\") required for universal acoustic scene analysis. To bridge this gap, we first introduce a hierarchical framework for Auditory Scene Analysis (ASA). Guided by this framework, we introduce a system that enables models like Qwen2-Audio to understand and reason about the complex acoustic world. Our framework achieves this through three core contributions: First, we build a large-scale, synthesized binaural audio dataset to provide the rich spatial cues. Second, we design a hybrid feature projector, which leverages parallel semantic and spatial encoders to extract decoupled representations. These distinct streams are integrated via a dense fusion mechanism, ensuring the model receives a holistic view of the acoustic scene. Finally, we employ a progressive training curriculum, advancing from supervised fine-tuning (SFT) to reinforcement learning via Group Relative Policy Optimization (GRPO), to explicitly evolve the model's capabilities towards reasoning. On our comprehensive benchmark, the model demonstrates comparatively strong capability for spatial understanding. By enabling this spatial perception, our work provides a clear pathway for leveraging the powerful reasoning abilities of large models towards holistic acoustic scene analysis, advancing from \"mono\" semantic recognition to spatial intelligence.",
      "url": "https://arxiv.org/abs/2601.02954",
      "pdfUrl": "https://arxiv.org/pdf/2601.02954.pdf",
      "titleJa": "世界は単一ではない：大規模音声言語モデルにおける空間理解の実現"
    },
    {
      "id": "2601.02688",
      "arxivId": "2601.02688",
      "title": "Multi-channel multi-speaker transformer for speech recognition",
      "authors": [
        "Guo Yifan",
        "Tian Yao",
        "Suo Hongbin",
        "Wan Yulong"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "With the development of teleconferencing and in-vehicle voice assistants, far-field multi-speaker speech recognition has become a hot research topic. Recently, a multi-channel transformer (MCT) has been proposed, which demonstrates the ability of the transformer to model far-field acoustic environments. However, MCT cannot encode high-dimensional acoustic features for each speaker from mixed input audio because of the interference between speakers. Based on these, we propose the multi-channel multi-speaker transformer (M2Former) for far-field multi-speaker ASR in this paper. Experiments on the SMS-WSJ benchmark show that the M2Former outperforms the neural beamformer, MCT, dual-path RNN with transform-average-concatenate and multi-channel deep clustering based end-to-end systems by 9.2%, 14.3%, 24.9%, and 52.2% respectively, in terms of relative word error rate reduction.",
      "url": "https://arxiv.org/abs/2601.02688",
      "pdfUrl": "https://arxiv.org/pdf/2601.02688.pdf",
      "titleJa": "音声認識用マルチチャンネルマルチスピーカートランス"
    },
    {
      "id": "2601.02455",
      "arxivId": "2601.02455",
      "title": "Dynamic Quantization Error Propagation in Encoder-Decoder ASR Quantization",
      "authors": [
        "Xinyu Wang",
        "Yajie Luo",
        "Yihong Wu",
        "Liheng Ma",
        "Ziyu Zhao",
        "Jingrui Tian",
        "Lei Ding",
        "Yufei Cui",
        "Xiao-Wen Chang"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Running Automatic Speech Recognition (ASR) models on memory-constrained edge devices requires efficient compression. While layer-wise post-training quantization is effective, it suffers from error accumulation, especially in encoder-decoder architectures. Existing solutions like Quantization Error Propagation (QEP) are suboptimal for ASR due to the model's heterogeneity, processing acoustic features in the encoder while generating text in the decoder. To address this, we propose Fine-grained Alpha for Dynamic Quantization Error Propagation (FADE), which adaptively controls the trade-off between cross-layer error correction and local quantization. Experiments show that FADE significantly improves stability by reducing performance variance across runs, while simultaneously surpassing baselines in mean WER.",
      "url": "https://arxiv.org/abs/2601.02455",
      "pdfUrl": "https://arxiv.org/pdf/2601.02455.pdf",
      "titleJa": "エンコーダ・デコーダASR量子化における動的量子化誤差の伝播"
    },
    {
      "id": "2601.02444",
      "arxivId": "2601.02444",
      "title": "VocalBridge: Latent Diffusion-Bridge Purification for Defeating Perturbation-Based Voiceprint Defenses",
      "authors": [
        "Maryam Abbasihafshejani",
        "AHM Nazmus Sakib",
        "Murtuza Jadliwala"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "The rapid advancement of speech synthesis technologies, including text-to-speech (TTS) and voice conversion (VC), has intensified security and privacy concerns related to voice cloning. Recent defenses attempt to prevent unauthorized cloning by embedding protective perturbations into speech to obscure speaker identity while maintaining intelligibility. However, adversaries can apply advanced purification techniques to remove these perturbations, recover authentic acoustic characteristics, and regenerate cloneable voices. Despite the growing realism of such attacks, the robustness of existing defenses under adaptive purification remains insufficiently studied. Most existing purification methods are designed to counter adversarial noise in automatic speech recognition (ASR) systems rather than speaker verification or voice cloning pipelines. As a result, they fail to suppress the fine-grained acoustic cues that define speaker identity and are often ineffective against speaker verification attacks (SVA). To address these limitations, we propose Diffusion-Bridge (VocalBridge), a purification framework that learns a latent mapping from perturbed to clean speech in the EnCodec latent space. Using a time-conditioned 1D U-Net with a cosine noise schedule, the model enables efficient, transcript-free purification while preserving speaker-discriminative structure. We further introduce a Whisper-guided phoneme variant that incorporates lightweight temporal guidance without requiring ground-truth transcripts. Experimental results show that our approach consistently outperforms existing purification methods in recovering cloneable voices from protected speech. Our findings demonstrate the fragility of current perturbation-based defenses and highlight the need for more robust protection mechanisms against evolving voice-cloning and speaker verification threats.",
      "url": "https://arxiv.org/abs/2601.02444",
      "pdfUrl": "https://arxiv.org/pdf/2601.02444.pdf",
      "titleJa": "VocalBridge: 摂動ベースの声紋防御を破るための潜在的拡散ブリッジ浄化"
    },
    {
      "id": "2601.02432",
      "arxivId": "2601.02432",
      "title": "Quantifying Quanvolutional Neural Networks Robustness for Speech in Healthcare Applications",
      "authors": [
        "Ha Tran",
        "Bipasha Kashyap",
        "Pubudu N. Pathirana"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Speech-based machine learning systems are sensitive to noise, complicating reliable deployment in emotion recognition and voice pathology detection. We evaluate the robustness of a hybrid quantum machine learning model, quanvolutional neural networks (QNNs) against classical convolutional neural networks (CNNs) under four acoustic corruptions (Gaussian noise, pitch shift, temporal shift, and speed variation) in a clean-train/corrupted-test regime. Using AVFAD (voice pathology) and TESS (speech emotion), we compare three QNN models (Random, Basic, Strongly) to a simple CNN baseline (CNN-Base), ResNet-18 and VGG-16 using accuracy and corruption metrics (CE, mCE, RCE, RmCE), and analyze architectural factors (circuit complexity or depth, convergence) alongside per-emotion robustness. QNNs generally outperform the CNN-Base under pitch shift, temporal shift, and speed variation (up to 22% lower CE/RCE at severe temporal shift), while the CNN-Base remains more resilient to Gaussian noise. Among quantum circuits, QNN-Basic achieves the best overall robustness on AVFAD, and QNN-Random performs strongest on TESS. Emotion-wise, fear is most robust (80-90% accuracy under severe corruptions), neutral can collapse under strong Gaussian noise (5.5% accuracy), and happy is most vulnerable to pitch, temporal, and speed distortions. QNNs also converge up to six times faster than the CNN-Base. To our knowledge, this is a systematic study of QNN robustness for speech under common non-adversarial acoustic corruptions, indicating that shallow entangling quantum front-ends can improve noise resilience while sensitivity to additive noise remains a challenge.",
      "url": "https://arxiv.org/abs/2601.02432",
      "pdfUrl": "https://arxiv.org/pdf/2601.02432.pdf",
      "titleJa": "医療アプリケーションにおける音声認識のための量子畳み込みニューラルネットワークの堅牢性の定量化"
    },
    {
      "id": "2601.01568",
      "arxivId": "2601.01568",
      "title": "MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning",
      "authors": [
        "Chunyu Qiang",
        "Jun Wang",
        "Xiaopeng Wang",
        "Kang Yin",
        "Yuxin Guo"
      ],
      "publishedDate": "2026-01-04",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Joint audio-video generation aims to synthesize synchronized multisensory content, yet current unified models struggle with fine-grained acoustic control, particularly for identity-preserving speech. Existing approaches either suffer from temporal misalignment due to cascaded generation or lack the capability to perform zero-shot voice cloning within a joint synthesis framework. In this work, we present MM-Sonate, a multimodal flow-matching framework that unifies controllable audio-video joint generation with zero-shot voice cloning capabilities. Unlike prior works that rely on coarse semantic descriptions, MM-Sonate utilizes a unified instruction-phoneme input to enforce strict linguistic and temporal alignment. To enable zero-shot voice cloning, we introduce a timbre injection mechanism that effectively decouples speaker identity from linguistic content. Furthermore, addressing the limitations of standard classifier-free guidance in multimodal settings, we propose a noise-based negative conditioning strategy that utilizes natural noise priors to significantly enhance acoustic fidelity. Empirical evaluations demonstrate that MM-Sonate establishes new state-of-the-art performance in joint generation benchmarks, significantly outperforming baselines in lip synchronization and speech intelligibility, while achieving voice cloning fidelity comparable to specialized Text-to-Speech systems.",
      "url": "https://arxiv.org/abs/2601.01568",
      "pdfUrl": "https://arxiv.org/pdf/2601.01568.pdf",
      "titleJa": "MM-Sonate: ゼロショット音声クローニングによるマルチモーダル制御可能オーディオ・ビデオ生成"
    },
    {
      "id": "2601.01459",
      "arxivId": "2601.01459",
      "title": "OV-InstructTTS: Towards Open-Vocabulary Instruct Text-to-Speech",
      "authors": [
        "Yong Ren",
        "Jiangyan Yi",
        "Jianhua Tao",
        "Haiyang Sun",
        "Zhengqi Wen",
        "Hao Gu",
        "Le Xu",
        "Ye Bai"
      ],
      "publishedDate": "2026-01-04",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Instruct Text-to-Speech (InstructTTS) leverages natural language descriptions as style prompts to guide speech synthesis. However, existing InstructTTS methods mainly rely on a direct combination of audio-related labels or their diverse rephrasings, making it difficult to handle flexible, high-level instructions. Such rigid control is insufficient for users such as content creators who wish to steer generation with descriptive instructions. To address these constraints, we introduce OV-InstructTTS, a new paradigm for open-vocabulary InstructTTS. We propose a comprehensive solution comprising a newly curated dataset, OV-Speech, and a novel reasoning-driven framework. The OV-Speech dataset pairs speech with open-vocabulary instructions, each augmented with a reasoning process that connects high-level instructions to acoustic features. The reasoning-driven framework infers emotional, acoustic, and paralinguistic information from open-vocabulary instructions before synthesizing speech. Evaluations show that this reasoning-driven approach significantly improves instruction-following fidelity and speech expressiveness. We believe this work can inspire the next user-friendly InstructTTS systems with stronger generalization and real-world applicability. The dataset and demos are publicly available on our project page.",
      "url": "https://arxiv.org/abs/2601.01459",
      "pdfUrl": "https://arxiv.org/pdf/2601.01459.pdf",
      "titleJa": "OV-InstructTTS: オープン語彙指示テキスト読み上げに向けて"
    },
    {
      "id": "2601.01392",
      "arxivId": "2601.01392",
      "title": "SAFE-QAQ: End-to-End Slow-Thinking Audio-Text Fraud Detection via Reinforcement Learning",
      "authors": [
        "Peidong Wang",
        "Zhiming Ma",
        "Xin Dai",
        "Yongkang Liu",
        "Shi Feng",
        "Xiaocui Yang",
        "Wenxing Hu",
        "Zhihao Wang",
        "Mingjun Pan",
        "Li Yuan",
        "Daling Wang"
      ],
      "publishedDate": "2026-01-04",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Existing fraud detection methods predominantly rely on transcribed text, suffering from ASR errors and missing crucial acoustic cues like vocal tone and environmental context. This limits their effectiveness against complex deceptive strategies. To address these challenges, we first propose \\textbf{SAFE-QAQ}, an end-to-end comprehensive framework for audio-based slow-thinking fraud detection. First, the SAFE-QAQ framework eliminates the impact of transcription errors on detection performance. Secondly, we propose rule-based slow-thinking reward mechanisms that systematically guide the system to identify fraud-indicative patterns by accurately capturing fine-grained audio details, through hierarchical reasoning processes. Besides, our framework introduces a dynamic risk assessment framework during live calls, enabling early detection and prevention of fraud. Experiments on the TeleAntiFraud-Bench demonstrate that SAFE-QAQ achieves dramatic improvements over existing methods in multiple key dimensions, including accuracy, inference efficiency, and real-time processing capabilities. Currently deployed and analyzing over 70,000 calls daily, SAFE-QAQ effectively automates complex fraud detection, reducing human workload and financial losses. Code: https://anonymous.4open.science/r/SAFE-QAQ.",
      "url": "https://arxiv.org/abs/2601.01392",
      "pdfUrl": "https://arxiv.org/pdf/2601.01392.pdf",
      "titleJa": "SAFE-QAQ: 強化学習によるエンドツーエンドのスローシンキング音声テキスト詐欺検出"
    },
    {
      "id": "2601.01373",
      "arxivId": "2601.01373",
      "title": "UltraEval-Audio: A Unified Framework for Comprehensive Evaluation of Audio Foundation Models",
      "authors": [
        "Qundong Shi",
        "Jie Zhou",
        "Biyuan Lin",
        "Junbo Cui",
        "Guoyang Zeng",
        "Yixuan Zhou",
        "Ziyang Wang",
        "Xin Liu",
        "Zhen Luo",
        "Yudong Wang",
        "Zhiyuan Liu"
      ],
      "publishedDate": "2026-01-04",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "The development of audio foundation models has accelerated rapidly since the emergence of GPT-4o. However, the lack of comprehensive evaluation has become a critical bottleneck for further progress in the field, particularly in audio generation. Current audio evaluation faces three major challenges: (1) audio evaluation lacks a unified framework, with datasets and code scattered across various sources, hindering fair and efficient cross-model comparison;(2) audio codecs, as a key component of audio foundation models, lack a widely accepted and holistic evaluation methodology; (3) existing speech benchmarks are heavily reliant on English, making it challenging to objectively assess models' performance on Chinese. To address the first issue, we introduce UltraEval-Audio, a unified evaluation framework for audio foundation models, specifically designed for both audio understanding and generation tasks. UltraEval-Audio features a modular architecture, supporting 10 languages and 14 core task categories, while seamlessly integrating 24 mainstream models and 36 authoritative benchmarks. To enhance research efficiency, the framework provides a one-command evaluation feature, accompanied by real-time public leaderboards. For the second challenge, UltraEval-Audio adopts a novel comprehensive evaluation scheme for audio codecs, evaluating performance across three key dimensions: semantic accuracy, timbre fidelity, and acoustic quality. To address the third issue, we propose two new Chinese benchmarks, SpeechCMMLU and SpeechHSK, designed to assess Chinese knowledge proficiency and language fluency. We wish that UltraEval-Audio will provide both academia and industry with a transparent, efficient, and fair platform for comparison of audio models. Our code, benchmarks, and leaderboards are available at https://github.com/OpenBMB/UltraEval-Audio.",
      "url": "https://arxiv.org/abs/2601.01373",
      "pdfUrl": "https://arxiv.org/pdf/2601.01373.pdf",
      "titleJa": "UltraEval-Audio: オーディオ基盤モデルの包括的な評価のための統合フレームワーク"
    },
    {
      "id": "2601.00557",
      "arxivId": "2601.00557",
      "title": "A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR",
      "authors": [
        "Yuang Zheng",
        "Yuxiang Mei",
        "Dongxing Xu",
        "Jie Chen",
        "Yanhua Long"
      ],
      "publishedDate": "2026-01-02",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Large-scale multilingual ASR (mASR) models such as Whisper achieve strong performance but incur high computational and latency costs, limiting their deployment on resource-constrained edge devices. In this study, we propose a lightweight and language-agnostic multilingual ASR system based on a CTC architecture with domain adaptation. Specifically, we introduce a Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model, enabling end-to-end decoding via LID-posterior-driven LoRA routing. The hierarchical design consists of a multilingual shared LoRA for learning language-invariant acoustic representations and language-specific LoRA experts for modeling language-dependent characteristics. The proposed routing mechanism removes the need for prior language identity information or explicit language labels during inference, achieving true language-agnostic decoding. Experiments on MSR-86K and the MLC-SLM 2025 Challenge datasets demonstrate that HLoRA achieves competitive performance with state-of-the-art two-stage inference methods using only single-pass decoding, significantly improving decoding efficiency for low-resource mASR applications.",
      "url": "https://arxiv.org/abs/2601.00557",
      "pdfUrl": "https://arxiv.org/pdf/2601.00557.pdf",
      "titleJa": "CTCベースの多言語ASRのための言語に依存しない階層型LoRA-MoEアーキテクチャ"
    }
  ],
  "lastUpdated": "2026-01-13T23:02:09.055703",
  "totalCount": 81
}