{
  "papers": [
    {
      "id": "2601.18694",
      "arxivId": "2601.18694",
      "title": "Neural Multi-Speaker Voice Cloning for Nepali in Low-Resource Settings",
      "authors": [
        "Aayush M. Shrestha",
        "Aditya Bajracharya",
        "Projan Shakya",
        "Dinesh B. Kshatri"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "This research presents a few-shot voice cloning system for Nepali speakers, designed to synthesize speech in a specific speaker's voice from Devanagari text using minimal data. Voice cloning in Nepali remains largely unexplored due to its low-resource nature. To address this, we constructed separate datasets: untranscribed audio for training a speaker encoder and paired text-audio data for training a Tacotron2-based synthesizer. The speaker encoder, optimized with Generative End2End loss, generates embeddings that capture the speaker's vocal identity, validated through Uniform Manifold Approximation and Projection (UMAP) for dimension reduction visualizations. These embeddings are fused with Tacotron2's text embeddings to produce mel-spectrograms, which are then converted into audio using a WaveRNN vocoder. Audio data were collected from various sources, including self-recordings, and underwent thorough preprocessing for quality and alignment. Training was performed using mel and gate loss functions under multiple hyperparameter settings. The system effectively clones speaker characteristics even for unseen voices, demonstrating the feasibility of few-shot voice cloning for the Nepali language and establishing a foundation for personalized speech synthesis in low-resource scenarios.",
      "url": "https://arxiv.org/abs/2601.18694",
      "pdfUrl": "https://arxiv.org/pdf/2601.18694.pdf",
      "titleJa": "低リソース環境におけるネパール語のニューラルマルチスピーカー音声クローニング"
    },
    {
      "id": "2601.18535",
      "arxivId": "2601.18535",
      "title": "Audio Inpainting in Time-Frequency Domain with Phase-Aware Prior",
      "authors": [
        "Peter Balušík",
        "Pavel Rajmic"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "The so-called audio inpainting problem in the time domain refers to estimating missing segments of samples within a signal. Over the years, several methods have been developed for such type of audio inpainting. In contrast to this case, a time-frequency variant of inpainting appeared in the literature, where the challenge is to reconstruct missing spectrogram columns with reliable information. We propose a method to address this time-frequency audio inpainting problem. Our approach is based on the recently introduced phase-aware signal prior that exploits an estimate of the instantaneous frequency. An optimization problem is formulated and solved using the generalized Chambolle-Pock algorithm. The proposed method is evaluated both objectively and subjectively against other time-frequency inpainting methods, specifically a deep-prior neural network and the autoregression-based approach known as Janssen-TF. Our proposed approach surpassed these methods in the objective evaluation as well as in the conducted listening test. Moreover, this outcome is achieved with a substantially reduced computational requirement compared to alternative methods.",
      "url": "https://arxiv.org/abs/2601.18535",
      "pdfUrl": "https://arxiv.org/pdf/2601.18535.pdf",
      "titleJa": "位相を考慮した事前分布を用いた時間周波数領域におけるオーディオ修復"
    },
    {
      "id": "2601.18456",
      "arxivId": "2601.18456",
      "title": "Geneses: Unified Generative Speech Enhancement and Separation",
      "authors": [
        "Kohei Asai",
        "Wataru Nakata",
        "Yuki Saito",
        "Hiroshi Saruwatari"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Real-world audio recordings often contain multiple speakers and various degradations, which limit both the quantity and quality of speech data available for building state-of-the-art speech processing models. Although end-to-end approaches that concatenate speech enhancement (SE) and speech separation (SS) to obtain a clean speech signal for each speaker are promising, conventional SE-SS methods suffer from complex degradations beyond additive noise. To this end, we propose \\textbf{Geneses}, a generative framework to achieve unified, high-quality SE--SS. Our Geneses leverages latent flow matching to estimate each speaker's clean speech features using multi-modal diffusion Transformer conditioned on self-supervised learning representation from noisy mixture. We conduct experimental evaluation using two-speaker mixtures from LibriTTS-R under two conditions: additive-noise-only and complex degradations. The results demonstrate that Geneses significantly outperforms a conventional mask-based SE--SS method across various objective metrics with high robustness against complex degradations. Audio samples are available in our demo page.",
      "url": "https://arxiv.org/abs/2601.18456",
      "pdfUrl": "https://arxiv.org/pdf/2601.18456.pdf",
      "titleJa": "Geneses: 統合型生成音声強調および分離"
    },
    {
      "id": "2601.18451",
      "arxivId": "2601.18451",
      "title": "3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control",
      "authors": [
        "Xuanmeng Sha",
        "Liyun Zhang",
        "Tomohiro Mashita",
        "Naoya Chiba",
        "Yuki Uranishi"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.",
      "url": "https://arxiv.org/abs/2601.18451",
      "pdfUrl": "https://arxiv.org/pdf/2601.18451.pdf",
      "titleJa": "3DGesPolicy: 動作制御に基づく音素を考慮した総合的な音声合成ジェスチャ生成"
    },
    {
      "id": "2601.18438",
      "arxivId": "2601.18438",
      "title": "UrgentMOS: Unified Multi-Metric and Preference Learning for Robust Speech Quality Assessment",
      "authors": [
        "Wei Wang",
        "Wangyou Zhang",
        "Chenda Li",
        "Jiahe Wang",
        "Samuele Cornell",
        "Marvin Sach",
        "Kohei Saijo",
        "Yihui Fu",
        "Zhaoheng Ni",
        "Bing Han",
        "Xun Gong",
        "Mengxiao Bi",
        "Tim Fingscheidt",
        "Shinji Watanabe",
        "Yanmin Qian"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Automatic speech quality assessment has become increasingly important as modern speech generation systems continue to advance, while human listening tests remain costly, time-consuming, and difficult to scale. Most existing learning-based assessment models rely primarily on scarce human-annotated mean opinion score (MOS) data, which limits robustness and generalization, especially when training across heterogeneous datasets. In this work, we propose UrgentMOS, a unified speech quality assessment framework that jointly learns from diverse objective and perceptual quality metrics, while explicitly tolerating the absence of arbitrary subsets of metrics during training. By leveraging complementary quality facets under heterogeneous supervision, UrgentMOS enables effective utilization of partially annotated data and improves robustness when trained on large-scale, multi-source datasets. Beyond absolute score prediction, UrgentMOS explicitly models pairwise quality preferences by directly predicting comparative MOS (CMOS), making it well suited for preference-based evaluation scenarios commonly adopted in system benchmarking. Extensive experiments across a wide range of speech quality datasets, including simulated distortions, speech enhancement, and speech synthesis, demonstrate that UrgentMOS consistently achieves state-of-the-art performance in both absolute and comparative evaluation settings.",
      "url": "https://arxiv.org/abs/2601.18438",
      "pdfUrl": "https://arxiv.org/pdf/2601.18438.pdf",
      "titleJa": "UrgentMOS: 堅牢な音声品質評価のための統合マルチメトリックおよび嗜好学習"
    },
    {
      "id": "2601.18415",
      "arxivId": "2601.18415",
      "title": "Pisets: A Robust Speech Recognition System for Lectures and Interviews",
      "authors": [
        "Ivan Bondarenko",
        "Daniil Grebenkin",
        "Oleg Sedukhin",
        "Mikhail Klementev",
        "Roman Derunets",
        "Lyudmila Budneva"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.",
      "url": "https://arxiv.org/abs/2601.18415",
      "pdfUrl": "https://arxiv.org/pdf/2601.18415.pdf",
      "titleJa": "Pisets: 講義やインタビューのための堅牢な音声認識システム"
    },
    {
      "id": "2601.18396",
      "arxivId": "2601.18396",
      "title": "Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder",
      "authors": [
        "Zhengyang Li",
        "Thomas Graave",
        "Björn Möller",
        "Zehang Wu",
        "Matthias Franz",
        "Tim Fingscheidt"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.CV",
        "cs.SD"
      ],
      "abstract": "In audiovisual automatic speech recognition (AV-ASR) systems, information fusion of visual features in a pre-trained ASR has been proven as a promising method to improve noise robustness. In this work, based on the prominent Whisper ASR, first, we propose a simple and effective visual fusion method -- use of visual features both in encoder and decoder (dual-use) -- to learn the audiovisual interactions in the encoder and to weigh modalities in the decoder. Second, we compare visual fusion methods in Whisper models of various sizes. Our proposed dual-use method shows consistent noise robustness improvement, e.g., a 35% relative improvement (WER: 4.41% vs. 6.83%) based on Whisper small, and a 57% relative improvement (WER: 4.07% vs. 9.53%) based on Whisper medium, compared to typical reference middle fusion in babble noise with a signal-to-noise ratio (SNR) of 0dB. Third, we conduct ablation studies examining the impact of various module designs and fusion options. Fine-tuned on 1929 hours of audiovisual data, our dual-use method using Whisper medium achieves 4.08% (MUSAN babble noise) and 4.43% (NoiseX babble noise) average WER across various SNRs, thereby establishing a new state-of-the-art in noisy conditions on the LRS3 AV-ASR benchmark. Our code is at https://github.com/ifnspaml/Dual-Use-AVASR",
      "url": "https://arxiv.org/abs/2601.18396",
      "pdfUrl": "https://arxiv.org/pdf/2601.18396.pdf",
      "titleJa": "ウィスパーエンコーダとデコーダの両方で視覚的特徴を利用したノイズ耐性AV-ASR"
    },
    {
      "id": "2601.18393",
      "arxivId": "2601.18393",
      "title": "OCR-Enhanced Multimodal ASR Can Read While Listening",
      "authors": [
        "Junli Chen",
        "Changli Tang",
        "Yixuan Li",
        "Guangzhi Sun",
        "Chao Zhang"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Visual information, such as subtitles in a movie, often helps automatic speech recognition. In this paper, we propose Donut-Whisper, an audio-visual ASR model with dual encoder to leverage visual information to improve speech recognition performance in both English and Chinese. Donut-Whisper combines the advantage of the linear and the Q-Former-based modality alignment structures via a cross-attention module, generating more powerful audio-visual features. Meanwhile, we propose a lightweight knowledge distillation scheme showcasing the potential of using audio-visual models to teach audio-only models to achieve better performance. Moreover, we propose a new multilingual audio-visual speech recognition dataset based on movie clips containing both Chinese and English partitions. As a result, Donut-Whisper achieved significantly better performance on both English and Chinese partition of the dataset compared to both Donut and Whisper large V3 baselines. In particular, an absolute 5.75% WER reduction and a 16.5% absolute CER reduction were achieved on the English and Chinese sets respectively compared to the Whisper ASR baseline.",
      "url": "https://arxiv.org/abs/2601.18393",
      "pdfUrl": "https://arxiv.org/pdf/2601.18393.pdf",
      "titleJa": "OCR強化マルチモーダルASRは聞きながら読むことができます"
    },
    {
      "id": "2601.18339",
      "arxivId": "2601.18339",
      "title": "A Dataset for Automatic Vocal Mode Classification",
      "authors": [
        "Reemt Hinrichs",
        "Sonja Stephan",
        "Alexander Lange",
        "Jörn Ostermann"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\\,\\% was achieved with a ResNet18. The dataset can be downloaded under https://zenodo.org/records/14276415.",
      "url": "https://arxiv.org/abs/2601.18339",
      "pdfUrl": "https://arxiv.org/pdf/2601.18339.pdf",
      "titleJa": "自動音声モード分類のためのデータセット"
    },
    {
      "id": "2601.18335",
      "arxivId": "2601.18335",
      "title": "Analytic Incremental Learning For Sound Source Localization With Imbalance Rectification",
      "authors": [
        "Zexia Fan",
        "Yu Chen",
        "Qiquan Zhang",
        "Kainan Chen",
        "Xinyuan Qian"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Sound source localization (SSL) demonstrates remarkable results in controlled settings but struggles in real-world deployment due to dual imbalance challenges: intra-task imbalance arising from long-tailed direction-of-arrival (DoA) distributions, and inter-task imbalance induced by cross-task skews and overlaps. These often lead to catastrophic forgetting, significantly degrading the localization accuracy. To mitigate these issues, we propose a unified framework with two key innovations. Specifically, we design a GCC-PHAT-based data augmentation (GDA) method that leverages peak characteristics to alleviate intra-task distribution skews. We also propose an Analytic dynamic imbalance rectifier (ADIR) with task-adaption regularization, which enables analytic updates that adapt to inter-task dynamics. On the SSLR benchmark, our proposal achieves state-of-the-art (SoTA) results of 89.0% accuracy, 5.3° mean absolute error, and 1.6 backward transfer, demonstrating robustness to evolving imbalances without exemplar storage.",
      "url": "https://arxiv.org/abs/2601.18335",
      "pdfUrl": "https://arxiv.org/pdf/2601.18335.pdf",
      "titleJa": "不均衡補正による音源定位のための解析的増分学習"
    },
    {
      "id": "2601.18322",
      "arxivId": "2601.18322",
      "title": "Residual Learning for Neural Ambisonics Encoders",
      "authors": [
        "Thomas Deppisch",
        "Yang Gao",
        "Manan Mittal",
        "Benjamin Stahl",
        "Christoph Hold",
        "David Alon",
        "Zamir Ben-Hur"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Emerging wearable devices such as smartglasses and extended reality headsets demand high-quality spatial audio capture from compact, head-worn microphone arrays. Ambisonics provides a device-agnostic spatial audio representation by mapping array signals to spherical harmonic (SH) coefficients. In practice, however, accurate encoding remains challenging. While traditional linear encoders are signal-independent and robust, they amplify low-frequency noise and suffer from high-frequency spatial aliasing. On the other hand, neural network approaches can outperform linear encoders but they often assume idealized microphones and may perform inconsistently in real-world scenarios. To leverage their complementary strengths, we introduce a residual-learning framework that refines a linear encoder with corrections from a neural network. Using measured array transfer functions from smartglasses, we compare a UNet-based encoder from the literature with a new recurrent attention model. Our analysis reveals that both neural encoders only consistently outperform the linear baseline when integrated within the residual learning framework. In the residual configuration, both neural models achieve consistent and significant improvements across all tested metrics for in-domain data and moderate gains for out-of-domain data. Yet, coherence analysis indicates that all neural encoder configurations continue to struggle with directionally accurate high-frequency encoding.",
      "url": "https://arxiv.org/abs/2601.18322",
      "pdfUrl": "https://arxiv.org/pdf/2601.18322.pdf",
      "titleJa": "ニューラル・アンビソニックス・エンコーダのための残差学習"
    },
    {
      "id": "2601.18295",
      "arxivId": "2601.18295",
      "title": "Noise-Robust Contrastive Learning with an MFCC-Conformer For Coronary Artery Disease Detection",
      "authors": [
        "Milan Marocchi",
        "Matthew Fynn",
        "Yue Rong"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Cardiovascular diseases (CVD) are the leading cause of death worldwide, with coronary artery disease (CAD) comprising the largest subcategory of CVDs. Recently, there has been increased focus on detecting CAD using phonocardiogram (PCG) signals, with high success in clinical environments with low noise and optimal sensor placement. Multichannel techniques have been found to be more robust to noise; however, achieving robust performance on real-world data remains a challenge. This work utilises a novel multichannel energy-based noisy-segment rejection algorithm, using heart and noise-reference microphones, to discard audio segments with large amounts of nonstationary noise before training a deep learning classifier. This conformer-based classifier takes mel-frequency cepstral coefficients (MFCCs) from multiple channels, further helping improve the model's noise robustness. The proposed method achieved 78.4% accuracy and 78.2% balanced accuracy on 297 subjects, representing improvements of 4.1% and 4.3%, respectively, compared to training without noisy-segment rejection.",
      "url": "https://arxiv.org/abs/2601.18295",
      "pdfUrl": "https://arxiv.org/pdf/2601.18295.pdf",
      "titleJa": "冠動脈疾患検出のためのMFCCコンフォーマーを用いたノイズロバストな対照学習"
    },
    {
      "id": "2601.18281",
      "arxivId": "2601.18281",
      "title": "Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue",
      "authors": [
        "Yuhang Jia",
        "Pei Liu",
        "Haoqin Sun",
        "Jiaming Zhou",
        "Xuxin Cheng",
        "Cao Liu",
        "Ke Zeng",
        "Xunliang Cai",
        "Yong Qin"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "End-to-end Spoken Language Models (SLMs) hold great potential for paralinguistic perception, and numerous studies have aimed to enhance their capabilities, particularly for empathetic dialogue. However, current approaches largely depend on rigid supervised signals, such as ground-truth response in supervised fine-tuning or preference scores in reinforcement learning. Such reliance is fundamentally limited for modeling complex empathy, as there is no single \"correct\" response and a simple numerical score cannot fully capture the nuances of emotional expression or the appropriateness of empathetic behavior. To address these limitations, we sequentially introduce EmpathyEval, a descriptive natural-language-based evaluation model for assessing empathetic quality in spoken dialogues. Building upon EmpathyEval, we propose ReEmpathy, an end-to-end SLM that enhances empathetic dialogue through a novel Empathetic Self-Reflective Alternating Inference mechanism, which interleaves spoken response generation with free-form, empathy-related reflective reasoning. Extensive experiments demonstrate that ReEmpathy substantially improves empathy-sensitive spoken dialogue by enabling reflective reasoning, offering a promising approach toward more emotionally intelligent and empathy-aware human-computer interactions.",
      "url": "https://arxiv.org/abs/2601.18281",
      "pdfUrl": "https://arxiv.org/pdf/2601.18281.pdf",
      "titleJa": "共感的に話す前に二度考える：共感を考慮したエンドツーエンドの音声対話のための自己反省的交互推論"
    },
    {
      "id": "2601.18266",
      "arxivId": "2601.18266",
      "title": "Efficient Rehearsal for Continual Learning in ASR via Singular Value Tuning",
      "authors": [
        "Steven Vander Eeckt",
        "Hugo Van hamme"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Continual Learning (CL) in Automatic Speech Recognition (ASR) suffers from catastrophic forgetting when adapting to new tasks, domains, or speakers. A common strategy to mitigate this is to store a subset of past data in memory for rehearsal. However, rehearsal-based methods face key limitations: storing data is often costly, infeasible with pre-trained models, or restricted by privacy regulations. Running existing rehearsal-based methods with smaller memory sizes to alleviate these issues usually leads to degraded performance. We propose a rehearsal-based CL method that remains effective even with minimal memory. It operates in two stages: first, fine-tuning on the new task; second, applying Singular Value Decomposition (SVD) to the changes in linear layers and, in a parameter-efficient manner, retraining only gating vectors on the singular values, which control to extent to which updates from the first stage are accepted, using rehearsal. We extensively test and analyze our method on two monolingual and two multilingual benchmarks. Our method reduces forgetting and outperforms state-of-the-art CL approaches for ASR, even when limited to a single utterance per previous task.",
      "url": "https://arxiv.org/abs/2601.18266",
      "pdfUrl": "https://arxiv.org/pdf/2601.18266.pdf",
      "titleJa": "特異値チューニングによるASRの継続学習のための効率的なリハーサル"
    },
    {
      "id": "2601.18220",
      "arxivId": "2601.18220",
      "title": "LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech",
      "authors": [
        "Bingshen Mu",
        "Xian Shi",
        "Xiong Wang",
        "Hexin Liu",
        "Jin Xu",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Forced alignment (FA) predicts start and end timestamps for words or characters in speech, but existing methods are language-specific and prone to cumulative temporal shifts. The multilingual speech understanding and long-sequence processing abilities of speech large language models (SLLMs) make them promising for FA in multilingual, crosslingual, and long-form speech settings. However, directly applying the next-token prediction paradigm of SLLMs to FA results in hallucinations and slow inference. To bridge the gap, we propose LLM-ForcedAligner, reformulating FA as a slot-filling paradigm: timestamps are treated as discrete indices, and special timestamp tokens are inserted as slots into the transcript. Conditioned on the speech embeddings and the transcript with slots, the SLLM directly predicts the time indices at slots. During training, causal attention masking with non-shifted input and label sequences allows each slot to predict its own timestamp index based on itself and preceding context, with loss computed only at slot positions. Dynamic slot insertion enables FA at arbitrary positions. Moreover, non-autoregressive inference is supported, avoiding hallucinations and improving speed. Experiments across multilingual, crosslingual, and long-form speech scenarios show that LLM-ForcedAligner achieves a 69%~78% relative reduction in accumulated averaging shift compared with prior methods. The checkpoint and inference code will be released later.",
      "url": "https://arxiv.org/abs/2601.18220",
      "pdfUrl": "https://arxiv.org/pdf/2601.18220.pdf",
      "titleJa": "LLM-ForcedAligner: 多言語および長文音声のための非自己回帰かつ高精度なLLMベースの強制アライナ"
    },
    {
      "id": "2601.18184",
      "arxivId": "2601.18184",
      "title": "VIBEVOICE-ASR Technical Report",
      "authors": [
        "Zhiliang Peng",
        "Jianwei Yu",
        "Yaoyao Chang",
        "Zilong Wang",
        "Li Dong",
        "Yingbo Hao",
        "Yujie Tu",
        "Chenyu Yang",
        "Wenhui Wang",
        "Songchen Xu",
        "Yutao Sun",
        "Hangbo Bao",
        "Weijiang Xu",
        "Yi Zhu",
        "Zehua Wang",
        "Ting Song",
        "Yan Xia",
        "Zewen Chi",
        "Shaohan Huang",
        "Liang Wang",
        "Chuang Ding",
        "Shuai Wang",
        "Xie Chen",
        "Furu Wei"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.",
      "url": "https://arxiv.org/abs/2601.18184",
      "pdfUrl": "https://arxiv.org/pdf/2601.18184.pdf",
      "titleJa": "VIBEVOICE-ASR技術レポート"
    },
    {
      "id": "2601.18094",
      "arxivId": "2601.18094",
      "title": "OneVoice: One Model, Triple Scenarios-Towards Unified Zero-shot Voice Conversion",
      "authors": [
        "Zhichao Wang",
        "Tao Li",
        "Wenshuo Ge",
        "Zihao Cui",
        "Shilei Zhang",
        "Junlan Feng"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Recent progress of voice conversion~(VC) has achieved a new milestone in speaker cloning and linguistic preservation. But the field remains fragmented, relying on specialized models for linguistic-preserving, expressive, and singing scenarios. We propose OneVoice, a unified zero-shot framework capable of handling all three scenarios within a single model. OneVoice is built upon a continuous language model trained with VAE-free next-patch diffusion, ensuring high fidelity and efficient sequence modeling. Its core design for unification lies in a Mixture-of-Experts (MoE) designed to explicitly model shared conversion knowledge and scenario-specific expressivity. Expert selection is coordinated by a dual-path routing mechanism, including shared expert isolation and scenario-aware domain expert assignment with global-local cues. For precise conditioning, scenario-specific prosodic features are fused into each layer via a gated mechanism, allowing adaptive usage of prosody information. Furthermore, to enable the core idea and alleviate the imbalanced issue (abundant speech vs. scarce singing), we adopt a two-stage progressive training that includes foundational pre-training and scenario enhancement with LoRA-based domain experts. Experiments show that OneVoice matches or surpasses specialized models across all three scenarios, while verifying flexible control over scenarios and offering a fast decoding version as few as 2 steps. Code and model will be released soon.",
      "url": "https://arxiv.org/abs/2601.18094",
      "pdfUrl": "https://arxiv.org/pdf/2601.18094.pdf",
      "titleJa": "OneVoice: 1つのモデル、3つのシナリオ - 統合ゼロショット音声変換に向けて"
    },
    {
      "id": "2601.18086",
      "arxivId": "2601.18086",
      "title": "From Human Speech to Ocean Signals: Transferring Speech Large Models for Underwater Acoustic Target Recognition",
      "authors": [
        "Mengcheng Huang",
        "Xue Zhou",
        "Chen Xu",
        "Dapeng Man"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Underwater acoustic target recognition (UATR) plays a vital role in marine applications but remains challenging due to limited labeled data and the complexity of ocean environments. This paper explores a central question: can speech large models (SLMs), trained on massive human speech corpora, be effectively transferred to underwater acoustics? To investigate this, we propose UATR-SLM, a simple framework that reuses the speech feature pipeline, adapts the SLM as an acoustic encoder, and adds a lightweight classifier.Experiments on the DeepShip and ShipsEar benchmarks show that UATR-SLM achieves over 99% in-domain accuracy, maintains strong robustness across variable signal lengths, and reaches up to 96.67% accuracy in cross-domain evaluation. These results highlight the strong transferability of SLMs to UATR, establishing a promising paradigm for leveraging speech foundation models in underwater acoustics.",
      "url": "https://arxiv.org/abs/2601.18086",
      "pdfUrl": "https://arxiv.org/pdf/2601.18086.pdf",
      "titleJa": "人間の音声から海洋信号へ：水中音響ターゲット認識のための音声大規模モデルの転送"
    },
    {
      "id": "2601.18037",
      "arxivId": "2601.18037",
      "title": "SpatialEmb: Extract and Encode Spatial Information for 1-Stage Multi-channel Multi-speaker ASR on Arbitrary Microphone Arrays",
      "authors": [
        "Yiwen Shao",
        "Yong Xu",
        "Sanjeev Khudanpur",
        "Dong Yu"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Spatial information is a critical clue for multi-channel multi-speaker target speech recognition. Most state-of-the-art multi-channel Automatic Speech Recognition (ASR) systems extract spatial features only during the speech separation stage, followed by standard single-channel ASR on the separated speech. This approach results in an inefficient, lengthy pipeline and sub-optimal ASR performance due to the accumulated errors from preprocessing modules. Furthermore, most spatial feature extraction methods depend on the knowledge of speaker positions and microphone topology, making the systems reliant on specific settings and challenging to adapt to new equipment. In this work, we propose a solution to these issues with a lightweight embedding module named SpatialEmb, which extracts and encodes spatial information directly for the ASR model, supporting both fixed and arbitrary microphone topology. We conduct comprehensive experiments on AliMeeting, a real meeting corpus, to determine the optimal model design for SpatialEmb in terms of both performance and efficiency. Our best model trained with 105 hours Train-Ali-far achieves 17.04% and 20.32% character error rates (CER) on the Eval and Test sets, establishing a new state-of-the-art result with the same training data.",
      "url": "https://arxiv.org/abs/2601.18037",
      "pdfUrl": "https://arxiv.org/pdf/2601.18037.pdf",
      "titleJa": "SpatialEmb: 任意のマイクアレイを用いた1段階マルチチャンネルマルチスピーカーASRのための空間情報の抽出とエンコード"
    },
    {
      "id": "2601.18010",
      "arxivId": "2601.18010",
      "title": "AmbER$^2$: Dual Ambiguity-Aware Emotion Recognition Applied to Speech and Text",
      "authors": [
        "Jingyao Wu",
        "Grace Lin",
        "Yinuo Song",
        "Rosalind Picard"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Emotion recognition is inherently ambiguous, with uncertainty arising both from rater disagreement and from discrepancies across modalities such as speech and text. There is growing interest in modeling rater ambiguity using label distributions. However, modality ambiguity remains underexplored, and multimodal approaches often rely on simple feature fusion without explicitly addressing conflicts between modalities. In this work, we propose AmbER$^2$, a dual ambiguity-aware framework that simultaneously models rater-level and modality-level ambiguity through a teacher-student architecture with a distribution-wise training objective. Evaluations on IEMOCAP and MSP-Podcast show that AmbER$^2$ consistently improves distributional fidelity over conventional cross-entropy baselines and achieves performance competitive with, or superior to, recent state-of-the-art systems. For example, on IEMOCAP, AmbER$^2$ achieves relative improvements of 20.3% on Bhattacharyya coefficient (0.83 vs. 0.69), 13.6% on R$^2$ (0.67 vs. 0.59), 3.8% on accuracy (0.683 vs. 0.658), and 4.5% on F1 (0.675 vs. 0.646). Further analysis across ambiguity levels shows that explicitly modeling ambiguity is particularly beneficial for highly uncertain samples. These findings highlight the importance of jointly addressing rater and modality ambiguity when building robust emotion recognition systems.",
      "url": "https://arxiv.org/abs/2601.18010",
      "pdfUrl": "https://arxiv.org/pdf/2601.18010.pdf",
      "titleJa": "AmbER$^2$: 音声とテキストの両方に適用された曖昧性を考慮した感情認識"
    },
    {
      "id": "2601.18766",
      "arxivId": "2601.18766",
      "title": "Learning to Discover: A Generalized Framework for Raga Identification without Forgetting",
      "authors": [
        "Parampreet Singh",
        "Somya Kumar",
        "Chaitanya Shailendra Nitawe",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.",
      "url": "https://arxiv.org/abs/2601.18766",
      "pdfUrl": "https://arxiv.org/pdf/2601.18766.pdf",
      "titleJa": "発見することを学ぶ：忘れずにラーガを識別するための一般化された枠組み"
    },
    {
      "id": "2601.17902",
      "arxivId": "2601.17902",
      "title": "dLLM-ASR: A Faster Diffusion LLM-based Framework for Speech Recognition",
      "authors": [
        "Wenjie Tian",
        "Bingshen Mu",
        "Guobin Ma",
        "Xuelong Geng",
        "Zhixian Zhao",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Automatic speech recognition (ASR) systems based on large language models (LLMs) achieve superior performance by leveraging pretrained LLMs as decoders, but their token-by-token generation mechanism leads to inference latency that grows linearly with sequence length. Meanwhile, discrete diffusion large language models (dLLMs) offer a promising alternative, enabling high-quality parallel sequence generation with pretrained decoders. However, directly applying native text-oriented dLLMs to ASR leads to a fundamental mismatch between open-ended text generation and the acoustically conditioned transcription paradigm required by ASR. As a result, it introduces unnecessary difficulty and computational redundancy, such as denoising from pure noise, inflexible generation lengths, and fixed denoising steps. We propose dLLM-ASR, an efficient dLLM-based ASR framework that formulates dLLM's decoding as a prior-guided and adaptive denoising process. It leverages an ASR prior to initialize the denoising process and provide an anchor for sequence length. Building upon this prior, length-adaptive pruning dynamically removes redundant tokens, while confidence-based denoising allows converged tokens to exit the denoising loop early, enabling token-level adaptive computation. Experiments demonstrate that dLLM-ASR achieves recognition accuracy comparable to autoregressive LLM-based ASR systems and delivers a 4.44$\\times$ inference speedup, establishing a practical and efficient paradigm for ASR.",
      "url": "https://arxiv.org/abs/2601.17902",
      "pdfUrl": "https://arxiv.org/pdf/2601.17902.pdf",
      "titleJa": "dLLM-ASR: 音声認識のための高速拡散LLMベースのフレームワーク"
    },
    {
      "id": "2601.17901",
      "arxivId": "2601.17901",
      "title": "Speech Emotion Recognition with ASR Integration",
      "authors": [
        "Yuanchao Li"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Speech Emotion Recognition (SER) plays a pivotal role in understanding human communication, enabling emotionally intelligent systems, and serving as a fundamental component in the development of Artificial General Intelligence (AGI). However, deploying SER in real-world, spontaneous, and low-resource scenarios remains a significant challenge due to the complexity of emotional expression and the limitations of current speech and language technologies. This thesis investigates the integration of Automatic Speech Recognition (ASR) into SER, with the goal of enhancing the robustness, scalability, and practical applicability of emotion recognition from spoken language.",
      "url": "https://arxiv.org/abs/2601.17901",
      "pdfUrl": "https://arxiv.org/pdf/2601.17901.pdf",
      "titleJa": "ASR統合による音声感情認識"
    },
    {
      "id": "2601.17711",
      "arxivId": "2601.17711",
      "title": "CaSNet: Compress-and-Send Network Based Multi-Device Speech Enhancement Model for Distributed Microphone Arrays",
      "authors": [
        "Chengqian Jiang",
        "Jie Zhang",
        "Haoyin Yan"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Distributed microphone array (DMA) is a promising next-generation platform for speech interaction, where speech enhancement (SE) is still required to improve the speech quality in noisy cases. Existing SE methods usually first gather raw waveforms at a fusion center (FC) from all devices and then design a multi-microphone model, causing high bandwidth and energy costs. In this work, we propose a \\emph{Compress-and-Send Network (CaSNet)} for resource-constrained DMAs, where one microphone serves as the FC and reference. Each of other devices encodes the measured raw data into a feature matrix, which is then compressed by singular value decomposition (SVD) to produce a more compact representation. The received features at the FC are aligned via cross window query with respect to the reference, followed by neural decoding to yield spatially coherent enhanced speech. Experiments on multiple datasets show that the proposed CaSNet can save the data amount with a negligible impact on the performance compared to the uncompressed case. The reproducible code is available at https://github.com/Jokejiangv/CaSNet.",
      "url": "https://arxiv.org/abs/2601.17711",
      "pdfUrl": "https://arxiv.org/pdf/2601.17711.pdf",
      "titleJa": "CaSNet: 分散型マイクロホンアレイのための圧縮・送信ネットワークベースのマルチデバイス音声強調モデル"
    },
    {
      "id": "2601.17690",
      "arxivId": "2601.17690",
      "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
      "authors": [
        "Ziling Gong",
        "Yunyan Ouyang",
        "Iram Kamdar",
        "Melody Ma",
        "Hongjie Chen",
        "Franck Dernoncourt",
        "Ryan A. Rossi",
        "Nesreen K. Ahmed"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.",
      "url": "https://arxiv.org/abs/2601.17690",
      "pdfUrl": "https://arxiv.org/pdf/2601.17690.pdf",
      "titleJa": "セグメント長の重要性：オーディオフィンガープリンティングの性能におけるセグメント長の研究"
    },
    {
      "id": "2601.17679",
      "arxivId": "2601.17679",
      "title": "BanglaRobustNet: A Hybrid Denoising-Attention Architecture for Robust Bangla Speech Recognition",
      "authors": [
        "Md Sazzadul Islam Ridoy",
        "Mubaswira Ibnat Zidney",
        "Sumi Akter",
        "Md. Aminur Rahman"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Bangla, one of the most widely spoken languages, remains underrepresented in state-of-the-art automatic speech recognition (ASR) research, particularly under noisy and speaker-diverse conditions. This paper presents BanglaRobustNet, a hybrid denoising-attention framework built on Wav2Vec-BERT, designed to address these challenges. The architecture integrates a diffusion-based denoising module to suppress environmental noise while preserving Bangla-specific phonetic cues, and a contextual cross-attention module that conditions recognition on speaker embeddings for robustness across gender, age, and dialects. Trained end-to-end with a composite objective combining CTC loss, phonetic consistency, and speaker alignment, BanglaRobustNet achieves substantial reductions in word error rate (WER) and character error rate (CER) compared to Wav2Vec-BERT and Whisper baselines. Evaluations on Mozilla Common Voice Bangla and augmented noisy speech confirm the effectiveness of our approach, establishing BanglaRobustNet as a robust ASR system tailored to low-resource, noise-prone linguistic settings.",
      "url": "https://arxiv.org/abs/2601.17679",
      "pdfUrl": "https://arxiv.org/pdf/2601.17679.pdf",
      "titleJa": "BanglaRobustNet: 堅牢なベンガル語音声認識のためのハイブリッドノイズ除去・アテンションアーキテクチャ"
    },
    {
      "id": "2601.18796",
      "arxivId": "2601.18796",
      "title": "ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models",
      "authors": [
        "Brian Ondov",
        "Chia-Hsuan Chang",
        "Yujia Zhou",
        "Mauro Giuffrè",
        "Hua Xu"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.",
      "url": "https://arxiv.org/abs/2601.18796",
      "pdfUrl": "https://arxiv.org/pdf/2601.18796.pdf",
      "titleJa": "ctELM: 埋め込み言語モデルを用いた臨床試験の埋め込みのデコードと操作"
    },
    {
      "id": "2601.18795",
      "arxivId": "2601.18795",
      "title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes",
      "authors": [
        "Amrith Setlur",
        "Zijian Wang",
        "Andrew Cohen",
        "Paria Rashidinejad",
        "Sang Michael Xie"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.",
      "url": "https://arxiv.org/abs/2601.18795",
      "pdfUrl": "https://arxiv.org/pdf/2601.18795.pdf",
      "titleJa": "FLOP を再利用: 非常にオフポリシーなプレフィックスを条件として難しい問題に RL をスケーリングする"
    },
    {
      "id": "2601.18791",
      "arxivId": "2601.18791",
      "title": "Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets",
      "authors": [
        "Iaroslav Chelombitko",
        "Mika Hämäläinen",
        "Aleksey Komissarov"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p < 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.",
      "url": "https://arxiv.org/abs/2601.18791",
      "pdfUrl": "https://arxiv.org/pdf/2601.18791.pdf",
      "titleJa": "Wikipedia Glottosetsを用いた242言語間のサブワードベースの比較言語学"
    },
    {
      "id": "2601.18785",
      "arxivId": "2601.18785",
      "title": "Design Techniques for LLM-Powered Interactive Storytelling: A Case Study of the Dramamancer System",
      "authors": [
        "Tiffany Wang",
        "Yuqian Sun",
        "Yi Wang",
        "Melissa Roemmele",
        "John Joon Young Chung",
        "Max Kreminski"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "The rise of Large Language Models (LLMs) has enabled a new paradigm for bridging authorial intent and player agency in interactive narrative. We consider this paradigm through the example of Dramamancer, a system that uses an LLM to transform author-created story schemas into player-driven playthroughs. This extended abstract outlines some design techniques and evaluation considerations associated with this system.",
      "url": "https://arxiv.org/abs/2601.18785",
      "pdfUrl": "https://arxiv.org/pdf/2601.18785.pdf",
      "titleJa": "LLMを活用したインタラクティブストーリーテリングのためのデザインテクニック：ドラママンサーシステムのケーススタディ"
    },
    {
      "id": "2601.18783",
      "arxivId": "2601.18783",
      "title": "Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic",
      "authors": [
        "Deepthi Pathare",
        "Leo Laine",
        "Morteza Haghir Chehreghani"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "abstract": "Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.",
      "url": "https://arxiv.org/abs/2601.18783",
      "pdfUrl": "https://arxiv.org/pdf/2601.18783.pdf",
      "titleJa": "高速道路交通におけるトラックの効率的な戦術的意思決定のための多目的強化学習"
    },
    {
      "id": "2601.18779",
      "arxivId": "2601.18779",
      "title": "POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration",
      "authors": [
        "Yuxiao Qu",
        "Amrith Setlur",
        "Virginia Smith",
        "Ruslan Salakhutdinov",
        "Aviral Kumar"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.",
      "url": "https://arxiv.org/abs/2601.18779",
      "pdfUrl": "https://arxiv.org/pdf/2601.18779.pdf",
      "titleJa": "POPE: 特権オンポリシー探索による難問の推論学習"
    },
    {
      "id": "2601.18777",
      "arxivId": "2601.18777",
      "title": "PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation",
      "authors": [
        "Abhishek Divekar",
        "Anirban Majumder"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.",
      "url": "https://arxiv.org/abs/2601.18777",
      "pdfUrl": "https://arxiv.org/pdf/2601.18777.pdf",
      "titleJa": "PRECISE: 予測に基づくランキング推定を用いた LLM 評価のバイアス軽減"
    },
    {
      "id": "2601.18771",
      "arxivId": "2601.18771",
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "authors": [
        "Yanming Liu",
        "Xinyue Peng",
        "Zixuan Yan",
        "Yanxin Shen",
        "Wenjie Xu",
        "Yuefeng Huang",
        "Xinyi Wang",
        "Jiannan Cao",
        "Jianwei Yin",
        "Xuhong Zhang"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.",
      "url": "https://arxiv.org/abs/2601.18771",
      "pdfUrl": "https://arxiv.org/pdf/2601.18771.pdf",
      "titleJa": "Dep-Search: 永続メモリを用いた依存性を考慮した推論トレースの学習"
    },
    {
      "id": "2601.18754",
      "arxivId": "2601.18754",
      "title": "$α^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks",
      "authors": [
        "Mohamed Amine Ferrag",
        "Abderrahmane Lakas",
        "Merouane Debbah"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "abstract": "Autonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical, networked environments where they must operate reliably in the presence of malicious adversaries. While recent benchmarks have evaluated large language model (LLM)-based UAV agents in reasoning, navigation, and efficiency, systematic assessment of security, resilience, and trust under adversarial conditions remains largely unexplored, particularly in emerging 6G-enabled settings. We introduce $α^{3}$-SecBench, the first large-scale evaluation suite for assessing the security-aware autonomy of LLM-based UAV agents under realistic adversarial interference. Building on multi-turn conversational UAV missions from $α^{3}$-Bench, the framework augments benign episodes with 20,000 validated security overlay attack scenarios targeting seven autonomy layers, including sensing, perception, planning, control, communication, edge/cloud infrastructure, and LLM reasoning. $α^{3}$-SecBench evaluates agents across three orthogonal dimensions: security (attack detection and vulnerability attribution), resilience (safe degradation behavior), and trust (policy-compliant tool usage). We evaluate 23 state-of-the-art LLMs from major industrial providers and leading AI labs using thousands of adversarially augmented UAV episodes sampled from a corpus of 113,475 missions spanning 175 threat types. While many models reliably detect anomalous behavior, effective mitigation, vulnerability attribution, and trustworthy control actions remain inconsistent. Normalized overall scores range from 12.9% to 57.1%, highlighting a significant gap between anomaly detection and security-aware autonomous decision-making. We release $α^{3}$-SecBench on GitHub: https://github.com/maferrag/AlphaSecBench",
      "url": "https://arxiv.org/abs/2601.18754",
      "pdfUrl": "https://arxiv.org/pdf/2601.18754.pdf",
      "titleJa": "$α^3$-SecBench: 6Gネットワーク上のLLMベースUAVエージェントのセキュリティ、レジリエンス、信頼性の大規模評価スイート"
    },
    {
      "id": "2601.18753",
      "arxivId": "2601.18753",
      "title": "HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs",
      "authors": [
        "Xinyue Zeng",
        "Junhong Lin",
        "Yujun Yan",
        "Feng Guo",
        "Liang Shi",
        "Jun Wu",
        "Dawei Zhou"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.",
      "url": "https://arxiv.org/abs/2601.18753",
      "pdfUrl": "https://arxiv.org/pdf/2601.18753.pdf",
      "titleJa": "HalluGuard: LLMにおけるデータ駆動型および推論駆動型の幻覚の解明"
    },
    {
      "id": "2601.18751",
      "arxivId": "2601.18751",
      "title": "Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback",
      "authors": [
        "Seyed Amir Hosseini",
        "Maryam Abdolali",
        "Amirhosein Tavakkoli",
        "Fardin Ayar",
        "Ehsan Javanmardi",
        "Manabu Tsukada",
        "Mahdi Javanmardi"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.",
      "url": "https://arxiv.org/abs/2601.18751",
      "pdfUrl": "https://arxiv.org/pdf/2601.18751.pdf",
      "titleJa": "信頼するか、信頼しないか、それともひっくり返すか：マルチエキスパートフィードバックによるロバストな選好ベースの強化学習"
    },
    {
      "id": "2601.18747",
      "arxivId": "2601.18747",
      "title": "Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval",
      "authors": [
        "Amir Aavani"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CC",
        "cs.CL",
        "cs.DB"
      ],
      "abstract": "Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic graphs; forcing them to execute such queries typically results in intractable runtime performance. Conversely, naive recursive approaches (Term-at-a-Time), while capable of supporting these structures, suffer from prohibitive memory consumption when enforcing broad logical exclusions. In this paper, we propose that a retrieval engine must be capable of ``Capturing $\\mathbf{P}$'' -- evaluating any polynomial-time property directly over its index in a computationally efficient manner. We define a formal Retrieval Language ($\\mathcal{L}_R$) based on Directed Acyclic Graphs (DAGs) and prove it precisely captures the complexity class $\\mathbf{P}$. We introduce \\texttt{ComputePN}, a novel evaluation algorithm that makes $\\mathcal{L}_R$ tractable. By combining native DAG traversal with a memory-efficient ``Positive-Negative'' response mechanism, \\texttt{ComputePN} ensures the efficient evaluation of any query in $\\mathcal{L}_R$. This work establishes the theoretical foundation for turning the search index into a general-purpose computational engine.",
      "url": "https://arxiv.org/abs/2601.18747",
      "pdfUrl": "https://arxiv.org/pdf/2601.18747.pdf",
      "titleJa": "Pの捕捉：ブール検索の表現力と効率的な評価について"
    },
    {
      "id": "2601.18744",
      "arxivId": "2601.18744",
      "title": "TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models",
      "authors": [
        "Fangxu Yu",
        "Xingang Guo",
        "Lingzhi Yuan",
        "Haoqiang Kang",
        "Hongyu Zhao",
        "Lianhui Qin",
        "Furong Huang",
        "Bin Hu",
        "Tianyi Zhou"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.",
      "url": "https://arxiv.org/abs/2601.18744",
      "pdfUrl": "https://arxiv.org/pdf/2601.18744.pdf",
      "titleJa": "TSRBench: ジェネラリストモデルのための包括的なマルチタスク・マルチモーダル時系列推論ベンチマーク"
    },
    {
      "id": "2601.18739",
      "arxivId": "2601.18739",
      "title": "SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification",
      "authors": [
        "Ignacio Antequera-Sánchez",
        "Juan Luis Suárez-Díaz",
        "Rosana Montes",
        "Francisco Herrera"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.",
      "url": "https://arxiv.org/abs/2601.18739",
      "pdfUrl": "https://arxiv.org/pdf/2601.18739.pdf",
      "titleJa": "SeNeDiF-OOD: オープンワールド分類における分布外検出手法のためのセマンティックネスト二分法融合。モニュメントスタイルの分類に関するケーススタディ"
    },
    {
      "id": "2601.18735",
      "arxivId": "2601.18735",
      "title": "Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems",
      "authors": [
        "Jusheng Zhang",
        "Yijia Fan",
        "Kaitong Cai",
        "Jing Yang",
        "Jiawei Yao",
        "Jian Wang",
        "Guanlong Qu",
        "Ziliang Chen",
        "Keze Wang"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.",
      "url": "https://arxiv.org/abs/2601.18735",
      "pdfUrl": "https://arxiv.org/pdf/2601.18735.pdf",
      "titleJa": "疑問を胸に秘めておくべき理由とは？マルチエージェント・バンディットシステムにおける視覚的不確実性のトレードオフ"
    },
    {
      "id": "2601.18733",
      "arxivId": "2601.18733",
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "authors": [
        "Li Kang",
        "Heng Zhou",
        "Xiufeng Song",
        "Rui Li",
        "Bruno N. Y. Chen",
        "Ziye Wang",
        "Ximeng Meng",
        "Stone Tao",
        "Yiran Qin",
        "Xiaohong Liu",
        "Ruimao Zhang",
        "Lei Bai",
        "Yilun Du",
        "Hao Su",
        "Philip Torr",
        "Zhenfei Yin",
        "Ruihao Gong",
        "Yejun Zeng",
        "Fengjun Zhong",
        "Shenghao Jin",
        "Jinyang Guo",
        "Xianglong Liu",
        "Xiaojun Jia",
        "Tianqi Shan",
        "Wenqi Ren",
        "Simeng Qin",
        "Jialing Yang",
        "Xiaoyu Ma",
        "Tianxing Chen",
        "Zixuan Li",
        "Zijian Cai",
        "Yan Qin",
        "Yusen Qin",
        "Qiangyu Chen",
        "Kaixuan Wang",
        "Zhaoming Han",
        "Yao Mu",
        "Ping Luo",
        "Yuanqi Yao",
        "Haoming Song",
        "Jan-Nico Zaech",
        "Fabien Despinoy",
        "Danda Pani Paudel",
        "Luc Van Gool"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.",
      "url": "https://arxiv.org/abs/2601.18733",
      "pdfUrl": "https://arxiv.org/pdf/2601.18733.pdf",
      "titleJa": "マルチエージェントロボットシステム（MARS）チャレンジにおける進歩と革新"
    },
    {
      "id": "2601.18732",
      "arxivId": "2601.18732",
      "title": "Optimal Use of Preferences in Artificial Intelligence Algorithms",
      "authors": [
        "Joshua S. Gans"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "econ.TH",
        "cs.AI"
      ],
      "abstract": "Machine learning systems embed preferences either in training losses or through post-processing of calibrated predictions. Applying information design methods from Strack and Yang (2024), this paper provides decision problem agnostic conditions under which separation training preference free and applying preferences ex post is optimal. Unlike prior work that requires specifying downstream objectives, the welfare results here apply uniformly across decision problems. The key primitive is a diminishing-value-of-information condition: relative to a fixed (normalised) preference-free loss, preference embedding makes informativeness less valuable at the margin, inducing a mean-preserving contraction of learned posteriors. Because the value of information is convex in beliefs, preference-free training weakly dominates for any expected utility decision problem. This provides theoretical foundations for modular AI pipelines that learn calibrated probabilities and implement asymmetric costs through downstream decision rules. However, separation requires users to implement optimal decision rules. When cognitive constraints bind, as documented in human AI decision-making, preference embedding can dominate by automating threshold computation. These results provide design guidance: preserve optionality through post-processing when objectives may shift; embed preferences when decision-stage frictions dominate.",
      "url": "https://arxiv.org/abs/2601.18732",
      "pdfUrl": "https://arxiv.org/pdf/2601.18732.pdf",
      "titleJa": "人工知能アルゴリズムにおける好みの最適利用"
    },
    {
      "id": "2601.18731",
      "arxivId": "2601.18731",
      "title": "One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment",
      "authors": [
        "Hongru Cai",
        "Yongqi Li",
        "Tiezheng Yu",
        "Fengbin Zhu",
        "Wenjie Wang",
        "Fuli Feng",
        "Wenjie Li"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.",
      "url": "https://arxiv.org/abs/2601.18731",
      "pdfUrl": "https://arxiv.org/pdf/2601.18731.pdf",
      "titleJa": "一つはどんなことにも適応する：パーソナライズされたLLMアライメントのためのメタ報酬モデリング"
    },
    {
      "id": "2601.18724",
      "arxivId": "2601.18724",
      "title": "HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences",
      "authors": [
        "Yusuke Sakai",
        "Hidetaka Kamigaito",
        "Taro Watanabe"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DL"
      ],
      "abstract": "Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as \"HalluCitation\" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.",
      "url": "https://arxiv.org/abs/2601.18724",
      "pdfUrl": "https://arxiv.org/pdf/2601.18724.pdf",
      "titleJa": "HalluCitation Matters: ACL会議における幻覚的論文300件で幻覚的引用の影響を明らかにする"
    },
    {
      "id": "2601.18716",
      "arxivId": "2601.18716",
      "title": "Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules",
      "authors": [
        "Naeyma N. Islam",
        "Thomas R. Caulfield"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.AI",
        "q-bio.BM"
      ],
      "abstract": "Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.",
      "url": "https://arxiv.org/abs/2601.18716",
      "pdfUrl": "https://arxiv.org/pdf/2601.18716.pdf",
      "titleJa": "分子接着剤の条件付き生成モデリング：合成可能な薬物様分子のための現実的なAIアプローチ"
    },
    {
      "id": "2601.17645",
      "arxivId": "2601.17645",
      "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
      "authors": [
        "Xilin Jiang",
        "Qiaolin Wang",
        "Junkai Wu",
        "Xiaomin He",
        "Zhongweiyang Xu",
        "Yinghao Ma",
        "Minshuo Piao",
        "Kaiyi Yang",
        "Xiuwen Zheng",
        "Riki Shimizu",
        "Yicong Chen",
        "Arsalan Firoozi",
        "Gavin Mischler",
        "Sukru Samet Dindar",
        "Richard Antonello",
        "Linyang He",
        "Tsun-An Hsieh",
        "Xulin Fan",
        "Yulun Wu",
        "Yuesheng Ma",
        "Chaitanya Amballa",
        "Weixiong Chen",
        "Jiarui Hai",
        "Ruisi Li",
        "Vishal Choudhari",
        "Cong Han",
        "Yinghao Aaron Li",
        "Adeen Flinker",
        "Mounya Elhilali",
        "Emmanouil Benetos",
        "Mark Hasegawa-Johnson",
        "Romit Roy Choudhury",
        "Nima Mesgarani"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public",
      "url": "https://arxiv.org/abs/2601.17645",
      "pdfUrl": "https://arxiv.org/pdf/2601.17645.pdf",
      "titleJa": "AVMeme試験：法学修士（LLM）の文脈的・文化的知識と思考力を評価するマルチモーダル・多言語・多文化ベンチマーク"
    },
    {
      "id": "2601.17517",
      "arxivId": "2601.17517",
      "title": "EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding",
      "authors": [
        "Luca Cerovaz",
        "Michele Mancusi",
        "Emanuele Rodolà"
      ],
      "publishedDate": "2026-01-24",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Audio codecs power discrete music generative modelling, music streaming, and immersive media by shrinking PCM audio to bandwidth-friendly bitrates. Recent works have gravitated towards processing in the spectral domain; however, spectrogram domains typically struggle with phase modeling, which is naturally complex-valued. Most frequency-domain neural codecs either disregard phase information or encode it as two separate real-valued channels, limiting spatial fidelity. This entails the need to introduce adversarial discriminators at the expense of convergence speed and training stability to compensate for the inadequate representation power of the audio signal. In this work we introduce an end-to-end complex-valued RVQ-VAE audio codec that preserves magnitude-phase coupling across the entire analysis-quantization-synthesis pipeline and removes adversarial discriminators and diffusion post-filters. Without GANs or diffusion, we match or surpass much longer-trained baselines in-domain and reach SOTA out-of-domain performance on phase coherence and waveform fidelity. Compared to standard baselines that train for hundreds of thousands of steps, our model, which reduces the training budget by an order of magnitude, is markedly more compute-efficient while preserving high perceptual quality.",
      "url": "https://arxiv.org/abs/2601.17517",
      "pdfUrl": "https://arxiv.org/pdf/2601.17517.pdf",
      "titleJa": "EuleroDec: 効率的かつ堅牢なオーディオコーディングのための複素値RVQ-VAE"
    },
    {
      "id": "2601.16675",
      "arxivId": "2601.16675",
      "title": "I Guess That's Why They Call it the Blues: Causal Analysis for Audio Classifiers",
      "authors": [
        "David A. Kelly",
        "Hana Chockler"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "It is well-known that audio classifiers often rely on non-musically relevant features and spurious correlations to classify audio. Hence audio classifiers are easy to manipulate or confuse, resulting in wrong classifications. While inducing a misclassification is not hard, until now the set of features that the classifiers rely on was not well understood. In this paper we introduce a new method that uses causal reasoning to discover features of the frequency space that are sufficient and necessary for a given classification. We describe an implementation of this algorithm in the tool FreqReX and provide experimental results on a number of standard benchmark datasets. Our experiments show that causally sufficient and necessary subsets allow us to manipulate the outputs of the models in a variety of ways by changing the input very slightly. Namely, a change to one out of 240,000 frequencies results in a change in classification 58% of the time, and the change can be so small that it is practically inaudible. These results show that causal analysis is useful for understanding the reasoning process of audio classifiers and can be used to successfully manipulate their outputs.",
      "url": "https://arxiv.org/abs/2601.16675",
      "pdfUrl": "https://arxiv.org/pdf/2601.16675.pdf",
      "titleJa": "ブルースと呼ばれる理由：音声分類器の因果分析"
    },
    {
      "id": "2601.16273",
      "arxivId": "2601.16273",
      "title": "The CMU-AIST submission for the ICME 2025 Audio Encoder Challenge",
      "authors": [
        "Shikhar Bharadwaj",
        "Samuele Cornell",
        "Kwanghee Choi",
        "Hye-jin Shim",
        "Soham Deshmukh",
        "Satoru Fukayama",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This technical report describes our submission to the ICME 2025 audio encoder challenge. Our submitted system is built on BEATs, a masked speech token prediction based audio encoder. We extend the BEATs model using 74,000 hours of data derived from various speech, music, and sound corpora and scale its architecture upto 300 million parameters. We experiment with speech-heavy and balanced pre-training mixtures to study the impact of different domains on final performance. Our submitted system consists of an ensemble of the Dasheng 1.2 billion model with two custom scaled-up BEATs models trained on the aforementioned pre-training data mixtures. We also propose a simple ensembling technique that retains the best capabilities of constituent models and surpasses both the baseline and Dasheng 1.2B. For open science, we publicly release our trained checkpoints via huggingface at https://huggingface.co/shikhar7ssu/OpenBEATs-ICME-SOUND and https://huggingface.co/shikhar7ssu/OpenBEATs-ICME.",
      "url": "https://arxiv.org/abs/2601.16273",
      "pdfUrl": "https://arxiv.org/pdf/2601.16273.pdf",
      "titleJa": "ICME 2025オーディオエンコーダチャレンジへのCMU-AISTの応募"
    },
    {
      "id": "2601.16150",
      "arxivId": "2601.16150",
      "title": "Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization",
      "authors": [
        "Maximos Kaliakatsos-Papakostas",
        "Dimos Makris",
        "Konstantinos Soiledis",
        "Konstantinos-Theodoros Tsamis",
        "Vassilis Katsouros",
        "Emilios Cambouropoulos"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody-harmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.",
      "url": "https://arxiv.org/abs/2601.16150",
      "pdfUrl": "https://arxiv.org/pdf/2601.16150.pdf",
      "titleJa": "メロディーに注意を払う（交差させる）：単一エンコーダーによるメロディーハーモニーのためのカリキュラムマスキング"
    },
    {
      "id": "2601.15872",
      "arxivId": "2601.15872",
      "title": "PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation",
      "authors": [
        "Jaekwon Im",
        "Natalia Polouliakh",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.",
      "url": "https://arxiv.org/abs/2601.15872",
      "pdfUrl": "https://arxiv.org/pdf/2601.15872.pdf",
      "titleJa": "PF-D2M: ユニバーサルなダンス・トゥ・ミュージック生成のためのポーズフリー拡散モデル"
    },
    {
      "id": "2601.15083",
      "arxivId": "2601.15083",
      "title": "Bangla Music Genre Classification Using Bidirectional LSTMS",
      "authors": [
        "Muntakimur Rahaman",
        "Md Mahmudul Hoque",
        "Md Mehedi Hassain"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Bangla music is enrich in its own music cultures. Now a days music genre classification is very significant because of the exponential increase in available music, both in digital and physical formats. It is necessary to index them accordingly to facilitate improved retrieval. Automatically classifying Bangla music by genre is essential for efficiently locating specific pieces within a vast and diverse music library. Prevailing methods for genre classification predominantly employ conventional machine learning or deep learning approaches. This work introduces a novel music dataset comprising ten distinct genres of Bangla music. For the task of audio classification, we utilize a recurrent neural network (RNN) architecture. Specifically, a Long Short-Term Memory (LSTM) network is implemented to train the model and perform the classification. Feature extraction represents a foundational stage in audio data processing. This study utilizes Mel-Frequency Cepstral Coefficients (MFCCs) to transform raw audio waveforms into a compact and representative set of features. The proposed framework facilitates music genre classification by leveraging these extracted features. Experimental results demonstrate a classification accuracy of 78%, indicating the system's strong potential to enhance and streamline the organization of Bangla music genres.",
      "url": "https://arxiv.org/abs/2601.15083",
      "pdfUrl": "https://arxiv.org/pdf/2601.15083.pdf",
      "titleJa": "双方向LSTMSを用いたバングラ音楽のジャンル分類"
    },
    {
      "id": "2601.14931",
      "arxivId": "2601.14931",
      "title": "Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali",
      "authors": [
        "Nouhoum Coulibaly",
        "Ousmane Ly",
        "Michael Leventhal",
        "Ousmane Goro"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "This study explores the capacity of generative artificial intelligence (Gen AI) to contribute to the construction of peace narratives and the revitalization of musical heritage in Mali. The study has been made in a political and social context where inter-community tensions and social fractures motivate a search for new symbolic frameworks for reconciliation. The study empirically explores three questions: (1) how Gen AI can be used as a tool for musical creation rooted in national languages and traditions; (2) to what extent Gen AI systems enable a balanced hybridization between technological innovation and cultural authenticity; and (3) how AI-assisted musical co-creation can strengthen social cohesion and cultural sovereignty. The experimental results suggest that Gen AI, embedded in a culturally conscious participatory framework, can act as a catalyst for symbolic diplomacy, amplifying local voices instead of standardizing them. However, challenges persist regarding the availability of linguistic corpora, algorithmic censorship, and the ethics of generating compositions derived from copyrighted sources.",
      "url": "https://arxiv.org/abs/2601.14931",
      "pdfUrl": "https://arxiv.org/pdf/2601.14931.pdf",
      "titleJa": "生成型人工知能、音楽遺産、そして平和物語の構築：マリにおける事例研究"
    },
    {
      "id": "2601.14786",
      "arxivId": "2601.14786",
      "title": "Training-Efficient Text-to-Music Generation with State-Space Modeling",
      "authors": [
        "Wei-Jaw Lee",
        "Fang-Chih Hsieh",
        "Xuanjun Chen",
        "Fang-Duo Tsai",
        "Yi-Hsuan Yang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in text-to-music generation (TTM) have yielded high-quality results, but often at the cost of extensive compute and the use of large proprietary internal data. To improve the affordability and openness of TTM training, an open-source generative model backbone that is more training- and data-efficient is needed. In this paper, we constrain the number of trainable parameters in the generative model to match that of the MusicGen-small benchmark (with about 300M parameters), and replace its Transformer backbone with the emerging class of state-space models (SSMs). Specifically, we explore different SSM variants for sequence modeling, and compare a single-stage SSM-based design with a decomposable two-stage SSM/diffusion hybrid design. All proposed models are trained from scratch on a purely public dataset comprising 457 hours of CC-licensed music, ensuring full openness. Our experimental findings are three-fold. First, we show that SSMs exhibit superior training efficiency compared to the Transformer counterpart. Second, despite using only 9% of the FLOPs and 2% of the training data size compared to the MusicGen-small benchmark, our model achieves competitive performance in both objective metrics and subjective listening tests based on MusicCaps captions. Finally, our scaling-down experiment demonstrates that SSMs can maintain competitive performance relative to the Transformer baseline even at the same training budget (measured in iterations), when the model size is reduced to four times smaller. To facilitate the democratization of TTM research, the processed captions, model checkpoints, and source code are available on GitHub via the project page: https://lonian6.github.io/ssmttm/.",
      "url": "https://arxiv.org/abs/2601.14786",
      "pdfUrl": "https://arxiv.org/pdf/2601.14786.pdf",
      "titleJa": "状態空間モデリングによる効率的なテキストから音楽への生成"
    },
    {
      "id": "2601.14684",
      "arxivId": "2601.14684",
      "title": "Dissecting Performance Degradation in Audio Source Separation under Sampling Frequency Mismatch",
      "authors": [
        "Kanami Imamura",
        "Tomohiko Nakamura",
        "Kohei Yatabe",
        "Hiroshi Saruwatari"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio processing methods based on deep neural networks are typically trained at a single sampling frequency (SF). To handle untrained SFs, signal resampling is commonly employed, but it can degrade performance, particularly when the input SF is lower than the trained SF. This paper investigates the causes of this degradation through two hypotheses: (i) the lack of high-frequency components introduced by up-sampling, and (ii) the greater importance of their presence than their precise representation. To examine these hypotheses, we compare conventional resampling with three alternatives: post-resampling noise addition, which adds Gaussian noise to the resampled signal; noisy-kernel resampling, which perturbs the kernel with Gaussian noise to enrich high-frequency components; and trainable-kernel resampling, which adapts the interpolation kernel through training. Experiments on music source separation show that noisy-kernel and trainable-kernel resampling alleviate the degradation observed with conventional resampling. We further demonstrate that noisy-kernel resampling is effective across diverse models, highlighting it as a simple yet practical option.",
      "url": "https://arxiv.org/abs/2601.14684",
      "pdfUrl": "https://arxiv.org/pdf/2601.14684.pdf",
      "titleJa": "サンプリング周波数の不一致による音源分離の性能劣化の分析"
    },
    {
      "id": "2601.15348",
      "arxivId": "2601.15348",
      "title": "Abusive music and song transformation using GenAI and LLMs",
      "authors": [
        "Jiyang Choi",
        "Rohitash Chandra"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Repeated exposure to violence and abusive content in music and song content can influence listeners' emotions and behaviours, potentially normalising aggression or reinforcing harmful stereotypes. In this study, we explore the use of generative artificial intelligence (GenAI) and Large Language Models (LLMs) to automatically transform abusive words (vocal delivery) and lyrical content in popular music. Rather than simply muting or replacing a single word, our approach transforms the tone, intensity, and sentiment, thus not altering just the lyrics, but how it is expressed. We present a comparative analysis of four selected English songs and their transformed counterparts, evaluating changes through both acoustic and sentiment-based lenses. Our findings indicate that Gen-AI significantly reduces vocal aggressiveness, with acoustic analysis showing improvements in Harmonic to Noise Ratio, Cepstral Peak Prominence, and Shimmer. Sentiment analysis reduced aggression by 63.3-85.6\\% across artists, with major improvements in chorus sections (up to 88.6\\% reduction). The transformed versions maintained musical coherence while mitigating harmful content, offering a promising alternative to traditional content moderation that avoids triggering the \"forbidden fruit\" effect, where the censored content becomes more appealing simply because it is restricted. This approach demonstrates the potential for GenAI to create safer listening experiences while preserving artistic expression.",
      "url": "https://arxiv.org/abs/2601.15348",
      "pdfUrl": "https://arxiv.org/pdf/2601.15348.pdf",
      "titleJa": "GenAIとLLMを使用した虐待的な音楽と歌の変換"
    },
    {
      "id": "2601.14356",
      "arxivId": "2601.14356",
      "title": "Single-step Controllable Music Bandwidth Extension With Flow Matching",
      "authors": [
        "Carlos Hernandez-Olivan",
        "Hendrik Vincent Koops",
        "Hao Hao Tan",
        "Elio Quinton"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio restoration consists in inverting degradations of a digital audio signal to recover what would have been the pristine quality signal before the degradation occurred. This is valuable in contexts such as archives of music recordings, particularly those of precious historical value, for which a clean version may have been lost or simply does not exist. Recent work applied generative models to audio restoration, showing promising improvement over previous methods, and opening the door to the ability to perform restoration operations that were not possible before. However, making these models finely controllable remains a challenge. In this paper, we propose an extension of FLowHigh and introduce the Dynamic Spectral Contour (DSC) as a control signal for bandwidth extension via classifier-free guidance. Our experiments show competitive model performance, and indicate that DSC is a promising feature to support fine-grained conditioning.",
      "url": "https://arxiv.org/abs/2601.14356",
      "pdfUrl": "https://arxiv.org/pdf/2601.14356.pdf",
      "titleJa": "フローマッチングによるシングルステップ制御可能な音楽帯域幅拡張"
    },
    {
      "id": "2601.14157",
      "arxivId": "2601.14157",
      "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models",
      "authors": [
        "Bruno Sienkiewicz",
        "Łukasz Neumann",
        "Mateusz Modrzejewski"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.",
      "url": "https://arxiv.org/abs/2601.14157",
      "pdfUrl": "https://arxiv.org/pdf/2601.14157.pdf",
      "titleJa": "ConceptCaps - 音楽モデルの解釈可能性のための蒸留概念データセット"
    },
    {
      "id": "2601.13931",
      "arxivId": "2601.13931",
      "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music",
      "authors": [
        "Yannis Vasilakis",
        "Rachel Bittner",
        "Johan Pauwels"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG"
      ],
      "abstract": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.",
      "url": "https://arxiv.org/abs/2601.13931",
      "pdfUrl": "https://arxiv.org/pdf/2601.13931.pdf",
      "titleJa": "音楽のための音声テキスト結合モデルにおける効果的な否定モデリングに向けて"
    },
    {
      "id": "2601.13647",
      "arxivId": "2601.13647",
      "title": "Fusion Segment Transformer: Bi-Directional Attention Guided Fusion Network for AI-Generated Music Detection",
      "authors": [
        "Yumin Kim",
        "Seonghyeon Go"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "With the rise of generative AI technology, anyone can now easily create and deploy AI-generated music, which has heightened the need for technical solutions to address copyright and ownership issues. While existing works mainly focused on short-audio, the challenge of full-audio detection, which requires modeling long-term structure and context, remains insufficiently explored. To address this, we propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. As in our previous work, we extract content embeddings from short music segments using diverse feature extractors. Furthermore, we enhance the architecture for full-audio AI-generated music detection by introducing a Gated Fusion Layer that effectively integrates content and structural information, enabling the capture of long-term context. Experiments on the SONICS and AIME datasets show that our approach outperforms the previous model and recent baselines, achieving state-of-the-art results in AI-generated music detection.",
      "url": "https://arxiv.org/abs/2601.13647",
      "pdfUrl": "https://arxiv.org/pdf/2601.13647.pdf",
      "titleJa": "Fusion Segment Transformer: AI生成音楽検出のための双方向アテンションガイド融合ネットワーク"
    },
    {
      "id": "2601.12961",
      "arxivId": "2601.12961",
      "title": "Supervised Learning for Game Music Segmentation",
      "authors": [
        "Shangxuan Luo",
        "Joshua Reiss"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD"
      ],
      "abstract": "At present, neural network-based models, including transformers, struggle to generate memorable and readily comprehensible music from unified and repetitive musical material due to a lack of understanding of musical structure. Consequently, these models are rarely employed by the games industry. It is hypothesised by many scholars that the modelling of musical structure may inform models at a higher level, thereby enhancing the quality of music generation. The aim of this study is to explore the performance of supervised learning methods in the task of structural segmentation, which is the initial step in music structure modelling. An audio game music dataset with 309 structural annotations was created to train the proposed method, which combines convolutional neural networks and recurrent neural networks, achieving performance comparable to the state-of-the-art unsupervised learning methods with fewer training resources.",
      "url": "https://arxiv.org/abs/2601.12961",
      "pdfUrl": "https://arxiv.org/pdf/2601.12961.pdf",
      "titleJa": "ゲーム音楽セグメンテーションのための教師あり学習"
    },
    {
      "id": "2601.12802",
      "arxivId": "2601.12802",
      "title": "UNMIXX: Untangling Highly Correlated Singing Voices Mixtures",
      "authors": [
        "Jihoo Jung",
        "Ji-Hoon Kim",
        "Doyeop Kwak",
        "Junwon Lee",
        "Juhan Nam",
        "Joon Son Chung"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We introduce UNMIXX, a novel framework for multiple singing voices separation (MSVS). While related to speech separation, MSVS faces unique challenges: data scarcity and the highly correlated nature of singing voices mixture. To address these issues, we propose UNMIXX with three key components: (1) musically informed mixing strategy to construct highly correlated, music-like mixtures, (2) cross-source attention that drives representations of two singers apart via reverse attention, and (3) magnitude penalty loss penalizing erroneously assigned interfering energy. UNMIXX not only addresses data scarcity by simulating realistic training data, but also excels at separating highly correlated mixtures through cross-source interactions at both the architectural and loss levels. Our extensive experiments demonstrate that UNMIXX greatly enhances performance, with SDRi gains exceeding 2.2 dB over prior work.",
      "url": "https://arxiv.org/abs/2601.12802",
      "pdfUrl": "https://arxiv.org/pdf/2601.12802.pdf",
      "titleJa": "UNMIXX: 相関性の高い歌声ミックスを解きほぐす"
    },
    {
      "id": "2601.12314",
      "arxivId": "2601.12314",
      "title": "A Similarity Network for Correlating Musical Structure to Military Strategy",
      "authors": [
        "Yiwen Zhang",
        "Hui Zhang",
        "Fanqin Meng"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Music perception, a multi-sensory process based on the synesthesia effect, is an essential component of music aesthetic education. Understanding music structure helps both perception and aesthetic education. Music structure incorporates a range of information, the coordination of which forms the melody, just as different military actions cooperate to produce a military strategy. However, there are a few ways for assessing music perception from the perspectives of system operation and information management. In this paper, we explore the similarities between music structure and military strategy while creating the Music Clips Correlation Network (MCCN) based on Mel-frequency Cepstral Coefficients (MFCCs). The inspiration comes from the comparison between a concert conductor's musical score and a military war commander's sand table exercise. Specifically, we create MCCNs for various kinds of war movie soundtracks, then relate military tactics (Sun Tzu's Art of War, etc.) and political institutions to military operations networks. Our primary findings suggest a few similarities, implying that music perception and aesthetic education can be approached from a military strategy and management perspective through this interdisciplinary research. Similarly, we can discover similarities between the art of military scheming and the art of musical structure based on network analysis in order to facilitate the understanding of the relationship between technology and art.",
      "url": "https://arxiv.org/abs/2601.12314",
      "pdfUrl": "https://arxiv.org/pdf/2601.12314.pdf",
      "titleJa": "音楽構造と軍事戦略を相関させる類似性ネットワーク"
    },
    {
      "id": "2601.16774",
      "arxivId": "2601.16774",
      "title": "E2E-AEC: Implementing an end-to-end neural network learning approach for acoustic echo cancellation",
      "authors": [
        "Yiheng Jiang",
        "Biao Tian",
        "Haoxu Wang",
        "Shengkui Zhao",
        "Bin Ma",
        "Daren Chen",
        "Xiangang Li"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We propose a novel neural network-based end-to-end acoustic echo cancellation (E2E-AEC) method capable of streaming inference, which operates effectively without reliance on traditional linear AEC (LAEC) techniques and time delay estimation. Our approach includes several key strategies: First, we introduce and refine progressive learning to gradually enhance echo suppression. Second, our model employs knowledge transfer by initializing with a pre-trained LAECbased model, harnessing the insights gained from LAEC training. Third, we optimize the attention mechanism with a loss function applied on attention weights to achieve precise time alignment between the reference and microphone signals. Lastly, we incorporate voice activity detection to enhance speech quality and improve echo removal by masking the network output when near-end speech is absent. The effectiveness of our approach is validated through experiments conducted on public datasets.",
      "url": "https://arxiv.org/abs/2601.16774",
      "pdfUrl": "https://arxiv.org/pdf/2601.16774.pdf",
      "titleJa": "E2E-AEC: 音響エコーキャンセルのためのエンドツーエンドのニューラルネットワーク学習アプローチの実装"
    },
    {
      "id": "2601.17086",
      "arxivId": "2601.17086",
      "title": "SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS",
      "authors": [
        "Ayush Pratap Singh",
        "Harshit Singh",
        "Nityanand Mathur",
        "Akshat Mandloi",
        "Sudarshan Kamath"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Neural text-to-speech (TTS) systems systematically mispronounce low-resource proper nouns, particularly non-English names, brands, and geographic locations, due to their underrepresentation in predominantly English training corpora. Existing solutions typically rely on expensive multilingual data collection, supervised finetuning, or manual phonetic annotation, which limits the deployment of TTS systems in linguistically diverse settings. We introduce SonoEdit, a model editing technique that surgically corrects pronunciation errors in pre-trained TTS models without retraining. Instead of costly finetuning or explicit phoneme injection, we propose a parsimonious alternative based on Null-Space Pronunciation Editing, which performs a single-shot parameter update to modify the pronunciation of specific words while provably preserving all other model behavior. We first adapt Acoustic Causal Tracing to identify the Transformer layers responsible for text-to-pronunciation mapping. We then apply Null-Space Constrained Editing to compute a closed-form weight update that corrects the target pronunciation while remaining mathematically orthogonal to the subspace governing general speech generation. This constrained update steers the model's acoustic output toward a desired pronunciation exemplar while guaranteeing zero first-order change on a preserved speech corpus.",
      "url": "https://arxiv.org/abs/2601.17086",
      "pdfUrl": "https://arxiv.org/pdf/2601.17086.pdf",
      "titleJa": "SonoEdit: LLMベースのTTSにおける発音訂正のためのヌル空間制約知識編集"
    },
    {
      "id": "2601.16547",
      "arxivId": "2601.16547",
      "title": "CORD: Bridging the Audio-Text Reasoning Gap via Weighted On-policy Cross-modal Distillation",
      "authors": [
        "Jing Hu",
        "Danxiang Zhu",
        "Xianlong Luo",
        "Dan Zhang",
        "Shuwei He",
        "Yishu Lei",
        "Haitao Zheng",
        "Shikun Feng",
        "Jingzhou He",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Large Audio Language Models (LALMs) have garnered significant research interest. Despite being built upon text-based large language models (LLMs), LALMs frequently exhibit a degradation in knowledge and reasoning capabilities. We hypothesize that this limitation stems from the failure of current training paradigms to effectively bridge the acoustic-semantic gap within the feature representation space. To address this challenge, we propose CORD, a unified alignment framework that performs online cross-modal self-distillation. Specifically, it aligns audio-conditioned reasoning with its text-conditioned counterpart within a unified model. Leveraging the text modality as an internal teacher, CORD performs multi-granularity alignment throughout the audio rollout process. At the token level, it employs on-policy reverse KL divergence with importance-aware weighting to prioritize early and semantically critical tokens. At the sequence level, CORD introduces a judge-based global reward to optimize complete reasoning trajectories via Group Relative Policy Optimization (GRPO). Empirical results across multiple benchmarks demonstrate that CORD consistently enhances audio-conditioned reasoning and substantially bridges the audio-text performance gap with only 80k synthetic training samples, validating the efficacy and data efficiency of our on-policy, multi-level cross-modal alignment approach.",
      "url": "https://arxiv.org/abs/2601.16547",
      "pdfUrl": "https://arxiv.org/pdf/2601.16547.pdf",
      "titleJa": "CORD: 重み付けされたポリシーに基づくクロスモーダル蒸留による音声テキスト推論ギャップの橋渡し"
    },
    {
      "id": "2601.17085",
      "arxivId": "2601.17085",
      "title": "Recovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration",
      "authors": [
        "Esther Sun",
        "Abinay Reddy Naini",
        "Carlos Busso"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Discrete speech tokens offer significant advantages for storage and language model integration, but their application in speech emotion recognition (SER) is limited by paralinguistic information loss during quantization. This paper presents a comprehensive investigation of discrete tokens for SER. Using a fine-tuned WavLM-Large model, we systematically quantify performance degradation across different layer configurations and k-means quantization granularities. To recover the information loss, we propose two key strategies: (1) attention-based multi-layer fusion to recapture complementary information from different layers, and (2) integration of openSMILE features to explicitly reintroduce paralinguistic cues. We also compare mainstream neural codec tokenizers (SpeechTokenizer, DAC, EnCodec) and analyze their behaviors when fused with acoustic features. Our findings demonstrate that through multi-layer fusion and acoustic feature integration, discrete tokens can close the performance gap with continuous representations in SER tasks.",
      "url": "https://arxiv.org/abs/2601.17085",
      "pdfUrl": "https://arxiv.org/pdf/2601.17085.pdf",
      "titleJa": "多層融合とパラ言語的特徴の統合による離散トークンからの音声感情認識のパフォーマンス回復"
    },
    {
      "id": "2601.16316",
      "arxivId": "2601.16316",
      "title": "EdgeSpot: Efficient and High-Performance Few-Shot Model for Keyword Spotting",
      "authors": [
        "Oguzhan Buyuksolak",
        "Alican Gok",
        "Osman Erman Okman"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "We introduce an efficient few-shot keyword spotting model for edge devices, EdgeSpot, that pairs an optimized version of a BC-ResNet-based acoustic backbone with a trainable Per-Channel Energy Normalization frontend and lightweight temporal self-attention. Knowledge distillation is utilized during training by employing a self-supervised teacher model, optimized with Sub-center ArcFace loss. This study demonstrates that the EdgeSpot model consistently provides better accuracy at a fixed false-alarm rate (FAR) than strong BC-ResNet baselines. The largest variant, EdgeSpot-4, improves the 10-shot accuracy at 1% FAR from 73.7% to 82.0%, which requires only 29.4M MACs with 128k parameters.",
      "url": "https://arxiv.org/abs/2601.16316",
      "pdfUrl": "https://arxiv.org/pdf/2601.16316.pdf",
      "titleJa": "EdgeSpot: キーワードスポッティングのための効率的で高性能な少数ショットモデル"
    },
    {
      "id": "2601.16023",
      "arxivId": "2601.16023",
      "title": "Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs",
      "authors": [
        "Lalaram Arya",
        "Mrinmoy Bhattacharjee",
        "Adarsh C. R.",
        "S. R. Mahadeva Prasanna"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "cs.HC"
      ],
      "abstract": "Direct Speech-to-Speech Translation (S2ST) has gained increasing attention for its ability to translate speech from one language to another, while reducing error propagation and latency inherent in traditional cascaded pipelines. However, existing direct S2ST systems continue to face notable challenges, including instability in semantic-acoustic alignment when parallel speech data is scarce, difficulty in preserving speaker identity, and limited multilingual scalability. In this work, we introduce DS2ST-LM, a scalable, single-stage direct S2ST framework leveraging a multilingual Large Language Model (LLM). The architecture integrates a Whisper speech encoder, a learnable projection module, a Qwen2-0.5B LLM, and a timbre-controlled vocoder. We construct GigaS2S-1000, a 1000-hour bilingual corpus by extending the GigaST dataset with high-fidelity synthetic target speech, and show that this synthetic data alleviates data scarcity to some extent. We investigate two semantic token generation strategies: speech-derived S3 tokens and text-derived tokens generated by a pre-trained LLM, and analyze their impact on training stability and semantic consistency. We further evaluate three projection architectures (Linear, Conv1D-Linear, and Q-Former) and observe that while higher-capacity projectors converge faster, the simple Linear projector achieves higher performance. Extensive experiments demonstrate that DS2ST-LM outperforms traditional cascaded and ST (Qwen-Audio) + TTS baselines across both lexical (BLEU, METEOR) and semantic (BLEURT, COMET) metrics, while extending to multiple language pairs, including French, Spanish, German, Hindi, Bengali, and Urdu. Furthermore, we incorporate timbre-aware speech synthesis to preserve speaker information, enabling DS2ST-LM to surpass prior direct S2ST systems in both speaker similarity and perceptual naturalness.",
      "url": "https://arxiv.org/abs/2601.16023",
      "pdfUrl": "https://arxiv.org/pdf/2601.16023.pdf",
      "titleJa": "音色を考慮したLLMベースの直接音声翻訳は複数の言語ペアに拡張可能"
    },
    {
      "id": "2601.15668",
      "arxivId": "2601.15668",
      "title": "EmotionThinker: Prosody-Aware Reinforcement Learning for Explainable Speech Emotion Reasoning",
      "authors": [
        "Dingdong Wang",
        "Shujie Liu",
        "Tianhua Zhang",
        "Youjun Chen",
        "Jinyu Li",
        "Helen Meng"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Emotional information in speech plays a unique role in multimodal perception. However, current Speech Large Language Models (SpeechLLMs), similar to conventional speech emotion recognition (SER) systems, still treat emotion understanding as a simple classification problem. This provides limited interpretability of predictions, while leaving the LLMs' expressive and reasoning capabilities underutilized. In this work, we take the first step to reformulate SER as a deep reasoning problem through reinforcement learning (RL). We propose EmotionThinker, which is designed to generate accurate emotion predictions with interpretable explanations grounded in fine-grained acoustic cues. To achieve this, we first construct EmotionCoT-35K, an emotional reasoning dataset with Chain-of-Thought annotations and detailed captions. Second, we observe that current SpeechLLMs exhibit weak prosody perception, whereas prosodic cues constitute fundamental signals for interpreting emotions. To address this, we develop the prosody-enhanced foundation model EmotionThinker-Base, and demonstrate that prosody enhancement improves emotion understanding. Third, we introduce Group-Relative-Policy-Optimization with Progressive-Trust-aware-Reasoning-Reward (GRPO-PTR) for RL. Different from standard GRPO, which relies only on rule-based outcome rewards, GRPO-PTR progressively introduces reasoning reward, dynamically adjusts it with a trustworthiness weight reflecting the alignment between reasoning and outcome, and evaluates the overall reasoning quality with a reward model based on multi-dimensional criteria. EmotionThinker outperforms previous state-of-the-art evaluation models both in emotion accuracy and explanation quality, advancing SER toward interpretable multimodal reasoning. Project page: https://github.com/dingdongwang/EmotionThinker",
      "url": "https://arxiv.org/abs/2601.15668",
      "pdfUrl": "https://arxiv.org/pdf/2601.15668.pdf",
      "titleJa": "EmotionThinker: 説明可能な音声感情推論のための韻律を考慮した強化学習"
    },
    {
      "id": "2601.15596",
      "arxivId": "2601.15596",
      "title": "DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice",
      "authors": [
        "Leying Zhang",
        "Tingxiao Zhou",
        "Haiyang Sun",
        "Mengxiao Bi",
        "Yanmin Qian"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "While modern Text-to-Speech (TTS) systems achieve high fidelity for read-style speech, they struggle to generate Autonomous Sensory Meridian Response (ASMR), a specialized, low-intensity speech style essential for relaxation. The inherent challenges include ASMR's subtle, often unvoiced characteristics and the demand for zero-shot speaker adaptation. In this paper, we introduce DeepASMR, the first framework designed for zero-shot ASMR generation. We demonstrate that a single short snippet of a speaker's ordinary, read-style speech is sufficient to synthesize high-fidelity ASMR in their voice, eliminating the need for whispered training data from the target speaker. Methodologically, we first identify that discrete speech tokens provide a soft factorization of ASMR style from speaker timbre. Leveraging this insight, we propose a two-stage pipeline incorporating a Large Language Model (LLM) for content-style encoding and a flow-matching acoustic decoder for timbre reconstruction. Furthermore, we contribute DeepASMR-DB, a comprehensive 670-hour English-Chinese multi-speaker ASMR speech corpus, and introduce a novel evaluation protocol integrating objective metrics, human listening tests, LLM-based scoring and unvoiced speech analysis. Extensive experiments confirm that DeepASMR achieves state-of-the-art naturalness and style fidelity in ASMR generation for anyone of any voice, while maintaining competitive performance on normal speech synthesis.",
      "url": "https://arxiv.org/abs/2601.15596",
      "pdfUrl": "https://arxiv.org/pdf/2601.15596.pdf",
      "titleJa": "DeepASMR: LLMベースのゼロショットASMR音声生成（あらゆる声質の人向け）"
    },
    {
      "id": "2601.15433",
      "arxivId": "2601.15433",
      "title": "DynamicSound simulator for simulating moving sources and microphone arrays",
      "authors": [
        "Luca Barbisan",
        "Marco Levorato",
        "Fabrizio Riente"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Developing algorithms for sound classification, detection, and localization requires large amounts of flexible and realistic audio data, especially when leveraging modern machine learning and beamforming techniques. However, most existing acoustic simulators are tailored for indoor environments and are limited to static sound sources, making them unsuitable for scenarios involving moving sources, moving microphones, or long-distance propagation. This paper presents DynamicSound an open-source acoustic simulation framework for generating multichannel audio from one or more sound sources with the possibility to move them continuously in three-dimensional space and recorded by arbitrarily configured microphone arrays. The proposed model explicitly accounts for finite sound propagation delays, Doppler effects, distance-dependent attenuation, air absorption, and first-order reflections from planar surfaces, yielding temporally consistent spatial audio signals. Unlike conventional mono or stereo simulators, the proposed system synthesizes audio for an arbitrary number of virtual microphones, accurately reproducing inter-microphone time delays, level differences, and spectral coloration induced by the environment. Comparative evaluations with existing open-source tools demonstrate that the generated signals preserve high spatial fidelity across varying source positions and acoustic conditions. By enabling the generation of realistic multichannel audio under controlled and repeatable conditions, the proposed open framework provides a flexible and reproducible tool for the development, training, and evaluation of modern spatial audio and sound-source localization algorithms.",
      "url": "https://arxiv.org/abs/2601.15433",
      "pdfUrl": "https://arxiv.org/pdf/2601.15433.pdf",
      "titleJa": "移動音源とマイクアレイをシミュレートするDynamicSoundシミュレータ"
    },
    {
      "id": "2601.14227",
      "arxivId": "2601.14227",
      "title": "Transformer Architectures for Respiratory Sound Analysis and Multimodal Diagnosis",
      "authors": [
        "Theodore Aptekarev",
        "Vladimir Sokolovsky",
        "Gregory Furman"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Respiratory sound analysis is a crucial tool for screening asthma and other pulmonary pathologies, yet traditional auscultation remains subjective and experience-dependent. Our prior research established a CNN baseline using DenseNet201, which demonstrated high sensitivity in classifying respiratory sounds. In this work, we (i) adapt the Audio Spectrogram Transformer (AST) for respiratory sound analysis and (ii) evaluate a multimodal Vision-Language Model (VLM) that integrates spectrograms with structured patient metadata. AST is initialized from publicly available weights and fine-tuned on a medical dataset containing hundreds of recordings per diagnosis. The VLM experiment uses a compact Moondream-type model that processes spectrogram images alongside a structured text prompt (sex, age, recording site) to output a JSON-formatted diagnosis. Results indicate that AST achieves approximately 97% accuracy with an F1-score around 97% and ROC AUC of 0.98 for asthma detection, significantly outperforming both the internal CNN baseline and typical external benchmarks. The VLM reaches 86-87% accuracy, performing comparably to the CNN baseline while demonstrating the capability to integrate clinical context into the inference process. These results confirm the effectiveness of self-attention for acoustic screening and highlight the potential of multimodal architectures for holistic diagnostic tools.",
      "url": "https://arxiv.org/abs/2601.14227",
      "pdfUrl": "https://arxiv.org/pdf/2601.14227.pdf",
      "titleJa": "呼吸音解析とマルチモーダル診断のためのトランスフォーマーアーキテクチャ"
    },
    {
      "id": "2601.13849",
      "arxivId": "2601.13849",
      "title": "Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control",
      "authors": [
        "Ziyi Yang",
        "Li Rao",
        "Zhengding Luo",
        "Dongyuan Shi",
        "Qirui Huang",
        "Woon-Seng Gan"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.LG",
        "eess.SP"
      ],
      "abstract": "Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train.",
      "url": "https://arxiv.org/abs/2601.13849",
      "pdfUrl": "https://arxiv.org/pdf/2601.13849.pdf",
      "titleJa": "能動騒音制御のためのメタ学習による制御フィルタと二次パスの共初期化"
    },
    {
      "id": "2601.13847",
      "arxivId": "2601.13847",
      "title": "Emotion and Acoustics Should Agree: Cross-Level Inconsistency Analysis for Audio Deepfake Detection",
      "authors": [
        "Jinhua Zhang",
        "Zhenqi Jia",
        "Rui Liu"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio Deepfake Detection (ADD) aims to detect spoof speech from bonafide speech. Most prior studies assume that stronger correlations within or across acoustic and emotional features imply authenticity, and thus focus on enhancing or measuring such correlations. However, existing methods often treat acoustic and emotional features in isolation or rely on correlation metrics, which overlook subtle desynchronization between them and smooth out abrupt discontinuities. To address these issues, we propose EAI-ADD, which treats cross level emotion acoustic inconsistency as the primary detection signal. We first project emotional and acoustic representations into a comparable space. Then we progressively integrate frame level and utterance level emotion features with acoustic features to capture cross level emotion acoustic inconsistencies across different temporal granularities. Experimental results on the ASVspoof 2019LA and 2021LA datasets demonstrate that the proposed EAI-ADD outperforms baselines, providing a more effective solution for audio anti spoofing detection.",
      "url": "https://arxiv.org/abs/2601.13847",
      "pdfUrl": "https://arxiv.org/pdf/2601.13847.pdf",
      "titleJa": "感情と音響は一致するはず：オーディオディープフェイク検出のためのクロスレベル不整合分析"
    },
    {
      "id": "2601.13679",
      "arxivId": "2601.13679",
      "title": "Ultra-Lightweight Network for Ship-Radiated Sound Classification on Embedded Deployment",
      "authors": [
        "Sangwon Park",
        "Dongjun Kim",
        "Sung-Hoon Byun",
        "Sangwook Park"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This letter presents ShuffleFAC, a lightweight acoustic model for ship-radiated sound classification in resource-constrained maritime monitoring systems. ShuffleFAC integrates Frequency-Aware convolution into an efficiency-oriented backbone using separable convolution, point-wise group convolution, and channel shuffle, enabling frequency-sensitive feature extraction with low computational cost. Experiments on the DeepShip dataset show that ShuffleFAC achieves competitive performance with substantially reduced complexity. In particular, ShuffleFAC ($γ=16$) attains a macro F1-score of 71.45 $\\pm$ 1.18% using 39K parameters and 3.06M MACs, and achieves an inference latency of 6.05 $\\pm$ 0.95ms on a Raspberry Pi. Compared with MicroNet0, it improves macro F1-score by 1.82 % while reducing model size by 9.7x and latency by 2.5x. These results indicate that ShuffleFAC is suitable for real-time embedded UATR.",
      "url": "https://arxiv.org/abs/2601.13679",
      "pdfUrl": "https://arxiv.org/pdf/2601.13679.pdf",
      "titleJa": "組み込み型船舶放射音分類用超軽量ネットワーク"
    },
    {
      "id": "2601.13589",
      "arxivId": "2601.13589",
      "title": "Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification",
      "authors": [
        "HyeYoung Lee"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.",
      "url": "https://arxiv.org/abs/2601.13589",
      "pdfUrl": "https://arxiv.org/pdf/2601.13589.pdf",
      "titleJa": "リアルタイム安全性検証を備えたマルチエージェントAIシステムによる動作応答コンテンツ生成"
    },
    {
      "id": "2601.13513",
      "arxivId": "2601.13513",
      "title": "Event Classification by Physics-informed Inpainting for Distributed Multichannel Acoustic Sensor with Partially Degraded Channels",
      "authors": [
        "Noriyuki Tonami",
        "Wataru Kohno",
        "Yoshiyuki Yajima",
        "Sakiko Mishima",
        "Yumi Arai",
        "Reishi Kondo",
        "Tomoyuki Hino"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Distributed multichannel acoustic sensing (DMAS) enables large-scale sound event classification (SEC), but performance drops when many channels are degraded and when sensor layouts at test time differ from training layouts. We propose a learning-free, physics-informed inpainting frontend based on reverse time migration (RTM). In this approach, observed multichannel spectrograms are first back-propagated on a 3D grid using an analytic Green's function to form a scene-consistent image, and then forward-projected to reconstruct inpainted signals before log-mel feature extraction and Transformer-based classification. We evaluate the method on ESC-50 with 50 sensors and three layouts (circular, linear, right-angle), where per-channel SNRs are sampled from -30 to 0 dB. Compared with an AST baseline, scaling-sparsemax channel selection, and channel-swap augmentation, the proposed RTM frontend achieves the best or competitive accuracy across all layouts, improving accuracy by 13.1 points on the right-angle layout (from 9.7% to 22.8%). Correlation analyses show that spatial weights align more strongly with SNR than with channel--source distance, and that higher SNR--weight correlation corresponds to higher SEC accuracy. These results demonstrate that a reconstruct-then-project, physics-based preprocessing effectively complements learning-only methods for DMAS under layout-open configurations and severe channel degradation.",
      "url": "https://arxiv.org/abs/2601.13513",
      "pdfUrl": "https://arxiv.org/pdf/2601.13513.pdf",
      "titleJa": "部分的に劣化したチャネルを持つ分散型マルチチャネル音響センサーのための物理学に基づくインペインティングによるイベント分類"
    }
  ],
  "lastUpdated": "2026-01-28T00:55:24.460341",
  "totalCount": 79
}