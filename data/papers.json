{
  "papers": [
    {
      "id": "2601.10547",
      "arxivId": "2601.10547",
      "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
      "authors": [
        "Dongchao Yang",
        "Yuxin Xie",
        "Yuguo Yin",
        "Zheyu Wang",
        "Xiaoyu Yi",
        "Gongxi Zhu",
        "Xiaolong Weng",
        "Zihan Xiong",
        "Yingzhe Ma",
        "Dading Cong",
        "Jingliang Liu",
        "Zihang Huang",
        "Jinghan Ru",
        "Rongjie Huang",
        "Haoran Wan",
        "Peixu Wang",
        "Kuoxi Yu",
        "Helin Wang",
        "Liming Liang",
        "Xianwei Zhuang",
        "Yuanyuan Wang",
        "Haohan Guo",
        "Junjie Cao",
        "Zeqian Ju",
        "Songxiang Liu",
        "Yuewen Cao",
        "Heming Weng",
        "Yuexian Zou"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD"
      ],
      "abstract": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio-text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "url": "https://arxiv.org/abs/2601.10547",
      "pdfUrl": "https://arxiv.org/pdf/2601.10547.pdf",
      "titleJa": "HeartMuLa: オープンソースの音楽基盤モデルファミリー"
    },
    {
      "id": "2601.10453",
      "arxivId": "2601.10453",
      "title": "Stable Differentiable Modal Synthesis for Learning Nonlinear Dynamics",
      "authors": [
        "Victor Zheleznov",
        "Stefan Bilbao",
        "Alec Wright",
        "Simon King"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS",
        "physics.comp-ph"
      ],
      "abstract": "Modal methods are a long-standing approach to physical modelling synthesis. Extensions to nonlinear problems are possible, including the case of a high-amplitude vibration of a string. A modal decomposition leads to a densely coupled nonlinear system of ordinary differential equations. Recent work in scalar auxiliary variable techniques has enabled construction of explicit and stable numerical solvers for such classes of nonlinear systems. On the other hand, machine learning approaches (in particular neural ordinary differential equations) have been successful in modelling nonlinear systems automatically from data. In this work, we examine how scalar auxiliary variable techniques can be combined with neural ordinary differential equations to yield a stable differentiable model capable of learning nonlinear dynamics. The proposed approach leverages the analytical solution for linear vibration of system's modes so that physical parameters of a system remain easily accessible after the training without the need for a parameter encoder in the model architecture. As a proof of concept, we generate synthetic data for the nonlinear transverse vibration of a string and show that the model can be trained to reproduce the nonlinear dynamics of the system. Sound examples are presented.",
      "url": "https://arxiv.org/abs/2601.10453",
      "pdfUrl": "https://arxiv.org/pdf/2601.10453.pdf",
      "titleJa": "非線形動力学の学習のための安定微分可能モード合成"
    },
    {
      "id": "2601.10384",
      "arxivId": "2601.10384",
      "title": "RSA-Bench: Benchmarking Audio Large Models in Real-World Acoustic Scenarios",
      "authors": [
        "Yibo Zhang",
        "Liang Lin",
        "Kaiwen Luo",
        "Shilinlu Yan",
        "Jin Wang",
        "Yaoqi Guo",
        "Yitian Chen",
        "Yalan Qin",
        "Zhenhong Zhou",
        "Kun Wang",
        "Li Sun"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD"
      ],
      "abstract": "While Audio Large Models (ALMs) have achieved remarkable proficiency, their robustness remains brittle in real-world deployment. Existing evaluations largely rely on synthetic Gaussian noise or simplistic single-source interference, failing to capture the intricate, multi-layered acoustic dynamics -- or ``Acoustic Ecology'' -- that characterize authentic physical environments. To bridge this ecological gap, we introduce \\textbf{RSA-Bench}, a comprehensive robustness benchmark designed to stress-test ALLMs through high-fidelity auditory scene simulations. Unlike traditional methods, we construct evaluation samples by naturally superimposing diverse environmental soundscapes -- spanning \\textit{Pasture}, \\textit{Extreme Weather}, \\textit{Classroom}, and \\textit{Outdoors} -- onto clean speech signals across a spectrum of interference intensities. By evaluating models on six core tasks ranging from fundamental perception to complex reasoning, our study unveils three macro-level insights: \\textbf{(I) The Perception-Cognition Gap:} Models maintain relative resilience in low-level recognition but suffer a \\textbf{functional collapse} in high-order reasoning tasks under stress; \\textbf{(II) Scenario Sensitivity:} ``Vocal-like'' interference (e.g., background laughter) proves significantly more destructive than mechanical noise, challenging the model's auditory attention mechanisms; and \\textbf{(III) The Denoising Paradox:} Standard speech enhancement often exacerbates performance degradation, as ALLMs prove highly sensitive to the semantic distortions introduced by denoising artifacts.",
      "url": "https://arxiv.org/abs/2601.10384",
      "pdfUrl": "https://arxiv.org/pdf/2601.10384.pdf",
      "titleJa": "RSA-Bench: 実世界の音響シナリオにおける大規模オーディオモデルのベンチマーク"
    },
    {
      "id": "2601.10345",
      "arxivId": "2601.10345",
      "title": "Self-supervised restoration of singing voice degraded by pitch shifting using shallow diffusion",
      "authors": [
        "Yunyi Liu",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Pitch shifting has been an essential feature in singing voice production. However, conventional signal processing approaches exhibit well known trade offs such as formant shifts and robotic coloration that becomes more severe at larger transposition jumps. This paper targets high quality pitch shifting for singing by reframing it as a restoration problem: given an audio track that has been pitch shifted (and thus contaminated by artifacts), we recover a natural sounding performance while preserving its melody and timing. Specifically, we use a lightweight, mel space diffusion model driven by frame level acoustic features such as f0, volume, and content features. We construct training pairs in a self supervised manner by applying pitch shifts and reversing them to simulate realistic artifacts while retaining ground truth. On a curated singing set, the proposed approach substantially reduces pitch shift artifacts compared to representative classical baselines, as measured by both statistical metrics and pairwise acoustic measures. The results suggest that restoration based pitch shifting could be a viable approach towards artifact resistant transposition in vocal production workflows.",
      "url": "https://arxiv.org/abs/2601.10345",
      "pdfUrl": "https://arxiv.org/pdf/2601.10345.pdf",
      "titleJa": "浅い拡散を用いたピッチシフトによって劣化した歌声の自己教師あり復元"
    },
    {
      "id": "2601.10272",
      "arxivId": "2601.10272",
      "title": "MoST: Mixing Speech and Text with Modality-Aware Mixture of Experts",
      "authors": [
        "Yuxuan Lou",
        "Kai Yang",
        "Yang You"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "We present MoST (Mixture of Speech and Text), a novel multimodal large language model that seamlessly integrates speech and text processing through our proposed Modality-Aware Mixture of Experts (MAMoE) architecture. While current multimodal models typically process diverse modality representations with identical parameters, disregarding their inherent representational differences, we introduce specialized routing pathways that direct tokens to modality-appropriate experts based on input type. MAMoE simultaneously enhances modality-specific learning and cross-modal understanding through two complementary components: modality-specific expert groups that capture domain-specific patterns and shared experts that facilitate information transfer between modalities. Building on this architecture, we develop an efficient transformation pipeline that adapts the pretrained MoE language model through strategic post-training on ASR and TTS datasets, followed by fine-tuning with a carefully curated speech-text instruction dataset. A key feature of this pipeline is that it relies exclusively on fully accessible, open-source datasets to achieve strong performance and data efficiency. Comprehensive evaluations across ASR, TTS, audio language modeling, and spoken question answering benchmarks show that MoST consistently outperforms existing models of comparable parameter counts. Our ablation studies confirm that the modality-specific routing mechanism and shared experts design significantly contribute to performance gains across all tested domains. To our knowledge, MoST represents the first fully open-source speech-text LLM built on a Mixture of Experts architecture. \\footnote{We release MoST model, training code, inference code, and training data at https://github.com/NUS-HPC-AI-Lab/MoST",
      "url": "https://arxiv.org/abs/2601.10272",
      "pdfUrl": "https://arxiv.org/pdf/2601.10272.pdf",
      "titleJa": "MoST: モダリティを考慮した専門家の混合による音声とテキストの混合"
    },
    {
      "id": "2601.09931",
      "arxivId": "2601.09931",
      "title": "Diffusion-based Frameworks for Unsupervised Speech Enhancement",
      "authors": [
        "Jean-Eudes Ayilo",
        "Mostafa Sadeghi",
        "Romain Serizel",
        "Xavier Alameda-Pineda"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This paper addresses $\\textit{unsupervised}$ diffusion-based single-channel speech enhancement (SE). Prior work in this direction combines a score-based diffusion model trained on clean speech with a Gaussian noise model whose covariance is structured by non-negative matrix factorization (NMF). This combination is used within an iterative expectation-maximization (EM) scheme, in which a diffusion-based posterior-sampling E-step estimates the clean speech. We first revisit this framework and propose to explicitly model both speech and acoustic noise as latent variables, jointly sampling them in the E-step instead of sampling speech alone as in previous approaches. We then introduce a new unsupervised SE framework that replaces the NMF noise prior with a diffusion-based noise model, learned jointly with the speech prior in a single conditional score model. Within this framework, we derive two variants: one that implicitly accounts for noise and one that explicitly treats noise as a latent variable. Experiments on WSJ0-QUT and VoiceBank-DEMAND show that explicit noise modeling systematically improves SE performance for both NMF-based and diffusion-based noise priors. Under matched conditions, the diffusion-based noise model attains the best overall quality and intelligibility among unsupervised methods, while under mismatched conditions the proposed NMF-based explicit-noise framework is more robust and suffers less degradation than several supervised baselines. Our code will be publicly available on this $\\href{https://github.com/jeaneudesAyilo/enudiffuse}{URL}$.",
      "url": "https://arxiv.org/abs/2601.09931",
      "pdfUrl": "https://arxiv.org/pdf/2601.09931.pdf",
      "titleJa": "教師なし音声強調のための拡散ベースフレームワーク"
    },
    {
      "id": "2601.09603",
      "arxivId": "2601.09603",
      "title": "Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer",
      "authors": [
        "Petros Vavaroutsos",
        "Theodoros Palamas",
        "Pantelis Vikatos"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "In recent years, foundation models have become very popular due to their exceptional performance, mainly in natural language (NLP) tasks where they were first introduced. These models usually consist of hundreds of millions, or even billions, of parameters, making them resource-intensive during training and in production systems, leading to increased costs. This paper focuses on the reduction of a foundation's model size when applied to music information retrieval (MIR) tasks. Our research combines the Branchformer architecture with SummaryMixing, which were first applied in speech recognition, along with a random quantization process. To facilitate reproducibility, we conduct pre-training on publicly available datasets, complemented by a proprietary dataset comparable in scale to other private datasets reported in the literature. We ensure robust evaluation by using a framework consisting of a variety of downstream MIR tasks. Our results show that our architecture achieves competitive performance when compared with other state-of-the-art models that use multi-head self-attention, while reducing the model size from 8.5% up to 12.3%.",
      "url": "https://arxiv.org/abs/2601.09603",
      "pdfUrl": "https://arxiv.org/pdf/2601.09603.pdf",
      "titleJa": "ランダム量子化器を用いた音楽理解のための線形複雑度自己教師学習"
    },
    {
      "id": "2601.09520",
      "arxivId": "2601.09520",
      "title": "Towards Realistic Synthetic Data for Automatic Drum Transcription",
      "authors": [
        "Pierfrancesco Melucci",
        "Paolo Merialdo",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Deep learning models define the state-of-the-art in Automatic Drum Transcription (ADT), yet their performance is contingent upon large-scale, paired audio-MIDI datasets, which are scarce. Existing workarounds that use synthetic data often introduce a significant domain gap, as they typically rely on low-fidelity SoundFont libraries that lack acoustic diversity. While high-quality one-shot samples offer a better alternative, they are not available in a standardized, large-scale format suitable for training. This paper introduces a new paradigm for ADT that circumvents the need for paired audio-MIDI training data. Our primary contribution is a semi-supervised method to automatically curate a large and diverse corpus of one-shot drum samples from unlabeled audio sources. We then use this corpus to synthesize a high-quality dataset from MIDI files alone, which we use to train a sequence-to-sequence transcription model. We evaluate our model on the ENST and MDB test sets, where it achieves new state-of-the-art results, significantly outperforming both fully supervised methods and previous synthetic-data approaches. The code for reproducing our experiments is publicly available at https://github.com/pier-maker92/ADT_STR",
      "url": "https://arxiv.org/abs/2601.09520",
      "pdfUrl": "https://arxiv.org/pdf/2601.09520.pdf",
      "titleJa": "自動ドラム転写のための現実的な合成データに向けて"
    },
    {
      "id": "2601.09461",
      "arxivId": "2601.09461",
      "title": "Analysis of the Maximum Prediction Gain of Short-Term Prediction on Sustained Speech",
      "authors": [
        "Reemt Hinrichs",
        "Muhamad Fadli Damara",
        "Stephan Preihs",
        "Jörn Ostermann"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Signal prediction is widely used in, e.g., economic forecasting, echo cancellation and in data compression, particularly in predictive coding of speech and music. Predictive coding algorithms reduce the bit-rate required for data transmission or storage by signal prediction. The prediction gain is a classic measure in applied signal coding of the quality of a predictor, as it links the mean-squared prediction error to the signal-to-quantization-noise of predictive coders. To evaluate predictor models, knowledge about the maximum achievable prediction gain independent of a predictor model is desirable. In this manuscript, Nadaraya-Watson kernel-regression (NWKR) and an information theoretic upper bound are applied to analyze the upper bound of the prediction gain on a newly recorded dataset of sustained speech/phonemes. It was found that for unvoiced speech a linear predictor always achieves the maximum prediction gain within at most 0.3 dB. On voiced speech, the optimum one-tap predictor was found to be linear but starting with two taps, the maximum achievable prediction gain was found to be about 2 dB to 6 dB above the prediction gain of the linear predictor. Significant differences between speakers/subjects were observed. The created dataset as well as the code can be obtained for research purpose upon request.",
      "url": "https://arxiv.org/abs/2601.09461",
      "pdfUrl": "https://arxiv.org/pdf/2601.09461.pdf",
      "titleJa": "持続音声における短期予測の最大予測利得の分析"
    },
    {
      "id": "2601.09448",
      "arxivId": "2601.09448",
      "title": "Population-Aligned Audio Reproduction With LLM-Based Equalizers",
      "authors": [
        "Ioannis Stylianou",
        "Jon Francombe",
        "Pablo Martinez-Nuevo",
        "Sven Ewan Shepstone",
        "Zheng-Hua Tan"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Conventional audio equalization is a static process that requires manual and cumbersome adjustments to adapt to changing listening contexts (e.g., mood, location, or social setting). In this paper, we introduce a Large Language Model (LLM)-based alternative that maps natural language text prompts to equalization settings. This enables a conversational approach to sound system control. By utilizing data collected from a controlled listening experiment, our models exploit in-context learning and parameter-efficient fine-tuning techniques to reliably align with population-preferred equalization settings. Our evaluation methods, which leverage distributional metrics that capture users' varied preferences, show statistically significant improvements in distributional alignment over random sampling and static preset baselines. These results indicate that LLMs could function as \"artificial equalizers,\" contributing to the development of more accessible, context-aware, and expert-level audio tuning methods.",
      "url": "https://arxiv.org/abs/2601.09448",
      "pdfUrl": "https://arxiv.org/pdf/2601.09448.pdf",
      "titleJa": "LLMベースのイコライザーによる人口整合オーディオ再生"
    },
    {
      "id": "2601.09413",
      "arxivId": "2601.09413",
      "title": "Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception",
      "authors": [
        "Zhen Wan",
        "Chao-Han Huck Yang",
        "Jinchuan Tian",
        "Hanrong Ye",
        "Ankita Pasad",
        "Szu-wei Fu",
        "Arushi Goel",
        "Ryo Hachiuma",
        "Shizhe Diao",
        "Kunal Dhawan",
        "Sreyan Ghosh",
        "Yusuke Hirota",
        "Zhehuai Chen",
        "Rafael Valle",
        "Ehsan Hosseini Asl",
        "Chenhui Chu",
        "Shinji Watanabe",
        "Yu-Chiang Frank Wang",
        "Boris Ginsburg"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MA",
        "eess.AS"
      ],
      "abstract": "We introduce a voice-agentic framework that learns one critical omni-understanding skill: knowing when to trust itself versus when to consult external audio perception. Our work is motivated by a crucial yet counterintuitive finding: naively fine-tuning an omni-model on both speech recognition and external sound understanding tasks often degrades performance, as the model can be easily misled by noisy hypotheses. To address this, our framework, Speech-Hands, recasts the problem as an explicit self-reflection decision. This learnable reflection primitive proves effective in preventing the model from being derailed by flawed external candidates. We show that this agentic action mechanism generalizes naturally from speech recognition to complex, multiple-choice audio reasoning. Across the OpenASR leaderboard, Speech-Hands consistently outperforms strong baselines by 12.1% WER on seven benchmarks. The model also achieves 77.37% accuracy and high F1 on audio QA decisions, showing robust generalization and reliability across diverse audio question answering datasets. By unifying perception and decision-making, our work offers a practical path toward more reliable and resilient audio intelligence.",
      "url": "https://arxiv.org/abs/2601.09413",
      "pdfUrl": "https://arxiv.org/pdf/2601.09413.pdf",
      "titleJa": "Speech-Hands: 全方位知覚による音声認識とオーディオ推論への自己反映型音声エージェントアプローチ"
    },
    {
      "id": "2601.09385",
      "arxivId": "2601.09385",
      "title": "SLAM-LLM: A Modular, Open-Source Multimodal Large Language Model Framework and Best Practice for Speech, Language, Audio and Music Processing",
      "authors": [
        "Ziyang Ma",
        "Guanrou Yang",
        "Wenxi Chen",
        "Zhifu Gao",
        "Yexing Du",
        "Xiquan Li",
        "Zhisheng Zheng",
        "Haina Zhu",
        "Jianheng Zhuo",
        "Zheshu Song",
        "Ruiyang Xu",
        "Tiranrui Wang",
        "Yifan Yang",
        "Yanqiao Zhu",
        "Zhikang Niu",
        "Liumeng Xue",
        "Yinghao Ma",
        "Ruibin Yuan",
        "Shiliang Zhang",
        "Kai Yu",
        "Eng Siong Chng",
        "Xie Chen"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.MM"
      ],
      "abstract": "The recent surge in open-source Multimodal Large Language Models (MLLM) frameworks, such as LLaVA, provides a convenient kickoff for artificial intelligence developers and researchers. However, most of the MLLM frameworks take vision as the main input modality, and provide limited in-depth support for the modality of speech, audio, and music. This situation hinders the development of audio-language models, and forces researchers to spend a lot of effort on code writing and hyperparameter tuning. We present SLAM-LLM, an open-source deep learning framework designed to train customized MLLMs, focused on speech, language, audio, and music processing. SLAM-LLM provides a modular configuration of different encoders, projectors, LLMs, and parameter-efficient fine-tuning plugins. SLAM-LLM also includes detailed training and inference recipes for mainstream tasks, along with high-performance checkpoints like LLM-based Automatic Speech Recognition (ASR), Automated Audio Captioning (AAC), and Music Captioning (MC). Some of these recipes have already reached or are nearing state-of-the-art performance, and some relevant techniques have also been accepted by academic papers. We hope SLAM-LLM will accelerate iteration, development, data engineering, and model training for researchers. We are committed to continually pushing forward audio-based MLLMs through this open-source framework, and call on the community to contribute to the LLM-based speech, audio and music processing.",
      "url": "https://arxiv.org/abs/2601.09385",
      "pdfUrl": "https://arxiv.org/pdf/2601.09385.pdf",
      "titleJa": "SLAM-LLM: 音声、言語、オーディオ、音楽処理のためのモジュール式オープンソースマルチモーダル大規模言語モデルフレームワークとベストプラクティス"
    },
    {
      "id": "2601.09333",
      "arxivId": "2601.09333",
      "title": "Research on Piano Timbre Transformation System Based on Diffusion Model",
      "authors": [
        "Chun-Chieh Hsu",
        "Tsai-Ling Hsu",
        "Chen-Chen Yeh",
        "Shao-Chien Lu",
        "Cheng-Han Wu",
        "Bing-Ze Liu",
        "Timothy K. Shih",
        "Yu-Cheng Lin"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.MM"
      ],
      "abstract": "We propose a timbre conversion model based on the Diffusion architecture de-signed to precisely translate music played by various instruments into piano ver-sions. The model employs a Pitch Encoder and Loudness Encoder to extract pitch and loudness features of the music, which serve as conditional inputs to the Dif-fusion Model's decoder, generating high-quality piano timbres. Case analysis re-sults show that the model performs excellently in terms of pitch accuracy and timbral similarity, maintaining stable conversion across different musical styles (classical, jazz, pop) and lengths (from short clips to full pieces). Particularly, the model maintains high sound quality and accuracy even when dealing with rapidly changing notes and complex musical structures, demonstrating good generaliza-tion capability. Additionally, the model has the potential for real-time musical conversion and is suitable for live performances and digital music creation tools. Future research will focus on enhancing the handling of loudness dynamics and incorporating additional musical features (such as timbral variations and rhythmic complexity) to improve the model's adaptability and expressiveness. We plan to explore the model's application potential in other timbre conversion tasks, such as converting vocals to instrumental sounds or integration with MIDI digital pianos, further expanding the application scope of the Diffusion-based timbre conversion model in the field of music generation.",
      "url": "https://arxiv.org/abs/2601.09333",
      "pdfUrl": "https://arxiv.org/pdf/2601.09333.pdf",
      "titleJa": "拡散モデルに基づくピアノ音色変換システムの研究"
    },
    {
      "id": "2601.09239",
      "arxivId": "2601.09239",
      "title": "DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion",
      "authors": [
        "Hanlin Zhang",
        "Daxin Tan",
        "Dehua Tao",
        "Xiao Chen",
        "Haochen Tan",
        "Yunhe Li",
        "Yuchen Cao",
        "Jianping Wang",
        "Linqi Song"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Speech tokenizers serve as the cornerstone of discrete Speech Large Language Models (Speech LLMs). Existing tokenizers either prioritize semantic encoding, fuse semantic content with acoustic style inseparably, or achieve incomplete semantic-acoustic disentanglement. To achieve better disentanglement, we propose DSA-Tokenizer, which explicitly disentangles speech into discrete semantic and acoustic tokens via distinct optimization constraints. Specifically, semantic tokens are supervised by ASR to capture linguistic content, while acoustic tokens focus on mel-spectrograms restoration to encode style. To eliminate rigid length constraints between the two sequences, we introduce a hierarchical Flow-Matching decoder that further improve speech generation quality. Furthermore, We employ a joint reconstruction-recombination training strategy to enforce this separation. DSA-Tokenizer enables high fidelity reconstruction and flexible recombination through robust disentanglement, facilitating controllable generation in speech LLMs. Our analysis highlights disentangled tokenization as a pivotal paradigm for future speech modeling. Audio samples are avaialble at https://anonymous.4open.science/w/DSA_Tokenizer_demo/. The code and model will be made publicly available after the paper has been accepted.",
      "url": "https://arxiv.org/abs/2601.09239",
      "pdfUrl": "https://arxiv.org/pdf/2601.09239.pdf",
      "titleJa": "DSA-Tokenizer: フローマッチングに基づく階層的融合による意味音響分離トークン化"
    },
    {
      "id": "2601.08764",
      "arxivId": "2601.08764",
      "title": "FusID: Modality-Fused Semantic IDs for Generative Music Recommendation",
      "authors": [
        "Haven Kim",
        "Yupeng Hou",
        "Julian McAuley"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.IR",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Generative recommendation systems have achieved significant advances by leveraging semantic IDs to represent items. However, existing approaches that tokenize each modality independently face two critical limitations: (1) redundancy across modalities that reduces efficiency, and (2) failure to capture inter-modal interactions that limits item representation. We introduce FusID, a modality-fused semantic ID framework that addresses these limitations through three key components: (i) multimodal fusion that learns unified representations by jointly encoding information across modalities, (ii) representation learning that brings frequently co-occurring item embeddings closer while maintaining distinctiveness and preventing feature redundancy, and (iii) product quantization that converts the fused continuous embeddings into multiple discrete tokens to mitigate ID conflict. Evaluated on a multimodal next-song recommendation (i.e., playlist continuation) benchmark, FusID achieves zero ID conflicts, ensuring that each token sequence maps to exactly one song, mitigates codebook underutilization, and outperforms baselines in terms of MRR and Recall@k (k = 1, 5, 10, 20).",
      "url": "https://arxiv.org/abs/2601.08764",
      "pdfUrl": "https://arxiv.org/pdf/2601.08764.pdf",
      "titleJa": "FusID: 生成的音楽推薦のためのモダリティ融合セマンティックID"
    },
    {
      "id": "2601.08516",
      "arxivId": "2601.08516",
      "title": "Robust CAPTCHA Using Audio Illusions in the Era of Large Language Models: from Evaluation to Advances",
      "authors": [
        "Ziqi Ding",
        "Yunfeng Wan",
        "Wei Song",
        "Yi Liu",
        "Gelei Deng",
        "Nan Sun",
        "Huadong Mo",
        "Jingling Xue",
        "Shidong Pan",
        "Yuekang Li"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.SD",
        "cs.CY",
        "eess.AS"
      ],
      "abstract": "CAPTCHAs are widely used by websites to block bots and spam by presenting challenges that are easy for humans but difficult for automated programs to solve. To improve accessibility, audio CAPTCHAs are designed to complement visual ones. However, the robustness of audio CAPTCHAs against advanced Large Audio Language Models (LALMs) and Automatic Speech Recognition (ASR) models remains unclear. In this paper, we introduce AI-CAPTCHA, a unified framework that offers (i) an evaluation framework, ACEval, which includes advanced LALM- and ASR-based solvers, and (ii) a novel audio CAPTCHA approach, IllusionAudio, leveraging audio illusions. Through extensive evaluations of seven widely deployed audio CAPTCHAs, we show that most existing methods can be solved with high success rates by advanced LALMs and ASR models, exposing critical security weaknesses. To address these vulnerabilities, we design a new audio CAPTCHA approach, IllusionAudio, which exploits perceptual illusion cues rooted in human auditory mechanisms. Extensive experiments demonstrate that our method defeats all tested LALM- and ASR-based attacks while achieving a 100% human pass rate, significantly outperforming existing audio CAPTCHA methods.",
      "url": "https://arxiv.org/abs/2601.08516",
      "pdfUrl": "https://arxiv.org/pdf/2601.08516.pdf",
      "titleJa": "大規模言語モデルの時代における音響錯覚を利用した堅牢なCAPTCHA：評価から発展まで"
    },
    {
      "id": "2601.08450",
      "arxivId": "2601.08450",
      "title": "Decoding Order Matters in Autoregressive Speech Synthesis",
      "authors": [
        "Minghui Zhao",
        "Anton Ragni"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Autoregressive speech synthesis often adopts a left-to-right order, yet generation order is a modelling choice. We investigate decoding order through masked diffusion framework, which progressively unmasks positions and allows arbitrary decoding orders during training and inference. By interpolating between identity and random permutations, we show that randomness in decoding order affects speech quality. We further compare fixed strategies, such as \\texttt{l2r} and \\texttt{r2l} with adaptive ones, such as Top-$K$, finding that fixed-order decoding, including the dominating left-to-right approach, is suboptimal, while adaptive decoding yields better performance. Finally, since masked diffusion requires discrete inputs, we quantise acoustic representations and find that even 1-bit quantisation can support reasonably high-quality speech.",
      "url": "https://arxiv.org/abs/2601.08450",
      "pdfUrl": "https://arxiv.org/pdf/2601.08450.pdf",
      "titleJa": "自己回帰音声合成におけるデコード順序の重要性"
    },
    {
      "id": "2601.08358",
      "arxivId": "2601.08358",
      "title": "Decodable but not structured: linear probing enables Underwater Acoustic Target Recognition with pretrained audio embeddings",
      "authors": [
        "Hilde I. Hummel",
        "Sandjai Bhulai",
        "Rob D. van der Mei",
        "Burooj Ghani"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Increasing levels of anthropogenic noise from ships contribute significantly to underwater sound pollution, posing risks to marine ecosystems. This makes monitoring crucial to understand and quantify the impact of the ship radiated noise. Passive Acoustic Monitoring (PAM) systems are widely deployed for this purpose, generating years of underwater recordings across diverse soundscapes. Manual analysis of such large-scale data is impractical, motivating the need for automated approaches based on machine learning. Recent advances in automatic Underwater Acoustic Target Recognition (UATR) have largely relied on supervised learning, which is constrained by the scarcity of labeled data. Transfer Learning (TL) offers a promising alternative to mitigate this limitation. In this work, we conduct the first empirical comparative study of transfer learning for UATR, evaluating multiple pretrained audio models originating from diverse audio domains. The pretrained model weights are frozen, and the resulting embeddings are analyzed through classification, clustering, and similarity-based evaluations. The analysis shows that the geometrical structure of the embedding space is largely dominated by recording-specific characteristics. However, a simple linear probe can effectively suppress this recording-specific information and isolate ship-type features from these embeddings. As a result, linear probing enables effective automatic UATR using pretrained audio models at low computational cost, significantly reducing the need for a large amounts of high-quality labeled ship recordings.",
      "url": "https://arxiv.org/abs/2601.08358",
      "pdfUrl": "https://arxiv.org/pdf/2601.08358.pdf",
      "titleJa": "デコード可能だが構造化されていない：線形プローブにより、事前学習済みのオーディオ埋め込みによる水中音響ターゲット認識が可能になる"
    },
    {
      "id": "2601.08074",
      "arxivId": "2601.08074",
      "title": "Elastic overtones: an equal temperament 12 tone music system with \"perfect\" fifths",
      "authors": [
        "X. Hernandez",
        "Luis Nasser",
        "Pablo Garcia-Valenzuela"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "physics.soc-ph",
        "cs.SD",
        "eess.AS",
        "physics.pop-ph"
      ],
      "abstract": "The impossibility of a transposable 12 semitone tuning of the octave arises from the mathematical fact that $2 \\times 2^{7/12} \\neq 3$ i.e., the second harmonic of the fifth can not exactly match the third harmonic of the fundamental. This in turn, stems from the whole number harmonic structure of western music, and the subsequent fundamental character of the octave interval as multiples of 2 in frequency, a property inherited by our music system from the physics of instruments with vibrating elements being to a good approximation one dimensional. In the current era of electronic music, one can relax the above assumptions to construct an analogous music system where all the structural properties of the standard music system are preserved, but where harmonics are not whole number multiples of the fundamental frequency, and the octave is no longer a factor of 2 in frequency. This now allows to construct a transposable 12 semitone music system where the second harmonic of the fifth exactly matches the third harmonic of the fundamental. The enhanced harmonic qualities of this system recover to a good approximation the musical qualities of Just Intonation, whilst retaining by construction all the versatility and modulating ability of 12TET.",
      "url": "https://arxiv.org/abs/2601.08074",
      "pdfUrl": "https://arxiv.org/pdf/2601.08074.pdf",
      "titleJa": "弾性倍音: 完全五度を含む平均律12音音楽システム"
    },
    {
      "id": "2601.07999",
      "arxivId": "2601.07999",
      "title": "VoxCog: Towards End-to-End Multilingual Cognitive Impairment Classification through Dialectal Knowledge",
      "authors": [
        "Tiantian Feng",
        "Anfeng Xu",
        "Jinkook Lee",
        "Shrikanth Narayanan"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "In this work, we present a novel perspective on cognitive impairment classification from speech by integrating speech foundation models that explicitly recognize speech dialects. Our motivation is based on the observation that individuals with Alzheimer's Disease (AD) or mild cognitive impairment (MCI) often produce measurable speech characteristics, such as slower articulation rate and lengthened sounds, in a manner similar to dialectal phonetic variations seen in speech. Building on this idea, we introduce VoxCog, an end-to-end framework that uses pre-trained dialect models to detect AD or MCI without relying on additional modalities such as text or images. Through experiments on multiple multilingual datasets for AD and MCI detection, we demonstrate that model initialization with a dialect classifier on top of speech foundation models consistently improves the predictive performance of AD or MCI. Our trained models yield similar or often better performance compared to previous approaches that ensembled several computational methods using different signal modalities. Particularly, our end-to-end speech-based model achieves 87.5% and 85.9% accuracy on the ADReSS 2020 challenge and ADReSSo 2021 challenge test sets, outperforming existing solutions that use multimodal ensemble-based computation or LLMs.",
      "url": "https://arxiv.org/abs/2601.07999",
      "pdfUrl": "https://arxiv.org/pdf/2601.07999.pdf",
      "titleJa": "VoxCog: 方言知識によるエンドツーエンドの多言語認知障害分類に向けて"
    },
    {
      "id": "2601.10629",
      "arxivId": "2601.10629",
      "title": "VoiceSculptor: Your Voice, Designed By You",
      "authors": [
        "Jingbin Hu",
        "Huakang Chen",
        "Linhan Ma",
        "Dake Guo",
        "Qirui Zhan",
        "Wenhao Li",
        "Haoyu Zhang",
        "Kangxiang Xia",
        "Ziyu Zhang",
        "Wenjie Tian",
        "Chengyou Wang",
        "Jinrui Liang",
        "Shuhan Guo",
        "Zihang Yang",
        "Bengu Wu",
        "Binbin Zhang",
        "Pengcheng Zhu",
        "Pengyuan Xie",
        "Chuan Xie",
        "Qiang Zhang",
        "Jie Liu",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Despite rapid progress in text-to-speech (TTS), open-source systems still lack truly instruction-following, fine-grained control over core speech attributes (e.g., pitch, speaking rate, age, emotion, and style). We present VoiceSculptor, an open-source unified system that bridges this gap by integrating instruction-based voice design and high-fidelity voice cloning in a single framework. It generates controllable speaker timbre directly from natural-language descriptions, supports iterative refinement via Retrieval-Augmented Generation (RAG), and provides attribute-level edits across multiple dimensions. The designed voice is then rendered into a prompt waveform and fed into a cloning model to enable high-fidelity timbre transfer for downstream speech synthesis. VoiceSculptor achieves open-source state-of-the-art (SOTA) on InstructTTSEval-Zh, and is fully open-sourced, including code and pretrained models, to advance reproducible instruction-controlled TTS research.",
      "url": "https://arxiv.org/abs/2601.10629",
      "pdfUrl": "https://arxiv.org/pdf/2601.10629.pdf",
      "titleJa": "VoiceSculptor: あなたの声を、あなたがデザインする"
    },
    {
      "id": "2601.10078",
      "arxivId": "2601.10078",
      "title": "Nearest Kronecker Product Decomposition Based Subband Adaptive Filter: Algorithms and Applications",
      "authors": [
        "Jianhong Ye",
        "Haiquan Zhao"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "eess.AS",
        "cs.IT"
      ],
      "abstract": "Recently, the nearest Kronecker product (NKP) decomposition-based normalized least mean square (NLMS-NKP) algorithm has demonstrated superior convergence performance compared to the conventional NLMS algorithm. However, its convergence rate exhibits significant degradation when processing highly correlated input signals. To address this problem, we propose a type-I NKP-based normalized subband adaptive filter (NSAF) algorithm, namely NSAF-NKP-I. Nevertheless, this algorithm incurs substantially higher computational overhead than the NLMS-NKP algorithm. Remarkably, our enhanced type-II NKP-based NSAF (NSAF-NKP-II) algorithm achieves equivalent convergence performance while substantially reducing computational complexity. Furthermore, to enhance robustness against impulsive noise interference, we develop two robust variants: the maximum correntropy criterion-based robust NSAF-NKP (RNSAF-NKP-MCC) and logarithmic criterion-based robust NSAF-NKP (RNSAF-NKP-LC) algorithms. Additionally, detailed analyses of computational complexity, step-size range, and theoretical steady-state performance are provided for theproposed algorithms. To enhance the practicability of the NSAF-NKP-II algorithm in complex nonlinear environments, we further devise two nonlinear implementations: the trigonometric functional link network-based NKP-NSAF (TFLN-NSAF-NKP) and Volterra series expansion-based NKP-NSAF (Volterra-NKP-NSAF) algorithms. In active noise control (ANC) systems, we further propose the filtered-x NSAF-NKP-II (NKP-FxNSAF) algorithm. Simulation experiments in echo cancellation, sparse system identification, nonlinear processing, and ANC scenarios are conducted to validate the superiority of the proposed algorithms over existing state-of-the-art counterparts.",
      "url": "https://arxiv.org/abs/2601.10078",
      "pdfUrl": "https://arxiv.org/pdf/2601.10078.pdf",
      "titleJa": "最近傍クロネッカー積分解に基づくサブバンド適応フィルタ：アルゴリズムと応用"
    },
    {
      "id": "2601.08537",
      "arxivId": "2601.08537",
      "title": "Weakly Supervised Tabla Stroke Transcription via TI-SDRM: A Rhythm-Aware Lattice Rescoring Framework",
      "authors": [
        "Rahul Bapusaheb Kodag",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Tabla Stroke Transcription (TST) is central to the analysis of rhythmic structure in Hindustani classical music, yet remains challenging due to complex rhythmic organization and the scarcity of strongly annotated data. Existing approaches largely rely on fully supervised learning with onset-level annotations, which are costly and impractical at scale. This work addresses TST in a weakly supervised setting, using only symbolic stroke sequences without temporal alignment. We propose a framework that combines a CTC-based acoustic model with sequence-level rhythmic rescoring. The acoustic model produces a decoding lattice, which is refined using a \\textbf{$T\\bar{a}la$}-Independent Static--Dynamic Rhythmic Model (TI-SDRM) that integrates long-term rhythmic structure with short-term adaptive dynamics through an adaptive interpolation mechanism. We curate a new real-world tabla solo dataset and a complementary synthetic dataset, establishing the first benchmark for weakly supervised TST in Hindustani classical music. Experiments demonstrate consistent and substantial reductions in stroke error rate over acoustic-only decoding, confirming the importance of explicit rhythmic structure for accurate transcription.",
      "url": "https://arxiv.org/abs/2601.08537",
      "pdfUrl": "https://arxiv.org/pdf/2601.08537.pdf",
      "titleJa": "TI-SDRMによる弱教師付きタブラストローク転写：リズムを考慮したラティス再採点フレームワーク"
    },
    {
      "id": "2601.08480",
      "arxivId": "2601.08480",
      "title": "Quantitative Analysis of Proxy Tasks for Anomalous Sound Detection",
      "authors": [
        "Seunghyeon Shin",
        "Seokjin Lee"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Anomalous sound detection (ASD) typically involves self-supervised proxy tasks to learn feature representations from normal sound data, owing to the scarcity of anomalous samples. In ASD research, proxy tasks such as AutoEncoders operate under the explicit assumption that models trained on normal data will increase the reconstruction errors related to anomalies. A natural extension suggests that improved proxy task performance should improve ASD capability; however, this relationship has received little systematic attention. This study addresses this research gap by quantitatively analyzing the relationship between proxy task metrics and ASD performance across five configurations, namely, AutoEncoders, classification, source separation, contrastive learning, and pre-trained models. We evaluate the learned representations using linear probe (linear separability) and Mahalanobis distance (distributional compactness). Our experiments reveal that strong proxy performance does not necessarily improve anomalous sound detection performance. Specifically, classification tasks experience performance saturation owing to insufficient task difficulty, whereas contrastive learning fails to learn meaningful features owing to limited data diversity. Notably, source separation is the only task demonstrating a strong positive correlation, such that improved separation consistently improves anomaly detection. Based on these findings, we highlight the critical importance of task difficulty and objective alignment. Finally, we propose a three-stage alignment verification protocol to guide the design of highly effective proxy tasks for ASD systems.",
      "url": "https://arxiv.org/abs/2601.08480",
      "pdfUrl": "https://arxiv.org/pdf/2601.08480.pdf",
      "titleJa": "異常音検知のための代理タスクの定量分析"
    },
    {
      "id": "2601.07969",
      "arxivId": "2601.07969",
      "title": "Tuberculosis Screening from Cough Audio: Baseline Models, Clinical Variables, and Uncertainty Quantification",
      "authors": [
        "George P. Kafentzis",
        "Efstratios Selisios"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "In this paper, we propose a standardized framework for automatic tuberculosis (TB) detection from cough audio and routinely collected clinical data using machine learning. While TB screening from audio has attracted growing interest, progress is difficult to measure because existing studies vary substantially in datasets, cohort definitions, feature representations, model families, validation protocols, and reported metrics. Consequently, reported gains are often not directly comparable, and it remains unclear whether improvements stem from modeling advances or from differences in data and evaluation. We address this gap by establishing a strong, well-documented baseline for TB prediction using cough recordings and accompanying clinical metadata from a recently compiled dataset from several countries. Our pipeline is reproducible end-to-end, covering feature extraction, multimodal fusion, cougher-independent evaluation, and uncertainty quantification, and it reports a consistent suite of clinically relevant metrics to enable fair comparison. We further quantify performance for cough audio-only and fused (audio + clinical metadata) models, and release the full experimental protocol to facilitate benchmarking. This baseline is intended to serve as a common reference point and to reduce methodological variance that currently holds back progress in the field.",
      "url": "https://arxiv.org/abs/2601.07969",
      "pdfUrl": "https://arxiv.org/pdf/2601.07969.pdf",
      "titleJa": "咳嗽音による結核スクリーニング：ベースラインモデル、臨床変数、不確実性の定量化"
    },
    {
      "id": "2601.07958",
      "arxivId": "2601.07958",
      "title": "LJ-Spoof: A Generatively Varied Corpus for Audio Anti-Spoofing and Synthesis Source Tracing",
      "authors": [
        "Surya Subramani",
        "Hashim Ali",
        "Hafiz Malik"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Speaker-specific anti-spoofing and synthesis-source tracing are central challenges in audio anti-spoofing. Progress has been hampered by the lack of datasets that systematically vary model architectures, synthesis pipelines, and generative parameters. To address this gap, we introduce LJ-Spoof, a speaker-specific, generatively diverse corpus that systematically varies prosody, vocoders, generative hyperparameters, bona fide prompt sources, training regimes, and neural post-processing. The corpus spans one speakers-including studio-quality recordings-30 TTS families, 500 generatively variant subsets, 10 bona fide neural-processing variants, and more than 3 million utterances. This variation-dense design enables robust speaker-conditioned anti-spoofing and fine-grained synthesis-source tracing. We further position this dataset as both a practical reference training resource and a benchmark evaluation suite for anti-spoofing and source tracing.",
      "url": "https://arxiv.org/abs/2601.07958",
      "pdfUrl": "https://arxiv.org/pdf/2601.07958.pdf",
      "titleJa": "LJ-Spoof: 音声スプーフィング防止と合成音源追跡のための生成的に多様なコーパス"
    },
    {
      "id": "2601.08879",
      "arxivId": "2601.08879",
      "title": "Echoes of Ideology: Toward an Audio Analysis Pipeline to Unveil Character Traits in Historical Nazi Propaganda Films",
      "authors": [
        "Nicolas Ruth",
        "Manuel Burghardt"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This study investigates the use of computational audio analysis to examine ideological narratives in Nazi propaganda films. Employing a three-step pipeline, speaker diarization, audio transcription and psycholinguistic analysis, it reveals ideological patterns in characters. Despite current issues with speaker diarization, the methodology provides insights into character traits and propaganda narratives, suggesting scalable applications.",
      "url": "https://arxiv.org/abs/2601.08879",
      "pdfUrl": "https://arxiv.org/pdf/2601.08879.pdf",
      "titleJa": "イデオロギーの響き：ナチスの歴史的プロパガンダ映画の登場人物の特徴を明らかにする音声分析パイプラインの構築に向けて"
    },
    {
      "id": "2601.07481",
      "arxivId": "2601.07481",
      "title": "Directional reflection modeling via wavenumber-domain reflection coefficient for 3D acoustic field simulation",
      "authors": [
        "Satoshi Hoshika",
        "Takahiro Iwami",
        "Akira Omoto"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This study proposes a framework for incorporating wavenumber-domain acoustic reflection coefficients into sound field analysis to characterize direction-dependent material reflection and scattering phenomena. The reflection coefficient is defined as the amplitude ratio between incident and reflected waves for each propagation direction and is estimated from spatial Fourier transforms of the incident and reflected sound fields. The resulting wavenumber-domain reflection coefficients are converted into an acoustic admittance representation that is directly compatible with numerical methods such as the Boundary Element Method (BEM), enabling simulation of reflections beyond simple specular components. Unlike conventional extended reaction models, the proposed approach avoids explicit modeling of the material interior. This significantly reduces computational cost while allowing direct use of measured data, empirical models, or user-defined directional reflection characteristics. The validity of the proposed formulation was previously demonstrated by the authors through two-dimensional sound field simulations, in which accurate reproduction of direction-dependent reflection behavior was confirmed. In the present work, the framework is extended to three-dimensional analysis, demonstrating its applicability to more realistic and complex acoustic environments. The proposed approach provides a practical and flexible tool for simulating direction-dependent acoustic reflections and scattering, with potential applications in architectural acoustics, material characterization, and noise control.",
      "url": "https://arxiv.org/abs/2601.07481",
      "pdfUrl": "https://arxiv.org/pdf/2601.07481.pdf",
      "titleJa": "3次元音響場シミュレーションのための波数領域反射係数による方向反射モデリング"
    },
    {
      "id": "2601.07237",
      "arxivId": "2601.07237",
      "title": "The ICASSP 2026 Automatic Song Aesthetics Evaluation Challenge",
      "authors": [
        "Guobin Ma",
        "Yuxuan Xia",
        "Jixun Yao",
        "Huixin Xue",
        "Hexin Liu",
        "Shuai Wang",
        "Hao Liu",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "This paper summarizes the ICASSP 2026 Automatic Song Aesthetics Evaluation (ASAE) Challenge, which focuses on predicting the subjective aesthetic scores of AI-generated songs. The challenge consists of two tracks: Track 1 targets the prediction of the overall musicality score, while Track 2 focuses on predicting five fine-grained aesthetic scores. The challenge attracted strong interest from the research community and received numerous submissions from both academia and industry. Top-performing systems significantly surpassed the official baseline, demonstrating substantial progress in aligning objective metrics with human aesthetic preferences. The outcomes establish a standardized benchmark and advance human-aligned evaluation methodologies for modern music generation systems.",
      "url": "https://arxiv.org/abs/2601.07237",
      "pdfUrl": "https://arxiv.org/pdf/2601.07237.pdf",
      "titleJa": "ICASSP 2026 自動歌曲美学評価チャレンジ"
    },
    {
      "id": "2601.08871",
      "arxivId": "2601.08871",
      "title": "Semantic visually-guided acoustic highlighting with large vision-language models",
      "authors": [
        "Junhua Huang",
        "Chao Huang",
        "Chenliang Xu"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Balancing dialogue, music, and sound effects with accompanying video is crucial for immersive storytelling, yet current audio mixing workflows remain largely manual and labor-intensive. While recent advancements have introduced the visually guided acoustic highlighting task, which implicitly rebalances audio sources using multimodal guidance, it remains unclear which visual aspects are most effective as conditioning signals.We address this gap through a systematic study of whether deep video understanding improves audio remixing. Using textual descriptions as a proxy for visual analysis, we prompt large vision-language models to extract six types of visual-semantic aspects, including object and character appearance, emotion, camera focus, tone, scene background, and inferred sound-related cues. Through extensive experiments, camera focus, tone, and scene background consistently yield the largest improvements in perceptual mix quality over state-of-the-art baselines. Our findings (i) identify which visual-semantic cues most strongly support coherent and visually aligned audio remixing, and (ii) outline a practical path toward automating cinema-grade sound design using lightweight guidance derived from large vision-language models.",
      "url": "https://arxiv.org/abs/2601.08871",
      "pdfUrl": "https://arxiv.org/pdf/2601.08871.pdf",
      "titleJa": "大規模視覚言語モデルを用いた意味的視覚誘導音響強調表示"
    },
    {
      "id": "2601.07064",
      "arxivId": "2601.07064",
      "title": "Bridging Attribution and Open-Set Detection using Graph-Augmented Instance Learning in Synthetic Speech",
      "authors": [
        "Mohd Mujtaba Akhtar",
        " Girish",
        "Farhan Sheth",
        "Muskaan Singh"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We propose a unified framework for not only attributing synthetic speech to its source but also for detecting speech generated by synthesizers that were not encountered during training. This requires methods that move beyond simple detection to support both detailed forensic analysis and open-set generalization. To address this, we introduce SIGNAL, a hybrid framework that combines speech foundation models (SFMs) with graph-based modeling and open-set-aware inference. Our framework integrates Graph Neural Networks (GNNs) and a k-Nearest Neighbor (KNN) classifier, allowing it to capture meaningful relationships between utterances and recognize speech that doesn`t belong to any known generator. It constructs a query-conditioned graph over generator class prototypes, enabling the GNN to reason over relationships among candidate generators, while the KNN branch supports open-set detection via confidence-based thresholding. We evaluate SIGNAL using the DiffSSD dataset, which offers a diverse mix of real speech and synthetic audio from both open-source and commercial diffusion-based TTS systems. To further assess generalization, we also test on the SingFake benchmark. Our results show that SIGNAL consistently improves performance across both tasks, with Mamba-based embeddings delivering especially strong results. To the best of our knowledge, this is the first study to unify graph-based learning and open-set detection for tracing synthetic speech back to its origin.",
      "url": "https://arxiv.org/abs/2601.07064",
      "pdfUrl": "https://arxiv.org/pdf/2601.07064.pdf",
      "titleJa": "合成音声におけるグラフ拡張インスタンス学習を用いた帰属とオープンセット検出の橋渡し"
    },
    {
      "id": "2601.07014",
      "arxivId": "2601.07014",
      "title": "DIVINE: Coordinating Multimodal Disentangled Representations for Oro-Facial Neurological Disorder Assessment",
      "authors": [
        "Mohd Mujtaba Akhtar",
        " Girish",
        "Muskaan Singh"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "In this study, we present a multimodal framework for predicting neuro-facial disorders by capturing both vocal and facial cues. We hypothesize that explicitly disentangling shared and modality-specific representations within multimodal foundation model embeddings can enhance clinical interpretability and generalization. To validate this hypothesis, we propose DIVINE a fully disentangled multimodal framework that operates on representations extracted from state-of-the-art (SOTA) audio and video foundation models, incorporating hierarchical variational bottlenecks, sparse gated fusion, and learnable symptom tokens. DIVINE operates in a multitask learning setup to jointly predict diagnostic categories (Healthy Control,ALS, Stroke) and severity levels (Mild, Moderate, Severe). The model is trained using synchronized audio and video inputs and evaluated on the Toronto NeuroFace dataset under full (audio-video) as well as single-modality (audio-only and video-only) test conditions. Our proposed approach, DIVINE achieves SOTA result, with the DeepSeek-VL2 and TRILLsson combination reaching 98.26% accuracy and 97.51% F1-score. Under modality-constrained scenarios, the framework performs well, showing strong generalization when tested with video-only or audio-only inputs. It consistently yields superior performance compared to unimodal models and baseline fusion techniques. To the best of our knowledge, DIVINE is the first framework that combines cross-modal disentanglement, adaptive fusion, and multitask learning to comprehensively assess neurological disorders using synchronized speech and facial video.",
      "url": "https://arxiv.org/abs/2601.07014",
      "pdfUrl": "https://arxiv.org/pdf/2601.07014.pdf",
      "titleJa": "DIVINE: 口腔顔面神経疾患評価のためのマルチモーダル分離表現の調整"
    },
    {
      "id": "2601.10712",
      "arxivId": "2601.10712",
      "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
      "authors": [
        "Changle Qu",
        "Sunhao Dai",
        "Hengyi Cai",
        "Jun Xu",
        "Shuaiqiang Wang",
        "Dawei Yin"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.",
      "url": "https://arxiv.org/abs/2601.10712",
      "pdfUrl": "https://arxiv.org/pdf/2601.10712.pdf",
      "titleJa": "MatchTIR: 二部マッチングによるツール統合推論のための細粒度監督"
    },
    {
      "id": "2601.10702",
      "arxivId": "2601.10702",
      "title": "Grounding Agent Memory in Contextual Intent",
      "authors": [
        "Ruozhen Yang",
        "Yucheng Jiang",
        "Yueqi Jiang",
        "Priyanka Kargupta",
        "Yunyi Zhang",
        "Jiawei Han"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "abstract": "Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step's intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history. For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.",
      "url": "https://arxiv.org/abs/2601.10702",
      "pdfUrl": "https://arxiv.org/pdf/2601.10702.pdf",
      "titleJa": "文脈的意図におけるエージェントの記憶のグラウンディング"
    },
    {
      "id": "2601.10700",
      "arxivId": "2601.10700",
      "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
      "authors": [
        "Gilat Toker",
        "Nitay Calderon",
        "Ohad Amosy",
        "Roi Reichart"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.",
      "url": "https://arxiv.org/abs/2601.10700",
      "pdfUrl": "https://arxiv.org/pdf/2601.10700.pdf",
      "titleJa": "LIBERTy: 構造的反事実を用いたLLMの概念ベース説明のベンチマークのための因果フレームワーク"
    },
    {
      "id": "2601.10696",
      "arxivId": "2601.10696",
      "title": "The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load",
      "authors": [
        "Han Jiang",
        "Yao Xiao",
        "Rachel Hurley",
        "Shichao Liu"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.",
      "url": "https://arxiv.org/abs/2601.10696",
      "pdfUrl": "https://arxiv.org/pdf/2601.10696.pdf",
      "titleJa": "生成AIが建築概念設計に与える影響：パフォーマンス、創造的自己効力感、認知負荷"
    },
    {
      "id": "2601.10684",
      "arxivId": "2601.10684",
      "title": "On the origin of neural scaling laws: from random graphs to natural language",
      "authors": [
        "Maissam Barkeshli",
        "Alberto Alfarano",
        "Andrey Gromov"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "stat.ML"
      ],
      "abstract": "Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erdös-Renyi and scale-free Barabási-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.",
      "url": "https://arxiv.org/abs/2601.10684",
      "pdfUrl": "https://arxiv.org/pdf/2601.10684.pdf",
      "titleJa": "神経スケーリング則の起源：ランダムグラフから自然言語まで"
    },
    {
      "id": "2601.10681",
      "arxivId": "2601.10681",
      "title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems",
      "authors": [
        "Amir Khurshid",
        "Abhishek Sehgal"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.",
      "url": "https://arxiv.org/abs/2601.10681",
      "pdfUrl": "https://arxiv.org/pdf/2601.10681.pdf",
      "titleJa": "エンタープライズ検索拡張システムのための構造と多様性を考慮したコンテキストバブル構築"
    },
    {
      "id": "2601.10679",
      "arxivId": "2601.10679",
      "title": "Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models",
      "authors": [
        "Zirui Ren",
        "Ziming Liu"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) \"Grokking\" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM \"guesses\" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be \"guessing\" instead of \"reasoning\". Leveraging this \"guessing\" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models \"reason\".",
      "url": "https://arxiv.org/abs/2601.10679",
      "pdfUrl": "https://arxiv.org/pdf/2601.10679.pdf",
      "titleJa": "あなたの推論モデルは推論しているのか、推測しているのか？階層的推論モデルのメカニズム分析"
    },
    {
      "id": "2601.10651",
      "arxivId": "2601.10651",
      "title": "Multi-Property Synthesis",
      "authors": [
        "Christoph Weinhuber",
        "Yannik Schnitzer",
        "Alessandro Abate",
        "David Parker",
        "Giuseppe De Giacomo",
        "Moshe Y. Vardi"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "abstract": "We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.",
      "url": "https://arxiv.org/abs/2601.10651",
      "pdfUrl": "https://arxiv.org/pdf/2601.10651.pdf",
      "titleJa": "多特性合成"
    },
    {
      "id": "2601.10611",
      "arxivId": "2601.10611",
      "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
      "authors": [
        "Christopher Clark",
        "Jieyu Zhang",
        "Zixian Ma",
        "Jae Sung Park",
        "Mohammadreza Salehi",
        "Rohun Tripathi",
        "Sangho Lee",
        "Zhongzheng Ren",
        "Chris Dongjoo Kim",
        "Yinuo Yang",
        "Vincent Shao",
        "Yue Yang",
        "Weikai Huang",
        "Ziqi Gao",
        "Taira Anderson",
        "Jianrui Zhang",
        "Jitesh Jain",
        "George Stoica",
        "Winson Han",
        "Ali Farhadi",
        "Ranjay Krishna"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).",
      "url": "https://arxiv.org/abs/2601.10611",
      "pdfUrl": "https://arxiv.org/pdf/2601.10611.pdf",
      "titleJa": "Molmo2: ビデオ理解とグラウンディングを備えた視覚言語モデルのためのオープンな重みとデータ"
    },
    {
      "id": "2601.10600",
      "arxivId": "2601.10600",
      "title": "Procedural Fairness in Multi-Agent Bandits",
      "authors": [
        "Joshua Caiata",
        "Carter Blair",
        "Kate Larson"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "abstract": "In the context of multi-agent multi-armed bandits (MA-MAB), fairness is often reduced to outcomes: maximizing welfare, reducing inequality, or balancing utilities. However, evidence in psychology, economics, and Rawlsian theory suggests that fairness is also about process and who gets a say in the decisions being made. We introduce a new fairness objective, procedural fairness, which provides equal decision-making power for all agents, lies in the core, and provides for proportionality in outcomes. Empirical results confirm that fairness notions based on optimizing for outcomes sacrifice equal voice and representation, while the sacrifice in outcome-based fairness objectives (like equality and utilitarianism) is minimal under procedurally fair policies. We further prove that different fairness notions prioritize fundamentally different and incompatible values, highlighting that fairness requires explicit normative choices. This paper argues that procedural legitimacy deserves greater focus as a fairness objective, and provides a framework for putting procedural fairness into practice.",
      "url": "https://arxiv.org/abs/2601.10600",
      "pdfUrl": "https://arxiv.org/pdf/2601.10600.pdf",
      "titleJa": "マルチエージェントバンディットにおける手続き的公平性"
    },
    {
      "id": "2601.10591",
      "arxivId": "2601.10591",
      "title": "ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition",
      "authors": [
        "Arundeep Chinta",
        "Lucas Vinh Tran",
        "Jay Katukuri"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.RM",
        "q-fin.TR"
      ],
      "abstract": "Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.",
      "url": "https://arxiv.org/abs/2601.10591",
      "pdfUrl": "https://arxiv.org/pdf/2601.10591.pdf",
      "titleJa": "ProbFM: 不確実性分解を伴う確率的時系列基礎モデル"
    },
    {
      "id": "2601.10587",
      "arxivId": "2601.10587",
      "title": "Adversarial Evasion Attacks on Computer Vision using SHAP Values",
      "authors": [
        "Frank Mollard",
        "Marcus Becker",
        "Florian Roehrbein"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "The paper introduces a white-box attack on computer vision models using SHAP values. It demonstrates how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications. Such attacks are particularly insidious as they can deceive the perception of an algorithm while eluding human perception due to their imperceptibility to the human eye. The proposed attack leverages SHAP values to quantify the significance of individual inputs to the output at the inference stage. A comparison is drawn between the SHAP attack and the well-known Fast Gradient Sign Method. We find evidence that SHAP attacks are more robust in generating misclassifications particularly in gradient hiding scenarios.",
      "url": "https://arxiv.org/abs/2601.10587",
      "pdfUrl": "https://arxiv.org/pdf/2601.10587.pdf",
      "titleJa": "SHAP値を用いたコンピュータビジョンに対する敵対的回避攻撃"
    },
    {
      "id": "2601.10581",
      "arxivId": "2601.10581",
      "title": "From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA",
      "authors": [
        "Kimia Abedini",
        "Farzad Shami",
        "Gianmaria Silvello"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "abstract": "Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.",
      "url": "https://arxiv.org/abs/2601.10581",
      "pdfUrl": "https://arxiv.org/pdf/2601.10581.pdf",
      "titleJa": "シングルエージェントからマルチエージェント推論へ：ゲノムQAのためのGeneGPTの進化"
    },
    {
      "id": "2601.10567",
      "arxivId": "2601.10567",
      "title": "Generative AI collective behavior needs an interactionist paradigm",
      "authors": [
        "Laura Ferrarotti",
        "Gian Maria Campedelli",
        "Roberto Dessì",
        "Andrea Baronchelli",
        "Giovanni Iacca",
        "Kathleen M. Carley",
        "Alex Pentland",
        "Joel Z. Leibo",
        "James Evans",
        "Bruno Lepri"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ],
      "abstract": "In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.",
      "url": "https://arxiv.org/abs/2601.10567",
      "pdfUrl": "https://arxiv.org/pdf/2601.10567.pdf",
      "titleJa": "生成AIの集団行動には相互作用主義パラダイムが必要"
    },
    {
      "id": "2601.10562",
      "arxivId": "2601.10562",
      "title": "Process-Guided Concept Bottleneck Model",
      "authors": [
        "Reza M. Asiyabi",
        "SEOSAW Partnership",
        "Steven Hancock",
        "Casey Ryan"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "abstract": "Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.",
      "url": "https://arxiv.org/abs/2601.10562",
      "pdfUrl": "https://arxiv.org/pdf/2601.10562.pdf",
      "titleJa": "プロセス誘導コンセプトボトルネックモデル"
    },
    {
      "id": "2601.10560",
      "arxivId": "2601.10560",
      "title": "Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems",
      "authors": [
        "Xi Shi",
        "Mengxin Zheng",
        "Qian Lou"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Multi-agent systems (MAS) enable complex reasoning by coordinating multiple agents, but often incur high inference latency due to multi-step execution and repeated model invocations, severely limiting their scalability and usability in time-sensitive scenarios. Most existing approaches primarily optimize task performance and inference cost, and explicitly or implicitly assume sequential execution, making them less optimal for controlling latency under parallel execution. In this work, we investigate learning-based orchestration of multi-agent systems with explicit latency supervision under parallel execution. We propose Latency-Aware Multi-agent System (LAMaS), a latency-aware multi-agent orchestration framework that enables parallel execution and explicitly optimizes the critical execution path, allowing the controller to construct execution topology graphs with lower latency under parallel execution. Our experiments show that our approach reduces critical path length by 38-46% compared to the state-of-the-art baseline for multi-agent architecture search across multiple benchmarks, while maintaining or even improving task performance. These results highlight the importance of explicitly optimizing latency under parallel execution when designing efficient multi-agent systems. The code is available at https://github.com/xishi404/LAMaS",
      "url": "https://arxiv.org/abs/2601.10560",
      "pdfUrl": "https://arxiv.org/pdf/2601.10560.pdf",
      "titleJa": "並列マルチエージェントシステムのための遅延を考慮したオーケストレーションの学習"
    },
    {
      "id": "2601.10543",
      "arxivId": "2601.10543",
      "title": "Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing",
      "authors": [
        "Yinzhi Zhao",
        "Ming Wang",
        "Shi Feng",
        "Xiaocui Yang",
        "Daling Wang",
        "Yifei Zhang"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.",
      "url": "https://arxiv.org/abs/2601.10543",
      "pdfUrl": "https://arxiv.org/pdf/2601.10543.pdf",
      "titleJa": "デコード中の安全性認識プロービングによる大規模言語モデルの脱獄攻撃からの防御"
    },
    {
      "id": "2601.10527",
      "arxivId": "2601.10527",
      "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
      "authors": [
        "Xingjun Ma",
        "Yixu Wang",
        "Hengyuan Xu",
        "Yutao Wu",
        "Yifan Ding",
        "Yunhan Zhao",
        "Zilong Wang",
        "Jiabin Hua",
        "Ming Wen",
        "Jianan Liu",
        "Ranjie Duan",
        "Yifeng Gao",
        "Yingshui Tan",
        "Yunhao Chen",
        "Hui Xue",
        "Xin Wang",
        "Wei Cheng",
        "Jingjing Chen",
        "Zuxuan Wu",
        "Bo Li",
        "Yu-Gang Jiang"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "abstract": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
      "url": "https://arxiv.org/abs/2601.10527",
      "pdfUrl": "https://arxiv.org/pdf/2601.10527.pdf",
      "titleJa": "GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro、Seedream 4.5 の安全性レポート"
    },
    {
      "id": "2601.10524",
      "arxivId": "2601.10524",
      "title": "Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection",
      "authors": [
        "Frank Bobe",
        "Gregory D. Vetaw",
        "Chase Pavlick",
        "Darshan Bryner",
        "Matthew Cook",
        "Jose Salas-Vernis"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.AI"
      ],
      "abstract": "The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.",
      "url": "https://arxiv.org/abs/2601.10524",
      "pdfUrl": "https://arxiv.org/pdf/2601.10524.pdf",
      "titleJa": "微調整されたLLMにおける一般化の失敗の診断：フィッシング検出に関するクロスアーキテクチャ研究"
    },
    {
      "id": "2601.10520",
      "arxivId": "2601.10520",
      "title": "Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment",
      "authors": [
        "Felix Jahn",
        "Yannic Muskalla",
        "Lisa Dargasz",
        "Patrick Schramowski",
        "Kevin Baum"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "abstract": "As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.",
      "url": "https://arxiv.org/abs/2601.10520",
      "pdfUrl": "https://arxiv.org/pdf/2601.10520.pdf",
      "titleJa": "GRACEによる規範的モノリシックエージェンシーからの脱却：安全で倫理的なAIアライメントのための理性に基づくニューロシンボリックアーキテクチャ"
    },
    {
      "id": "2601.06406",
      "arxivId": "2601.06406",
      "title": "Representing Sounds as Neural Amplitude Fields: A Benchmark of Coordinate-MLPs and A Fourier Kolmogorov-Arnold Framework",
      "authors": [
        "Linfei Li",
        "Lin Zhang",
        "Zhong Wang",
        "Fengyi Zhang",
        "Zelin Li",
        "Ying Shen"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Although Coordinate-MLP-based implicit neural representations have excelled in representing radiance fields, 3D shapes, and images, their application to audio signals remains underexplored. To fill this gap, we investigate existing implicit neural representations, from which we extract 3 types of positional encoding and 16 commonly used activation functions. Through combinatorial design, we establish the first benchmark for Coordinate-MLPs in audio signal representations. Our benchmark reveals that Coordinate-MLPs require complex hyperparameter tuning and frequency-dependent initialization, limiting their robustness. To address these issues, we propose Fourier-ASR, a novel framework based on the Fourier series theorem and the Kolmogorov-Arnold representation theorem. Fourier-ASR introduces Fourier Kolmogorov-Arnold Networks (Fourier-KAN), which leverage periodicity and strong nonlinearity to represent audio signals, eliminating the need for additional positional encoding. Furthermore, a Frequency-adaptive Learning Strategy (FaLS) is proposed to enhance the convergence of Fourier-KAN by capturing high-frequency components and preventing overfitting of low-frequency signals. Extensive experiments conducted on natural speech and music datasets reveal that: (1) well-designed positional encoding and activation functions in Coordinate-MLPs can effectively improve audio representation quality; and (2) Fourier-ASR can robustly represent complex audio signals without extensive hyperparameter tuning. Looking ahead, the continuity and infinite resolution of implicit audio representations make our research highly promising for tasks such as audio compression, synthesis, and generation. The source code will be released publicly to ensure reproducibility. The code is available at https://github.com/lif314/Fourier-ASR.",
      "url": "https://arxiv.org/abs/2601.06406",
      "pdfUrl": "https://arxiv.org/pdf/2601.06406.pdf",
      "titleJa": "音を神経振幅場として表現する：座標MLPのベンチマークとフーリエ・コルモゴロフ・アーノルド枠組み"
    },
    {
      "id": "2601.04592",
      "arxivId": "2601.04592",
      "title": "Density Matrix RNN (DM-RNN): A Quantum Information Theoretic Framework for Modeling Musical Context and Polyphony",
      "authors": [
        "Joonwon Seo",
        "Mariana Montiel"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.LG",
        "cs.SD",
        "math-ph"
      ],
      "abstract": "Classical Recurrent Neural Networks (RNNs) summarize musical context into a deterministic hidden state vector, imposing an information bottleneck that fails to capture the inherent ambiguity in music. We propose the Density Matrix RNN (DM-RNN), a novel theoretical architecture utilizing the Density Matrix. This allows the model to maintain a statistical ensemble of musical interpretations (a mixed state), capturing both classical probabilities and quantum coherences. We rigorously define the temporal dynamics using Quantum Channels (CPTP maps). Crucially, we detail a parameterization strategy based on the Choi-Jamiolkowski isomorphism, ensuring the learned dynamics remain physically valid (CPTP) by construction. We introduce an analytical framework using Von Neumann Entropy to quantify musical uncertainty and Quantum Mutual Information (QMI) to measure entanglement between voices. The DM-RNN provides a mathematically rigorous framework for modeling complex, ambiguous musical structures.",
      "url": "https://arxiv.org/abs/2601.04592",
      "pdfUrl": "https://arxiv.org/pdf/2601.04592.pdf",
      "titleJa": "密度行列RNN（DM-RNN）：音楽的文脈とポリフォニーをモデル化する量子情報理論的枠組み"
    },
    {
      "id": "2601.04343",
      "arxivId": "2601.04343",
      "title": "Summary of The Inaugural Music Source Restoration Challenge",
      "authors": [
        "Yongyi Zang",
        "Jiarui Hai",
        "Wanying Ge",
        "Qiuqiang Kong",
        "Zheqi Dai",
        "Helin Wang",
        "Yuki Mitsufuji",
        "Mark D. Plumbley"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Music Source Restoration (MSR) aims to recover original, unprocessed instrument stems from professionally mixed and degraded audio, requiring the reversal of both production effects and real-world degradations. We present the inaugural MSR Challenge, which features objective evaluation on studio-produced mixtures using Multi-Mel-SNR, Zimtohrli, and FAD-CLAP, alongside subjective evaluation on real-world degraded recordings. Five teams participated in the challenge. The winning system achieved 4.46 dB Multi-Mel-SNR and 3.47 MOS-Overall, corresponding to relative improvements of 91% and 18% over the second-place system, respectively. Per-stem analysis reveals substantial variation in restoration difficulty across instruments, with bass averaging 4.59 dB across all teams, while percussion averages only 0.29 dB. The dataset, evaluation protocols, and baselines are available at https://msrchallenge.com/.",
      "url": "https://arxiv.org/abs/2601.04343",
      "pdfUrl": "https://arxiv.org/pdf/2601.04343.pdf",
      "titleJa": "第1回音楽ソース修復チャレンジの概要"
    },
    {
      "id": "2601.03973",
      "arxivId": "2601.03973",
      "title": "Muse: Towards Reproducible Long-Form Song Generation with Fine-Grained Style Control",
      "authors": [
        "Changhao Jiang",
        "Jiahao Chen",
        "Zhenghao Xiang",
        "Zhixiong Yang",
        "Hanchen Wang",
        "Jiabao Zhuang",
        "Xinmeng Che",
        "Jiajun Sun",
        "Hui Li",
        "Yifei Cao",
        "Shihan Dou",
        "Ming Zhang",
        "Junjie Ye",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Recent commercial systems such as Suno demonstrate strong capabilities in long-form song generation, while academic research remains largely non-reproducible due to the lack of publicly available training data, hindering fair comparison and progress. To this end, we release a fully open-source system for long-form song generation with fine-grained style conditioning, including a licensed synthetic dataset, training and evaluation pipelines, and Muse, an easy-to-deploy song generation model. The dataset consists of 116k fully licensed synthetic songs with automatically generated lyrics and style descriptions paired with audio synthesized by SunoV5. We train Muse via single-stage supervised finetuning of a Qwen-based language model extended with discrete audio tokens using MuCodec, without task-specific losses, auxiliary objectives, or additional architectural components. Our evaluations find that although Muse is trained with a modest data scale and model size, it achieves competitive performance on phoneme error rate, text--music style similarity, and audio aesthetic quality, while enabling controllable segment-level generation across different musical structures. All data, model weights, and training and evaluation pipelines will be publicly released, paving the way for continued progress in controllable long-form song generation research. The project repository is available at https://github.com/yuhui1038/Muse.",
      "url": "https://arxiv.org/abs/2601.03973",
      "pdfUrl": "https://arxiv.org/pdf/2601.03973.pdf",
      "titleJa": "Muse: きめ細かなスタイル制御による再現性の高い長編楽曲生成に向けて"
    },
    {
      "id": "2601.03626",
      "arxivId": "2601.03626",
      "title": "Learning from Limited Labels: Transductive Graph Label Propagation for Indian Music Analysis",
      "authors": [
        "Parampreet Singh",
        "Akshay Raina",
        "Sayeedul Islam Sheikh",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Supervised machine learning frameworks rely on extensive labeled datasets for robust performance on real-world tasks. However, there is a lack of large annotated datasets in audio and music domains, as annotating such recordings is resource-intensive, laborious, and often require expert domain knowledge. In this work, we explore the use of label propagation (LP), a graph-based semi-supervised learning technique, for automatically labeling the unlabeled set in an unsupervised manner. By constructing a similarity graph over audio embeddings, we propagate limited label information from a small annotated subset to a larger unlabeled corpus in a transductive, semi-supervised setting. We apply this method to two tasks in Indian Art Music (IAM): Raga identification and Instrument classification. For both these tasks, we integrate multiple public datasets along with additional recordings we acquire from Prasar Bharati Archives to perform LP. Our experiments demonstrate that LP significantly reduces labeling overhead and produces higher-quality annotations compared to conventional baseline methods, including those based on pretrained inductive models. These results highlight the potential of graph-based semi-supervised learning to democratize data annotation and accelerate progress in music information retrieval.",
      "url": "https://arxiv.org/abs/2601.03626",
      "pdfUrl": "https://arxiv.org/pdf/2601.03626.pdf",
      "titleJa": "限定ラベルからの学習：インド音楽分析のためのトランスダクティブグラフラベル伝播"
    },
    {
      "id": "2601.03612",
      "arxivId": "2601.03612",
      "title": "Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias",
      "authors": [
        "Joonwon Seo"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This monograph introduces a novel approach to polyphonic music generation by addressing the \"Missing Middle\" problem through structural inductive bias. Focusing on Beethoven's piano sonatas as a case study, we empirically verify the independence of pitch and hand attributes using normalized mutual information (NMI=0.167) and propose the Smart Embedding architecture, achieving a 48.30% reduction in parameters. We provide rigorous mathematical proofs using information theory (negligible loss bounded at 0.153 bits), Rademacher complexity (28.09% tighter generalization bound), and category theory to demonstrate improved stability and generalization. Empirical results show a 9.47% reduction in validation loss, confirmed by SVD analysis and an expert listening study (N=53). This dual theoretical and applied framework bridges gaps in AI music generation, offering verifiable insights for mathematically grounded deep learning.",
      "url": "https://arxiv.org/abs/2601.03612",
      "pdfUrl": "https://arxiv.org/pdf/2601.03612.pdf",
      "titleJa": "構造的帰納的バイアスによるポリフォニック音楽生成の数学的基礎"
    },
    {
      "id": "2601.03443",
      "arxivId": "2601.03443",
      "title": "Discriminating real and synthetic super-resolved audio samples using embedding-based classifiers",
      "authors": [
        "Mikhail Silaev",
        "Konstantinos Drossos",
        "Tuomas Virtanen"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "Generative adversarial networks (GANs) and diffusion models have recently achieved state-of-the-art performance in audio super-resolution (ADSR), producing perceptually convincing wideband audio from narrowband inputs. However, existing evaluations primarily rely on signal-level or perceptual metrics, leaving open the question of how closely the distributions of synthetic super-resolved and real wideband audio match. Here we address this problem by analyzing the separability of real and super-resolved audio in various embedding spaces. We consider both middle-band ($4\\to 16$~kHz) and full-band ($16\\to 48$~kHz) upsampling tasks for speech and music, training linear classifiers to distinguish real from synthetic samples based on multiple types of audio embeddings. Comparisons with objective metrics and subjective listening tests reveal that embedding-based classifiers achieve near-perfect separation, even when the generated audio attains high perceptual quality and state-of-the-art metric scores. This behavior is consistent across datasets and models, including recent diffusion-based approaches, highlighting a persistent gap between perceptual quality and true distributional fidelity in ADSR models.",
      "url": "https://arxiv.org/abs/2601.03443",
      "pdfUrl": "https://arxiv.org/pdf/2601.03443.pdf",
      "titleJa": "埋め込みベースの分類器を用いた実在および合成の超解像オーディオサンプルの識別"
    },
    {
      "id": "2601.02983",
      "arxivId": "2601.02983",
      "title": "Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning",
      "authors": [
        "Yuankun Xie",
        "Xiaoxuan Guo",
        "Jiayi Zhou",
        "Tao Wang",
        "Jian Liu",
        "Ruibo Fu",
        "Xiaopeng Wang",
        "Haonan Cheng",
        "Long Ye"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in audio large language models (ALLMs) have made high-quality synthetic audio widely accessible, increasing the risk of malicious audio deepfakes across speech, environmental sounds, singing voice, and music. Real-world audio deepfake detection (ADD) therefore requires all-type detectors that generalize across heterogeneous audio and provide interpretable decisions. Given the strong multi-task generalization ability of ALLMs, we first investigate their performance on all-type ADD under both supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). However, SFT using only binary real/fake labels tends to reduce the model to a black-box classifier, sacrificing interpretability. Meanwhile, vanilla RFT under sparse supervision is prone to reward hacking and can produce hallucinated, ungrounded rationales. To address this, we propose an automatic annotation and polishing pipeline that constructs Frequency-Time structured chain-of-thought (CoT) rationales, producing ~340K cold-start demonstrations. Building on CoT data, we propose Frequency Time-Group Relative Policy Optimization (FT-GRPO), a two-stage training paradigm that cold-starts ALLMs with SFT and then applies GRPO under rule-based frequency-time constraints. Experiments demonstrate that FT-GRPO achieves state-of-the-art performance on all-type ADD while producing interpretable, FT-grounded rationales. The data and code are available online.",
      "url": "https://arxiv.org/abs/2601.02983",
      "pdfUrl": "https://arxiv.org/pdf/2601.02983.pdf",
      "titleJa": "周波数時間強化学習によるオーディオLLMを用いた解釈可能な全タイプオーディオディープフェイク検出"
    },
    {
      "id": "2601.02967",
      "arxivId": "2601.02967",
      "title": "MoE Adapter for Large Audio Language Models: Sparsity, Disentanglement, and Gradient-Conflict-Free",
      "authors": [
        "Yishu Lei",
        "Shuwei He",
        "Jing Hu",
        "Dan Zhang",
        "Xianlong Luo",
        "Danxiang Zhu",
        "Shikun Feng",
        "Rui Liu",
        "Jingzhou He",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Extending the input modality of Large Language Models~(LLMs) to the audio domain is essential for achieving comprehensive multimodal perception. However, it is well-known that acoustic information is intrinsically \\textit{heterogeneous}, entangling attributes such as speech, music, and environmental context. Existing research is limited to a dense, parameter-shared adapter to model these diverse patterns, which induces \\textit{gradient conflict} during optimization, as parameter updates required for distinct attributes contradict each other. To address this limitation, we introduce the \\textit{\\textbf{MoE-Adapter}}, a sparse Mixture-of-Experts~(MoE) architecture designed to decouple acoustic information. Specifically, it employs a dynamic gating mechanism that routes audio tokens to specialized experts capturing complementary feature subspaces while retaining shared experts for global context, thereby mitigating gradient conflicts and enabling fine-grained feature learning. Comprehensive experiments show that the MoE-Adapter achieves superior performance on both audio semantic and paralinguistic tasks, consistently outperforming dense linear baselines with comparable computational costs. Furthermore, we will release the related code and models to facilitate future research.",
      "url": "https://arxiv.org/abs/2601.02967",
      "pdfUrl": "https://arxiv.org/pdf/2601.02967.pdf",
      "titleJa": "大規模音声言語モデルのためのMoEアダプタ：スパース性、分離、勾配衝突フリー"
    },
    {
      "id": "2601.02591",
      "arxivId": "2601.02591",
      "title": "A Music Information Retrieval Approach to Classify Sub-Genres in Role Playing Games",
      "authors": [
        "Daeun Hwang",
        "Xuyuan Cai",
        "Edward F. Melcer",
        "Elin Carstensdottir"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.IR"
      ],
      "abstract": "Video game music (VGM) is often studied under the same lens as film music, which largely focuses on its theoretical functionality with relation to the identified genres of the media. However, till date, we are unaware of any systematic approach that analyzes the quantifiable musical features in VGM across several identified game genres. Therefore, we extracted musical features from VGM in games from three sub-genres of Role-Playing Games (RPG), and then hypothesized how different musical features are correlated to the perceptions and portrayals of each genre. This observed correlation may be used to further suggest such features are relevant to the expected storytelling elements or play mechanics associated with the sub-genre.",
      "url": "https://arxiv.org/abs/2601.02591",
      "pdfUrl": "https://arxiv.org/pdf/2601.02591.pdf",
      "titleJa": "ロールプレイングゲームのサブジャンルを分類するための音楽情報検索アプローチ"
    },
    {
      "id": "2601.06621",
      "arxivId": "2601.06621",
      "title": "Stereo Audio Rendering for Personal Sound Zones Using a Binaural Spatially Adaptive Neural Network (BSANN)",
      "authors": [
        "Hao Jiang",
        "Edgar Choueiri"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "A binaural rendering framework for personal sound zones (PSZs) is proposed to enable multiple head-tracked listeners to receive fully independent stereo audio programs. Current PSZ systems typically rely on monophonic rendering and therefore cannot control the left and right ears separately, which limits the quality and accuracy of spatial imaging. The proposed method employs a Binaural Spatially Adaptive Neural Network (BSANN) to generate ear-optimized loudspeaker filters that reconstruct the desired acoustic field at each ear of multiple listeners. The framework integrates anechoically measured loudspeaker frequency responses, analytically modeled transducer directivity, and rigid-sphere head-related transfer functions (HRTFs) to enhance acoustic accuracy and spatial rendering fidelity. An explicit active crosstalk cancellation (XTC) stage further improves three-dimensional spatial perception. Experiments show significant gains in measured objective performance metrics, including inter-zone isolation (IZI), inter-program isolation (IPI), and crosstalk cancellation (XTC), with log-frequency-weighted values of 10.23/10.03 dB (IZI), 11.11/9.16 dB (IPI), and 10.55/11.13 dB (XTC), respectively, over 100-20,000 Hz. The combined use of ear-wise control, accurate acoustic modeling, and integrated active XTC produces a unified rendering method that delivers greater isolation performance, increased robustness to room asymmetry, and more faithful spatial reproduction in real acoustic environments.",
      "url": "https://arxiv.org/abs/2601.06621",
      "pdfUrl": "https://arxiv.org/pdf/2601.06621.pdf",
      "titleJa": "バイノーラル空間適応型ニューラルネットワーク（BSANN）を用いたパーソナルサウンドゾーン向けステレオオーディオレンダリング"
    },
    {
      "id": "2601.06006",
      "arxivId": "2601.06006",
      "title": "Discriminative-Generative Target Speaker Extraction with Decoder-Only Language Models",
      "authors": [
        "Bang Zeng",
        "Beilong Tang",
        "Wang Xiang",
        "Ming Li"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Target speaker extraction (TSE) aims to recover the speech signal of a desired speaker from a mixed audio recording, given a short enrollment utterance. Most existing TSE approaches are based on discriminative modeling paradigms. Although effective at suppressing interfering speakers, these methods often struggle to produce speech with high perceptual quality and naturalness. To address this limitation, we first propose LauraTSE, a generative TSE model built upon an auto-regressive decoder-only language model. However, purely generative approaches may suffer from hallucinations, content drift, and limited controllability, which may undermine their reliability in complex acoustic scenarios. To overcome these challenges, we further introduce a discriminative-generative TSE framework. In this framework, a discriminative front-end is employed to robustly extract the target speaker's speech, yielding stable and controllable intermediate representations. A generative back-end then operates in the neural audio codec representation space to reconstruct fine-grained speech details and enhance perceptual quality. This two-stage design effectively combines the robustness and controllability of discriminative models with the superior naturalness and quality enhancement capabilities of generative models. Moreover, we systematically investigate collaborative training strategies for the proposed framework, including freezing or fine-tuning the front-end, incorporating an auxiliary SI-SDR loss, and exploring both auto-regressive and non-auto-regressive inference mechanisms. Experimental results demonstrate that the proposed framework achieves a more favorable trade-off among speech quality, intelligibility, and speaker consistency.",
      "url": "https://arxiv.org/abs/2601.06006",
      "pdfUrl": "https://arxiv.org/pdf/2601.06006.pdf",
      "titleJa": "デコーダのみの言語モデルを用いた識別的・生成的ターゲット話者抽出"
    },
    {
      "id": "2601.05554",
      "arxivId": "2601.05554",
      "title": "SPAM: Style Prompt Adherence Metric for Prompt-based TTS",
      "authors": [
        "Chanhee Cho",
        "Nayeon Kim",
        "Bugeun Kim"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Prompt-based text-to-speech (TTS) aims to generate speech that adheres to fine-grained style cues provided in a text prompt. However, most prior works depend on neither plausible nor faithful measures to evaluate prompt adherence. That is, they cannot ensure whether the evaluation is grounded on the prompt and is similar to a human. Thus, we present a new automatic metric, the Style Prompt Adherence Metric, which explicitly satisfies both plausibility and faithfulness. Inspired by the CLAP, our approach factorizes speech into acoustic attributes and aligns them with the style prompt. Also, we trained the scorer with a supervised contrastive loss, which could provide a clearer distinction between different semantics. We conducted two experiments on two perspectives. The plausibility experiment showed that SPAM achieved a strong correlation with the mean opinion score (MOS). Also, the faithfulness experiment demonstrated that SPAM is successfully grounded to the given style prompt, as it can discriminate different semantics of the prompt. We believe that SPAM can provide a viable automatic solution for evaluating style prompt adherence of synthesized speech.",
      "url": "https://arxiv.org/abs/2601.05554",
      "pdfUrl": "https://arxiv.org/pdf/2601.05554.pdf",
      "titleJa": "SPAM: プロンプトベースの TTS におけるスタイルプロンプト遵守指標"
    },
    {
      "id": "2601.04744",
      "arxivId": "2601.04744",
      "title": "Semi-Supervised Diseased Detection from Speech Dialogues with Multi-Level Data Modeling",
      "authors": [
        "Xingyuan Li",
        "Mengyue Wu"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Detecting medical conditions from speech acoustics is fundamentally a weakly-supervised learning problem: a single, often noisy, session-level label must be linked to nuanced patterns within a long, complex audio recording. This task is further hampered by severe data scarcity and the subjective nature of clinical annotations. While semi-supervised learning (SSL) offers a viable path to leverage unlabeled data, existing audio methods often fail to address the core challenge that pathological traits are not uniformly expressed in a patient's speech. We propose a novel, audio-only SSL framework that explicitly models this hierarchy by jointly learning from frame-level, segment-level, and session-level representations within unsegmented clinical dialogues. Our end-to-end approach dynamically aggregates these multi-granularity features and generates high-quality pseudo-labels to efficiently utilize unlabeled data. Extensive experiments show the framework is model-agnostic, robust across languages and conditions, and highly data-efficient-achieving, for instance, 90\\% of fully-supervised performance using only 11 labeled samples. This work provides a principled approach to learning from weak, far-end supervision in medical speech analysis.",
      "url": "https://arxiv.org/abs/2601.04744",
      "pdfUrl": "https://arxiv.org/pdf/2601.04744.pdf",
      "titleJa": "多層データモデリングを用いた音声対話からの半教師付き疾患検出"
    },
    {
      "id": "2601.04564",
      "arxivId": "2601.04564",
      "title": "When Tone and Words Disagree: Towards Robust Speech Emotion Recognition under Acoustic-Semantic Conflict",
      "authors": [
        "Dawei Huang",
        "Yongjie Lv",
        "Ruijie Xiong",
        "Chunxiang Jin",
        "Xiaojiang Peng"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Speech Emotion Recognition (SER) systems often assume congruence between vocal emotion and lexical semantics. However, in real-world interactions, acoustic-semantic conflict is common yet overlooked, where the emotion conveyed by tone contradicts the literal meaning of spoken words. We show that state-of-the-art SER models, including ASR-based, self-supervised learning (SSL) approaches and Audio Language Models (ALMs), suffer performance degradation under such conflicts due to semantic bias or entangled acoustic-semantic representations. To address this, we propose the Fusion Acoustic-Semantic (FAS) framework, which explicitly disentangles acoustic and semantic pathways and bridges them through a lightweight, query-based attention module. To enable systematic evaluation, we introduce the Conflict in Acoustic-Semantic Emotion (CASE), the first dataset dominated by clear and interpretable acoustic-semantic conflicts in varied scenarios. Extensive experiments demonstrate that FAS consistently outperforms existing methods in both in-domain and zero-shot settings. Notably, on the CASE benchmark, conventional SER models fail dramatically, while FAS sets a new SOTA with 59.38% accuracy. Our code and datasets is available at https://github.com/24DavidHuang/FAS.",
      "url": "https://arxiv.org/abs/2601.04564",
      "pdfUrl": "https://arxiv.org/pdf/2601.04564.pdf",
      "titleJa": "音調と言葉が一致しないとき：音響的・意味的矛盾下におけるロバストな音声感情認識に向けて"
    },
    {
      "id": "2601.03712",
      "arxivId": "2601.03712",
      "title": "TellWhisper: Tell Whisper Who Speaks When",
      "authors": [
        "Yifan Hu",
        "Peiji Yang",
        "Zhisheng Wang",
        "Yicheng Zhong",
        "Rui Liu"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Multi-speaker automatic speech recognition (MASR) aims to predict ''who spoke when and what'' from multi-speaker speech, a key technology for multi-party dialogue understanding. However, most existing approaches decouple temporal modeling and speaker modeling when addressing ''when'' and ''who'': some inject speaker cues before encoding (e.g., speaker masking), which can cause irreversible information loss; others fuse identity by mixing speaker posteriors after encoding, which may entangle acoustic content with speaker identity. This separation is brittle under rapid turn-taking and overlapping speech, often leading to degraded performance. To address these limitations, we propose TellWhisper, a unified framework that jointly models speaker identity and temporal within the speech encoder. Specifically, we design TS-RoPE, a time-speaker rotary positional encoding: time coordinates are derived from frame indices, while speaker coordinates are derived from speaker activity and pause cues. By applying region-specific rotation angles, the model explicitly captures per-speaker continuity, speaker-turn transitions, and state dynamics, enabling the attention mechanism to simultaneously attend to ''when'' and ''who''. Moreover, to estimate frame-level speaker activity, we develop Hyper-SD, which casts speaker classification in hyperbolic space to enhance inter-class separation and refine speaker-activity estimates. Extensive experiments demonstrate the effectiveness of the proposed approach.",
      "url": "https://arxiv.org/abs/2601.03712",
      "pdfUrl": "https://arxiv.org/pdf/2601.03712.pdf",
      "titleJa": "TellWhisper: 誰がいつ話すかを知らせる"
    },
    {
      "id": "2601.03615",
      "arxivId": "2601.03615",
      "title": "Analyzing Reasoning Shifts in Audio Deepfake Detection under Adversarial Attacks: The Reasoning Tax versus Shield Bifurcation",
      "authors": [
        "Binh Nguyen",
        "Thai Le"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Audio Language Models (ALMs) offer a promising shift towards explainable audio deepfake detections (ADDs), moving beyond \\textit{black-box} classifiers by providing some level of transparency into their predictions via reasoning traces. This necessitates a new class of model robustness analysis: robustness of the predictive reasoning under adversarial attacks, which goes beyond existing paradigm that mainly focuses on the shifts of the final predictions (e.g., fake v.s. real). To analyze such reasoning shifts, we introduce a forensic auditing framework to evaluate the robustness of ALMs' reasoning under adversarial attacks in three inter-connected dimensions: acoustic perception, cognitive coherence, and cognitive dissonance. Our systematic analysis reveals that explicit reasoning does not universally enhance robustness. Instead, we observe a bifurcation: for models exhibiting robust acoustic perception, reasoning acts as a defensive \\textit{``shield''}, protecting them from adversarial attacks. However, for others, it imposes a performance \\textit{``tax''}, particularly under linguistic attacks which reduce cognitive coherence and increase attack success rate. Crucially, even when classification fails, high cognitive dissonance can serve as a \\textit{silent alarm}, flagging potential manipulation. Overall, this work provides a critical evaluation of the role of reasoning in forensic audio deepfake analysis and its vulnerabilities.",
      "url": "https://arxiv.org/abs/2601.03615",
      "pdfUrl": "https://arxiv.org/pdf/2601.03615.pdf",
      "titleJa": "敵対的攻撃下における音声ディープフェイク検出の推論シフトの分析：推論税とシールド分岐"
    },
    {
      "id": "2601.03610",
      "arxivId": "2601.03610",
      "title": "Investigation into respiratory sound classification for an imbalanced data set using hybrid LSTM-KAN architectures",
      "authors": [
        "Nithinkumar K.",
        "Anand R"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Respiratory sounds captured via auscultation contain critical clues for diagnosing pulmonary conditions. Automated classification of these sounds faces challenges due to subtle acoustic differences and severe class imbalance in clinical datasets. This study investigates respiratory sound classification with a focus on mitigating pronounced class imbalance. We propose a hybrid deep learning model that combines a Long Short-Term Memory (LSTM) network for sequential feature encoding with a Kolmogorov-Arnold Network (KAN) for classification. The model is integrated with a comprehensive feature extraction pipeline and targeted imbalance mitigation strategies. Experiments were conducted on a public respiratory sound database comprising six classes with a highly skewed distribution. Techniques such as focal loss, class-specific data augmentation, and Synthetic Minority Over-sampling Technique (SMOTE) were employed to enhance minority class recognition. The proposed Hybrid LSTM-KAN model achieves an overall accuracy of 94.6 percent and a macro-averaged F1 score of 0.703, despite the dominant COPD class accounting for over 86 percent of the data. Improved detection performance is observed for minority classes compared to baseline approaches, demonstrating the effectiveness of the proposed architecture for imbalanced respiratory sound classification.",
      "url": "https://arxiv.org/abs/2601.03610",
      "pdfUrl": "https://arxiv.org/pdf/2601.03610.pdf",
      "titleJa": "ハイブリッドLSTM-KANアーキテクチャを用いた不均衡なデータセットの呼吸音分類の調査"
    }
  ],
  "lastUpdated": "2026-01-19T00:58:18.027375",
  "totalCount": 70
}