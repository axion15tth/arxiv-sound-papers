{
  "papers": [
    {
      "id": "2601.19786",
      "arxivId": "2601.19786",
      "title": "Rethinking Discrete Speech Representation Tokens for Accent Generation",
      "authors": [
        "Jinzuomu Zhong",
        "Yi Wang",
        "Korin Richmond",
        "Peter Bell"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "Discrete Speech Representation Tokens (DSRTs) have become a foundational component in speech generation. While prior work has extensively studied phonetic and speaker information in DSRTs, how accent information is encoded in DSRTs remains largely unexplored. In this paper, we present the first systematic investigation of accent information in DSRTs. We propose a unified evaluation framework that measures both accessibility of accent information via a novel Accent ABX task and recoverability via cross-accent Voice Conversion (VC) resynthesis. Using this framework, we analyse DSRTs derived from a variety of speech encoders. Our results reveal that accent information is substantially reduced when ASR supervision is used to fine-tune the encoder, but cannot be effectively disentangled from phonetic and speaker information through naive codebook size reduction. Based on these findings, we propose new content-only and content-accent DSRTs that significantly outperform existing designs in controllable accent generation. Our work highlights the importance of accent-aware evaluation and provides practical guidance for designing DSRTs for accent-controlled speech generation.",
      "url": "https://arxiv.org/abs/2601.19786",
      "pdfUrl": "https://arxiv.org/pdf/2601.19786.pdf",
      "titleJa": "アクセント生成のための離散音声表現トークンの再考"
    },
    {
      "id": "2601.19781",
      "arxivId": "2601.19781",
      "title": "Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means",
      "authors": [
        "Kentaro Onda",
        "Hayato Futami",
        "Yosuke Kashiwagi",
        "Emiru Tsunoo",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In recent years, there has been growing interest in representing speech with discrete tokens, which serve as pseudo-text for speech language models (speechLMs) and as efficient intermediate representations for downstream tasks. These tokens are typically categorized as acoustic and phonetic tokens: the former holds detailed acoustic information for reconstruction while the latter mainly captures linguistic content. In human speech communication, however, unnecessary acoustic details such as speaker information are abstracted, while both linguistic and prosodic information are utilized for speech comprehension and production. Given this, neither type of token seems an ideal representation for tasks sensitive to prosody, such as speechLMs. In this study, we propose the Phonological Tokenizer, a method that fine-tunes phonetic tokens via differentiable k-means with a multi-task objective of ASR and speech resynthesis. Experimental validation on diverse tasks confirms that our tokens retain phonological (both linguistic and prosodic) information while appropriately discarding speaker identity.",
      "url": "https://arxiv.org/abs/2601.19781",
      "pdfUrl": "https://arxiv.org/pdf/2601.19781.pdf",
      "titleJa": "音韻論トークナイザー: 微分可能K平均法を用いた多目的微調整による韻律を考慮した音声トークン"
    },
    {
      "id": "2601.19767",
      "arxivId": "2601.19767",
      "title": "Advanced Modeling of Interlanguage Speech Intelligibility Benefit with L1-L2 Multi-Task Learning Using Differentiable K-Means for Accent-Robust Discrete Token-Based ASR",
      "authors": [
        "Kentaro Onda",
        "Satoru Fukayama",
        "Daisuke Saito",
        "Nobuaki Minematsu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Building ASR systems robust to foreign-accented speech is an important challenge in today's globalized world. A prior study explored the way to enhance the performance of phonetic token-based ASR on accented speech by reproducing the phenomenon known as interlanguage speech intelligibility benefit (ISIB), where foreign-accented speech is more intelligible to listeners sharing the speaker's native language than to native listeners. ISIB was technically implemented by using the speaker's L1 to learn k-means cluster centroids in an SSL feature space to obtain phonetic tokens. In this study, we propose a more advanced modeling of ISIB. By employing differentiable k-means and optimizing the entire module for both L1 and L2 ASR, the proposed method outperformed the baselines, both when using only native speech and when additionally incorporating a limited amount of accented speech. Notably, in the latter scenario, our method achieved approximately a 20% relative improvement in recognition accuracy.",
      "url": "https://arxiv.org/abs/2601.19767",
      "pdfUrl": "https://arxiv.org/pdf/2601.19767.pdf",
      "titleJa": "アクセントロバストな離散トークンベース音声認識のための微分可能K平均法を用いたL1-L2マルチタスク学習による中間言語音声明瞭度の利点の高度なモデリング"
    },
    {
      "id": "2601.19712",
      "arxivId": "2601.19712",
      "title": "Physics-Aware Novel-View Acoustic Synthesis with Vision-Language Priors and 3D Acoustic Environment Modeling",
      "authors": [
        "Congyi Fan",
        "Jian Guan",
        "Youtian Lin",
        "Dongli Xu",
        "Tong Ye",
        "Qiaoxi Zhu",
        "Pengming Feng",
        "Wenwu Wang"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.MM"
      ],
      "abstract": "Spatial audio is essential for immersive experiences, yet novel-view acoustic synthesis (NVAS) remains challenging due to complex physical phenomena such as reflection, diffraction, and material absorption. Existing methods based on single-view or panoramic inputs improve spatial fidelity but fail to capture global geometry and semantic cues such as object layout and material properties. To address this, we propose Phys-NVAS, the first physics-aware NVAS framework that integrates spatial geometry modeling with vision-language semantic priors. A global 3D acoustic environment is reconstructed from multi-view images and depth maps to estimate room size and shape, enhancing spatial awareness of sound propagation. Meanwhile, a vision-language model extracts physics-aware priors of objects, layouts, and materials, capturing absorption and reflection beyond geometry. An acoustic feature fusion adapter unifies these cues into a physics-aware representation for binaural generation. Experiments on RWAVS demonstrate that Phys-NVAS yields binaural audio with improved realism and physical consistency.",
      "url": "https://arxiv.org/abs/2601.19712",
      "pdfUrl": "https://arxiv.org/pdf/2601.19712.pdf",
      "titleJa": "視覚言語事前分布と3D音響環境モデリングを用いた物理を考慮した新規視点音響合成"
    },
    {
      "id": "2601.19709",
      "arxivId": "2601.19709",
      "title": "Hyperbolic Additive Margin Softmax with Hierarchical Information for Speaker Verification",
      "authors": [
        "Zhihua Fang",
        "Liang He"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Speaker embedding learning based on Euclidean space has achieved significant progress, but it is still insufficient in modeling hierarchical information within speaker features. Hyperbolic space, with its negative curvature geometric properties, can efficiently represent hierarchical information within a finite volume, making it more suitable for the feature distribution of speaker embeddings. In this paper, we propose Hyperbolic Softmax (H-Softmax) and Hyperbolic Additive Margin Softmax (HAM-Softmax) based on hyperbolic space. H-Softmax incorporates hierarchical information into speaker embeddings by projecting embeddings and speaker centers into hyperbolic space and computing hyperbolic distances. HAM-Softmax further enhances inter-class separability by introducing margin constraint on this basis. Experimental results show that H-Softmax and HAM-Softmax achieve average relative EER reductions of 27.84% and 14.23% compared with standard Softmax and AM-Softmax, respectively, demonstrating that the proposed methods effectively improve speaker verification performance and at the same time preserve the capability of hierarchical structure modeling. The code will be released at https://github.com/PunkMale/HAM-Softmax.",
      "url": "https://arxiv.org/abs/2601.19709",
      "pdfUrl": "https://arxiv.org/pdf/2601.19709.pdf",
      "titleJa": "話者認証のための階層情報を用いた双曲型加法マージンソフトマックス"
    },
    {
      "id": "2601.19673",
      "arxivId": "2601.19673",
      "title": "A Benchmark for Audio Reasoning Capabilities of Multimodal Large Language Models",
      "authors": [
        "Iwona Christop",
        "Mateusz Czyżnikiewicz",
        "Paweł Skórzewski",
        "Łukasz Bondaruk",
        "Jakub Kubiak",
        "Marcin Lewandowski",
        "Marek Kubis"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "The present benchmarks for testing the audio modality of multimodal large language models concentrate on testing various audio tasks such as speaker diarization or gender identification in isolation. Whether a multimodal model can answer the questions that require reasoning skills to combine audio tasks of different categories, cannot be verified with their use. To address this issue, we propose Audio Reasoning Tasks (ART), a new benchmark for assessing the ability of multimodal models to solve problems that require reasoning over audio signal.",
      "url": "https://arxiv.org/abs/2601.19673",
      "pdfUrl": "https://arxiv.org/pdf/2601.19673.pdf",
      "titleJa": "マルチモーダル大規模言語モデルの音声推論能力のベンチマーク"
    },
    {
      "id": "2601.19606",
      "arxivId": "2601.19606",
      "title": "GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining",
      "authors": [
        "Shentong Mo",
        "Zehua Chen",
        "Jun Zhu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Recent advances in video-audio (V-A) understanding and generation have increasingly relied on joint V-A embeddings, which serve as the foundation for tasks such as cross-modal retrieval and generation. While prior methods like CAVP effectively model semantic and temporal correspondences between modalities using contrastive objectives, their performance remains suboptimal. A key limitation is the insufficient modeling of the dense, multi-scale nature of both video and audio signals, correspondences often span fine- to coarse-grained spatial-temporal structures, which are underutilized in existing frameworks. To this end, we propose GMS-CAVP, a novel framework that combines Multi-Scale Video-Audio Alignment and Multi-Scale Spatial-Temporal Diffusion-based pretraining objectives to enhance V-A correspondence modeling. First, GMS-CAVP introduces a multi-scale contrastive learning strategy that captures semantic and temporal relations across varying granularities. Second, we go beyond traditional contrastive learning by incorporating a diffusion-based generative objective, enabling modality translation and synthesis between video and audio. This unified discriminative-generative formulation facilitates deeper cross-modal understanding and paves the way for high-fidelity generation. Extensive experiments on VGGSound, AudioSet, and Panda70M demonstrate that GMS-CAVP outperforms previous methods in generation and retrieval.",
      "url": "https://arxiv.org/abs/2601.19606",
      "pdfUrl": "https://arxiv.org/pdf/2601.19606.pdf",
      "titleJa": "GMS-CAVP: マルチスケールの対照的および生成的事前学習による音声とビデオの対応の改善"
    },
    {
      "id": "2601.19533",
      "arxivId": "2601.19533",
      "title": "SLM-SS: Speech Language Model for Generative Speech Separation",
      "authors": [
        "Tianhua Li",
        "Chenda Li",
        "Wei Wang",
        "Xin Zhou",
        "Xihui Chen",
        "Jianqing Gao",
        "Yanmin Qian"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Speech separation (SS) has advanced significantly with neural network-based methods, showing improved performance on signal-level metrics. However, these methods often struggle to maintain speech intelligibility in the separated signals, which can negatively affect the performance of downstream tasks such as speech recognition. In this work, we propose SLM-SS, a novel approach that applies speech language models to SS, aiming to enhance the intelligibility and coherence of the separated signals. We frame SS as discrete multi-codebook sequence generation, using Encoder-Decoder models to map quantized speech mixtures to target tokens. In addition to the autoregressive modeling strategy, we introduce a non-autoregressive model to improve decoding efficiency for residual tokens. Experimental results on the LibriMix dataset demonstrate that our approach shows significantly better preservation of speech intelligibility, leading to improved linguistic consistency in a variety of downstream tasks compared to existing approaches.",
      "url": "https://arxiv.org/abs/2601.19533",
      "pdfUrl": "https://arxiv.org/pdf/2601.19533.pdf",
      "titleJa": "SLM-SS: 生成的音声分離のための音声言語モデル"
    },
    {
      "id": "2601.19472",
      "arxivId": "2601.19472",
      "title": "Dual-Strategy-Enhanced ConBiMamba for Neural Speaker Diarization",
      "authors": [
        "Zhen Liao",
        "Gaole Dai",
        "Mengqiao Chen",
        "Wenqing Cheng",
        "Wei Xu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Conformer and Mamba have achieved strong performance in speech modeling but face limitations in speaker diarization. Mamba is efficient but struggles with local details and nonlinear patterns. Conformer's self-attention incurs high memory overhead for long speech sequences and may cause instability in long-range dependency modeling. These limitations are critical for diarization, which requires both precise modeling of local variations and robust speaker consistency over extended spans. To address these challenges, we first apply ConBiMamba for speaker diarization. We follow the Pyannote pipeline and propose the Dual-Strategy-Enhanced ConBiMamba neural speaker diarization system. ConBiMamba integrates the strengths of Conformer and Mamba, where Conformer's convolutional and feed-forward structures are utilized to improve local feature extraction. By replacing Conformer's self-attention with ExtBiMamba, ConBiMamba efficiently handles long audio sequences while alleviating the high memory cost of self-attention. Furthermore, to address the problem of the higher DER around speaker change points, we introduce the Boundary-Enhanced Transition Loss to enhance the detection of speaker change points. We also propose Layer-wise Feature Aggregation to enhance the utilization of multi-layer representations. The system is evaluated on six diarization datasets and achieves state-of-the-art performance on four of them. The source code of our study is available at https://github.com/lz-hust/DSE-CBM.",
      "url": "https://arxiv.org/abs/2601.19472",
      "pdfUrl": "https://arxiv.org/pdf/2601.19472.pdf",
      "titleJa": "ニューラル話者ダイアライゼーションのためのデュアル戦略強化ConBiMamba"
    },
    {
      "id": "2601.19399",
      "arxivId": "2601.19399",
      "title": "Residual Tokens Enhance Masked Autoencoders for Speech Modeling",
      "authors": [
        "Samir Sadok",
        "Stéphane Lathuilière",
        "Xavier Alameda-Pineda"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent speech modeling relies on explicit attributes such as pitch, content, and speaker identity, but these alone cannot capture the full richness of natural speech. We introduce RT-MAE, a novel masked autoencoder framework that augments the supervised attributes-based modeling with unsupervised residual trainable tokens, designed to encode the information not explained by explicit labeled factors (e.g., timbre variations, noise, emotion etc). Experiments show that RT-MAE improves reconstruction quality, preserving content and speaker similarity while enhancing expressivity. We further demonstrate its applicability to speech enhancement, removing noise at inference while maintaining controllability and naturalness.",
      "url": "https://arxiv.org/abs/2601.19399",
      "pdfUrl": "https://arxiv.org/pdf/2601.19399.pdf",
      "titleJa": "残差トークンは音声モデリングのためのマスクオートエンコーダを強化する"
    },
    {
      "id": "2601.19297",
      "arxivId": "2601.19297",
      "title": "Phase-Retrieval-Based Physics-Informed Neural Networks For Acoustic Magnitude Field Reconstruction",
      "authors": [
        "Karl Schrader",
        "Shoichi Koyama",
        "Tomohiko Nakamura",
        "Mirco Pezzoli"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We propose a method for estimating the magnitude distribution of an acoustic field from spatially sparse magnitude measurements. Such a method is useful when phase measurements are unreliable or inaccessible. Physics-informed neural networks (PINNs) have shown promise for sound field estimation by incorporating constraints derived from governing partial differential equations (PDEs) into neural networks. However, they do not extend to settings where phase measurements are unavailable, as the loss function based on the governing PDE relies on phase information. To remedy this, we propose a phase-retrieval-based PINN for magnitude field estimation. By representing the magnitude and phase distributions with separate networks, the PDE loss can be computed based on the reconstructed complex amplitude. We demonstrate the effectiveness of our phase-retrieval-based PINN through experimental evaluation.",
      "url": "https://arxiv.org/abs/2601.19297",
      "pdfUrl": "https://arxiv.org/pdf/2601.19297.pdf",
      "titleJa": "音響振幅場再構成のための位相回復ベースの物理学に基づくニューラルネットワーク"
    },
    {
      "id": "2601.19113",
      "arxivId": "2601.19113",
      "title": "A Hybrid Discriminative and Generative System for Universal Speech Enhancement",
      "authors": [
        "Yinghao Liu",
        "Chengwei Liu",
        "Xiaotao Liang",
        "Haoyin Yan",
        "Shaofei Xue",
        "Zheng Xue"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Universal speech enhancement aims at handling inputs with various speech distortions and recording conditions. In this work, we propose a novel hybrid architecture that synergizes the signal fidelity of discriminative modeling with the reconstruction capabilities of generative modeling. Our system utilizes the discriminative TF-GridNet model with the Sampling-Frequency-Independent strategy to handle variable sampling rates universally. In parallel, an autoregressive model combined with spectral mapping modeling generates detail-rich speech while effectively suppressing generative artifacts. Finally, a fusion network learns adaptive weights of the two outputs under the optimization of signal-level losses and the comprehensive Speech Quality Assessment (SQA) loss. Our proposed system is evaluated in the ICASSP 2026 URGENT Challenge (Track 1) and ranks the third place.",
      "url": "https://arxiv.org/abs/2601.19113",
      "pdfUrl": "https://arxiv.org/pdf/2601.19113.pdf",
      "titleJa": "ユニバーサル音声強調のためのハイブリッド識別・生成システム"
    },
    {
      "id": "2601.19112",
      "arxivId": "2601.19112",
      "title": "Uncertainty-Aware 3D Emotional Talking Face Synthesis with Emotion Prior Distillation",
      "authors": [
        "Nanhan Shen",
        "Zhilei Liu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.AI",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Emotional Talking Face synthesis is pivotal in multimedia and signal processing, yet existing 3D methods suffer from two critical challenges: poor audio-vision emotion alignment, manifested as difficult audio emotion extraction and inadequate control over emotional micro-expressions; and a one-size-fits-all multi-view fusion strategy that overlooks uncertainty and feature quality differences, undermining rendering quality. We propose UA-3DTalk, Uncertainty-Aware 3D Emotional Talking Face Synthesis with emotion prior distillation, which has three core modules: the Prior Extraction module disentangles audio into content-synchronized features for alignment and person-specific complementary features for individualization; the Emotion Distillation module introduces a multi-modal attention-weighted fusion mechanism and 4D Gaussian encoding with multi-resolution code-books, enabling fine-grained audio emotion extraction and precise control of emotional micro-expressions; the Uncertainty-based Deformation deploys uncertainty blocks to estimate view-specific aleatoric (input noise) and epistemic (model parameters) uncertainty, realizing adaptive multi-view fusion and incorporating a multi-head decoder for Gaussian primitive optimization to mitigate the limitations of uniform-weight fusion. Extensive experiments on regular and emotional datasets show UA-3DTalk outperforms state-of-the-art methods like DEGSTalk and EDTalk by 5.2% in E-FID for emotion alignment, 3.1% in SyncC for lip synchronization, and 0.015 in LPIPS for rendering quality. Project page: https://mrask999.github.io/UA-3DTalk",
      "url": "https://arxiv.org/abs/2601.19112",
      "pdfUrl": "https://arxiv.org/pdf/2601.19112.pdf",
      "titleJa": "感情事前抽出による不確実性を考慮した3D感情会話顔合成"
    },
    {
      "id": "2601.19109",
      "arxivId": "2601.19109",
      "title": "Interpretable and Perceptually-Aligned Music Similarity with Pretrained Embeddings",
      "authors": [
        "Arhan Vohra",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Perceptual similarity representations enable music retrieval systems to determine which songs sound most similar to listeners. State-of-the-art approaches based on task-specific training via self-supervised metric learning show promising alignment with human judgment, but are difficult to interpret or generalize due to limited dataset availability. We show that pretrained text-audio embeddings (CLAP and MuQ-MuLan) offer comparable perceptual alignment on similarity tasks without any additional fine-tuning. To surpass this baseline, we introduce a novel method to perceptually align pretrained embeddings with source separation and linear optimization on ABX preference data from listening tests. Our model provides interpretable and controllable instrument-wise weights, allowing music producers to retrieve stem-level loops and samples based on mixed reference songs.",
      "url": "https://arxiv.org/abs/2601.19109",
      "pdfUrl": "https://arxiv.org/pdf/2601.19109.pdf",
      "titleJa": "事前学習済みの埋め込みによる解釈可能かつ知覚的に整合された音楽類似性"
    },
    {
      "id": "2601.19063",
      "arxivId": "2601.19063",
      "title": "Optimizing Conversational Quality in Spoken Dialogue Systems with Reinforcement Learning from AI Feedback",
      "authors": [
        "Siddhant Arora",
        "Jinchuan Tian",
        "Jiatong Shi",
        "Hayato Futami",
        "Yosuke Kashiwagi",
        "Emiru Tsunoo",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "Reinforcement learning from human or AI feedback (RLHF/RLAIF) for speech-in/speech-out dialogue systems (SDS) remains underexplored, with prior work largely limited to single semantic rewards applied at the utterance level. Such setups overlook the multi-dimensional and multi-modal nature of conversational quality, which encompasses semantic coherence, audio naturalness, speaker consistency, emotion alignment, and turn-taking behavior. Moreover, they are fundamentally mismatched with duplex spoken dialogue systems that generate responses incrementally, where agents must make decisions based on partial utterances. We address these limitations with the first multi-reward RLAIF framework for SDS, combining semantic, audio-quality, and emotion-consistency rewards. To align utterance-level preferences with incremental, blockwise decoding in duplex models, we apply turn-level preference sampling and aggregate per-block log-probabilities within a single DPO objective. We present the first systematic study of preference learning for improving SDS quality in both multi-turn Chain-of-Thought and blockwise duplex models, and release a multi-reward DPO dataset to support reproducible research. Experiments show that single-reward RLAIF selectively improves its targeted metric, while joint multi-reward training yields consistent gains across semantic quality and audio naturalness. These results highlight the importance of holistic, multi-reward alignment for practical conversational SDS.",
      "url": "https://arxiv.org/abs/2601.19063",
      "pdfUrl": "https://arxiv.org/pdf/2601.19063.pdf",
      "titleJa": "AIフィードバックからの強化学習による音声対話システムにおける会話品質の最適化"
    },
    {
      "id": "2601.19029",
      "arxivId": "2601.19029",
      "title": "Audio Foundation Models Outperform Symbolic Representations for Piano Performance Evaluation",
      "authors": [
        "Jai Dhiman"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Automated piano performance evaluation traditionally relies on symbolic (MIDI) representations, which capture note-level information but miss the acoustic nuances that characterize expressive playing. I propose using pre-trained audio foundation models, specifically MuQ and MERT, to predict 19 perceptual dimensions of piano performance quality. Using synthesized audio from PercePiano MIDI files (rendered via Pianoteq), I compare audio and symbolic approaches under controlled conditions where both derive from identical source data. The best model, MuQ layers 9-12 with Pianoteq soundfont augmentation, achieves R^2 = 0.537 (95% CI: [0.465, 0.575]), representing a 55% improvement over the symbolic baseline (R^2 = 0.347). Statistical analysis confirms significance (p < 10^-25) with audio outperforming symbolic on all 19 dimensions. I validate the approach through cross-soundfont generalization (R^2 = 0.534 +/- 0.075), difficulty correlation with an external dataset (rho = 0.623), and multi-performer consistency analysis. Analysis of audio-symbolic fusion reveals high error correlation (r = 0.738), explaining why fusion provides minimal benefit: audio representations alone are sufficient. I release the complete training pipeline, pretrained models, and inference code.",
      "url": "https://arxiv.org/abs/2601.19029",
      "pdfUrl": "https://arxiv.org/pdf/2601.19029.pdf",
      "titleJa": "ピアノ演奏評価において、オーディオ基礎モデルは記号表現よりも優れている"
    },
    {
      "id": "2601.19017",
      "arxivId": "2601.19017",
      "title": "A Framework for Evaluating Faithfulness in Explainable AI for Machine Anomalous Sound Detection Using Frequency-Band Perturbation",
      "authors": [
        "Alexander Buck",
        "Georgina Cosma",
        "Iain Phillips",
        "Paul Conway",
        "Patrick Baker"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Explainable AI (XAI) is commonly applied to anomalous sound detection (ASD) models to identify which time-frequency regions of an audio signal contribute to an anomaly decision. However, most audio explanations rely on qualitative inspection of saliency maps, leaving open the question of whether these attributions accurately reflect the spectral cues the model uses. In this work, we introduce a new quantitative framework for evaluating XAI faithfulness in machine-sound analysis by directly linking attribution relevance to model behaviour through systematic frequency-band removal. This approach provides an objective measure of whether an XAI method for machine ASD correctly identifies frequency regions that influence an ASD model's predictions. By using four widely adopted methods, namely Integrated Gradients, Occlusion, Grad-CAM and SmoothGrad, we show that XAI techniques differ in reliability, with Occlusion demonstrating the strongest alignment with true model sensitivity and gradient-+based methods often failing to accurately capture spectral dependencies. The proposed framework offers a reproducible way to benchmark audio explanations and enables more trustworthy interpretation of spectrogram-based ASD systems.",
      "url": "https://arxiv.org/abs/2601.19017",
      "pdfUrl": "https://arxiv.org/pdf/2601.19017.pdf",
      "titleJa": "周波数帯域摂動を用いた機械異常音検出のための説明可能AIの忠実性評価フレームワーク"
    },
    {
      "id": "2601.18908",
      "arxivId": "2601.18908",
      "title": "Enhancing Speech Emotion Recognition using Dynamic Spectral Features and Kalman Smoothing",
      "authors": [
        "Marouane El Hizabri",
        "Abdelfattah Bezzaz",
        "Ismail Hayoukane",
        "Youssef Taki"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Speech Emotion Recognition systems often use static features like Mel-Frequency Cepstral Coefficients (MFCCs), Zero Crossing Rate (ZCR), and Root Mean Square Energy (RMSE). Because of this, they can misclassify emotions when there is acoustic noise in vocal signals. To address this, we added dynamic features using Dynamic Spectral features (Deltas and Delta-Deltas) along with the Kalman Smoothing algorithm. This approach reduces noise and improves emotion classification. Since emotion changes over time, the Kalman Smoothing filter also helped make the classifier outputs more stable. Tests on the RAVDESS dataset showed that this method achieved a state-of-the-art accuracy of 87\\% and reduced misclassification between emotions with similar acoustic features",
      "url": "https://arxiv.org/abs/2601.18908",
      "pdfUrl": "https://arxiv.org/pdf/2601.18908.pdf",
      "titleJa": "動的スペクトル特徴とカルマンスムージングを用いた音声感情認識の強化"
    },
    {
      "id": "2601.18904",
      "arxivId": "2601.18904",
      "title": "SICL-AT: Another way to adapt Auditory LLM to low-resource task",
      "authors": [
        "Haolong Zheng",
        "Siyin Wang",
        "Zengrui Jin",
        "Mark Hasegawa-Johnson"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Auditory Large Language Models (LLMs) have demonstrated strong performance across a wide range of speech and audio understanding tasks. Nevertheless, they often struggle when applied to low-resource or unfamiliar tasks. In case of labeled in-domain data is scarce or mismatched to the true test distribution, direct fine-tuning can be brittle. In-Context Learning (ICL) provides a training-free, inference-time solution by adapting auditory LLMs through conditioning on a few in-domain demonstrations. In this work, we first show that \\emph{Vanilla ICL}, improves zero-shot performance across diverse speech and audio tasks for selected models which suggest this ICL adaptation capability can be generalized to multimodal setting. Building on this, we propose \\textbf{Speech In-Context Learning Adaptation Training (SICL-AT)}, a post-training recipe utilizes only high resource speech data intending to strengthen model's in-context learning capability. The enhancement can generalize to audio understanding/reasoning task. Experiments indicate our proposed method consistently outperforms direct fine-tuning in low-resource scenario.",
      "url": "https://arxiv.org/abs/2601.18904",
      "pdfUrl": "https://arxiv.org/pdf/2601.18904.pdf",
      "titleJa": "SICL-AT: 聴覚LLMを低リソースタスクに適応させるもう一つの方法"
    },
    {
      "id": "2601.18899",
      "arxivId": "2601.18899",
      "title": "Language Family Matters: Evaluating LLM-Based ASR Across Linguistic Boundaries",
      "authors": [
        "Yuchen Zhang",
        "Ravi Shekhar",
        "Haralambos Mouratidis"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Large Language Model (LLM)-powered Automatic Speech Recognition (ASR) systems achieve strong performance with limited resources by linking a frozen speech encoder to a pretrained LLM via a lightweight connector. Prior work trains a separate connector per language, overlooking linguistic relatedness. We propose an efficient and novel connector-sharing strategy based on linguistic family membership, enabling one connector per family, and empirically validate its effectiveness across two multilingual LLMs and two real-world corpora spanning curated and crowd-sourced speech. Our results show that family-based connectors reduce parameter count while improving generalization across domains, offering a practical and scalable strategy for multilingual ASR deployment.",
      "url": "https://arxiv.org/abs/2601.18899",
      "pdfUrl": "https://arxiv.org/pdf/2601.18899.pdf",
      "titleJa": "言語ファミリーの重要性：言語の境界を越えたLLMベースのASRの評価"
    },
    {
      "id": "2601.19702",
      "arxivId": "2601.19702",
      "title": "SAM Audio Judge: A Unified Multimodal Framework for Perceptual Evaluation of Audio Separation",
      "authors": [
        "Helin Wang",
        "Bowen Shi",
        "Andros Tjandra",
        "John Hoffman",
        "Yi-Chiao Wu",
        "Apoorv Vyas",
        "Najim Dehak",
        "Ann Lee",
        "Wei-Ning Hsu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "The performance evaluation remains a complex challenge in audio separation, and existing evaluation metrics are often misaligned with human perception, course-grained, relying on ground truth signals. On the other hand, subjective listening tests remain the gold standard for real-world evaluation, but they are expensive, time-consuming, and difficult to scale. This paper addresses the growing need for automated systems capable of evaluating audio separation without human intervention. The proposed evaluation metric, SAM Audio Judge (SAJ), is a multimodal fine-grained reference-free objective metric, which shows highly alignment with human perceptions. SAJ supports three audio domains (speech, music and general sound events) and three prompt inputs (text, visual and span), covering four different dimensions of evaluation (recall, percision, faithfulness, and overall). SAM Audio Judge also shows potential applications in data filtering, pseudo-labeling large datasets and reranking in audio separation models. We release our code and pre-trained models at: https://github.com/facebookresearch/sam-audio.",
      "url": "https://arxiv.org/abs/2601.19702",
      "pdfUrl": "https://arxiv.org/pdf/2601.19702.pdf",
      "titleJa": "SAM Audio Judge: 音声分離の知覚評価のための統合マルチモーダルフレームワーク"
    },
    {
      "id": "2601.19573",
      "arxivId": "2601.19573",
      "title": "Audio Deepfake Detection at the First Greeting: \"Hi!\"",
      "authors": [
        "Haohan Shi",
        "Xiyu Shi",
        "Safak Dogan",
        "Tianjin Huang",
        "Yunxiao Zhang"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This paper focuses on audio deepfake detection under real-world communication degradations, with an emphasis on ultra-short inputs (0.5-2.0s), targeting the capability to detect synthetic speech at a conversation opening, e.g., when a scammer says \"Hi.\" We propose Short-MGAA (S-MGAA), a novel lightweight extension of Multi-Granularity Adaptive Time-Frequency Attention, designed to enhance discriminative representation learning for short, degraded inputs subjected to communication processing and perturbations. The S-MGAA integrates two tailored modules: a Pixel-Channel Enhanced Module (PCEM) that amplifies fine-grained time-frequency saliency, and a Frequency Compensation Enhanced Module (FCEM) to supplement limited temporal evidence via multi-scale frequency modeling and adaptive frequency-temporal interaction. Extensive experiments demonstrate that S-MGAA consistently surpasses nine state-of-the-art baselines while achieving strong robustness to degradations and favorable efficiency-accuracy trade-offs, including low RTF, competitive GFLOPs, compact parameters, and reduced training cost, highlighting its strong potential for real-time deployment in communication systems and edge devices.",
      "url": "https://arxiv.org/abs/2601.19573",
      "pdfUrl": "https://arxiv.org/pdf/2601.19573.pdf",
      "titleJa": "最初の挨拶「こんにちは！」でのオーディオディープフェイク検出"
    },
    {
      "id": "2601.19491",
      "arxivId": "2601.19491",
      "title": "Permutation-Invariant Physics-Informed Neural Network for Region-to-Region Sound Field Reconstruction",
      "authors": [
        "Xingyu Chen",
        "Sipei Zhao",
        "Fei Ma",
        "Eva Cheng",
        "Ian S. Burnett"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Most existing sound field reconstruction methods target point-to-region reconstruction, interpolating the Acoustic Transfer Functions (ATFs) between a fixed-position sound source and a receiver region. The applicability of these methods is limited because real-world ATFs tend to varying continuously with respect to the positions of sound sources and receiver regions. This paper presents a permutation-invariant physics-informed neural network for region-to-region sound field reconstruction, which aims to interpolate the ATFs across continuously varying sound sources and measurement regions. The proposed method employs a deep set architecture to process the receiver and sound source positions as an unordered set, preserving acoustic reciprocity. Furthermore, it incorporates the Helmholtz equation as a physical constraint to guide network training, ensuring physically consistent predictions.",
      "url": "https://arxiv.org/abs/2601.19491",
      "pdfUrl": "https://arxiv.org/pdf/2601.19491.pdf",
      "titleJa": "領域間音場再構成のための順列不変な物理学に基づくニューラルネットワーク"
    },
    {
      "id": "2601.19194",
      "arxivId": "2601.19194",
      "title": "SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper",
      "authors": [
        "Alexander Polok",
        "Dominik Klement",
        "Samuele Cornell",
        "Matthew Wiesner",
        "Jan Černocký",
        "Sanjeev Khudanpur",
        "Lukáš Burget"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Speaker-attributed automatic speech recognition (ASR) in multi-speaker environments remains a major challenge. While some approaches achieve strong performance when fine-tuned on specific domains, few systems generalize well across out-of-domain datasets. Our prior work, Diarization-Conditioned Whisper (DiCoW), leverages speaker diarization outputs as conditioning information and, with minimal fine-tuning, demonstrated strong multilingual and multi-domain performance. In this paper, we address a key limitation of DiCoW: ambiguity in Silence-Target-Non-target-Overlap (STNO) masks, where two or more fully overlapping speakers may have nearly identical conditioning despite differing transcriptions. We introduce SE-DiCoW (Self-Enrolled Diarization-Conditioned Whisper), which uses diarization output to locate an enrollment segment anywhere in the conversation where the target speaker is most active. This enrollment segment is used as fixed conditioning via cross-attention at each encoder layer. We further refine DiCoW with improved data segmentation, model initialization, and augmentation. Together, these advances yield substantial gains: SE-DiCoW reduces macro-averaged tcpWER by 52.4% relative to the original DiCoW on the EMMA MT-ASR benchmark.",
      "url": "https://arxiv.org/abs/2601.19194",
      "pdfUrl": "https://arxiv.org/pdf/2601.19194.pdf",
      "titleJa": "SE-DiCoW: 自己登録型ダイアライゼーション条件付きウィスパー"
    },
    {
      "id": "2601.19153",
      "arxivId": "2601.19153",
      "title": "LuSeeL: Language-queried Binaural Universal Sound Event Extraction and Localization",
      "authors": [
        "Zexu Pan",
        "Shengkui Zhao",
        "Yukun Ma",
        "Haoxu Wang",
        "Yiheng Jiang",
        "Biao Tian",
        "Bin Ma"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Most universal sound extraction algorithms focus on isolating a target sound event from single-channel audio mixtures. However, the real world is three-dimensional, and binaural audio, which mimics human hearing, can capture richer spatial information, including sound source location. This spatial context is crucial for understanding and modeling complex auditory scenes, as it inherently informs sound detection and extraction. In this work, we propose a language-driven universal sound extraction network that isolates text-described sound events from binaural mixtures by effectively leveraging the spatial cues present in binaural signals. Additionally, we jointly predict the direction of arrival (DoA) of the target sound using spatial features from the extraction network. This dual-task approach exploits complementary location information to improve extraction performance while enabling accurate DoA estimation. Experimental results on the in-the-wild AudioCaps dataset show that our proposed LuSeeL model significantly outperforms single-channel and uni-task baselines.",
      "url": "https://arxiv.org/abs/2601.19153",
      "pdfUrl": "https://arxiv.org/pdf/2601.19153.pdf",
      "titleJa": "LuSeeL: 言語クエリによるバイノーラルユニバーサルサウンドイベントの抽出と定位"
    },
    {
      "id": "2601.19130",
      "arxivId": "2601.19130",
      "title": "Beyond Lips: Integrating Gesture and Lip Cues for Robust Audio-visual Speaker Extraction",
      "authors": [
        "Zexu Pan",
        "Xinyuan Qian",
        "Shengkui Zhao",
        "Kun Zhou",
        "Bin Ma"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Most audio-visual speaker extraction methods rely on synchronized lip recording to isolate the speech of a target speaker from a multi-talker mixture. However, in natural human communication, co-speech gestures are also temporally aligned with speech, often emphasizing specific words or syllables. These gestures provide complementary visual cues that can be especially valuable when facial or lip regions are occluded or distant. In this work, we move beyond lip-centric approaches and propose SeLG, a model that integrates both lip and upper-body gesture information for robust speaker extraction. SeLG features a cross-attention-based fusion mechanism that enables each visual modality to query and selectively attend to relevant speech features in the mixture. To improve the alignment of gesture representations with speech dynamics, SeLG also employs a contrastive InfoNCE loss that encourages gesture embeddings to align more closely with corresponding lip embeddings, which are more strongly correlated with speech. Experimental results on the YGD dataset, containing TED talks, demonstrate that the proposed contrastive learning strategy significantly improves gesture-based speaker extraction, and that our proposed SeLG model, by effectively fusing lip and gesture cues with an attention mechanism and InfoNCE loss, achieves superior performance compared to baselines, across both complete and partial (i.e., missing-modality) conditions.",
      "url": "https://arxiv.org/abs/2601.19130",
      "pdfUrl": "https://arxiv.org/pdf/2601.19130.pdf",
      "titleJa": "唇を超えて：ジェスチャーと唇の手がかりを統合した堅牢なオーディオビジュアル話者抽出"
    },
    {
      "id": "2601.18766",
      "arxivId": "2601.18766",
      "title": "Learning to Discover: A Generalized Framework for Raga Identification without Forgetting",
      "authors": [
        "Parampreet Singh",
        "Somya Kumar",
        "Chaitanya Shailendra Nitawe",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.",
      "url": "https://arxiv.org/abs/2601.18766",
      "pdfUrl": "https://arxiv.org/pdf/2601.18766.pdf",
      "titleJa": "発見することを学ぶ：忘れずにラーガを識別するための一般化された枠組み"
    },
    {
      "id": "2601.18535",
      "arxivId": "2601.18535",
      "title": "Audio Inpainting in Time-Frequency Domain with Phase-Aware Prior",
      "authors": [
        "Peter Balušík",
        "Pavel Rajmic"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "The so-called audio inpainting problem in the time domain refers to estimating missing segments of samples within a signal. Over the years, several methods have been developed for such type of audio inpainting. In contrast to this case, a time-frequency variant of inpainting appeared in the literature, where the challenge is to reconstruct missing spectrogram columns with reliable information. We propose a method to address this time-frequency audio inpainting problem. Our approach is based on the recently introduced phase-aware signal prior that exploits an estimate of the instantaneous frequency. An optimization problem is formulated and solved using the generalized Chambolle-Pock algorithm. The proposed method is evaluated both objectively and subjectively against other time-frequency inpainting methods, specifically a deep-prior neural network and the autoregression-based approach known as Janssen-TF. Our proposed approach surpassed these methods in the objective evaluation as well as in the conducted listening test. Moreover, this outcome is achieved with a substantially reduced computational requirement compared to alternative methods.",
      "url": "https://arxiv.org/abs/2601.18535",
      "pdfUrl": "https://arxiv.org/pdf/2601.18535.pdf",
      "titleJa": "位相を考慮した事前分布を用いた時間周波数領域におけるオーディオ修復"
    },
    {
      "id": "2601.18456",
      "arxivId": "2601.18456",
      "title": "Geneses: Unified Generative Speech Enhancement and Separation",
      "authors": [
        "Kohei Asai",
        "Wataru Nakata",
        "Yuki Saito",
        "Hiroshi Saruwatari"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Real-world audio recordings often contain multiple speakers and various degradations, which limit both the quantity and quality of speech data available for building state-of-the-art speech processing models. Although end-to-end approaches that concatenate speech enhancement (SE) and speech separation (SS) to obtain a clean speech signal for each speaker are promising, conventional SE-SS methods suffer from complex degradations beyond additive noise. To this end, we propose \\textbf{Geneses}, a generative framework to achieve unified, high-quality SE--SS. Our Geneses leverages latent flow matching to estimate each speaker's clean speech features using multi-modal diffusion Transformer conditioned on self-supervised learning representation from noisy mixture. We conduct experimental evaluation using two-speaker mixtures from LibriTTS-R under two conditions: additive-noise-only and complex degradations. The results demonstrate that Geneses significantly outperforms a conventional mask-based SE--SS method across various objective metrics with high robustness against complex degradations. Audio samples are available in our demo page.",
      "url": "https://arxiv.org/abs/2601.18456",
      "pdfUrl": "https://arxiv.org/pdf/2601.18456.pdf",
      "titleJa": "Geneses: 統合型生成音声強調および分離"
    },
    {
      "id": "2601.18415",
      "arxivId": "2601.18415",
      "title": "Pisets: A Robust Speech Recognition System for Lectures and Interviews",
      "authors": [
        "Ivan Bondarenko",
        "Daniil Grebenkin",
        "Oleg Sedukhin",
        "Mikhail Klementev",
        "Roman Derunets",
        "Lyudmila Budneva"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.",
      "url": "https://arxiv.org/abs/2601.18415",
      "pdfUrl": "https://arxiv.org/pdf/2601.18415.pdf",
      "titleJa": "Pisets: 講義やインタビューのための堅牢な音声認識システム"
    },
    {
      "id": "2601.18396",
      "arxivId": "2601.18396",
      "title": "Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder",
      "authors": [
        "Zhengyang Li",
        "Thomas Graave",
        "Björn Möller",
        "Zehang Wu",
        "Matthias Franz",
        "Tim Fingscheidt"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.CV",
        "cs.SD"
      ],
      "abstract": "In audiovisual automatic speech recognition (AV-ASR) systems, information fusion of visual features in a pre-trained ASR has been proven as a promising method to improve noise robustness. In this work, based on the prominent Whisper ASR, first, we propose a simple and effective visual fusion method -- use of visual features both in encoder and decoder (dual-use) -- to learn the audiovisual interactions in the encoder and to weigh modalities in the decoder. Second, we compare visual fusion methods in Whisper models of various sizes. Our proposed dual-use method shows consistent noise robustness improvement, e.g., a 35% relative improvement (WER: 4.41% vs. 6.83%) based on Whisper small, and a 57% relative improvement (WER: 4.07% vs. 9.53%) based on Whisper medium, compared to typical reference middle fusion in babble noise with a signal-to-noise ratio (SNR) of 0dB. Third, we conduct ablation studies examining the impact of various module designs and fusion options. Fine-tuned on 1929 hours of audiovisual data, our dual-use method using Whisper medium achieves 4.08% (MUSAN babble noise) and 4.43% (NoiseX babble noise) average WER across various SNRs, thereby establishing a new state-of-the-art in noisy conditions on the LRS3 AV-ASR benchmark. Our code is at https://github.com/ifnspaml/Dual-Use-AVASR",
      "url": "https://arxiv.org/abs/2601.18396",
      "pdfUrl": "https://arxiv.org/pdf/2601.18396.pdf",
      "titleJa": "ウィスパーエンコーダとデコーダの両方で視覚的特徴を利用したノイズ耐性AV-ASR"
    },
    {
      "id": "2601.18393",
      "arxivId": "2601.18393",
      "title": "OCR-Enhanced Multimodal ASR Can Read While Listening",
      "authors": [
        "Junli Chen",
        "Changli Tang",
        "Yixuan Li",
        "Guangzhi Sun",
        "Chao Zhang"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Visual information, such as subtitles in a movie, often helps automatic speech recognition. In this paper, we propose Donut-Whisper, an audio-visual ASR model with dual encoder to leverage visual information to improve speech recognition performance in both English and Chinese. Donut-Whisper combines the advantage of the linear and the Q-Former-based modality alignment structures via a cross-attention module, generating more powerful audio-visual features. Meanwhile, we propose a lightweight knowledge distillation scheme showcasing the potential of using audio-visual models to teach audio-only models to achieve better performance. Moreover, we propose a new multilingual audio-visual speech recognition dataset based on movie clips containing both Chinese and English partitions. As a result, Donut-Whisper achieved significantly better performance on both English and Chinese partition of the dataset compared to both Donut and Whisper large V3 baselines. In particular, an absolute 5.75% WER reduction and a 16.5% absolute CER reduction were achieved on the English and Chinese sets respectively compared to the Whisper ASR baseline.",
      "url": "https://arxiv.org/abs/2601.18393",
      "pdfUrl": "https://arxiv.org/pdf/2601.18393.pdf",
      "titleJa": "OCR強化マルチモーダルASRは聞きながら読むことができます"
    },
    {
      "id": "2601.18322",
      "arxivId": "2601.18322",
      "title": "Residual Learning for Neural Ambisonics Encoders",
      "authors": [
        "Thomas Deppisch",
        "Yang Gao",
        "Manan Mittal",
        "Benjamin Stahl",
        "Christoph Hold",
        "David Alon",
        "Zamir Ben-Hur"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Emerging wearable devices such as smartglasses and extended reality headsets demand high-quality spatial audio capture from compact, head-worn microphone arrays. Ambisonics provides a device-agnostic spatial audio representation by mapping array signals to spherical harmonic (SH) coefficients. In practice, however, accurate encoding remains challenging. While traditional linear encoders are signal-independent and robust, they amplify low-frequency noise and suffer from high-frequency spatial aliasing. On the other hand, neural network approaches can outperform linear encoders but they often assume idealized microphones and may perform inconsistently in real-world scenarios. To leverage their complementary strengths, we introduce a residual-learning framework that refines a linear encoder with corrections from a neural network. Using measured array transfer functions from smartglasses, we compare a UNet-based encoder from the literature with a new recurrent attention model. Our analysis reveals that both neural encoders only consistently outperform the linear baseline when integrated within the residual learning framework. In the residual configuration, both neural models achieve consistent and significant improvements across all tested metrics for in-domain data and moderate gains for out-of-domain data. Yet, coherence analysis indicates that all neural encoder configurations continue to struggle with directionally accurate high-frequency encoding.",
      "url": "https://arxiv.org/abs/2601.18322",
      "pdfUrl": "https://arxiv.org/pdf/2601.18322.pdf",
      "titleJa": "ニューラル・アンビソニックス・エンコーダのための残差学習"
    },
    {
      "id": "2601.18295",
      "arxivId": "2601.18295",
      "title": "Noise-Robust Contrastive Learning with an MFCC-Conformer For Coronary Artery Disease Detection",
      "authors": [
        "Milan Marocchi",
        "Matthew Fynn",
        "Yue Rong"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Cardiovascular diseases (CVD) are the leading cause of death worldwide, with coronary artery disease (CAD) comprising the largest subcategory of CVDs. Recently, there has been increased focus on detecting CAD using phonocardiogram (PCG) signals, with high success in clinical environments with low noise and optimal sensor placement. Multichannel techniques have been found to be more robust to noise; however, achieving robust performance on real-world data remains a challenge. This work utilises a novel multichannel energy-based noisy-segment rejection algorithm, using heart and noise-reference microphones, to discard audio segments with large amounts of nonstationary noise before training a deep learning classifier. This conformer-based classifier takes mel-frequency cepstral coefficients (MFCCs) from multiple channels, further helping improve the model's noise robustness. The proposed method achieved 78.4% accuracy and 78.2% balanced accuracy on 297 subjects, representing improvements of 4.1% and 4.3%, respectively, compared to training without noisy-segment rejection.",
      "url": "https://arxiv.org/abs/2601.18295",
      "pdfUrl": "https://arxiv.org/pdf/2601.18295.pdf",
      "titleJa": "冠動脈疾患検出のためのMFCCコンフォーマーを用いたノイズロバストな対照学習"
    },
    {
      "id": "2601.19888",
      "arxivId": "2601.19888",
      "title": "M-SGWR: Multiscale Similarity and Geographically Weighted Regression",
      "authors": [
        "M. Naser Lessani",
        "Zhenlong Li",
        "Manzhu Yu",
        "Helen Greatrex",
        "Chan Shen"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "The first law of geography is a cornerstone of spatial analysis, emphasizing that nearby and related locations tend to be more similar, however, defining what constitutes \"near\" and \"related\" remains challenging, as different phenomena exhibit distinct spatial patterns. Traditional local regression models, such as Geographically Weighted Regression (GWR) and Multiscale GWR (MGWR), quantify spatial relationships solely through geographic proximity. In an era of globalization and digital connectivity, however, geographic proximity alone may be insufficient to capture how locations are interconnected. To address this limitation, we propose a new multiscale local regression framework, termed M-SGWR, which characterizes spatial interaction across two dimensions: geographic proximity and attribute (variable) similarity. For each predictor, geographic and attribute-based weight matrices are constructed separately and then combined using an optimized parameter, alpha, which governs their relative contribution to local model fitting. Analogous to variable-specific bandwidths in MGWR, the optimal alpha varies by predictor, allowing the model to flexibly account for geographic, mixed, or non-spatial (remote similarity) effects. Results from two simulation experiments and one empirical application demonstrate that M-SGWR consistently outperforms GWR, SGWR, and MGWR across all goodness-of-fit metrics.",
      "url": "https://arxiv.org/abs/2601.19888",
      "pdfUrl": "https://arxiv.org/pdf/2601.19888.pdf",
      "titleJa": "M-SGWR: マルチスケール類似度と地理的加重回帰"
    },
    {
      "id": "2601.19886",
      "arxivId": "2601.19886",
      "title": "AI Cap-and-Trade: Efficiency Incentives for Accessibility and Sustainability",
      "authors": [
        "Marco Bornstein",
        "Amrit Singh Bedi"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.CY",
        "cs.GT"
      ],
      "abstract": "The race for artificial intelligence (AI) dominance often prioritizes scale over efficiency. Hyper-scaling is the common industry approach: larger models, more data, and as many computational resources as possible. Using more resources is a simpler path to improved AI performance. Thus, efficiency has been de-emphasized. Consequently, the need for costly computational resources has marginalized academics and smaller companies. Simultaneously, increased energy expenditure, due to growing AI use, has led to mounting environmental costs. In response to accessibility and sustainability concerns, we argue for research into, and implementation of, market-based methods that incentivize AI efficiency. We believe that incentivizing efficient operations and approaches will reduce emissions while opening new opportunities for academics and smaller companies. As a call to action, we propose a cap-and-trade system for AI. Our system provably reduces computations for AI deployment, thereby lowering emissions and monetizing efficiency to the benefit of of academics and smaller companies.",
      "url": "https://arxiv.org/abs/2601.19886",
      "pdfUrl": "https://arxiv.org/pdf/2601.19886.pdf",
      "titleJa": "AIキャップ・アンド・トレード：アクセシビリティと持続可能性のための効率性インセンティブ"
    },
    {
      "id": "2601.19839",
      "arxivId": "2601.19839",
      "title": "HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs",
      "authors": [
        "Jeanne Malécot",
        "Hamed Rahimi",
        "Jeanne Cattoni",
        "Marie Samson",
        "Mouad Abrini",
        "Mahdi Khoramshahi",
        "Maribel Pino",
        "Mohamed Chetouani"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "abstract": "Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.",
      "url": "https://arxiv.org/abs/2601.19839",
      "pdfUrl": "https://arxiv.org/pdf/2601.19839.pdf",
      "titleJa": "HARMONI: LLM によるマルチユーザー ヒューマンロボットインタラクションのマルチモーダルパーソナライゼーション"
    },
    {
      "id": "2601.19834",
      "arxivId": "2601.19834",
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "authors": [
        "Jialong Wu",
        "Xiaoying Zhang",
        "Hongyi Yuan",
        "Xiangcheng Zhang",
        "Tianhao Huang",
        "Changjing He",
        "Chaoyi Deng",
        "Renrui Zhang",
        "Youbin Wu",
        "Mingsheng Long"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.",
      "url": "https://arxiv.org/abs/2601.19834",
      "pdfUrl": "https://arxiv.org/pdf/2601.19834.pdf",
      "titleJa": "ビジュアル生成はマルチモーダル世界モデルを通じて人間のような推論を可能にする"
    },
    {
      "id": "2601.19827",
      "arxivId": "2601.19827",
      "title": "When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering",
      "authors": [
        "Mahdi Astaraki",
        "Mohammad Arshi Saloot",
        "Ali Shiraee Kasmaee",
        "Hamidreza Mahyar",
        "Soheila Samiee"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) extends large language models (LLMs) beyond parametric knowledge, yet it is unclear when iterative retrieval-reasoning loops meaningfully outperform static RAG, particularly in scientific domains with multi-hop reasoning, sparse domain knowledge, and heterogeneous evidence. We provide the first controlled, mechanism-level diagnostic study of whether synchronized iterative retrieval and reasoning can surpass an idealized static upper bound (Gold Context) RAG. We benchmark eleven state-of-the-art LLMs under three regimes: (i) No Context, measuring reliance on parametric memory; (ii) Gold Context, where all oracle evidence is supplied at once; and (iii) Iterative RAG, a training-free controller that alternates retrieval, hypothesis refinement, and evidence-aware stopping. Using the chemistry-focused ChemKGMultiHopQA dataset, we isolate questions requiring genuine retrieval and analyze behavior with diagnostics spanning retrieval coverage gaps, anchor-carry drop, query quality, composition fidelity, and control calibration. Across models, Iterative RAG consistently outperforms Gold Context, with gains up to 25.6 percentage points, especially for non-reasoning fine-tuned models. Staged retrieval reduces late-hop failures, mitigates context overload, and enables dynamic correction of early hypothesis drift, but remaining failure modes include incomplete hop coverage, distractor latch trajectories, early stopping miscalibration, and high composition failure rates even with perfect retrieval. Overall, staged retrieval is often more influential than the mere presence of ideal evidence; we provide practical guidance for deploying and diagnosing RAG systems in specialized scientific settings and a foundation for more reliable, controllable iterative retrieval-reasoning frameworks.",
      "url": "https://arxiv.org/abs/2601.19827",
      "pdfUrl": "https://arxiv.org/pdf/2601.19827.pdf",
      "titleJa": "反復型RAGが理想的な証拠に勝ったとき：科学的マルチホップ質問応答の診断的研究"
    },
    {
      "id": "2601.19825",
      "arxivId": "2601.19825",
      "title": "Routing End User Queries to Enterprise Databases",
      "authors": [
        "Saikrishna Sudarshan",
        "Tanay Kulkarni",
        "Manasi Patwardhan",
        "Lovekesh Vig",
        "Ashwin Srinivasan",
        "Tanmay Tulsidas Verlekar"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "abstract": "We address the task of routing natural language queries in multi-database enterprise environments. We construct realistic benchmarks by extending existing NL-to-SQL datasets. Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions. By explicitly modelling schema coverage, structural connectivity, and fine-grained semantic alignment, the proposed modular, reasoning-driven reranking strategy consistently outperforms embedding-only and direct LLM-prompting baselines across all the metrics.",
      "url": "https://arxiv.org/abs/2601.19825",
      "pdfUrl": "https://arxiv.org/pdf/2601.19825.pdf",
      "titleJa": "エンドユーザークエリをエンタープライズデータベースにルーティングする"
    },
    {
      "id": "2601.19824",
      "arxivId": "2601.19824",
      "title": "An Interpretable Recommendation Model for Psychometric Data, With an Application to Gerontological Primary Care",
      "authors": [
        "Andre Paulino de Lima",
        "Paula Castro",
        "Suzana Carvalho Vaz de Andrade",
        "Rosa Maria Marcucci",
        "Ruth Caldeira de Melo",
        "Marcelo Garcia Manzato"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.IR",
        "cs.SI"
      ],
      "abstract": "There are challenges that must be overcome to make recommender systems useful in healthcare settings. The reasons are varied: the lack of publicly available clinical data, the difficulty that users may have in understanding the reasons why a recommendation was made, the risks that may be involved in following that recommendation, and the uncertainty about its effectiveness. In this work, we address these challenges with a recommendation model that leverages the structure of psychometric data to provide visual explanations that are faithful to the model and interpretable by care professionals. We focus on a narrow healthcare niche, gerontological primary care, to show that the proposed recommendation model can assist the attending professional in the creation of personalised care plans. We report results of a comparative offline performance evaluation of the proposed model on healthcare datasets that were collected by research partners in Brazil, as well as the results of a user study that evaluates the interpretability of the visual explanations the model generates. The results suggest that the proposed model can advance the application of recommender systems in this healthcare niche, which is expected to grow in demand , opportunities, and information technology needs as demographic changes become more pronounced.",
      "url": "https://arxiv.org/abs/2601.19824",
      "pdfUrl": "https://arxiv.org/pdf/2601.19824.pdf",
      "titleJa": "老年学プライマリケアへの応用を伴う心理測定データのための解釈可能な推奨モデル"
    },
    {
      "id": "2601.19811",
      "arxivId": "2601.19811",
      "title": "Revisiting Incremental Stochastic Majorization-Minimization Algorithms with Applications to Mixture of Experts",
      "authors": [
        "TrungKhang Tran",
        "TrungTin Nguyen",
        "Gersende Fort",
        "Tung Doan",
        "Hien Duy Nguyen",
        "Binh T. Nguyen",
        "Florence Forbes",
        "Christopher Drovandi"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST",
        "stat.ME"
      ],
      "abstract": "Processing high-volume, streaming data is increasingly common in modern statistics and machine learning, where batch-mode algorithms are often impractical because they require repeated passes over the full dataset. This has motivated incremental stochastic estimation methods, including the incremental stochastic Expectation-Maximization (EM) algorithm formulated via stochastic approximation. In this work, we revisit and analyze an incremental stochastic variant of the Majorization-Minimization (MM) algorithm, which generalizes incremental stochastic EM as a special case. Our approach relaxes key EM requirements, such as explicit latent-variable representations, enabling broader applicability and greater algorithmic flexibility. We establish theoretical guarantees for the incremental stochastic MM algorithm, proving consistency in the sense that the iterates converge to a stationary point characterized by a vanishing gradient of the objective. We demonstrate these advantages on a softmax-gated mixture of experts (MoE) regression problem, for which no stochastic EM algorithm is available. Empirically, our method consistently outperforms widely used stochastic optimizers, including stochastic gradient descent, root mean square propagation, adaptive moment estimation, and second-order clipped stochastic optimization. These results support the development of new incremental stochastic algorithms, given the central role of softmax-gated MoE architectures in contemporary deep neural networks for heterogeneous data modeling. Beyond synthetic experiments, we also validate practical effectiveness on two real-world datasets, including a bioinformatics study of dent maize genotypes under drought stress that integrates high-dimensional proteomics with ecophysiological traits, where incremental stochastic MM yields stable gains in predictive performance.",
      "url": "https://arxiv.org/abs/2601.19811",
      "pdfUrl": "https://arxiv.org/pdf/2601.19811.pdf",
      "titleJa": "漸進的確率的主要化-最小化アルゴリズムの再検討と専門家混合への応用"
    },
    {
      "id": "2601.19810",
      "arxivId": "2601.19810",
      "title": "Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals",
      "authors": [
        "Octavio Pappalardo"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "abstract": "Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent's post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent's capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula.",
      "url": "https://arxiv.org/abs/2601.19810",
      "pdfUrl": "https://arxiv.org/pdf/2601.19810.pdf",
      "titleJa": "効率的な探索のための教師なし学習：自己課せられた目標による適応型ポリシーの事前学習"
    },
    {
      "id": "2601.19793",
      "arxivId": "2601.19793",
      "title": "CASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration via Context-Aware Strategy for Task Efficient Routing",
      "authors": [
        "Shanyv Liu",
        "Xuyang Yuan",
        "Tao Chen",
        "Zijun Zhan",
        "Zhu Han",
        "Danyang Zheng",
        "Weishan Zhang",
        "Shaohua Cao"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Graph-based Multi-Agent Systems (MAS) enable complex cyclic workflows but suffer from inefficient static model allocation, where deploying strong models uniformly wastes computation on trivial sub-tasks. We propose CASTER (Context-Aware Strategy for Task Efficient Routing), a lightweight router for dynamic model selection in graph-based MAS. CASTER employs a Dual-Signal Router that combines semantic embeddings with structural meta-features to estimate task difficulty. During training, the router self-optimizes through a Cold Start to Iterative Evolution paradigm, learning from its own routing failures via on-policy negative feedback. Experiments using LLM-as-a-Judge evaluation across Software Engineering, Data Analysis, Scientific Discovery, and Cybersecurity demonstrate that CASTER reduces inference cost by up to 72.4% compared to strong-model baselines while matching their success rates, and consistently outperforms both heuristic routing and FrugalGPT across all domains.",
      "url": "https://arxiv.org/abs/2601.19793",
      "pdfUrl": "https://arxiv.org/pdf/2601.19793.pdf",
      "titleJa": "CASTER: タスク効率の高いルーティングのためのコンテキストアウェア戦略によるマルチエージェントオーケストレーションのコストパフォーマンスの壁を打破"
    },
    {
      "id": "2601.19792",
      "arxivId": "2601.19792",
      "title": "LVLMs and Humans Ground Differently in Referential Communication",
      "authors": [
        "Peter Zeng",
        "Weiling Li",
        "Amie Paige",
        "Zhengxiang Wang",
        "Panagiotis Kaliosis",
        "Dimitris Samaras",
        "Gregory Zelinsky",
        "Susan Brennan",
        "Owen Rambow"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "abstract": "For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that interact with multiple turns in repeated rounds to match pictures of objects not associated with any obvious lexicalized labels. We release the online pipeline for data collection, the tools and analyses for accuracy, efficiency, and lexical overlap, and a corpus of 356 dialogues (89 pairs over 4 rounds each) that unmasks LVLMs' limitations in interactively resolving referring expressions, a crucial skill that underlies human language use.",
      "url": "https://arxiv.org/abs/2601.19792",
      "pdfUrl": "https://arxiv.org/pdf/2601.19792.pdf",
      "titleJa": "LVLMと人間は参照コミュニケーションにおいて異なる基盤を持つ"
    },
    {
      "id": "2601.19778",
      "arxivId": "2601.19778",
      "title": "Reimagining Peer Review Process Through Multi-Agent Mechanism Design",
      "authors": [
        "Ahmad Farooq",
        "Kamran Iqbal"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CY",
        "cs.GT",
        "cs.SE"
      ],
      "abstract": "The software engineering research community faces a systemic crisis: peer review is failing under growing submissions, misaligned incentives, and reviewer fatigue. Community surveys reveal that researchers perceive the process as \"broken.\" This position paper argues that these dysfunctions are mechanism design failures amenable to computational solutions. We propose modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design incentive-compatible protocols. We outline three interventions: a credit-based submission economy, MARL-optimized reviewer assignment, and hybrid verification of review consistency. We present threat models, equity considerations, and phased pilot metrics. This vision charts a research agenda toward sustainable peer review.",
      "url": "https://arxiv.org/abs/2601.19778",
      "pdfUrl": "https://arxiv.org/pdf/2601.19778.pdf",
      "titleJa": "マルチエージェントメカニズム設計によるピアレビュープロセスの再構築"
    },
    {
      "id": "2601.19768",
      "arxivId": "2601.19768",
      "title": "GAVEL: Towards rule-based safety through activation monitoring",
      "authors": [
        "Shir Rozenfeld",
        "Rahul Pankajakshan",
        "Itay Zloczower",
        "Eyal Lenga",
        "Gilad Gressel",
        "Yisroel Mirsky"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "abstract": "Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.",
      "url": "https://arxiv.org/abs/2601.19768",
      "pdfUrl": "https://arxiv.org/pdf/2601.19768.pdf",
      "titleJa": "GAVEL: アクティベーション監視によるルールベースの安全性の実現"
    },
    {
      "id": "2601.19752",
      "arxivId": "2601.19752",
      "title": "Agentic Design Patterns: A System-Theoretic Framework",
      "authors": [
        "Minh-Dung Dao",
        "Quy Minh Le",
        "Hoang Thanh Lam",
        "Duc-Trong Le",
        "Quoc-Viet Pham",
        "Barry O'Sullivan",
        "Hoang D. Nguyen"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.AI"
      ],
      "abstract": "With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning & World Model, Perception & Grounding, Action Execution, Learning & Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.",
      "url": "https://arxiv.org/abs/2601.19752",
      "pdfUrl": "https://arxiv.org/pdf/2601.19752.pdf",
      "titleJa": "エージェントデザインパターン：システム理論的フレームワーク"
    },
    {
      "id": "2601.19747",
      "arxivId": "2601.19747",
      "title": "Veri-Sure: A Contract-Aware Multi-Agent Framework with Temporal Tracing and Formal Verification for Correct RTL Code Generation",
      "authors": [
        "Jiale Liu",
        "Taiyu Zhou",
        "Tianqi Jiang"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.SE"
      ],
      "abstract": "In the rapidly evolving field of Electronic Design Automation (EDA), the deployment of Large Language Models (LLMs) for Register-Transfer Level (RTL) design has emerged as a promising direction. However, silicon-grade correctness remains bottlenecked by: (i) limited test coverage and reliability of simulation-centric evaluation, (ii) regressions and repair hallucinations introduced by iterative debugging, and (iii) semantic drift as intent is reinterpreted across agent handoffs. In this work, we propose Veri-Sure, a multi-agent framework that establishes a design contract to align agents' intent and uses a patching mechanism guided by static dependency slicing to perform precise, localized repairs. By integrating a multi-branch verification pipeline that combines trace-driven temporal analysis with formal verification consisting of assertion-based checking and boolean equivalence proofs, Veri-Sure enables functional correctness beyond pure simulations. We also introduce VerilogEval-v2-EXT, extending the original benchmark with 53 more industrial-grade design tasks and stratified difficulty levels, and show that Veri-Sure achieves state-of-the-art verified-correct RTL code generation performance, surpassing standalone LLMs and prior agentic systems.",
      "url": "https://arxiv.org/abs/2601.19747",
      "pdfUrl": "https://arxiv.org/pdf/2601.19747.pdf",
      "titleJa": "Veri-Sure: 正しい RTL コード生成のための時間的トレースと形式検証を備えた契約対応マルチエージェントフレームワーク"
    },
    {
      "id": "2601.19739",
      "arxivId": "2601.19739",
      "title": "TokenSeek: Memory Efficient Fine Tuning via Instance-Aware Token Ditching",
      "authors": [
        "Runjia Zeng",
        "Qifan Wang",
        "Qiang Guan",
        "Ruixiang Tang",
        "Lifu Huang",
        "Zhenting Wang",
        "Xueling Zhang",
        "Cheng Han",
        "Dongfang Liu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Fine tuning has been regarded as a de facto approach for adapting large language models (LLMs) to downstream tasks, but the high training memory consumption inherited from LLMs makes this process inefficient. Among existing memory efficient approaches, activation-related optimization has proven particularly effective, as activations consistently dominate overall memory consumption. Although prior arts offer various activation optimization strategies, their data-agnostic nature ultimately results in ineffective and unstable fine tuning. In this paper, we propose TokenSeek, a universal plugin solution for various transformer-based models through instance-aware token seeking and ditching, achieving significant fine-tuning memory savings (e.g., requiring only 14.8% of the memory on Llama3.2 1B) with on-par or even better performance. Furthermore, our interpretable token seeking process reveals the underlying reasons for its effectiveness, offering valuable insights for future research on token efficiency. Homepage: https://runjia.tech/iclr_tokenseek/",
      "url": "https://arxiv.org/abs/2601.19739",
      "pdfUrl": "https://arxiv.org/pdf/2601.19739.pdf",
      "titleJa": "TokenSeek: インスタンスを考慮したトークン破棄によるメモリ効率の高い微調整"
    },
    {
      "id": "2601.19738",
      "arxivId": "2601.19738",
      "title": "Quantum Circuit Pre-Synthesis: Learning Local Edits to Reduce $T$-count",
      "authors": [
        "Daniele Lizzio Bosco",
        "Lukasz Cincio",
        "Giuseppe Serra",
        "M. Cerezo"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Compiling quantum circuits into Clifford+$T$ gates is a central task for fault-tolerant quantum computing using stabilizer codes. In the near term, $T$ gates will dominate the cost of fault tolerant implementations, and any reduction in the number of such expensive gates could mean the difference between being able to run a circuit or not. While exact synthesis is exponentially hard in the number of qubits, local synthesis approaches are commonly used to compile large circuits by decomposing them into substructures. However, composing local methods leads to suboptimal compilations in key metrics such as $T$-count or circuit depth, and their performance strongly depends on circuit representation. In this work, we address this challenge by proposing \\textsc{Q-PreSyn}, a strategy that, given a set of local edits preserving circuit equivalence, uses a RL agent to identify effective sequences of such actions and thereby obtain circuit representations that yield a reduced $T$-count upon synthesis. Experimental results of our proposed strategy, applied on top of well-known synthesis algorithms, show up to a $20\\%$ reduction in $T$-count on circuits with up to 25 qubits, without introducing any additional approximation error prior to synthesis.",
      "url": "https://arxiv.org/abs/2601.19738",
      "pdfUrl": "https://arxiv.org/pdf/2601.19738.pdf",
      "titleJa": "量子回路の事前合成：Tカウントを削減するための局所編集の学習"
    },
    {
      "id": "2601.19726",
      "arxivId": "2601.19726",
      "title": "RvB: Automating AI System Hardening via Iterative Red-Blue Games",
      "authors": [
        "Lige Huang",
        "Zicheng Liu",
        "Jie Zhang",
        "Lewen Yan",
        "Dongrui Liu",
        "Jing Shao"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "The dual offensive and defensive utility of Large Language Models (LLMs) highlights a critical gap in AI security: the lack of unified frameworks for dynamic, iterative adversarial adaptation hardening. To bridge this gap, we propose the Red Team vs. Blue Team (RvB) framework, formulated as a training-free, sequential, imperfect-information game. In this process, the Red Team exposes vulnerabilities, driving the Blue Team to learning effective solutions without parameter updates. We validate our framework across two challenging domains: dynamic code hardening against CVEs and guardrail optimization against jailbreaks. Our empirical results show that this interaction compels the Blue Team to learn fundamental defensive principles, leading to robust remediations that are not merely overfitted to specific exploits. RvB achieves Defense Success Rates of 90\\% and 45\\% across the respective tasks while maintaining near 0\\% False Positive Rates, significantly surpassing baselines. This work establishes the iterative adversarial interaction framework as a practical paradigm that automates the continuous hardening of AI systems.",
      "url": "https://arxiv.org/abs/2601.19726",
      "pdfUrl": "https://arxiv.org/pdf/2601.19726.pdf",
      "titleJa": "RvB: 反復的な赤青ゲームによる AI システム強化の自動化"
    },
    {
      "id": "2601.19723",
      "arxivId": "2601.19723",
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "authors": [
        "Yifan Wang",
        "Jichen Zheng",
        "Jingyuan Sun",
        "Yunhao Zhang",
        "Chunyu Ye",
        "Jixing Li",
        "Chengqing Zong",
        "Shaonan Wang"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca's and Wernicke's aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions.",
      "url": "https://arxiv.org/abs/2601.19723",
      "pdfUrl": "https://arxiv.org/pdf/2601.19723.pdf",
      "titleJa": "言語モデルの構成要素レベルの損傷により、臨床的に整合した失語症の表現型が明らかになる"
    },
    {
      "id": "2601.18339",
      "arxivId": "2601.18339",
      "title": "A Dataset for Automatic Vocal Mode Classification",
      "authors": [
        "Reemt Hinrichs",
        "Sonja Stephan",
        "Alexander Lange",
        "Jörn Ostermann"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\\,\\% was achieved with a ResNet18. The dataset can be downloaded under https://zenodo.org/records/14276415.",
      "url": "https://arxiv.org/abs/2601.18339",
      "pdfUrl": "https://arxiv.org/pdf/2601.18339.pdf",
      "titleJa": "自動音声モード分類のためのデータセット"
    },
    {
      "id": "2601.17645",
      "arxivId": "2601.17645",
      "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
      "authors": [
        "Xilin Jiang",
        "Qiaolin Wang",
        "Junkai Wu",
        "Xiaomin He",
        "Zhongweiyang Xu",
        "Yinghao Ma",
        "Minshuo Piao",
        "Kaiyi Yang",
        "Xiuwen Zheng",
        "Riki Shimizu",
        "Yicong Chen",
        "Arsalan Firoozi",
        "Gavin Mischler",
        "Sukru Samet Dindar",
        "Richard Antonello",
        "Linyang He",
        "Tsun-An Hsieh",
        "Xulin Fan",
        "Yulun Wu",
        "Yuesheng Ma",
        "Chaitanya Amballa",
        "Weixiong Chen",
        "Jiarui Hai",
        "Ruisi Li",
        "Vishal Choudhari",
        "Cong Han",
        "Yinghao Aaron Li",
        "Adeen Flinker",
        "Mounya Elhilali",
        "Emmanouil Benetos",
        "Mark Hasegawa-Johnson",
        "Romit Roy Choudhury",
        "Nima Mesgarani"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public",
      "url": "https://arxiv.org/abs/2601.17645",
      "pdfUrl": "https://arxiv.org/pdf/2601.17645.pdf",
      "titleJa": "AVMeme試験：法学修士（LLM）の文脈的・文化的知識と思考力を評価するマルチモーダル・多言語・多文化ベンチマーク"
    },
    {
      "id": "2601.17517",
      "arxivId": "2601.17517",
      "title": "EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding",
      "authors": [
        "Luca Cerovaz",
        "Michele Mancusi",
        "Emanuele Rodolà"
      ],
      "publishedDate": "2026-01-24",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Audio codecs power discrete music generative modelling, music streaming, and immersive media by shrinking PCM audio to bandwidth-friendly bitrates. Recent works have gravitated towards processing in the spectral domain; however, spectrogram domains typically struggle with phase modeling, which is naturally complex-valued. Most frequency-domain neural codecs either disregard phase information or encode it as two separate real-valued channels, limiting spatial fidelity. This entails the need to introduce adversarial discriminators at the expense of convergence speed and training stability to compensate for the inadequate representation power of the audio signal. In this work we introduce an end-to-end complex-valued RVQ-VAE audio codec that preserves magnitude-phase coupling across the entire analysis-quantization-synthesis pipeline and removes adversarial discriminators and diffusion post-filters. Without GANs or diffusion, we match or surpass much longer-trained baselines in-domain and reach SOTA out-of-domain performance on phase coherence and waveform fidelity. Compared to standard baselines that train for hundreds of thousands of steps, our model, which reduces the training budget by an order of magnitude, is markedly more compute-efficient while preserving high perceptual quality.",
      "url": "https://arxiv.org/abs/2601.17517",
      "pdfUrl": "https://arxiv.org/pdf/2601.17517.pdf",
      "titleJa": "EuleroDec: 効率的かつ堅牢なオーディオコーディングのための複素値RVQ-VAE"
    },
    {
      "id": "2601.16675",
      "arxivId": "2601.16675",
      "title": "I Guess That's Why They Call it the Blues: Causal Analysis for Audio Classifiers",
      "authors": [
        "David A. Kelly",
        "Hana Chockler"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "It is well-known that audio classifiers often rely on non-musically relevant features and spurious correlations to classify audio. Hence audio classifiers are easy to manipulate or confuse, resulting in wrong classifications. While inducing a misclassification is not hard, until now the set of features that the classifiers rely on was not well understood. In this paper we introduce a new method that uses causal reasoning to discover features of the frequency space that are sufficient and necessary for a given classification. We describe an implementation of this algorithm in the tool FreqReX and provide experimental results on a number of standard benchmark datasets. Our experiments show that causally sufficient and necessary subsets allow us to manipulate the outputs of the models in a variety of ways by changing the input very slightly. Namely, a change to one out of 240,000 frequencies results in a change in classification 58% of the time, and the change can be so small that it is practically inaudible. These results show that causal analysis is useful for understanding the reasoning process of audio classifiers and can be used to successfully manipulate their outputs.",
      "url": "https://arxiv.org/abs/2601.16675",
      "pdfUrl": "https://arxiv.org/pdf/2601.16675.pdf",
      "titleJa": "ブルースと呼ばれる理由：音声分類器の因果分析"
    },
    {
      "id": "2601.16273",
      "arxivId": "2601.16273",
      "title": "The CMU-AIST submission for the ICME 2025 Audio Encoder Challenge",
      "authors": [
        "Shikhar Bharadwaj",
        "Samuele Cornell",
        "Kwanghee Choi",
        "Hye-jin Shim",
        "Soham Deshmukh",
        "Satoru Fukayama",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This technical report describes our submission to the ICME 2025 audio encoder challenge. Our submitted system is built on BEATs, a masked speech token prediction based audio encoder. We extend the BEATs model using 74,000 hours of data derived from various speech, music, and sound corpora and scale its architecture upto 300 million parameters. We experiment with speech-heavy and balanced pre-training mixtures to study the impact of different domains on final performance. Our submitted system consists of an ensemble of the Dasheng 1.2 billion model with two custom scaled-up BEATs models trained on the aforementioned pre-training data mixtures. We also propose a simple ensembling technique that retains the best capabilities of constituent models and surpasses both the baseline and Dasheng 1.2B. For open science, we publicly release our trained checkpoints via huggingface at https://huggingface.co/shikhar7ssu/OpenBEATs-ICME-SOUND and https://huggingface.co/shikhar7ssu/OpenBEATs-ICME.",
      "url": "https://arxiv.org/abs/2601.16273",
      "pdfUrl": "https://arxiv.org/pdf/2601.16273.pdf",
      "titleJa": "ICME 2025オーディオエンコーダチャレンジへのCMU-AISTの応募"
    },
    {
      "id": "2601.16150",
      "arxivId": "2601.16150",
      "title": "Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization",
      "authors": [
        "Maximos Kaliakatsos-Papakostas",
        "Dimos Makris",
        "Konstantinos Soiledis",
        "Konstantinos-Theodoros Tsamis",
        "Vassilis Katsouros",
        "Emilios Cambouropoulos"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody-harmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.",
      "url": "https://arxiv.org/abs/2601.16150",
      "pdfUrl": "https://arxiv.org/pdf/2601.16150.pdf",
      "titleJa": "メロディーに注意を払う（交差させる）：単一エンコーダーによるメロディーハーモニーのためのカリキュラムマスキング"
    },
    {
      "id": "2601.15872",
      "arxivId": "2601.15872",
      "title": "PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation",
      "authors": [
        "Jaekwon Im",
        "Natalia Polouliakh",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.",
      "url": "https://arxiv.org/abs/2601.15872",
      "pdfUrl": "https://arxiv.org/pdf/2601.15872.pdf",
      "titleJa": "PF-D2M: ユニバーサルなダンス・トゥ・ミュージック生成のためのポーズフリー拡散モデル"
    },
    {
      "id": "2601.15083",
      "arxivId": "2601.15083",
      "title": "Bangla Music Genre Classification Using Bidirectional LSTMS",
      "authors": [
        "Muntakimur Rahaman",
        "Md Mahmudul Hoque",
        "Md Mehedi Hassain"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Bangla music is enrich in its own music cultures. Now a days music genre classification is very significant because of the exponential increase in available music, both in digital and physical formats. It is necessary to index them accordingly to facilitate improved retrieval. Automatically classifying Bangla music by genre is essential for efficiently locating specific pieces within a vast and diverse music library. Prevailing methods for genre classification predominantly employ conventional machine learning or deep learning approaches. This work introduces a novel music dataset comprising ten distinct genres of Bangla music. For the task of audio classification, we utilize a recurrent neural network (RNN) architecture. Specifically, a Long Short-Term Memory (LSTM) network is implemented to train the model and perform the classification. Feature extraction represents a foundational stage in audio data processing. This study utilizes Mel-Frequency Cepstral Coefficients (MFCCs) to transform raw audio waveforms into a compact and representative set of features. The proposed framework facilitates music genre classification by leveraging these extracted features. Experimental results demonstrate a classification accuracy of 78%, indicating the system's strong potential to enhance and streamline the organization of Bangla music genres.",
      "url": "https://arxiv.org/abs/2601.15083",
      "pdfUrl": "https://arxiv.org/pdf/2601.15083.pdf",
      "titleJa": "双方向LSTMSを用いたバングラ音楽のジャンル分類"
    },
    {
      "id": "2601.14931",
      "arxivId": "2601.14931",
      "title": "Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali",
      "authors": [
        "Nouhoum Coulibaly",
        "Ousmane Ly",
        "Michael Leventhal",
        "Ousmane Goro"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "This study explores the capacity of generative artificial intelligence (Gen AI) to contribute to the construction of peace narratives and the revitalization of musical heritage in Mali. The study has been made in a political and social context where inter-community tensions and social fractures motivate a search for new symbolic frameworks for reconciliation. The study empirically explores three questions: (1) how Gen AI can be used as a tool for musical creation rooted in national languages and traditions; (2) to what extent Gen AI systems enable a balanced hybridization between technological innovation and cultural authenticity; and (3) how AI-assisted musical co-creation can strengthen social cohesion and cultural sovereignty. The experimental results suggest that Gen AI, embedded in a culturally conscious participatory framework, can act as a catalyst for symbolic diplomacy, amplifying local voices instead of standardizing them. However, challenges persist regarding the availability of linguistic corpora, algorithmic censorship, and the ethics of generating compositions derived from copyrighted sources.",
      "url": "https://arxiv.org/abs/2601.14931",
      "pdfUrl": "https://arxiv.org/pdf/2601.14931.pdf",
      "titleJa": "生成型人工知能、音楽遺産、そして平和物語の構築：マリにおける事例研究"
    },
    {
      "id": "2601.14786",
      "arxivId": "2601.14786",
      "title": "Training-Efficient Text-to-Music Generation with State-Space Modeling",
      "authors": [
        "Wei-Jaw Lee",
        "Fang-Chih Hsieh",
        "Xuanjun Chen",
        "Fang-Duo Tsai",
        "Yi-Hsuan Yang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in text-to-music generation (TTM) have yielded high-quality results, but often at the cost of extensive compute and the use of large proprietary internal data. To improve the affordability and openness of TTM training, an open-source generative model backbone that is more training- and data-efficient is needed. In this paper, we constrain the number of trainable parameters in the generative model to match that of the MusicGen-small benchmark (with about 300M parameters), and replace its Transformer backbone with the emerging class of state-space models (SSMs). Specifically, we explore different SSM variants for sequence modeling, and compare a single-stage SSM-based design with a decomposable two-stage SSM/diffusion hybrid design. All proposed models are trained from scratch on a purely public dataset comprising 457 hours of CC-licensed music, ensuring full openness. Our experimental findings are three-fold. First, we show that SSMs exhibit superior training efficiency compared to the Transformer counterpart. Second, despite using only 9% of the FLOPs and 2% of the training data size compared to the MusicGen-small benchmark, our model achieves competitive performance in both objective metrics and subjective listening tests based on MusicCaps captions. Finally, our scaling-down experiment demonstrates that SSMs can maintain competitive performance relative to the Transformer baseline even at the same training budget (measured in iterations), when the model size is reduced to four times smaller. To facilitate the democratization of TTM research, the processed captions, model checkpoints, and source code are available on GitHub via the project page: https://lonian6.github.io/ssmttm/.",
      "url": "https://arxiv.org/abs/2601.14786",
      "pdfUrl": "https://arxiv.org/pdf/2601.14786.pdf",
      "titleJa": "状態空間モデリングによる効率的なテキストから音楽への生成"
    },
    {
      "id": "2601.14684",
      "arxivId": "2601.14684",
      "title": "Dissecting Performance Degradation in Audio Source Separation under Sampling Frequency Mismatch",
      "authors": [
        "Kanami Imamura",
        "Tomohiko Nakamura",
        "Kohei Yatabe",
        "Hiroshi Saruwatari"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio processing methods based on deep neural networks are typically trained at a single sampling frequency (SF). To handle untrained SFs, signal resampling is commonly employed, but it can degrade performance, particularly when the input SF is lower than the trained SF. This paper investigates the causes of this degradation through two hypotheses: (i) the lack of high-frequency components introduced by up-sampling, and (ii) the greater importance of their presence than their precise representation. To examine these hypotheses, we compare conventional resampling with three alternatives: post-resampling noise addition, which adds Gaussian noise to the resampled signal; noisy-kernel resampling, which perturbs the kernel with Gaussian noise to enrich high-frequency components; and trainable-kernel resampling, which adapts the interpolation kernel through training. Experiments on music source separation show that noisy-kernel and trainable-kernel resampling alleviate the degradation observed with conventional resampling. We further demonstrate that noisy-kernel resampling is effective across diverse models, highlighting it as a simple yet practical option.",
      "url": "https://arxiv.org/abs/2601.14684",
      "pdfUrl": "https://arxiv.org/pdf/2601.14684.pdf",
      "titleJa": "サンプリング周波数の不一致による音源分離の性能劣化の解析"
    },
    {
      "id": "2601.15348",
      "arxivId": "2601.15348",
      "title": "Abusive music and song transformation using GenAI and LLMs",
      "authors": [
        "Jiyang Choi",
        "Rohitash Chandra"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Repeated exposure to violence and abusive content in music and song content can influence listeners' emotions and behaviours, potentially normalising aggression or reinforcing harmful stereotypes. In this study, we explore the use of generative artificial intelligence (GenAI) and Large Language Models (LLMs) to automatically transform abusive words (vocal delivery) and lyrical content in popular music. Rather than simply muting or replacing a single word, our approach transforms the tone, intensity, and sentiment, thus not altering just the lyrics, but how it is expressed. We present a comparative analysis of four selected English songs and their transformed counterparts, evaluating changes through both acoustic and sentiment-based lenses. Our findings indicate that Gen-AI significantly reduces vocal aggressiveness, with acoustic analysis showing improvements in Harmonic to Noise Ratio, Cepstral Peak Prominence, and Shimmer. Sentiment analysis reduced aggression by 63.3-85.6\\% across artists, with major improvements in chorus sections (up to 88.6\\% reduction). The transformed versions maintained musical coherence while mitigating harmful content, offering a promising alternative to traditional content moderation that avoids triggering the \"forbidden fruit\" effect, where the censored content becomes more appealing simply because it is restricted. This approach demonstrates the potential for GenAI to create safer listening experiences while preserving artistic expression.",
      "url": "https://arxiv.org/abs/2601.15348",
      "pdfUrl": "https://arxiv.org/pdf/2601.15348.pdf",
      "titleJa": "GenAIとLLMを使用した虐待的な音楽と歌の変換"
    },
    {
      "id": "2601.14356",
      "arxivId": "2601.14356",
      "title": "Single-step Controllable Music Bandwidth Extension With Flow Matching",
      "authors": [
        "Carlos Hernandez-Olivan",
        "Hendrik Vincent Koops",
        "Hao Hao Tan",
        "Elio Quinton"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio restoration consists in inverting degradations of a digital audio signal to recover what would have been the pristine quality signal before the degradation occurred. This is valuable in contexts such as archives of music recordings, particularly those of precious historical value, for which a clean version may have been lost or simply does not exist. Recent work applied generative models to audio restoration, showing promising improvement over previous methods, and opening the door to the ability to perform restoration operations that were not possible before. However, making these models finely controllable remains a challenge. In this paper, we propose an extension of FLowHigh and introduce the Dynamic Spectral Contour (DSC) as a control signal for bandwidth extension via classifier-free guidance. Our experiments show competitive model performance, and indicate that DSC is a promising feature to support fine-grained conditioning.",
      "url": "https://arxiv.org/abs/2601.14356",
      "pdfUrl": "https://arxiv.org/pdf/2601.14356.pdf",
      "titleJa": "フローマッチングによるシングルステップ制御可能な音楽帯域幅拡張"
    },
    {
      "id": "2601.14157",
      "arxivId": "2601.14157",
      "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models",
      "authors": [
        "Bruno Sienkiewicz",
        "Łukasz Neumann",
        "Mateusz Modrzejewski"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.",
      "url": "https://arxiv.org/abs/2601.14157",
      "pdfUrl": "https://arxiv.org/pdf/2601.14157.pdf",
      "titleJa": "ConceptCaps - 音楽モデルの解釈可能性のための蒸留概念データセット"
    },
    {
      "id": "2601.13931",
      "arxivId": "2601.13931",
      "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music",
      "authors": [
        "Yannis Vasilakis",
        "Rachel Bittner",
        "Johan Pauwels"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG"
      ],
      "abstract": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.",
      "url": "https://arxiv.org/abs/2601.13931",
      "pdfUrl": "https://arxiv.org/pdf/2601.13931.pdf",
      "titleJa": "音楽のための音声テキスト結合モデルにおける効果的な否定モデリングに向けて"
    },
    {
      "id": "2601.13647",
      "arxivId": "2601.13647",
      "title": "Fusion Segment Transformer: Bi-Directional Attention Guided Fusion Network for AI-Generated Music Detection",
      "authors": [
        "Yumin Kim",
        "Seonghyeon Go"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "With the rise of generative AI technology, anyone can now easily create and deploy AI-generated music, which has heightened the need for technical solutions to address copyright and ownership issues. While existing works mainly focused on short-audio, the challenge of full-audio detection, which requires modeling long-term structure and context, remains insufficiently explored. To address this, we propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. As in our previous work, we extract content embeddings from short music segments using diverse feature extractors. Furthermore, we enhance the architecture for full-audio AI-generated music detection by introducing a Gated Fusion Layer that effectively integrates content and structural information, enabling the capture of long-term context. Experiments on the SONICS and AIME datasets show that our approach outperforms the previous model and recent baselines, achieving state-of-the-art results in AI-generated music detection.",
      "url": "https://arxiv.org/abs/2601.13647",
      "pdfUrl": "https://arxiv.org/pdf/2601.13647.pdf",
      "titleJa": "Fusion Segment Transformer: AI生成音楽検出のための双方向アテンションガイド融合ネットワーク"
    },
    {
      "id": "2601.12961",
      "arxivId": "2601.12961",
      "title": "Supervised Learning for Game Music Segmentation",
      "authors": [
        "Shangxuan Luo",
        "Joshua Reiss"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD"
      ],
      "abstract": "At present, neural network-based models, including transformers, struggle to generate memorable and readily comprehensible music from unified and repetitive musical material due to a lack of understanding of musical structure. Consequently, these models are rarely employed by the games industry. It is hypothesised by many scholars that the modelling of musical structure may inform models at a higher level, thereby enhancing the quality of music generation. The aim of this study is to explore the performance of supervised learning methods in the task of structural segmentation, which is the initial step in music structure modelling. An audio game music dataset with 309 structural annotations was created to train the proposed method, which combines convolutional neural networks and recurrent neural networks, achieving performance comparable to the state-of-the-art unsupervised learning methods with fewer training resources.",
      "url": "https://arxiv.org/abs/2601.12961",
      "pdfUrl": "https://arxiv.org/pdf/2601.12961.pdf",
      "titleJa": "ゲーム音楽セグメンテーションのための教師あり学習"
    },
    {
      "id": "2601.18086",
      "arxivId": "2601.18086",
      "title": "From Human Speech to Ocean Signals: Transferring Speech Large Models for Underwater Acoustic Target Recognition",
      "authors": [
        "Mengcheng Huang",
        "Xue Zhou",
        "Chen Xu",
        "Dapeng Man"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Underwater acoustic target recognition (UATR) plays a vital role in marine applications but remains challenging due to limited labeled data and the complexity of ocean environments. This paper explores a central question: can speech large models (SLMs), trained on massive human speech corpora, be effectively transferred to underwater acoustics? To investigate this, we propose UATR-SLM, a simple framework that reuses the speech feature pipeline, adapts the SLM as an acoustic encoder, and adds a lightweight classifier.Experiments on the DeepShip and ShipsEar benchmarks show that UATR-SLM achieves over 99% in-domain accuracy, maintains strong robustness across variable signal lengths, and reaches up to 96.67% accuracy in cross-domain evaluation. These results highlight the strong transferability of SLMs to UATR, establishing a promising paradigm for leveraging speech foundation models in underwater acoustics.",
      "url": "https://arxiv.org/abs/2601.18086",
      "pdfUrl": "https://arxiv.org/pdf/2601.18086.pdf",
      "titleJa": "人間の音声から海洋信号へ：水中音響ターゲット認識のための音声大規模モデルの転送"
    },
    {
      "id": "2601.17902",
      "arxivId": "2601.17902",
      "title": "dLLM-ASR: A Faster Diffusion LLM-based Framework for Speech Recognition",
      "authors": [
        "Wenjie Tian",
        "Bingshen Mu",
        "Guobin Ma",
        "Xuelong Geng",
        "Zhixian Zhao",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Automatic speech recognition (ASR) systems based on large language models (LLMs) achieve superior performance by leveraging pretrained LLMs as decoders, but their token-by-token generation mechanism leads to inference latency that grows linearly with sequence length. Meanwhile, discrete diffusion large language models (dLLMs) offer a promising alternative, enabling high-quality parallel sequence generation with pretrained decoders. However, directly applying native text-oriented dLLMs to ASR leads to a fundamental mismatch between open-ended text generation and the acoustically conditioned transcription paradigm required by ASR. As a result, it introduces unnecessary difficulty and computational redundancy, such as denoising from pure noise, inflexible generation lengths, and fixed denoising steps. We propose dLLM-ASR, an efficient dLLM-based ASR framework that formulates dLLM's decoding as a prior-guided and adaptive denoising process. It leverages an ASR prior to initialize the denoising process and provide an anchor for sequence length. Building upon this prior, length-adaptive pruning dynamically removes redundant tokens, while confidence-based denoising allows converged tokens to exit the denoising loop early, enabling token-level adaptive computation. Experiments demonstrate that dLLM-ASR achieves recognition accuracy comparable to autoregressive LLM-based ASR systems and delivers a 4.44$\\times$ inference speedup, establishing a practical and efficient paradigm for ASR.",
      "url": "https://arxiv.org/abs/2601.17902",
      "pdfUrl": "https://arxiv.org/pdf/2601.17902.pdf",
      "titleJa": "dLLM-ASR: 音声認識のための高速拡散LLMベースのフレームワーク"
    },
    {
      "id": "2601.17690",
      "arxivId": "2601.17690",
      "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
      "authors": [
        "Ziling Gong",
        "Yunyan Ouyang",
        "Iram Kamdar",
        "Melody Ma",
        "Hongjie Chen",
        "Franck Dernoncourt",
        "Ryan A. Rossi",
        "Nesreen K. Ahmed"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.",
      "url": "https://arxiv.org/abs/2601.17690",
      "pdfUrl": "https://arxiv.org/pdf/2601.17690.pdf",
      "titleJa": "セグメント長の重要性：オーディオフィンガープリンティングの性能におけるセグメント長の研究"
    },
    {
      "id": "2601.16774",
      "arxivId": "2601.16774",
      "title": "E2E-AEC: Implementing an end-to-end neural network learning approach for acoustic echo cancellation",
      "authors": [
        "Yiheng Jiang",
        "Biao Tian",
        "Haoxu Wang",
        "Shengkui Zhao",
        "Bin Ma",
        "Daren Chen",
        "Xiangang Li"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We propose a novel neural network-based end-to-end acoustic echo cancellation (E2E-AEC) method capable of streaming inference, which operates effectively without reliance on traditional linear AEC (LAEC) techniques and time delay estimation. Our approach includes several key strategies: First, we introduce and refine progressive learning to gradually enhance echo suppression. Second, our model employs knowledge transfer by initializing with a pre-trained LAECbased model, harnessing the insights gained from LAEC training. Third, we optimize the attention mechanism with a loss function applied on attention weights to achieve precise time alignment between the reference and microphone signals. Lastly, we incorporate voice activity detection to enhance speech quality and improve echo removal by masking the network output when near-end speech is absent. The effectiveness of our approach is validated through experiments conducted on public datasets.",
      "url": "https://arxiv.org/abs/2601.16774",
      "pdfUrl": "https://arxiv.org/pdf/2601.16774.pdf",
      "titleJa": "E2E-AEC: 音響エコーキャンセルのためのエンドツーエンドのニューラルネットワーク学習アプローチの実装"
    },
    {
      "id": "2601.17086",
      "arxivId": "2601.17086",
      "title": "SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS",
      "authors": [
        "Ayush Pratap Singh",
        "Harshit Singh",
        "Nityanand Mathur",
        "Akshat Mandloi",
        "Sudarshan Kamath"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Neural text-to-speech (TTS) systems systematically mispronounce low-resource proper nouns, particularly non-English names, brands, and geographic locations, due to their underrepresentation in predominantly English training corpora. Existing solutions typically rely on expensive multilingual data collection, supervised finetuning, or manual phonetic annotation, which limits the deployment of TTS systems in linguistically diverse settings. We introduce SonoEdit, a model editing technique that surgically corrects pronunciation errors in pre-trained TTS models without retraining. Instead of costly finetuning or explicit phoneme injection, we propose a parsimonious alternative based on Null-Space Pronunciation Editing, which performs a single-shot parameter update to modify the pronunciation of specific words while provably preserving all other model behavior. We first adapt Acoustic Causal Tracing to identify the Transformer layers responsible for text-to-pronunciation mapping. We then apply Null-Space Constrained Editing to compute a closed-form weight update that corrects the target pronunciation while remaining mathematically orthogonal to the subspace governing general speech generation. This constrained update steers the model's acoustic output toward a desired pronunciation exemplar while guaranteeing zero first-order change on a preserved speech corpus.",
      "url": "https://arxiv.org/abs/2601.17086",
      "pdfUrl": "https://arxiv.org/pdf/2601.17086.pdf",
      "titleJa": "SonoEdit: LLMベースのTTSにおける発音訂正のためのヌル空間制約知識編集"
    },
    {
      "id": "2601.16547",
      "arxivId": "2601.16547",
      "title": "CORD: Bridging the Audio-Text Reasoning Gap via Weighted On-policy Cross-modal Distillation",
      "authors": [
        "Jing Hu",
        "Danxiang Zhu",
        "Xianlong Luo",
        "Dan Zhang",
        "Shuwei He",
        "Yishu Lei",
        "Haitao Zheng",
        "Shikun Feng",
        "Jingzhou He",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Large Audio Language Models (LALMs) have garnered significant research interest. Despite being built upon text-based large language models (LLMs), LALMs frequently exhibit a degradation in knowledge and reasoning capabilities. We hypothesize that this limitation stems from the failure of current training paradigms to effectively bridge the acoustic-semantic gap within the feature representation space. To address this challenge, we propose CORD, a unified alignment framework that performs online cross-modal self-distillation. Specifically, it aligns audio-conditioned reasoning with its text-conditioned counterpart within a unified model. Leveraging the text modality as an internal teacher, CORD performs multi-granularity alignment throughout the audio rollout process. At the token level, it employs on-policy reverse KL divergence with importance-aware weighting to prioritize early and semantically critical tokens. At the sequence level, CORD introduces a judge-based global reward to optimize complete reasoning trajectories via Group Relative Policy Optimization (GRPO). Empirical results across multiple benchmarks demonstrate that CORD consistently enhances audio-conditioned reasoning and substantially bridges the audio-text performance gap with only 80k synthetic training samples, validating the efficacy and data efficiency of our on-policy, multi-level cross-modal alignment approach.",
      "url": "https://arxiv.org/abs/2601.16547",
      "pdfUrl": "https://arxiv.org/pdf/2601.16547.pdf",
      "titleJa": "CORD: 重み付けされたポリシーに基づくクロスモーダル蒸留による音声テキスト推論ギャップの橋渡し"
    },
    {
      "id": "2601.17085",
      "arxivId": "2601.17085",
      "title": "Recovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration",
      "authors": [
        "Esther Sun",
        "Abinay Reddy Naini",
        "Carlos Busso"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Discrete speech tokens offer significant advantages for storage and language model integration, but their application in speech emotion recognition (SER) is limited by paralinguistic information loss during quantization. This paper presents a comprehensive investigation of discrete tokens for SER. Using a fine-tuned WavLM-Large model, we systematically quantify performance degradation across different layer configurations and k-means quantization granularities. To recover the information loss, we propose two key strategies: (1) attention-based multi-layer fusion to recapture complementary information from different layers, and (2) integration of openSMILE features to explicitly reintroduce paralinguistic cues. We also compare mainstream neural codec tokenizers (SpeechTokenizer, DAC, EnCodec) and analyze their behaviors when fused with acoustic features. Our findings demonstrate that through multi-layer fusion and acoustic feature integration, discrete tokens can close the performance gap with continuous representations in SER tasks.",
      "url": "https://arxiv.org/abs/2601.17085",
      "pdfUrl": "https://arxiv.org/pdf/2601.17085.pdf",
      "titleJa": "多層融合とパラ言語的特徴の統合による離散トークンからの音声感情認識のパフォーマンス回復"
    },
    {
      "id": "2601.16316",
      "arxivId": "2601.16316",
      "title": "EdgeSpot: Efficient and High-Performance Few-Shot Model for Keyword Spotting",
      "authors": [
        "Oguzhan Buyuksolak",
        "Alican Gok",
        "Osman Erman Okman"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "We introduce an efficient few-shot keyword spotting model for edge devices, EdgeSpot, that pairs an optimized version of a BC-ResNet-based acoustic backbone with a trainable Per-Channel Energy Normalization frontend and lightweight temporal self-attention. Knowledge distillation is utilized during training by employing a self-supervised teacher model, optimized with Sub-center ArcFace loss. This study demonstrates that the EdgeSpot model consistently provides better accuracy at a fixed false-alarm rate (FAR) than strong BC-ResNet baselines. The largest variant, EdgeSpot-4, improves the 10-shot accuracy at 1% FAR from 73.7% to 82.0%, which requires only 29.4M MACs with 128k parameters.",
      "url": "https://arxiv.org/abs/2601.16316",
      "pdfUrl": "https://arxiv.org/pdf/2601.16316.pdf",
      "titleJa": "EdgeSpot: キーワードスポッティングのための効率的で高性能な少数ショットモデル"
    },
    {
      "id": "2601.16023",
      "arxivId": "2601.16023",
      "title": "Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs",
      "authors": [
        "Lalaram Arya",
        "Mrinmoy Bhattacharjee",
        "Adarsh C. R.",
        "S. R. Mahadeva Prasanna"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "cs.HC"
      ],
      "abstract": "Direct Speech-to-Speech Translation (S2ST) has gained increasing attention for its ability to translate speech from one language to another, while reducing error propagation and latency inherent in traditional cascaded pipelines. However, existing direct S2ST systems continue to face notable challenges, including instability in semantic-acoustic alignment when parallel speech data is scarce, difficulty in preserving speaker identity, and limited multilingual scalability. In this work, we introduce DS2ST-LM, a scalable, single-stage direct S2ST framework leveraging a multilingual Large Language Model (LLM). The architecture integrates a Whisper speech encoder, a learnable projection module, a Qwen2-0.5B LLM, and a timbre-controlled vocoder. We construct GigaS2S-1000, a 1000-hour bilingual corpus by extending the GigaST dataset with high-fidelity synthetic target speech, and show that this synthetic data alleviates data scarcity to some extent. We investigate two semantic token generation strategies: speech-derived S3 tokens and text-derived tokens generated by a pre-trained LLM, and analyze their impact on training stability and semantic consistency. We further evaluate three projection architectures (Linear, Conv1D-Linear, and Q-Former) and observe that while higher-capacity projectors converge faster, the simple Linear projector achieves higher performance. Extensive experiments demonstrate that DS2ST-LM outperforms traditional cascaded and ST (Qwen-Audio) + TTS baselines across both lexical (BLEU, METEOR) and semantic (BLEURT, COMET) metrics, while extending to multiple language pairs, including French, Spanish, German, Hindi, Bengali, and Urdu. Furthermore, we incorporate timbre-aware speech synthesis to preserve speaker information, enabling DS2ST-LM to surpass prior direct S2ST systems in both speaker similarity and perceptual naturalness.",
      "url": "https://arxiv.org/abs/2601.16023",
      "pdfUrl": "https://arxiv.org/pdf/2601.16023.pdf",
      "titleJa": "音色を考慮したLLMベースの直接音声翻訳は複数の言語ペアに拡張可能"
    },
    {
      "id": "2601.15668",
      "arxivId": "2601.15668",
      "title": "EmotionThinker: Prosody-Aware Reinforcement Learning for Explainable Speech Emotion Reasoning",
      "authors": [
        "Dingdong Wang",
        "Shujie Liu",
        "Tianhua Zhang",
        "Youjun Chen",
        "Jinyu Li",
        "Helen Meng"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Emotional information in speech plays a unique role in multimodal perception. However, current Speech Large Language Models (SpeechLLMs), similar to conventional speech emotion recognition (SER) systems, still treat emotion understanding as a simple classification problem. This provides limited interpretability of predictions, while leaving the LLMs' expressive and reasoning capabilities underutilized. In this work, we take the first step to reformulate SER as a deep reasoning problem through reinforcement learning (RL). We propose EmotionThinker, which is designed to generate accurate emotion predictions with interpretable explanations grounded in fine-grained acoustic cues. To achieve this, we first construct EmotionCoT-35K, an emotional reasoning dataset with Chain-of-Thought annotations and detailed captions. Second, we observe that current SpeechLLMs exhibit weak prosody perception, whereas prosodic cues constitute fundamental signals for interpreting emotions. To address this, we develop the prosody-enhanced foundation model EmotionThinker-Base, and demonstrate that prosody enhancement improves emotion understanding. Third, we introduce Group-Relative-Policy-Optimization with Progressive-Trust-aware-Reasoning-Reward (GRPO-PTR) for RL. Different from standard GRPO, which relies only on rule-based outcome rewards, GRPO-PTR progressively introduces reasoning reward, dynamically adjusts it with a trustworthiness weight reflecting the alignment between reasoning and outcome, and evaluates the overall reasoning quality with a reward model based on multi-dimensional criteria. EmotionThinker outperforms previous state-of-the-art evaluation models both in emotion accuracy and explanation quality, advancing SER toward interpretable multimodal reasoning. Project page: https://github.com/dingdongwang/EmotionThinker",
      "url": "https://arxiv.org/abs/2601.15668",
      "pdfUrl": "https://arxiv.org/pdf/2601.15668.pdf",
      "titleJa": "EmotionThinker: 説明可能な音声感情推論のための韻律を考慮した強化学習"
    },
    {
      "id": "2601.15596",
      "arxivId": "2601.15596",
      "title": "DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice",
      "authors": [
        "Leying Zhang",
        "Tingxiao Zhou",
        "Haiyang Sun",
        "Mengxiao Bi",
        "Yanmin Qian"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "While modern Text-to-Speech (TTS) systems achieve high fidelity for read-style speech, they struggle to generate Autonomous Sensory Meridian Response (ASMR), a specialized, low-intensity speech style essential for relaxation. The inherent challenges include ASMR's subtle, often unvoiced characteristics and the demand for zero-shot speaker adaptation. In this paper, we introduce DeepASMR, the first framework designed for zero-shot ASMR generation. We demonstrate that a single short snippet of a speaker's ordinary, read-style speech is sufficient to synthesize high-fidelity ASMR in their voice, eliminating the need for whispered training data from the target speaker. Methodologically, we first identify that discrete speech tokens provide a soft factorization of ASMR style from speaker timbre. Leveraging this insight, we propose a two-stage pipeline incorporating a Large Language Model (LLM) for content-style encoding and a flow-matching acoustic decoder for timbre reconstruction. Furthermore, we contribute DeepASMR-DB, a comprehensive 670-hour English-Chinese multi-speaker ASMR speech corpus, and introduce a novel evaluation protocol integrating objective metrics, human listening tests, LLM-based scoring and unvoiced speech analysis. Extensive experiments confirm that DeepASMR achieves state-of-the-art naturalness and style fidelity in ASMR generation for anyone of any voice, while maintaining competitive performance on normal speech synthesis.",
      "url": "https://arxiv.org/abs/2601.15596",
      "pdfUrl": "https://arxiv.org/pdf/2601.15596.pdf",
      "titleJa": "DeepASMR: LLMベースのゼロショットASMR音声生成（あらゆる声質の人向け）"
    },
    {
      "id": "2601.15433",
      "arxivId": "2601.15433",
      "title": "DynamicSound simulator for simulating moving sources and microphone arrays",
      "authors": [
        "Luca Barbisan",
        "Marco Levorato",
        "Fabrizio Riente"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Developing algorithms for sound classification, detection, and localization requires large amounts of flexible and realistic audio data, especially when leveraging modern machine learning and beamforming techniques. However, most existing acoustic simulators are tailored for indoor environments and are limited to static sound sources, making them unsuitable for scenarios involving moving sources, moving microphones, or long-distance propagation. This paper presents DynamicSound an open-source acoustic simulation framework for generating multichannel audio from one or more sound sources with the possibility to move them continuously in three-dimensional space and recorded by arbitrarily configured microphone arrays. The proposed model explicitly accounts for finite sound propagation delays, Doppler effects, distance-dependent attenuation, air absorption, and first-order reflections from planar surfaces, yielding temporally consistent spatial audio signals. Unlike conventional mono or stereo simulators, the proposed system synthesizes audio for an arbitrary number of virtual microphones, accurately reproducing inter-microphone time delays, level differences, and spectral coloration induced by the environment. Comparative evaluations with existing open-source tools demonstrate that the generated signals preserve high spatial fidelity across varying source positions and acoustic conditions. By enabling the generation of realistic multichannel audio under controlled and repeatable conditions, the proposed open framework provides a flexible and reproducible tool for the development, training, and evaluation of modern spatial audio and sound-source localization algorithms.",
      "url": "https://arxiv.org/abs/2601.15433",
      "pdfUrl": "https://arxiv.org/pdf/2601.15433.pdf",
      "titleJa": "移動音源とマイクアレイをシミュレートするDynamicSoundシミュレータ"
    }
  ],
  "lastUpdated": "2026-01-29T01:02:34.337103",
  "totalCount": 82
}