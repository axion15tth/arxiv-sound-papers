{
  "papers": [
    {
      "id": "2601.23174",
      "arxivId": "2601.23174",
      "title": "Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization",
      "authors": [
        "Luca Della Libera",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.",
      "url": "https://arxiv.org/abs/2601.23174",
      "pdfUrl": "https://arxiv.org/pdf/2601.23174.pdf",
      "titleJa": "固定フレームを超えて: 動的な文字揃え音声トークン化"
    },
    {
      "id": "2601.23161",
      "arxivId": "2601.23161",
      "title": "DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding",
      "authors": [
        "Jiaming Zhou",
        "Xuxin Cheng",
        "Shiwan Zhao",
        "Yuhang Jia",
        "Cao Liu",
        "Ke Zeng",
        "Xunliang Cai",
        "Yong Qin"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.",
      "url": "https://arxiv.org/abs/2601.23161",
      "pdfUrl": "https://arxiv.org/pdf/2601.23161.pdf",
      "titleJa": "DIFFA-2: 一般的な音声理解のための実用的な拡散大規模言語モデル"
    },
    {
      "id": "2601.23149",
      "arxivId": "2601.23149",
      "title": "Hearing is Believing? Evaluating and Analyzing Audio Language Model Sycophancy with SYAUDIO",
      "authors": [
        "Junchi Yao",
        "Lokranjan Lakshmikanthan",
        "Annie Zhao",
        "Danielle Zhao",
        "Shu Yang",
        "Zikang Ding",
        "Di Wang",
        "Lijie Hu"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio Language Models (ALMs) have recently shown strong capabilities in unified reasoning over speech, sound, and natural language; yet they inherit behavioral issues observed in Large Language Models, including sycophancy--the tendency to agree with user assertions even when they contradict objective evidence. While sycophancy has been extensively studied in text and vision-language models, its manifestation in audio-conditioned reasoning remains largely unexplored, despite the need for ALMs to rely on auditory cues such as acoustic events, speaker characteristics, and speech rate. To address this gap, we introduce SYAUDIO, the first benchmark dedicated to evaluating sycophancy in ALMs, consisting of 4,319 audio questions spanning Audio Perception, Audio Reasoning, Audio Math, and Audio Ethics. Built upon established audio benchmarks and augmented with TTS-generated arithmetic and moral reasoning tasks, SYAUDIO enables systematic evaluation across multiple domains and sycophancy types with carefully verified data quality. Furthermore, we analyze audio-specific sycophancy under realistic conditions involving noise and rate, and demonstrate that supervised fine-tuning with chain-of-thought data is an effective mitigation strategy for reducing sycophantic behavior in ALMs.",
      "url": "https://arxiv.org/abs/2601.23149",
      "pdfUrl": "https://arxiv.org/pdf/2601.23149.pdf",
      "titleJa": "聞くことは信じること？SYAUDIOによる音声言語モデルの追従性の評価と分析"
    },
    {
      "id": "2601.23066",
      "arxivId": "2601.23066",
      "title": "Towards Explicit Acoustic Evidence Perception in Audio LLMs for Speech Deepfake Detection",
      "authors": [
        "Xiaoxuan Guo",
        "Yuankun Xie",
        "Haonan Cheng",
        "Jiayi Zhou",
        "Jian Liu",
        "Hengyan Huang",
        "Long Ye",
        "Qin Zhang"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Speech deepfake detection (SDD) focuses on identifying whether a given speech signal is genuine or has been synthetically generated. Existing audio large language model (LLM)-based methods excel in content understanding; however, their predictions are often biased toward semantically correlated cues, which results in fine-grained acoustic artifacts being overlooked during the decisionmaking process. Consequently, fake speech with natural semantics can bypass detectors despite harboring subtle acoustic anomalies; this suggests that the challenge stems not from the absence of acoustic data, but from its inadequate accessibility when semantic-dominant reasoning prevails. To address this issue, we investigate SDD within the audio LLM paradigm and introduce SDD with Auditory Perception-enhanced Audio Large Language Model (SDD-APALLM), an acoustically enhanced framework designed to explicitly expose fine-grained time-frequency evidence as accessible acoustic cues. By combining raw audio with structured spectrograms, the proposed framework empowers audio LLMs to more effectively capture subtle acoustic inconsistencies without compromising their semantic understanding. Experimental results indicate consistent gains in detection accuracy and robustness, especially in cases where semantic cues are misleading. Further analysis reveals that these improvements stem from a coordinated utilization of semantic and acoustic information, as opposed to simple modality aggregation.",
      "url": "https://arxiv.org/abs/2601.23066",
      "pdfUrl": "https://arxiv.org/pdf/2601.23066.pdf",
      "titleJa": "音声ディープフェイク検出のためのオーディオLLMにおける明示的な音響証拠認識に向けて"
    },
    {
      "id": "2601.22889",
      "arxivId": "2601.22889",
      "title": "DiffuSpeech: Silent Thought, Spoken Answer via Unified Speech-Text Diffusion",
      "authors": [
        "Yuxuan Lou",
        "Ziming Wu",
        "Yaochen Wang",
        "Yong Liu",
        "Yingxuan Ren",
        "Fuming Lai",
        "Shaobing Lian",
        "Jie Tang",
        "Yang You"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Current speech language models generate responses directly without explicit reasoning, leading to errors that cannot be corrected once audio is produced. We introduce \\textbf{``Silent Thought, Spoken Answer''} -- a paradigm where speech LLMs generate internal text reasoning alongside spoken responses, with thinking traces informing speech quality. To realize this, we present \\method{}, the first diffusion-based speech-text language model supporting both understanding and generation, unifying discrete text and tokenized speech under a single masked diffusion framework. Unlike autoregressive approaches, \\method{} jointly generates reasoning traces and speech tokens through iterative denoising, with modality-specific masking schedules. We also construct \\dataset{}, the first speech QA dataset with paired text reasoning traces, containing 26K samples totaling 319 hours. Experiments show \\method{} achieves state-of-the-art speech-to-speech QA accuracy, outperforming the best baseline by up to 9 points, while attaining the best TTS quality among generative models (6.2\\% WER) and preserving language understanding (66.2\\% MMLU). Ablations confirm that both the diffusion architecture and thinking traces contribute to these gains.",
      "url": "https://arxiv.org/abs/2601.22889",
      "pdfUrl": "https://arxiv.org/pdf/2601.22889.pdf",
      "titleJa": "DiffuSpeech: 音声テキスト拡散技術による沈黙の思考と音声回答"
    },
    {
      "id": "2601.22873",
      "arxivId": "2601.22873",
      "title": "EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis",
      "authors": [
        "Li Zhou",
        "Hao Jiang",
        "Junjie Li",
        "Tianrui Wang",
        "Haizhou Li"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "Achieving precise and controllable emotional expression is crucial for producing natural and context-appropriate speech in text-to-speech (TTS) synthesis. However, many emotion-aware TTS systems, including large language model (LLM)-based designs, rely on scaling fixed emotion embeddings or external guidance, limiting their ability to model emotion-specific latent characteristics. To address this gap, we present EmoShift, a lightweight activation-steering framework incorporating a EmoSteer layer, which learns a steering vector for each target emotion in the output embedding space to capture its latent offset and maintain stable, appropriate expression across utterances and categories. With only 10M trainable parameters,less than 1/30 of full fine-tuning, EmoShift outperforms zero-shot and fully fine-tuned baselines in objective and subjective evaluations, enhancing emotional expressiveness while preserving naturalness and speaker similarity. Further analysis confirms the proposed EmoSteer layer's effectiveness and reveals its potential for controllable emotional intensity in speech synthesis.",
      "url": "https://arxiv.org/abs/2601.22873",
      "pdfUrl": "https://arxiv.org/pdf/2601.22873.pdf",
      "titleJa": "EmoShift: 感情認識音声合成を強化する軽量アクティベーションステアリング"
    },
    {
      "id": "2601.22792",
      "arxivId": "2601.22792",
      "title": "CALM: Joint Contextual Acoustic-Linguistic Modeling for Personalization of Multi-Speaker ASR",
      "authors": [
        "Muhammad Shakeel",
        "Yosuke Fukumoto",
        "Chikara Maeda",
        "Chyi-Jiunn Lin",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "We present CALM, a joint Contextual Acoustic-Linguistic Modeling framework for multi-speaker automatic speech recognition (ASR). In personalized AI scenarios, the joint availability of acoustic and linguistic cues naturally motivates the integration of target-speaker conditioning with contextual biasing in overlapping conversations. CALM implements this integration in an end-to-end framework through speaker embedding-driven target-speaker extraction and dynamic vocabulary-based contextual biasing. We evaluate CALM on simulated English (LibriSpeechMix) and Japanese (Corpus of Spontaneous Japanese mixtures, CSJMix). On two-speaker mixtures, CALM reduces biased word error rate (B-WER) from 12.7 to 4.7 on LibriSpeech2Mix and biased character error rate (B-CER) from 16.6 to 8.4 on CSJMix2 (eval3), demonstrating the effectiveness of joint acoustic-linguistic modeling across languages. We additionally report results on the AMI corpus (IHM-mix condition) to validate performance on standardized speech mixtures.",
      "url": "https://arxiv.org/abs/2601.22792",
      "pdfUrl": "https://arxiv.org/pdf/2601.22792.pdf",
      "titleJa": "CALM: 複数話者ASRのパーソナライゼーションのためのコンテキスト音響言語モデリング"
    },
    {
      "id": "2601.22783",
      "arxivId": "2601.22783",
      "title": "Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval",
      "authors": [
        "Ilyass Moummad",
        "Marius Miron",
        "David Robinson",
        "Kawtar Zaher",
        "Hervé Goëau",
        "Olivier Pietquin",
        "Pierre Bonnet",
        "Emmanuel Chemla",
        "Matthieu Geist",
        "Alexis Joly"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.IR",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.",
      "url": "https://arxiv.org/abs/2601.22783",
      "pdfUrl": "https://arxiv.org/pdf/2601.22783.pdf",
      "titleJa": "高速テキストベース野生生物観察検索のためのコンパクトハイパーキューブ埋め込み"
    },
    {
      "id": "2601.22779",
      "arxivId": "2601.22779",
      "title": "Streaming Speech Recognition with Decoder-Only Large Language Models and Latency Optimization",
      "authors": [
        "Genshun Wan",
        "Wenhui Zhang",
        "Jing-Xuan Zhang",
        "Shifu Xiong",
        "Jianqing Gao",
        "Zhongfu Ye"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Recent advances have demonstrated the potential of decoderonly large language models (LLMs) for automatic speech recognition (ASR). However, enabling streaming recognition within this framework remains a challenge. In this work, we propose a novel streaming ASR approach that integrates a read/write policy network with monotonic chunkwise attention (MoChA) to dynamically segment speech embeddings. These segments are interleaved with label sequences during training, enabling seamless integration with the LLM. During inference, the audio stream is buffered until the MoChA module triggers a read signal, at which point the buffered segment together with the previous token is fed into the LLM for the next token prediction. We also introduce a minimal-latency training objective to guide the policy network toward accurate segmentation boundaries. Furthermore, we adopt a joint training strategy in which a non-streaming LLM-ASR model and our streaming model share parameters. Experiments on the AISHELL-1 and AISHELL-2 Mandarin benchmarks demonstrate that our method consistently outperforms recent streaming ASR baselines, achieving character error rates of 5.1% and 5.5%, respectively. The latency optimization results in a 62.5% reduction in average token generation delay with negligible impact on recognition accuracy",
      "url": "https://arxiv.org/abs/2601.22779",
      "pdfUrl": "https://arxiv.org/pdf/2601.22779.pdf",
      "titleJa": "デコーダのみの大規模言語モデルとレイテンシ最適化によるストリーミング音声認識"
    },
    {
      "id": "2601.22764",
      "arxivId": "2601.22764",
      "title": "How Far Can Pretrained LLMs Go in Symbolic Music? Controlled Comparisons of Supervised and Preference-based Adaptation",
      "authors": [
        "Deepak Kumar",
        "Emmanouil Karystinaios",
        "Gerhard Widmer",
        "Markus Schedl"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Music often shares notable parallels with language, motivating the use of pretrained large language models (LLMs) for symbolic music understanding and generation. Despite growing interest, the practical effectiveness of adapting instruction-tuned LLMs to symbolic music remains insufficiently characterized. We present a controlled comparative study of finetuning strategies for ABC-based generation and understanding, comparing an off-the-shelf instruction-tuned backbone to domain-adapted variants and a music-specialized LLM baseline. Across multiple symbolic music corpora and evaluation signals, we provide some insights into adaptation choices for symbolic music applications. We highlight the domain adaptation vs.~preserving prior information tradeoff as well as the distinct behaviour of metrics used to measure the domain adaptation for symbolic music.",
      "url": "https://arxiv.org/abs/2601.22764",
      "pdfUrl": "https://arxiv.org/pdf/2601.22764.pdf",
      "titleJa": "事前学習済みLLMは象徴音楽においてどこまで到達できるか？教師あり適応と選好に基づく適応の制御比較"
    },
    {
      "id": "2601.22661",
      "arxivId": "2601.22661",
      "title": "Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability",
      "authors": [
        "Yong Ren",
        "Jingbei Li",
        "Haiyang Sun",
        "Yujie Chen",
        "Cheng Yi",
        "Yechang Huang",
        "Hao Gu",
        "Ye Bai",
        "Xuerui Yang"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Recent advances in Large Audio Language Models (LALMs) have extended Text-to-Speech (TTS) to interactive role-play scenarios, which demand high expressiveness and strict adherence to role-play instructions. However, existing models struggle to maintain stylistic consistency with character profiles and scene descriptions across multi-turn dialogues. A critical bottleneck is the lack of objective metrics for quantifying speaking style. To bridge this gap, we propose Mean Continuation Log-Probability (MCLP) as both an evaluation metric and a reward signal, validated on LALM-based Role-Play TTS (RP-TTS) tasks. Critically, we leverage the In-Context Learning capability of pre-trained LALMs to formulate MCLP via a continuation log-probability prediction. This metric quantifies stylistic consistency by measuring the likelihood of the ground-truth speech conditioned on the generated speech. Furthermore, we employ MCLP as a reinforcement learning reward to enhance the style alignment between generated speech and Role-Play instructions. To facilitate evaluation, we construct an RP-TTS dataset with rich scene and character annotations. Experimental results demonstrate that our method significantly outperforms strong LALM baselines on both objective and subjective metrics.",
      "url": "https://arxiv.org/abs/2601.22661",
      "pdfUrl": "https://arxiv.org/pdf/2601.22661.pdf",
      "titleJa": "平均継続対数確率による表現ロールプレイTTSのLALMの評価と報酬"
    },
    {
      "id": "2601.22599",
      "arxivId": "2601.22599",
      "title": "A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation",
      "authors": [
        "Kai Li",
        "Jintao Cheng",
        "Chang Zeng",
        "Zijun Yan",
        "Helin Wang",
        "Zixiong Su",
        "Bo Zheng",
        "Xiaolin Hu"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.HC"
      ],
      "abstract": "Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from a data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events. These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via a semantically consistent synthesis protocol. Utilizing this pipeline, we constructed Hive, a high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on a huge dataset $\\sim$500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-of-distribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency, offering a new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive.",
      "url": "https://arxiv.org/abs/2601.22599",
      "pdfUrl": "https://arxiv.org/pdf/2601.22599.pdf",
      "titleJa": "データ効率の高いクエリベースのユニバーサルサウンド分離のための意味的に一貫性のあるデータセット"
    },
    {
      "id": "2601.22501",
      "arxivId": "2601.22501",
      "title": "MIRRORTALK: Forging Personalized Avatars Via Disentangled Style and Hierarchical Motion Control",
      "authors": [
        "Renjie Lu",
        "Xulong Zhang",
        "Xiaoyang Qu",
        "Jianzong Wang",
        "Shangfei Wang"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.CV",
        "cs.SD"
      ],
      "abstract": "Synthesizing personalized talking faces that uphold and highlight a speaker's unique style while maintaining lip-sync accuracy remains a significant challenge. A primary limitation of existing approaches is the intrinsic confounding of speaker-specific talking style and semantic content within facial motions, which prevents the faithful transfer of a speaker's unique persona to arbitrary speech. In this paper, we propose MirrorTalk, a generative framework based on a conditional diffusion model, combined with a Semantically-Disentangled Style Encoder (SDSE) that can distill pure style representations from a brief reference video. To effectively utilize this representation, we further introduce a hierarchical modulation strategy within the diffusion process. This mechanism guides the synthesis by dynamically balancing the contributions of audio and style features across distinct facial regions, ensuring both precise lip-sync accuracy and expressive full-face dynamics. Extensive experiments demonstrate that MirrorTalk achieves significant improvements over state-of-the-art methods in terms of lip-sync accuracy and personalization preservation.",
      "url": "https://arxiv.org/abs/2601.22501",
      "pdfUrl": "https://arxiv.org/pdf/2601.22501.pdf",
      "titleJa": "MIRRORTALK: 分離したスタイルと階層的なモーションコントロールによるパーソナライズされたアバターの作成"
    },
    {
      "id": "2601.22480",
      "arxivId": "2601.22480",
      "title": "Rethinking Speech Representation Aggregation in Speech Enhancement: A Phonetic Mutual Information Perspective",
      "authors": [
        "Seungu Han",
        "Sungho Lee",
        "Kyogu Lee"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Recent speech enhancement (SE) models increasingly leverage self-supervised learning (SSL) representations for their rich semantic information. Typically, intermediate features are aggregated into a single representation via a lightweight adaptation module. However, most SSL models are not trained for noise robustness, which can lead to corrupted semantic representations. Moreover, the adaptation module is trained jointly with the SE model, potentially prioritizing acoustic details over semantic information, contradicting the original purpose. To address this issue, we first analyze the behavior of SSL models on noisy speech from an information-theoretic perspective. Specifically, we measure the mutual information (MI) between the corrupted SSL representations and the corresponding phoneme labels, focusing on preservation of linguistic contents. Building upon this analysis, we introduce the linguistic aggregation layer, which is pre-trained to maximize MI with phoneme labels (with optional dynamic aggregation) and then frozen during SE training. Experiments show that this decoupled approach improves Word Error Rate (WER) over jointly optimized baselines, demonstrating the benefit of explicitly aligning the adaptation module with linguistic contents.",
      "url": "https://arxiv.org/abs/2601.22480",
      "pdfUrl": "https://arxiv.org/pdf/2601.22480.pdf",
      "titleJa": "音声強調における音声表現集約の再考：音声相互情報量の観点"
    },
    {
      "id": "2601.22390",
      "arxivId": "2601.22390",
      "title": "An Effective Energy Mask-based Adversarial Evasion Attacks against Misclassification in Speaker Recognition Systems",
      "authors": [
        "Chanwoo Park",
        "Chanwoo Kim"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.SD",
        "cs.CR",
        "eess.AS"
      ],
      "abstract": "Evasion attacks pose significant threats to AI systems, exploiting vulnerabilities in machine learning models to bypass detection mechanisms. The widespread use of voice data, including deepfakes, in promising future industries is currently hindered by insufficient legal frameworks. Adversarial attack methods have emerged as the most effective countermeasure against the indiscriminate use of such data. This research introduces masked energy perturbation (MEP), a novel approach using power spectrum for energy masking of original voice data. MEP applies masking to small energy regions in the frequency domain before generating adversarial perturbations, targeting areas less noticeable to the human auditory model. The study primarily employs advanced speaker recognition models, including ECAPA-TDNN and ResNet34, which have shown remarkable performance in speaker verification tasks. The proposed MEP method demonstrated strong performance in both audio quality and evasion effectiveness. The energy masking approach effectively minimizes the perceptual evaluation of speech quality (PESQ) degradation, indicating that minimal perceptual distortion occurs to the human listener despite the adversarial perturbations. Specifically, in the PESQ evaluation, the relative performance of the MEP method was 26.68% when compared to the fast gradient sign method (FGSM) and iterative FGSM.",
      "url": "https://arxiv.org/abs/2601.22390",
      "pdfUrl": "https://arxiv.org/pdf/2601.22390.pdf",
      "titleJa": "話者認識システムにおける誤分類に対する効果的なエネルギーマスクベースの敵対的回避攻撃"
    },
    {
      "id": "2601.21960",
      "arxivId": "2601.21960",
      "title": "TidyVoice 2026 Challenge Evaluation Plan",
      "authors": [
        "Aref Farhadipour",
        "Jan Marquenie",
        "Srikanth Madikeri",
        "Teodora Vukovic",
        "Volker Dellwo",
        "Kathy Reid",
        "Francis M. Tyers",
        "Ingo Siegert",
        "Eleanor Chodroff"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "The performance of speaker verification systems degrades significantly under language mismatch, a critical challenge exacerbated by the field's reliance on English-centric data. To address this, we propose the TidyVoice Challenge for cross-lingual speaker verification. The challenge leverages the TidyVoiceX dataset from the novel TidyVoice benchmark, a large-scale, multilingual corpus derived from Mozilla Common Voice, and specifically curated to isolate the effect of language switching across approximately 40 languages. Participants will be tasked with building systems robust to this mismatch, with performance primarily evaluated using the Equal Error Rate on cross-language trials. By providing standardized data, open-source baselines, and a rigorous evaluation protocol, this challenge aims to drive research towards fairer, more inclusive, and language-independent speaker recognition technologies, directly aligning with the Interspeech 2026 theme, \"Speaking Together.\"",
      "url": "https://arxiv.org/abs/2601.21960",
      "pdfUrl": "https://arxiv.org/pdf/2601.21960.pdf",
      "titleJa": "TidyVoice 2026チャレンジ評価計画"
    },
    {
      "id": "2601.21925",
      "arxivId": "2601.21925",
      "title": "Localizing Speech Deepfakes Beyond Transitions via Segment-Aware Learning",
      "authors": [
        "Yuchen Mao",
        "Wen Huang",
        "Yanmin Qian"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Localizing partial deepfake audio, where only segments of speech are manipulated, remains challenging due to the subtle and scattered nature of these modifications. Existing approaches typically rely on frame-level predictions to identify spoofed segments, and some recent methods improve performance by concentrating on the transitions between real and fake audio. However, we observe that these models tend to over-rely on boundary artifacts while neglecting the manipulated content that follows. We argue that effective localization requires understanding the entire segments beyond just detecting transitions. Thus, we propose Segment-Aware Learning (SAL), a framework that encourages models to focus on the internal structure of segments. SAL introduces two core techniques: Segment Positional Labeling, which provides fine-grained frame supervision based on relative position within a segment; and Cross-Segment Mixing, a data augmentation method that generates diverse segment patterns. Experiments across multiple deepfake localization datasets show that SAL consistently achieves strong performance in both in-domain and out-of-domain settings, with notable gains in non-boundary regions and reduced reliance on transition artifacts. The code is available at https://github.com/SentryMao/SAL.",
      "url": "https://arxiv.org/abs/2601.21925",
      "pdfUrl": "https://arxiv.org/pdf/2601.21925.pdf",
      "titleJa": "セグメント認識学習による音声ディープフェイクのトランジションを超えたローカライズ"
    },
    {
      "id": "2601.21740",
      "arxivId": "2601.21740",
      "title": "MIDI-LLaMA: An Instruction-Following Multimodal LLM for Symbolic Music Understanding",
      "authors": [
        "Meng Yang",
        "Jon McCormack",
        "Maria Teresa Llano",
        "Wanchao Su",
        "Chao Lei"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLM) for audio music have demonstrated strong capabilities in music understanding, yet symbolic music, a fundamental representation of musical structure, remains unexplored. In this work, we introduce MIDI-LLaMA, the first instruction-following MLLM for symbolic music understanding. Our approach aligns the MIDI encoder MusicBERT and Llama-3-8B via a two-stage pipeline comprising feature alignment and instruction tuning. To support training, we design a scalable annotation pipeline that annotates GiantMIDI-Piano with fine-grained metadata, resulting in a MIDI-text dataset. Compared with the baseline trained on converting MIDI into ABC notation under the same instruction-tuning procedure, MIDI-LLaMA substantially outperforms in captioning and semantic alignment in question answering. Human evaluation further confirms the advantages of MIDI-LLaMA in music understanding, emotion recognition, creativity, and overall preference. These findings demonstrate that incorporating symbolic music into large language models enhances their capacity for musical understanding.",
      "url": "https://arxiv.org/abs/2601.21740",
      "pdfUrl": "https://arxiv.org/pdf/2601.21740.pdf",
      "titleJa": "MIDI-LLaMA: 記号的音楽理解のための指示追従型マルチモーダルLLM"
    },
    {
      "id": "2601.21612",
      "arxivId": "2601.21612",
      "title": "Representation-Regularized Convolutional Audio Transformer for Audio Understanding",
      "authors": [
        "Bing Han",
        "Chushu Zhou",
        "Yifan Yang",
        "Wei Wang",
        "Chenda Li",
        "Wangyou Zhang",
        "Yanmin Qian"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Bootstrap-based Self-Supervised Learning (SSL) has achieved remarkable progress in audio understanding. However, existing methods typically operate at a single level of granularity, limiting their ability to model the diverse temporal and spectral structures inherent in complex audio signals. Furthermore, bootstrapping representations from scratch is computationally expensive, often requiring extensive training to converge. In this work, we propose the Convolutional Audio Transformer (CAT), a unified framework designed to address these challenges. First, to capture hierarchical audio features, CAT incorporates a Multi-resolution Block that aggregates information across varying granularities. Second, to enhance training efficiency, we introduce a Representation Regularization objective. Drawing inspiration from generative modeling, this auxiliary task guides the student model by aligning its predictions with high-quality semantic representations from frozen, pre-trained external encoders. Experimental results demonstrate that CAT significantly outperforms baselines on audio understanding benchmarks. Notably, it achieves competitive performance on the AudioSet 20k dataset with 5 times faster convergence than existing methods. Codes and checkpoints will be released soon at https://github.com/realzhouchushu/CAT.",
      "url": "https://arxiv.org/abs/2601.21612",
      "pdfUrl": "https://arxiv.org/pdf/2601.21612.pdf",
      "titleJa": "音声理解のための表現正規化畳み込み音声変換器"
    },
    {
      "id": "2601.21463",
      "arxivId": "2601.21463",
      "title": "Unifying Speech Editing Detection and Content Localization via Prior-Enhanced Audio LLMs",
      "authors": [
        "Jun Xue",
        "Yi Chai",
        "Yanzhen Ren",
        "Jinshen He",
        "Zhiqiang Tang",
        "Zhuolin Yi",
        "Yihuan Huang",
        "Yuankun Xie",
        "Yujie Chen"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Speech editing achieves semantic inversion by performing fine-grained segment-level manipulation on original utterances, while preserving global perceptual naturalness. Existing detection studies mainly focus on manually edited speech with explicit splicing artifacts, and therefore struggle to cope with emerging end-to-end neural speech editing techniques that generate seamless acoustic transitions. To address this challenge, we first construct a large-scale bilingual dataset, AiEdit, which leverages large language models to drive precise semantic tampering logic and employs multiple advanced neural speech editing methods for data synthesis, thereby filling the gap of high-quality speech editing datasets. Building upon this foundation, we propose PELM (Prior-Enhanced Audio Large Language Model), the first large-model framework that unifies speech editing detection and content localization by formulating them as an audio question answering task. To mitigate the inherent forgery bias and semantic-priority bias observed in existing audio large models, PELM incorporates word-level probability priors to provide explicit acoustic cues, and further designs a centroid-aggregation-based acoustic consistency perception loss to explicitly enforce the modeling of subtle local distribution anomalies. Extensive experimental results demonstrate that PELM significantly outperforms state-of-the-art methods on both the HumanEdit and AiEdit datasets, achieving equal error rates (EER) of 0.57\\% and 9.28\\% (localization), respectively.",
      "url": "https://arxiv.org/abs/2601.21463",
      "pdfUrl": "https://arxiv.org/pdf/2601.21463.pdf",
      "titleJa": "事前強化オーディオ LLM による音声編集検出とコンテンツ ローカリゼーションの統合"
    },
    {
      "id": "2601.23196",
      "arxivId": "2601.23196",
      "title": "Beyond Omnidirectional: Neural Ambisonics Encoding for Arbitrary Microphone Directivity Patterns using Cross-Attention",
      "authors": [
        "Mikko Heikkinen",
        "Archontis Politis",
        "Konstantinos Drossos",
        "Tuomas Virtanen"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We present a deep neural network approach for encoding microphone array signals into Ambisonics that generalizes to arbitrary microphone array configurations with fixed microphone count but varying locations and frequency-dependent directional characteristics. Unlike previous methods that rely only on array geometry as metadata, our approach uses directional array transfer functions, enabling accurate characterization of real-world arrays. The proposed architecture employs separate encoders for audio and directional responses, combining them through cross-attention mechanisms to generate array-independent spatial audio representations. We evaluate the method on simulated data in two settings: a mobile phone with complex body scattering, and a free-field condition, both with varying numbers of sound sources in reverberant environments. Evaluations demonstrate that our approach outperforms both conventional digital signal processing-based methods and existing deep neural network solutions. Furthermore, using array transfer functions instead of geometry as metadata input improves accuracy on realistic arrays.",
      "url": "https://arxiv.org/abs/2601.23196",
      "pdfUrl": "https://arxiv.org/pdf/2601.23196.pdf",
      "titleJa": "全方向性を超えて：クロスアテンションを用いた任意のマイク指向性パターンのニューラルアンビソニックス符号化"
    },
    {
      "id": "2601.23004",
      "arxivId": "2601.23004",
      "title": "Layer-Aware Early Fusion of Acoustic and Linguistic Embeddings for Cognitive Status Classification",
      "authors": [
        "Krystof Novotny",
        "Laureano Moro-Velázquez",
        "Jiri Mekyska"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Speech contains both acoustic and linguistic patterns that reflect cognitive decline, and therefore models describing only one domain cannot fully capture such complexity. This study investigates how early fusion (EF) of speech and its corresponding transcription text embeddings, with attention to encoder layer depth, can improve cognitive status classification. Using a DementiaBank-derived collection of recordings (1,629 speakers; cognitively normal controls$\\unicode{x2013}$CN, Mild Cognitive Impairment$\\unicode{x2013}$MCI, and Alzheimer's Disease and Related Dementias$\\unicode{x2013}$ADRD), we extracted frame-aligned embeddings from different internal layers of wav2vec 2.0 or Whisper combined with DistilBERT or RoBERTa. Unimodal, EF and late fusion (LF) models were trained with a transformer classifier, optimized, and then evaluated across 10 seeds. Performance consistently peaked in mid encoder layers ($\\sim$8$\\unicode{x2013}$10), with the single best F1 at Whisper + RoBERTa layer 9 and the best log loss at Whisper + DistilBERT layer 10. Acoustic-only models consistently outperformed text-only variants. EF boosts discrimination for genuinely acoustic embeddings, whereas LF improves probability calibration. Layer choice critically shapes clinical multimodal synergy.",
      "url": "https://arxiv.org/abs/2601.23004",
      "pdfUrl": "https://arxiv.org/pdf/2601.23004.pdf",
      "titleJa": "認知状態分類のための音響と言語埋め込みのレイヤー認識型早期融合"
    },
    {
      "id": "2601.22504",
      "arxivId": "2601.22504",
      "title": "Class-Aware Permutation-Invariant Signal-to-Distortion Ratio for Semantic Segmentation of Sound Scene with Same-Class Sources",
      "authors": [
        "Binh Thien Nguyen",
        "Masahiro Yasuda",
        "Daiki Takeuchi",
        "Daisuke Niizumi",
        "Noboru Harada"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "eess.AS"
      ],
      "abstract": "To advance immersive communication, the Detection and Classification of Acoustic Scenes and Events (DCASE) 2025 Challenge recently introduced Task 4 on Spatial Semantic Segmentation of Sound Scenes (S5). An S5 system takes a multi-channel audio mixture as input and outputs single-channel dry sources along with their corresponding class labels. Although the DCASE 2025 Challenge simplifies the task by constraining class labels in each mixture to be mutually exclusive, real-world mixtures frequently contain multiple sources from the same class. The presence of duplicated labels can significantly degrade the performance of the label-queried source separation (LQSS) model, which is the key component of many existing S5 systems, and can also limit the validity of the official evaluation metric of DCASE 2025 Task 4. To address these issues, we propose a class-aware permutation-invariant loss function that enables the LQSS model to handle queries involving duplicated labels. In addition, we redesign the S5 evaluation metric to eliminate ambiguities caused by these same-class sources. To evaluate the proposed method within the S5 system, we extend the label prediction model to support same-class labels. Experimental results demonstrate the effectiveness of the proposed methods and the robustness of the new metric on mixtures both with and without same-class sources.",
      "url": "https://arxiv.org/abs/2601.22504",
      "pdfUrl": "https://arxiv.org/pdf/2601.22504.pdf",
      "titleJa": "同一クラス音源によるサウンドシーンの意味的セグメンテーションのためのクラスを考慮した順列不変な信号対歪み比"
    },
    {
      "id": "2601.22319",
      "arxivId": "2601.22319",
      "title": "Optimizing Domain-Adaptive Self-Supervised Learning for Clinical Voice-Based Disease Classification",
      "authors": [
        "Weixin Liu",
        "Bowen Qu",
        "Matthew Pontell",
        "Maria Powell",
        "Bradley Malin",
        "Zhijun Yin"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "The human voice is a promising non-invasive digital biomarker, yet deep learning for voice-based health analysis is hindered by data scarcity and domain mismatch, where models pre-trained on general audio fail to capture the subtle pathological features characteristic of clinical voice data. To address these challenges, we investigate domain-adaptive self-supervised learning (SSL) with Masked Autoencoders (MAE) and demonstrate that standard configurations are suboptimal for health-related audio. Using the Bridge2AI-Voice dataset, a multi-institutional collection of pathological voices, we systematically examine three performance-critical factors: reconstruction loss (Mean Absolute Error vs. Mean Squared Error), normalization (patch-wise vs. global), and masking (random vs. content-aware). Our optimized design, which combines Mean Absolute Error (MA-Error) loss, patch-wise normalization, and content-aware masking, achieves a Macro F1 of $0.688 \\pm 0.009$ (over 10 fine-tuning runs), outperforming a strong out-of-domain SSL baseline pre-trained on large-scale general audio, which has a Macro F1 of $0.663 \\pm 0.011$. The results show that MA-Error loss improves robustness and content-aware masking boosts performance by emphasizing information-rich regions. These findings highlight the importance of component-level optimization in data-constrained medical applications that rely on audio data.",
      "url": "https://arxiv.org/abs/2601.22319",
      "pdfUrl": "https://arxiv.org/pdf/2601.22319.pdf",
      "titleJa": "臨床音声に基づく疾患分類のためのドメイン適応型自己教師学習の最適化"
    },
    {
      "id": "2601.22306",
      "arxivId": "2601.22306",
      "title": "Sylber 2.0: A Universal Syllable Embedding",
      "authors": [
        "Cheol Jun Cho",
        "Nicholas Lee",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "abstract": "Scaling spoken language modeling requires speech tokens that are both efficient and universal. Recent work has proposed syllables as promising speech tokens at low temporal resolution, but existing models are constrained to English and fail to capture sufficient acoustic detail. To address this gap, we present Sylber 2.0, a self-supervised framework for coding speech at the syllable level that enables efficient temporal compression and high-fidelity reconstruction. Sylber 2.0 achieves a very low token frequency around 5 Hz, while retaining both linguistic and acoustic detail across multiple languages and expressive styles. Experiments show that it performs on par with previous models operating on high-frequency baselines. Furthermore, Sylber 2.0 enables efficient TTS modeling which can generate speech with competitive intelligibility and quality with SOTA models using only 72M parameters. Moreover, the universality of Sylber 2.0 provides more effective features for low resource ASR than previous speech coding frameworks. In sum, we establish an effective syllable-level abstraction for general spoken language.",
      "url": "https://arxiv.org/abs/2601.22306",
      "pdfUrl": "https://arxiv.org/pdf/2601.22306.pdf",
      "titleJa": "Sylber 2.0: ユニバーサル音節埋め込み"
    },
    {
      "id": "2601.22288",
      "arxivId": "2601.22288",
      "title": "PersonaCite: VoC-Grounded Interviewable Agentic Synthetic AI Personas for Verifiable User and Design Research",
      "authors": [
        "Mario Truss"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.HC",
        "cs.AI",
        "eess.AS",
        "eess.IV"
      ],
      "abstract": "LLM-based and agent-based synthetic personas are increasingly used in design and product decision-making, yet prior work shows that prompt-based personas often produce persuasive but unverifiable responses that obscure their evidentiary basis. We present PersonaCite, an agentic system that reframes AI personas as evidence-bounded research instruments through retrieval-augmented interaction. Unlike prior approaches that rely on prompt-based roleplaying, PersonaCite retrieves actual voice-of-customer artifacts during each conversation turn, constrains responses to retrieved evidence, explicitly abstains when evidence is missing, and provides response-level source attribution. Through semi-structured interviews and deployment study with 14 industry experts, we identify preliminary findings on perceived benefits, validity concerns, and design tensions, and propose Persona Provenance Cards as a documentation pattern for responsible AI persona use in human-centered design workflows.",
      "url": "https://arxiv.org/abs/2601.22288",
      "pdfUrl": "https://arxiv.org/pdf/2601.22288.pdf",
      "titleJa": "PersonaCite: 検証可能なユーザーおよびデザインリサーチのための、VoC に基づいたインタビュー可能なエージェント型合成 AI ペルソナ"
    },
    {
      "id": "2601.22260",
      "arxivId": "2601.22260",
      "title": "Brain-Informed Speech Separation for Cochlear Implants",
      "authors": [
        "Tom Gajecki",
        "Jonas Althoff",
        "Waldo Nogueira"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "We propose a brain-informed speech separation method for cochlear implants (CIs) that uses electroencephalography (EEG)-derived attention cues to guide enhancement toward the attended speaker. An attention-guided network fuses audio mixtures with EEG features through a lightweight fusion layer, producing attended-source electrodograms for CI stimulation while resolving the label-permutation ambiguity of audio-only separators. Robustness to degraded attention cues is improved with a mixed curriculum that varies cue quality during training, yielding stable gains even when EEG-speech correlation is moderate. In multi-talker conditions, the model achieves higher signal-to-interference ratio improvements than an audio-only electrodogram baseline while remaining slightly smaller (167k vs. 171k parameters). With 2 ms algorithmic latency and comparable cost, the approach highlights the promise of coupling auditory and neural cues for cognitively adaptive CI processing.",
      "url": "https://arxiv.org/abs/2601.22260",
      "pdfUrl": "https://arxiv.org/pdf/2601.22260.pdf",
      "titleJa": "人工内耳のための脳情報に基づく音声分離"
    },
    {
      "id": "2601.21940",
      "arxivId": "2601.21940",
      "title": "DisContSE: Single-Step Diffusion Speech Enhancement Based on Joint Discrete and Continuous Embeddings",
      "authors": [
        "Yihui Fu",
        "Tim Fingscheidt"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Diffusion speech enhancement on discrete audio codec features gain immense attention due to their improved speech component reconstruction capability. However, they usually suffer from high inference computational complexity due to multiple reverse process iterations. Furthermore, they generally achieve promising results on non-intrusive metrics but show poor performance on intrusive metrics, as they may struggle in reconstructing the correct phones. In this paper, we propose DisContSE, an efficient diffusion-based speech enhancement model on joint discrete codec tokens and continuous embeddings. Our contributions are three-fold. First, we formulate both a discrete and a continuous enhancement module operating on discrete audio codec tokens and continuous embeddings, respectively, to achieve improved fidelity and intelligibility simultaneously. Second, a semantic enhancement module is further adopted to achieve optimal phonetic accuracy. Third, we achieve a single-step efficient reverse process in inference with a novel quantization error mask initialization strategy, which, according to our knowledge, is the first successful single-step diffusion speech enhancement based on an audio codec. Trained and evaluated on URGENT 2024 Speech Enhancement Challenge data splits, the proposed DisContSE excels top-reported time- and frequency-domain diffusion baseline methods in PESQ, POLQA, UTMOS, and in a subjective ITU-T P.808 listening test, clearly achieving an overall top rank.",
      "url": "https://arxiv.org/abs/2601.21940",
      "pdfUrl": "https://arxiv.org/pdf/2601.21940.pdf",
      "titleJa": "DisContSE: 離散的および連続的な埋め込みに基づくシングルステップ拡散音声強調"
    },
    {
      "id": "2601.21886",
      "arxivId": "2601.21886",
      "title": "Speech Quality-Based Localization of Low-Quality Speech and Text-to-Speech Synthesis Artefacts",
      "authors": [
        "Michael Kuhlmann",
        "Alexander Werning",
        "Thilo von Neumann",
        "Reinhold Haeb-Umbach"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "eess.AS"
      ],
      "abstract": "A large number of works view the automatic assessment of speech from an utterance- or system-level perspective. While such approaches are good in judging overall quality, they cannot adequately explain why a certain score was assigned to an utterance. frame-level scores can provide better interpretability, but models predicting them are harder to tune and regularize since no strong targets are available during training. In this work, we show that utterance-level speech quality predictors can be regularized with a segment-based consistency constraint which notably reduces frame-level stochasticity. We then demonstrate two applications involving frame-level scores: The partial spoof scenario and the detection of synthesis artefacts in two state-of-the-art text-to-speech systems. For the latter, we perform listening tests and confirm that listeners rate segments to be of poor quality more often in the set defined by low frame-level scores than in a random control set.",
      "url": "https://arxiv.org/abs/2601.21886",
      "pdfUrl": "https://arxiv.org/pdf/2601.21886.pdf",
      "titleJa": "低品質音声およびテキスト音声合成アーティファクトの音声品質ベースの定位"
    },
    {
      "id": "2601.21402",
      "arxivId": "2601.21402",
      "title": "SemanticAudio: Audio Generation and Editing in Semantic Space",
      "authors": [
        "Zheqi Dai",
        "Guangyan Zhang",
        "Haolin He",
        "Xiquan Li",
        "Jingyu Li",
        "Chunyat Wu",
        "Yiwen Guo",
        "Qiuqiang Kong"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "In recent years, Text-to-Audio Generation has achieved remarkable progress, offering sound creators powerful tools to transform textual inspirations into vivid audio. However, existing models predominantly operate directly in the acoustic latent space of a Variational Autoencoder (VAE), often leading to suboptimal alignment between generated audio and textual descriptions. In this paper, we introduce SemanticAudio, a novel framework that conducts both audio generation and editing directly in a high-level semantic space. We define this semantic space as a compact representation capturing the global identity and temporal sequence of sound events, distinct from fine-grained acoustic details. SemanticAudio employs a two-stage Flow Matching architecture: the Semantic Planner first generates these compact semantic features to sketch the global semantic layout, and the Acoustic Synthesizer subsequently produces high-fidelity acoustic latents conditioned on this semantic plan. Leveraging this decoupled design, we further introduce a training-free text-guided editing mechanism that enables precise attribute-level modifications on general audio without retraining. Specifically, this is achieved by steering the semantic generation trajectory via the difference of velocity fields derived from source and target text prompts. Extensive experiments demonstrate that SemanticAudio surpasses existing mainstream approaches in semantic alignment. Demo available at: https://semanticaudio1.github.io/",
      "url": "https://arxiv.org/abs/2601.21402",
      "pdfUrl": "https://arxiv.org/pdf/2601.21402.pdf",
      "titleJa": "SemanticAudio: セマンティック空間におけるオーディオ生成と編集"
    },
    {
      "id": "2601.21347",
      "arxivId": "2601.21347",
      "title": "Towards Robust Dysarthric Speech Recognition: LLM-Agent Post-ASR Correction Beyond WER",
      "authors": [
        "Xiuwen Zheng",
        "Sixun Dong",
        "Bornali Phukon",
        "Mark Hasegawa-Johnson",
        "Chang D. Yoo"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "While Automatic Speech Recognition (ASR) is typically benchmarked by word error rate (WER), real-world applications ultimately hinge on semantic fidelity. This mismatch is particularly problematic for dysarthric speech, where articulatory imprecision and disfluencies can cause severe semantic distortions. To bridge this gap, we introduce a Large Language Model (LLM)-based agent for post-ASR correction: a Judge-Editor over the top-k ASR hypotheses that keeps high-confidence spans, rewrites uncertain segments, and operates in both zero-shot and fine-tuned modes. In parallel, we release SAP-Hypo5, the largest benchmark for dysarthric speech correction, to enable reproducibility and future exploration. Under multi-perspective evaluation, our agent achieves a 14.51% WER reduction alongside substantial semantic gains, including a +7.59 pp improvement in MENLI and +7.66 pp in Slot Micro F1 on challenging samples. Our analysis further reveals that WER is highly sensitive to domain shift, whereas semantic metrics correlate more closely with downstream task performance.",
      "url": "https://arxiv.org/abs/2601.21347",
      "pdfUrl": "https://arxiv.org/pdf/2601.21347.pdf",
      "titleJa": "堅牢な構音障害音声認識に向けて：WERを超えるLLMエージェントによるASR後修正"
    },
    {
      "id": "2601.21337",
      "arxivId": "2601.21337",
      "title": "Qwen3-ASR Technical Report",
      "authors": [
        "Xian Shi",
        "Xiong Wang",
        "Zhifang Guo",
        "Yongqi Wang",
        "Pei Zhang",
        "Xinyu Zhang",
        "Zishan Guo",
        "Hongkun Hao",
        "Yu Xi",
        "Baosong Yang",
        "Jin Xu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.",
      "url": "https://arxiv.org/abs/2601.21337",
      "pdfUrl": "https://arxiv.org/pdf/2601.21337.pdf",
      "titleJa": "Qwen3-ASR 技術レポート"
    },
    {
      "id": "2601.21264",
      "arxivId": "2601.21264",
      "title": "Evaluating Spatialized Auditory Cues for Rapid Attention Capture in XR",
      "authors": [
        "Yoonsang Kim",
        "Swapnil Dey",
        "Arie Kaufman"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "In time-critical eXtended reality (XR) scenarios where users must rapidly reorient their attention to hazards, alerts, or instructions while engaged in a primary task, spatial audio can provide an immediate directional cue without occupying visual bandwidth. However, such scenarios can afford only a brief auditory exposure, requiring users to interpret sound direction quickly and without extended listening or head-driven refinement. This paper reports a controlled exploratory study of rapid spatial-audio localization in XR. Using HRTF-rendered broadband stimuli presented from a semi-dense set of directions around the listener, we quantify how accurately users can infer coarse direction from brief audio alone. We further examine the effects of short-term visuo-auditory feedback training as a lightweight calibration mechanism. Our findings show that brief spatial cues can convey coarse directional information, and that even short calibration can improve users' perception of aural signals. While these results highlight the potential of spatial audio for rapid attention guidance, they also show that auditory cues alone may not provide sufficient precision for complex or high-stakes tasks, and that spatial audio may be most effective when complemented by other sensory modalities or visual cues, without relying on head-driven refinement. We leverage this study on spatial audio as a preliminary investigation into a first-stage attention-guidance channel for wearable XR (e.g., VR head-mounted displays and AR smart glasses), and provide design insights on stimulus selection and calibration for time-critical use.",
      "url": "https://arxiv.org/abs/2601.21264",
      "pdfUrl": "https://arxiv.org/pdf/2601.21264.pdf",
      "titleJa": "XRにおける迅速な注意喚起のための空間化された聴覚手がかりの評価"
    },
    {
      "id": "2601.23286",
      "arxivId": "2601.23286",
      "title": "VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation",
      "authors": [
        "Hongyang Du",
        "Junjie Ye",
        "Xiaoyan Cong",
        "Runhao Li",
        "Jingcheng Ni",
        "Aman Agarwal",
        "Zeqi Zhou",
        "Zekun Li",
        "Randall Balestriero",
        "Yue Wang"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.",
      "url": "https://arxiv.org/abs/2601.23286",
      "pdfUrl": "https://arxiv.org/pdf/2601.23286.pdf",
      "titleJa": "VideoGPA: 3D一貫性のあるビデオ生成のためのジオメトリ事前抽出"
    },
    {
      "id": "2601.23285",
      "arxivId": "2601.23285",
      "title": "End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms",
      "authors": [
        "MH Farhadi",
        "Ali Rabiee",
        "Sima Ghafoori",
        "Anna Cetera",
        "Andrew Fisher",
        "Reza Abiri"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "abstract": "Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline conditions collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Our results confirmed that integrated optimization is most beneficial in complex, goal-ambiguous scenarios, and is generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.",
      "url": "https://arxiv.org/abs/2601.23285",
      "pdfUrl": "https://arxiv.org/pdf/2601.23285.pdf",
      "titleJa": "共有自律パラダイムにおける信念とポリシー学習のエンドツーエンド最適化"
    },
    {
      "id": "2601.23266",
      "arxivId": "2601.23266",
      "title": "IRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models",
      "authors": [
        "Seyed Ahmad Hosseini Miangoleh",
        "Amin Jalal Aghdasian",
        "Farzaneh Abdollahi"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "abstract": "This paper proposes a novel inverse reinforcement learning framework using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are combined with an IRL discriminator signal to align with expert goals. Reinforcement learning (RL) is then performed with a hybrid reward that combines diffuse environmental feedback and targeted IRL rewards. A conditional diffusion model, which acts as a safety supervisor, plans safe paths. It stays in its lane, avoids obstacles, and moves smoothly. Then, a learnable adaptive mask (LAM) improves perception. It shifts visual attention based on vehicle speed and nearby hazards. After FSM-based imitation, the policy is fine-tuned with Proximal Policy Optimization (PPO). Training is run in the Webots simulator with a two-stage curriculum. A 96\\% success rate is reached, and collisions are reduced to 0.05 per 1k steps, marking a new benchmark for safe navigation. By applying the proposed approach, the agent not only drives in lane but also handles unsafe conditions at an expert level, increasing robustness.We make our code publicly available.",
      "url": "https://arxiv.org/abs/2601.23266",
      "pdfUrl": "https://arxiv.org/pdf/2601.23266.pdf",
      "titleJa": "IRL-DAL: エネルギー誘導拡散モデルによる自動運転のための安全かつ適応的な軌道計画"
    },
    {
      "id": "2601.23261",
      "arxivId": "2601.23261",
      "title": "TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training",
      "authors": [
        "Ruijie Zhang",
        "Yequan Zhao",
        "Ziyue Liu",
        "Zhengyang Wang",
        "Dongyang Li",
        "Yupeng Su",
        "Sijia Liu",
        "Zheng Zhang"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "The Muon optimizer has demonstrated strong empirical performance in pre-training large language models by performing matrix-level gradient (or momentum) orthogonalization in each layer independently. In this work, we propose TEON, a principled generalization of Muon that extends orthogonalization beyond individual layers by modeling the gradients of a neural network as a structured higher-order tensor. We present TEON's improved convergence guarantee over layer-wise Muon, and further develop a practical instantiation of TEON based on the theoretical analysis with corresponding ablation. We evaluate our approach on two widely adopted architectures: GPT-style models, ranging from 130M to 774M parameters, and LLaMA-style models, ranging from 60M to 1B parameters. Experimental results show that TEON consistently improves training and validation perplexity across model scales and exhibits strong robustness under various approximate SVD schemes.",
      "url": "https://arxiv.org/abs/2601.23261",
      "pdfUrl": "https://arxiv.org/pdf/2601.23261.pdf",
      "titleJa": "TEON: 大規模言語モデルの事前学習のための層別ミューオンを超えるテンソル化直交化"
    },
    {
      "id": "2601.23258",
      "arxivId": "2601.23258",
      "title": "Agnostic Language Identification and Generation",
      "authors": [
        "Mikael Møller Høgsgaard",
        "Chirag Pabbaraju"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impose no restrictions on the distribution of the input data. We propose objectives to study both language identification and generation in this more general \"agnostic\" setup. Across both problems, we obtain novel interesting characterizations and nearly tight rates.",
      "url": "https://arxiv.org/abs/2601.23258",
      "pdfUrl": "https://arxiv.org/pdf/2601.23258.pdf",
      "titleJa": "不可知論的言語識別と生成"
    },
    {
      "id": "2601.23255",
      "arxivId": "2601.23255",
      "title": "Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models",
      "authors": [
        "Ye Yu",
        "Haibo Jin",
        "Yaoning Yu",
        "Jun Zhuang",
        "Haohan Wang"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "abstract": "Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine the security implications of this modality shift by designing a text-to-audio jailbreak that embeds disallowed directives within a narrative-style audio stream. The attack leverages an advanced instruction-following text-to-speech (TTS) model to exploit structural and acoustic properties, thereby circumventing safety mechanisms primarily calibrated for text. When delivered through synthetic speech, the narrative format elicits restricted outputs from state-of-the-art models, including Gemini 2.0 Flash, achieving a 98.26% success rate that substantially exceeds text-only baselines. These results highlight the need for safety frameworks that jointly reason over linguistic and paralinguistic representations, particularly as speech-based interfaces become more prevalent.",
      "url": "https://arxiv.org/abs/2601.23255",
      "pdfUrl": "https://arxiv.org/pdf/2601.23255.pdf",
      "titleJa": "さあ、聞いてください: 大規模音声言語モデルに対する音声ナラティブ攻撃"
    },
    {
      "id": "2601.23236",
      "arxivId": "2601.23236",
      "title": "YuriiFormer: A Suite of Nesterov-Accelerated Transformers",
      "authors": [
        "Aleksandr Zimin",
        "Yury Polyanskiy",
        "Philippe Rigollet"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "abstract": "We propose a variational framework that interprets transformer layers as iterations of an optimization algorithm acting on token embeddings. In this view, self-attention implements a gradient step of an interaction energy, while MLP layers correspond to gradient updates of a potential energy. Standard GPT-style transformers emerge as vanilla gradient descent on the resulting composite objective, implemented via Lie--Trotter splitting between these two energy functionals. This perspective enables principled architectural design using classical optimization ideas. As a proof of concept, we introduce a Nesterov-style accelerated transformer that preserves the same attention and MLP oracles. The resulting architecture consistently outperforms a nanoGPT baseline on TinyStories and OpenWebText, demonstrating that optimization-theoretic insights can translate into practical gains.",
      "url": "https://arxiv.org/abs/2601.23236",
      "pdfUrl": "https://arxiv.org/pdf/2601.23236.pdf",
      "titleJa": "YuriiFormer: ネステロフ加速型トランスフォーマースイート"
    },
    {
      "id": "2601.23232",
      "arxivId": "2601.23232",
      "title": "ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search",
      "authors": [
        "Tao Yu",
        "Haopeng Jin",
        "Hao Wang",
        "Shenghua Chai",
        "Yujia Yang",
        "Junhao Gong",
        "Jiaming Guo",
        "Minghui Zhang",
        "Xinlong Chen",
        "Zhenghao Zhang",
        "Yuxuan Zhou",
        "Yanpei Gong",
        "YuanCheng Liu",
        "Yiming Ding",
        "Kangwei Zeng",
        "Pengfei Yang",
        "Zhongtian Luo",
        "Yufei Xiong",
        "Shanbin Zhang",
        "Shaoxiong Cheng",
        "Huang Ruilin",
        "Li Shuo",
        "Yuxi Niu",
        "Xinyuan Zhang",
        "Yueya Xu",
        "Jie Mao",
        "Ruixuan Ji",
        "Yaru Zhao",
        "Mingchen Zhang",
        "Jiabing Yang",
        "Jiaqi Liu",
        "YiFan Zhang",
        "Hongzhu Yi",
        "Xinming Wang",
        "Cheng Zhong",
        "Xiao Ma",
        "Zhang Zhang",
        "Yan Huang",
        "Liang Wang"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes editing requirements as keyframe-oriented shot descriptions and introduces five types of controllable single-factor constraints: Temporal order, Color, Visual style, Audio, and Resolution. We curate 1,210 high-quality samples from YouTube across 20 thematic categories, using large models for generation with human verification. Based on the benchmark, we propose ShotFinder, a text-driven three-stage retrieval and localization pipeline: (1) query expansion via video imagination, (2) candidate video retrieval with a search engine, and (3) description-guided temporal localization. Experiments on multiple closed-source and open-source models reveal a significant gap to human performance, with clear imbalance across constraints: temporal localization is relatively tractable, while color and visual style remain major challenges. These results reveal that open-domain video shot retrieval is still a critical capability that multimodal large models have yet to overcome.",
      "url": "https://arxiv.org/abs/2601.23232",
      "pdfUrl": "https://arxiv.org/pdf/2601.23232.pdf",
      "titleJa": "ShotFinder: ウェブ検索による想像力主導のオープンドメインビデオショット検索"
    },
    {
      "id": "2601.23229",
      "arxivId": "2601.23229",
      "title": "Strongly Polynomial Time Complexity of Policy Iteration for $L_\\infty$ Robust MDPs",
      "authors": [
        "Ali Asadi",
        "Krishnendu Chatterjee",
        "Ehsan Goharshady",
        "Mehrdad Karrabi",
        "Alipasha Montaseri",
        "Carlo Pagano"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.AI",
        "cs.CC"
      ],
      "abstract": "Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L_\\infty$ uncertainty sets form a fundamental and expressive model: they subsume classical MDPs and turn-based stochastic games. We consider this model with discounted payoffs. The existence of polynomial and strongly-polynomial time algorithms is a fundamental problem for these optimization models. For MDPs, linear programming yields polynomial-time algorithms for any arbitrary discount factor, and the seminal work of Ye established strongly--polynomial time for a fixed discount factor. The generalization of such results to RMDPs has remained an important open problem. In this work, we show that a robust policy iteration algorithm runs in strongly-polynomial time for $(s, a)$-rectangular $L_\\infty$ RMDPs with a constant (fixed) discount factor, resolving an important algorithmic question.",
      "url": "https://arxiv.org/abs/2601.23229",
      "pdfUrl": "https://arxiv.org/pdf/2601.23229.pdf",
      "titleJa": "$L_\\infty$ ロバストMDPに対する方策反復の強多項式時間計算量"
    },
    {
      "id": "2601.23228",
      "arxivId": "2601.23228",
      "title": "Scaling Multiagent Systems with Process Rewards",
      "authors": [
        "Ed Li",
        "Junyu Ren",
        "Cat Yan"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.ET",
        "cs.MA"
      ],
      "abstract": "While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.",
      "url": "https://arxiv.org/abs/2601.23228",
      "pdfUrl": "https://arxiv.org/pdf/2601.23228.pdf",
      "titleJa": "プロセス報酬を用いたマルチエージェントシステムのスケーリング"
    },
    {
      "id": "2601.23225",
      "arxivId": "2601.23225",
      "title": "Agile Reinforcement Learning through Separable Neural Architecture",
      "authors": [
        "Rajib Mostakim",
        "Reza T. Batley",
        "Sourav Saha"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Deep reinforcement learning (RL) is increasingly deployed in resource-constrained environments, yet the go-to function approximators - multilayer perceptrons (MLPs) - are often parameter-inefficient due to an imperfect inductive bias for the smooth structure of many value functions. This mismatch can also hinder sample efficiency and slow policy learning in this capacity-limited regime. Although model compression techniques exist, they operate post-hoc and do not improve learning efficiency. Recent spline-based separable architectures - such as Kolmogorov-Arnold Networks (KANs) - have been shown to offer parameter efficiency but are widely reported to exhibit significant computational overhead, especially at scale. In seeking to address these limitations, this work introduces SPAN (SPline-based Adaptive Networks), a novel function approximation approach to RL. SPAN adapts the low rank KHRONOS framework by integrating a learnable preprocessing layer with a separable tensor product B-spline basis. SPAN is evaluated across discrete (PPO) and high-dimensional continuous (SAC) control tasks, as well as offline settings (Minari/D4RL). Empirical results demonstrate that SPAN achieves a 30-50% improvement in sample efficiency and 1.3-9 times higher success rates across benchmarks compared to MLP baselines. Furthermore, SPAN demonstrates superior anytime performance and robustness to hyperparameter variations, suggesting it as a viable, high performance alternative for learning intrinsically efficient policies in resource-limited settings.",
      "url": "https://arxiv.org/abs/2601.23225",
      "pdfUrl": "https://arxiv.org/pdf/2601.23225.pdf",
      "titleJa": "分離可能なニューラルアーキテクチャによるアジャイル強化学習"
    },
    {
      "id": "2601.23220",
      "arxivId": "2601.23220",
      "title": "Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training",
      "authors": [
        "Anglin Liu",
        "Ruichao Chen",
        "Yi Lu",
        "Hongxia Xu",
        "Jintai Chen"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Despite recent Multimodal Large Language Models (MLLMs)' linguistic prowess in medical diagnosis, we find even state-of-the-art MLLMs suffer from a critical perceptual deficit: geometric blindness. This failure to ground outputs in objective geometric constraints leads to plausible yet factually incorrect hallucinations, rooted in training paradigms that prioritize linguistic fluency over geometric fidelity. This paper introduces Med-Scout, a novel framework that \"cures\" this blindness via Reinforcement Learning (RL) that leverages the intrinsic geometric logic latent within unlabeled medical images. Instead of relying on costly expert annotations, Med-Scout derives verifiable supervision signals through three strategic proxy tasks: Hierarchical Scale Localization, Topological Jigsaw Reconstruction, and Anomaly Consistency Detection. To rigorously quantify this deficit, we present Med-Scout-Bench, a new benchmark specifically designed to evaluate geometric perception. Extensive evaluations show that Med-Scout significantly mitigates geometric blindness, outperforming leading proprietary and open-source MLLMs by over 40% on our benchmark. Furthermore, this enhanced geometric perception generalizes to broader medical understanding, achieving superior results on radiological and comprehensive medical VQA tasks.",
      "url": "https://arxiv.org/abs/2601.23220",
      "pdfUrl": "https://arxiv.org/pdf/2601.23220.pdf",
      "titleJa": "Med-Scout: 幾何学を考慮した強化学習後トレーニングによる MLLM の医療認識における幾何学的盲点の解消"
    },
    {
      "id": "2601.23219",
      "arxivId": "2601.23219",
      "title": "MonoScale: Scaling Multi-Agent System with Monotonic Improvement",
      "authors": [
        "Shuai Shao",
        "Yixiang Liu",
        "Bingwei Lu",
        "Weinan Zhang"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "abstract": "In recent years, LLM-based multi-agent systems (MAS) have advanced rapidly, using a router to decompose tasks and delegate subtasks to specialized agents. A natural way to expand capability is to scale up the agent pool by continually integrating new functional agents or tool interfaces, but naive expansion can trigger performance collapse when the router cold-starts on newly added, heterogeneous, and unreliable agents. We propose MonoScale, an expansion-aware update framework that proactively generates a small set of agent-conditioned familiarization tasks, harvests evidence from both successful and failed interactions, and distills it into auditable natural-language memory to guide future routing. We formalize sequential augmentation as a contextual bandit and perform trust-region memory updates, yielding a monotonic non-decreasing performance guarantee across onboarding rounds. Experiments on GAIA and Humanity's Last Exam show stable gains as the agent pool grows, outperforming naive scale-up and strong-router fixed-pool baselines.",
      "url": "https://arxiv.org/abs/2601.23219",
      "pdfUrl": "https://arxiv.org/pdf/2601.23219.pdf",
      "titleJa": "MonoScale: 単調な改善を伴うマルチエージェントシステムのスケーリング"
    },
    {
      "id": "2601.23212",
      "arxivId": "2601.23212",
      "title": "Disentangling multispecific antibody function with graph neural networks",
      "authors": [
        "Joshua Southern",
        "Changpeng Lu",
        "Santrupti Nerli",
        "Samuel D. Stanton",
        "Andrew M. Watkins",
        "Franziska Seeger",
        "Frédéric A. Dreyer"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "abstract": "Multispecific antibodies offer transformative therapeutic potential by engaging multiple epitopes simultaneously, yet their efficacy is an emergent property governed by complex molecular architectures. Rational design is often bottlenecked by the inability to predict how subtle changes in domain topology influence functional outcomes, a challenge exacerbated by the scarcity of comprehensive experimental data. Here, we introduce a computational framework to address part of this gap. First, we present a generative method for creating large-scale, realistic synthetic functional landscapes that capture non-linear interactions where biological activity depends on domain connectivity. Second, we propose a graph neural network architecture that explicitly encodes these topological constraints, distinguishing between format configurations that appear identical to sequence-only models. We demonstrate that this model, trained on synthetic landscapes, recapitulates complex functional properties and, via transfer learning, has the potential to achieve high predictive accuracy on limited biological datasets. We showcase the model's utility by optimizing trade-offs between efficacy and toxicity in trispecific T-cell engagers and retrieving optimal common light chains. This work provides a robust benchmarking environment for disentangling the combinatorial complexity of multispecifics, accelerating the design of next-generation therapeutics.",
      "url": "https://arxiv.org/abs/2601.23212",
      "pdfUrl": "https://arxiv.org/pdf/2601.23212.pdf",
      "titleJa": "グラフニューラルネットワークによる多特異性抗体機能の解明"
    },
    {
      "id": "2601.23207",
      "arxivId": "2601.23207",
      "title": "Learning to Execute Graph Algorithms Exactly with Graph Neural Networks",
      "authors": [
        "Muhammad Fetrat Qharabagh",
        "Artur Back de Luca",
        "George Giapitzakis",
        "Kimon Fountoulakis"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Understanding what graph neural networks can learn, especially their ability to learn to execute algorithms, remains a central theoretical challenge. In this work, we prove exact learnability results for graph algorithms under bounded-degree and finite-precision constraints. Our approach follows a two-step process. First, we train an ensemble of multi-layer perceptrons (MLPs) to execute the local instructions of a single node. Second, during inference, we use the trained MLP ensemble as the update function within a graph neural network (GNN). Leveraging Neural Tangent Kernel (NTK) theory, we show that local instructions can be learned from a small training set, enabling the complete graph algorithm to be executed during inference without error and with high probability. To illustrate the learning power of our setting, we establish a rigorous learnability result for the LOCAL model of distributed computation. We further demonstrate positive learnability results for widely studied algorithms such as message flooding, breadth-first and depth-first search, and Bellman-Ford.",
      "url": "https://arxiv.org/abs/2601.23207",
      "pdfUrl": "https://arxiv.org/pdf/2601.23207.pdf",
      "titleJa": "グラフニューラルネットワークでグラフアルゴリズムを正確に実行する方法を学ぶ"
    },
    {
      "id": "2601.23206",
      "arxivId": "2601.23206",
      "title": "High-quality generation of dynamic game content via small language models: A proof of concept",
      "authors": [
        "Morten I. K. Munk",
        "Arturo Valdivia",
        "Paolo Burelli"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Large language models (LLMs) offer promise for dynamic game content generation, but they face critical barriers, including narrative incoherence and high operational costs. Due to their large size, they are often accessed in the cloud, limiting their application in offline games. Many of these practical issues are solved by pivoting to small language models (SLMs), but existing studies using SLMs have resulted in poor output quality. We propose a strategy of achieving high-quality SLM generation through aggressive fine-tuning on deliberately scoped tasks with narrow context, constrained structure, or both. In short, more difficult tasks require narrower scope and higher specialization to the training corpus. Training data is synthetically generated via a DAG-based approach, grounding models in the specific game world. Such models can form the basis for agentic networks designed around the narratological framework at hand, representing a more practical and robust solution than cloud-dependent LLMs. To validate this approach, we present a proof-of-concept focusing on a single specialized SLM as the fundamental building block. We introduce a minimal RPG loop revolving around rhetorical battles of reputations, powered by this model. We demonstrate that a simple retry-until-success strategy reaches adequate quality (as defined by an LLM-as-a-judge scheme) with predictable latency suitable for real-time generation. While local quality assessment remains an open question, our results demonstrate feasibility for real-time generation under typical game engine constraints.",
      "url": "https://arxiv.org/abs/2601.23206",
      "pdfUrl": "https://arxiv.org/pdf/2601.23206.pdf",
      "titleJa": "小規模言語モデルによる高品質な動的ゲームコンテンツ生成：概念実証"
    },
    {
      "id": "2601.23204",
      "arxivId": "2601.23204",
      "title": "TSAQA: Time Series Analysis Question And Answering Benchmark",
      "authors": [
        "Baoyu Jing",
        "Sanhorn Chen",
        "Lecheng Zheng",
        "Boyu Liu",
        "Zihao Li",
        "Jiaru Zou",
        "Tianxin Wei",
        "Zhining Liu",
        "Zhichen Zeng",
        "Ruizhong Qiu",
        "Xiao Lin",
        "Yuchen Yan",
        "Dongqi Fu",
        "Jingchao Ni",
        "Jingrui He",
        "Hanghang Tong"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection tasks. We introduce TSAQA, a novel unified benchmark designed to broaden task coverage and evaluate diverse temporal analysis capabilities. TSAQA integrates six diverse tasks under a single framework ranging from conventional analysis, including anomaly detection and classification, to advanced analysis, such as characterization, comparison, data transformation, and temporal relationship analysis. Spanning 210k samples across 13 domains, the dataset employs diverse formats, including true-or-false (TF), multiple-choice (MC), and a novel puzzling (PZ), to comprehensively assess time series analysis. Zero-shot evaluation demonstrates that these tasks are challenging for current Large Language Models (LLMs): the best-performing commercial LLM, Gemini-2.5-Flash, achieves an average score of only 65.08. Although instruction tuning boosts open-source performance: the best-performing open-source model, LLaMA-3.1-8B, shows significant room for improvement, highlighting the complexity of temporal analysis for LLMs.",
      "url": "https://arxiv.org/abs/2601.23204",
      "pdfUrl": "https://arxiv.org/pdf/2601.23204.pdf",
      "titleJa": "TSAQA: 時系列分析の質問と回答のベンチマーク"
    },
    {
      "id": "2601.23179",
      "arxivId": "2601.23179",
      "title": "Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization",
      "authors": [
        "Hui Lu",
        "Yi Yu",
        "Yiming Yang",
        "Chenyu Yi",
        "Xueyi Ke",
        "Qixing Zhang",
        "Bingquan Shen",
        "Alex Kot",
        "Xudong Jiang"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targeted Transferable Adversarial Attacks (UTTAA), where a single perturbation must consistently steer arbitrary inputs toward a specified target across unknown commercial MLLMs. Naively adapting existing sample-wise attacks to this universal setting faces three core difficulties: (i) target supervision becomes high-variance due to target-crop randomness, (ii) token-wise matching is unreliable because universality suppresses image-specific cues that would otherwise anchor alignment, and (iii) few-source per-target adaptation is highly initialization-sensitive, which can degrade the attainable performance. In this work, we propose MCRMO-Attack, which stabilizes supervision via Multi-Crop Aggregation with an Attention-Guided Crop, improves token-level reliability through alignability-gated Token Routing, and meta-learns a cross-target perturbation prior that yields stronger per-target solutions. Across commercial MLLMs, we boost unseen-image attack success rate by +23.7\\% on GPT-4o and +19.9\\% on Gemini-2.0 over the strongest universal baseline.",
      "url": "https://arxiv.org/abs/2601.23179",
      "pdfUrl": "https://arxiv.org/pdf/2601.23179.pdf",
      "titleJa": "あらゆるものをターゲットにマッチさせる：マルチクロップルーティングメタ最適化によるクローズドソースMLLMに対する普遍的な敵対的摂動"
    },
    {
      "id": "2601.23163",
      "arxivId": "2601.23163",
      "title": "Probing the Trajectories of Reasoning Traces in Large Language Models",
      "authors": [
        "Marthe Ballon",
        "Brecht Verbeken",
        "Vincent Ginis",
        "Andres Algaba"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Large language models (LLMs) increasingly solve difficult problems by producing \"reasoning traces\" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant information beyond generic length or stylistic effects. Here, we propose a protocol to systematically probe the trajectories of reasoning traces in LLMs by 1) generating a model's reasoning trace, 2) truncating it at fixed token-percentiles, and 3) injecting each partial trace back into the model (or a different model) to measure the induced distribution over answer choices via next-token probabilities. We apply this protocol to the open-source Qwen3-4B/-8B/-14B and gpt-oss-20b/-120b models across the multiple-choice GPQA Diamond and MMLU-Pro benchmarks. We find that accuracy and decision commitment consistently increase as the percentage of provided reasoning tokens grows. These gains are primarily driven by relevant content in the model generation rather than context length or generic \"reasoning style\" effects. Stronger models often backtrack successfully from incorrect partial traces, but immediate answers often remain anchored in the weaker model's incorrect response. More broadly, we show that trajectory probing provides diagnostics for efficient and safer deployment of reasoning models as the measurements can inform practical trace-handling and monitoring policies that improve reliability without assuming intermediate tokens are inherently faithful explanations.",
      "url": "https://arxiv.org/abs/2601.23163",
      "pdfUrl": "https://arxiv.org/pdf/2601.23163.pdf",
      "titleJa": "大規模言語モデルにおける推論痕跡の軌跡の探究"
    },
    {
      "id": "2601.21260",
      "arxivId": "2601.21260",
      "title": "Music Plagiarism Detection: Problem Formulation and a Segment-based Solution",
      "authors": [
        "Seonghyeon Go",
        "Yumin Kim"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Recently, the problem of music plagiarism has emerged as an even more pressing social issue. As music information retrieval research advances, there is a growing effort to address issues related to music plagiarism. However, many studies, including our previous work, have conducted research without clearly defining what the music plagiarism detection task actually involves. This lack of a clear definition has slowed research progress and made it hard to apply results to real-world scenarios. To fix this situation, we defined how Music Plagiarism Detection is different from other MIR tasks and explained what problems need to be solved. We introduce the Similar Music Pair dataset to support this newly defined task. In addition, we propose a method based on segment transcription as one way to solve the task. Our demo and dataset are available at https://github.com/Mippia/ICASSP2026-MPD.",
      "url": "https://arxiv.org/abs/2601.21260",
      "pdfUrl": "https://arxiv.org/pdf/2601.21260.pdf",
      "titleJa": "音楽盗作検出：問題の定式化とセグメントベースのソリューション"
    },
    {
      "id": "2601.20478",
      "arxivId": "2601.20478",
      "title": "On Every Note a Griff: Looking for a Useful Representation of Basso Continuo Performance Style",
      "authors": [
        "Adam Štefunko",
        "Carlos Eduardo Cancino-Chacón",
        "Jan Hajič"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.SD",
        "cs.IR"
      ],
      "abstract": "Basso continuo is a baroque improvisatory accompaniment style which involves improvising multiple parts above a given bass line in a musical score on a harpsichord or organ. Basso continuo is not merely a matter of history; moreover, it is a historically inspired living practice, and The Aligned Continuo Dataset (ACoRD) records the first sample of modern-day basso continuo playing in the symbolic domain. This dataset, containing 175 MIDI recordings of 5 basso continuo scores performed by 7 players, allows us to start observing and analyzing the variety that basso continuo improvisation brings. A recently proposed basso continuo performance-to-score alignment system provides a way of mapping improvised performance notes to score notes. In order to study aligned basso continuo performances, we need an appropriate feature representation. We propose griff, a representation inspired by historical basso continuo treatises. It enables us to encode both pitch content and structure of a basso continuo realization in a transposition-invariant way. Griffs are directly extracted from aligned basso continuo performances by grouping together performance notes aligned to the same score note in a onset-time ordered way, and they provide meaningful tokens that form a feature space in which we can analyze basso continuo performance styles. We statistically describe griffs extracted from the ACoRD dataset recordings, and show in two experiments how griffs can be used for statistical analysis of individuality of different players' basso continuo performance styles. We finally present an argument why it is desirable to preserve the structure of a basso continuo improvisation in order to conduct a refined analysis of personal performance styles of individual basso continuo practitioners, and why griffs can provide a meaningful historically informed feature space worthy of a more robust empirical validation.",
      "url": "https://arxiv.org/abs/2601.20478",
      "pdfUrl": "https://arxiv.org/pdf/2601.20478.pdf",
      "titleJa": "すべての音符にグリフ：通奏低音演奏スタイルの有用な表現を求めて"
    },
    {
      "id": "2601.20883",
      "arxivId": "2601.20883",
      "title": "VoxMorph: Scalable Zero-shot Voice Identity Morphing via Disentangled Embeddings",
      "authors": [
        "Bharath Krishnamurthy",
        "Ajita Rattani"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.CR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Morphing techniques generate artificial biometric samples that combine features from multiple individuals, allowing each contributor to be verified against a single enrolled template. While extensively studied in face recognition, this vulnerability remains largely unexplored in voice biometrics. Prior work on voice morphing is computationally expensive, non-scalable, and limited to acoustically similar identity pairs, constraining practical deployment. Moreover, existing sound-morphing methods target audio textures, music, or environmental sounds and are not transferable to voice identity manipulation. We propose VoxMorph, a zero-shot framework that produces high-fidelity voice morphs from as little as five seconds of audio per subject without model retraining. Our method disentangles vocal traits into prosody and timbre embeddings, enabling fine-grained interpolation of speaking style and identity. These embeddings are fused via Spherical Linear Interpolation (Slerp) and synthesized using an autoregressive language model coupled with a Conditional Flow Matching network. VoxMorph achieves state-of-the-art performance, delivering a 2.6x gain in audio quality, a 73% reduction in intelligibility errors, and a 67.8% morphing attack success rate on automated speaker verification systems under strict security thresholds. This work establishes a practical and scalable paradigm for voice morphing with significant implications for biometric security. The code and dataset are available on our project page: https://vcbsl.github.io/VoxMorph/",
      "url": "https://arxiv.org/abs/2601.20883",
      "pdfUrl": "https://arxiv.org/pdf/2601.20883.pdf",
      "titleJa": "VoxMorph: 分離埋め込みによるスケーラブルなゼロショット音声アイデンティティモーフィング"
    },
    {
      "id": "2601.19702",
      "arxivId": "2601.19702",
      "title": "SAM Audio Judge: A Unified Multimodal Framework for Perceptual Evaluation of Audio Separation",
      "authors": [
        "Helin Wang",
        "Bowen Shi",
        "Andros Tjandra",
        "John Hoffman",
        "Yi-Chiao Wu",
        "Apoorv Vyas",
        "Najim Dehak",
        "Ann Lee",
        "Wei-Ning Hsu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "The performance evaluation remains a complex challenge in audio separation, and existing evaluation metrics are often misaligned with human perception, course-grained, relying on ground truth signals. On the other hand, subjective listening tests remain the gold standard for real-world evaluation, but they are expensive, time-consuming, and difficult to scale. This paper addresses the growing need for automated systems capable of evaluating audio separation without human intervention. The proposed evaluation metric, SAM Audio Judge (SAJ), is a multimodal fine-grained reference-free objective metric, which shows highly alignment with human perceptions. SAJ supports three audio domains (speech, music and general sound events) and three prompt inputs (text, visual and span), covering four different dimensions of evaluation (recall, percision, faithfulness, and overall). SAM Audio Judge also shows potential applications in data filtering, pseudo-labeling large datasets and reranking in audio separation models. We release our code and pre-trained models at: https://github.com/facebookresearch/sam-audio.",
      "url": "https://arxiv.org/abs/2601.19702",
      "pdfUrl": "https://arxiv.org/pdf/2601.19702.pdf",
      "titleJa": "SAM Audio Judge: 音声分離の知覚評価のための統合マルチモーダルフレームワーク"
    },
    {
      "id": "2601.19109",
      "arxivId": "2601.19109",
      "title": "Interpretable and Perceptually-Aligned Music Similarity with Pretrained Embeddings",
      "authors": [
        "Arhan Vohra",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Perceptual similarity representations enable music retrieval systems to determine which songs sound most similar to listeners. State-of-the-art approaches based on task-specific training via self-supervised metric learning show promising alignment with human judgment, but are difficult to interpret or generalize due to limited dataset availability. We show that pretrained text-audio embeddings (CLAP and MuQ-MuLan) offer comparable perceptual alignment on similarity tasks without any additional fine-tuning. To surpass this baseline, we introduce a novel method to perceptually align pretrained embeddings with source separation and linear optimization on ABX preference data from listening tests. Our model provides interpretable and controllable instrument-wise weights, allowing music producers to retrieve stem-level loops and samples based on mixed reference songs.",
      "url": "https://arxiv.org/abs/2601.19109",
      "pdfUrl": "https://arxiv.org/pdf/2601.19109.pdf",
      "titleJa": "事前学習済みの埋め込みによる解釈可能かつ知覚的に整合された音楽類似性"
    },
    {
      "id": "2601.18766",
      "arxivId": "2601.18766",
      "title": "Learning to Discover: A Generalized Framework for Raga Identification without Forgetting",
      "authors": [
        "Parampreet Singh",
        "Somya Kumar",
        "Chaitanya Shailendra Nitawe",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.",
      "url": "https://arxiv.org/abs/2601.18766",
      "pdfUrl": "https://arxiv.org/pdf/2601.18766.pdf",
      "titleJa": "発見することを学ぶ：忘れずにラーガを識別するための一般化された枠組み"
    },
    {
      "id": "2601.18339",
      "arxivId": "2601.18339",
      "title": "A Dataset for Automatic Vocal Mode Classification",
      "authors": [
        "Reemt Hinrichs",
        "Sonja Stephan",
        "Alexander Lange",
        "Jörn Ostermann"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\\,\\% was achieved with a ResNet18. The dataset can be downloaded under https://zenodo.org/records/14276415.",
      "url": "https://arxiv.org/abs/2601.18339",
      "pdfUrl": "https://arxiv.org/pdf/2601.18339.pdf",
      "titleJa": "自動音声モード分類のためのデータセット"
    },
    {
      "id": "2601.19951",
      "arxivId": "2601.19951",
      "title": "Pianoroll-Event: A Novel Score Representation for Symbolic Music",
      "authors": [
        "Lekai Qian",
        "Haoyu Gu",
        "Dehan Li",
        "Boyu Cao",
        "Qi Liu"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Symbolic music representation is a fundamental challenge in computational musicology. While grid-based representations effectively preserve pitch-time spatial correspondence, their inherent data sparsity leads to low encoding efficiency. Discrete-event representations achieve compact encoding but fail to adequately capture structural invariance and spatial locality. To address these complementary limitations, we propose Pianoroll-Event, a novel encoding scheme that describes pianoroll representations through events, combining structural properties with encoding efficiency while maintaining temporal dependencies and local spatial patterns. Specifically, we design four complementary event types: Frame Events for temporal boundaries, Gap Events for sparse regions, Pattern Events for note patterns, and Musical Structure Events for musical metadata. Pianoroll-Event strikes an effective balance between sequence length and vocabulary size, improving encoding efficiency by 1.36\\times to 7.16\\times over representative discrete sequence methods. Experiments across multiple autoregressive architectures show models using our representation consistently outperform baselines in both quantitative and human evaluations.",
      "url": "https://arxiv.org/abs/2601.19951",
      "pdfUrl": "https://arxiv.org/pdf/2601.19951.pdf",
      "titleJa": "ピアノロールイベント：象徴音楽のための斬新な楽譜表現"
    },
    {
      "id": "2601.17645",
      "arxivId": "2601.17645",
      "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
      "authors": [
        "Xilin Jiang",
        "Qiaolin Wang",
        "Junkai Wu",
        "Xiaomin He",
        "Zhongweiyang Xu",
        "Yinghao Ma",
        "Minshuo Piao",
        "Kaiyi Yang",
        "Xiuwen Zheng",
        "Riki Shimizu",
        "Yicong Chen",
        "Arsalan Firoozi",
        "Gavin Mischler",
        "Sukru Samet Dindar",
        "Richard Antonello",
        "Linyang He",
        "Tsun-An Hsieh",
        "Xulin Fan",
        "Yulun Wu",
        "Yuesheng Ma",
        "Chaitanya Amballa",
        "Weixiong Chen",
        "Jiarui Hai",
        "Ruisi Li",
        "Vishal Choudhari",
        "Cong Han",
        "Yinghao Aaron Li",
        "Adeen Flinker",
        "Mounya Elhilali",
        "Emmanouil Benetos",
        "Mark Hasegawa-Johnson",
        "Romit Roy Choudhury",
        "Nima Mesgarani"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public",
      "url": "https://arxiv.org/abs/2601.17645",
      "pdfUrl": "https://arxiv.org/pdf/2601.17645.pdf",
      "titleJa": "AVMeme試験：法学修士（LLM）の文脈的・文化的知識と思考力を評価するマルチモーダル・多言語・多文化ベンチマーク"
    },
    {
      "id": "2601.17517",
      "arxivId": "2601.17517",
      "title": "EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding",
      "authors": [
        "Luca Cerovaz",
        "Michele Mancusi",
        "Emanuele Rodolà"
      ],
      "publishedDate": "2026-01-24",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Audio codecs power discrete music generative modelling, music streaming and immersive media by shrinking PCM audio to bandwidth-friendly bit-rates. Recent works have gravitated towards processing in the spectral domain; however, spectrogram-domains typically struggle with phase modeling which is naturally complex-valued. Most frequency-domain neural codecs either disregard phase information or encode it as two separate real-valued channels, limiting spatial fidelity. This entails the need to introduce adversarial discriminators at the expense of convergence speed and training stability to compensate for the inadequate representation power of the audio signal. In this work we introduce an end-to-end complex-valued RVQ-VAE audio codec that preserves magnitude-phase coupling across the entire analysis-quantization-synthesis pipeline and removes adversarial discriminators and diffusion post-filters. Without GANs or diffusion we match or surpass much longer-trained baselines in-domain and reach SOTA out-of-domain performance. Compared to standard baselines that train for hundreds of thousands of steps, our model reducing training budget by an order of magnitude is markedly more compute-efficient while preserving high perceptual quality.",
      "url": "https://arxiv.org/abs/2601.17517",
      "pdfUrl": "https://arxiv.org/pdf/2601.17517.pdf",
      "titleJa": "EuleroDec: 効率的かつ堅牢なオーディオコーディングのための複素値RVQ-VAE"
    },
    {
      "id": "2601.16675",
      "arxivId": "2601.16675",
      "title": "I Guess That's Why They Call it the Blues: Causal Analysis for Audio Classifiers",
      "authors": [
        "David A. Kelly",
        "Hana Chockler"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "It is well-known that audio classifiers often rely on non-musically relevant features and spurious correlations to classify audio. Hence audio classifiers are easy to manipulate or confuse, resulting in wrong classifications. While inducing a misclassification is not hard, until now the set of features that the classifiers rely on was not well understood. In this paper we introduce a new method that uses causal reasoning to discover features of the frequency space that are sufficient and necessary for a given classification. We describe an implementation of this algorithm in the tool FreqReX and provide experimental results on a number of standard benchmark datasets. Our experiments show that causally sufficient and necessary subsets allow us to manipulate the outputs of the models in a variety of ways by changing the input very slightly. Namely, a change to one out of 240,000 frequencies results in a change in classification 58% of the time, and the change can be so small that it is practically inaudible. These results show that causal analysis is useful for understanding the reasoning process of audio classifiers and can be used to successfully manipulate their outputs.",
      "url": "https://arxiv.org/abs/2601.16675",
      "pdfUrl": "https://arxiv.org/pdf/2601.16675.pdf",
      "titleJa": "ブルースと呼ばれる理由：音声分類器の因果分析"
    },
    {
      "id": "2601.16273",
      "arxivId": "2601.16273",
      "title": "The CMU-AIST submission for the ICME 2025 Audio Encoder Challenge",
      "authors": [
        "Shikhar Bharadwaj",
        "Samuele Cornell",
        "Kwanghee Choi",
        "Hye-jin Shim",
        "Soham Deshmukh",
        "Satoru Fukayama",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This technical report describes our submission to the ICME 2025 audio encoder challenge. Our submitted system is built on BEATs, a masked speech token prediction based audio encoder. We extend the BEATs model using 74,000 hours of data derived from various speech, music, and sound corpora and scale its architecture upto 300 million parameters. We experiment with speech-heavy and balanced pre-training mixtures to study the impact of different domains on final performance. Our submitted system consists of an ensemble of the Dasheng 1.2 billion model with two custom scaled-up BEATs models trained on the aforementioned pre-training data mixtures. We also propose a simple ensembling technique that retains the best capabilities of constituent models and surpasses both the baseline and Dasheng 1.2B. For open science, we publicly release our trained checkpoints via huggingface at https://huggingface.co/shikhar7ssu/OpenBEATs-ICME-SOUND and https://huggingface.co/shikhar7ssu/OpenBEATs-ICME.",
      "url": "https://arxiv.org/abs/2601.16273",
      "pdfUrl": "https://arxiv.org/pdf/2601.16273.pdf",
      "titleJa": "ICME 2025 オーディオエンコーダチャレンジへの CMU-AIST の応募"
    },
    {
      "id": "2601.16150",
      "arxivId": "2601.16150",
      "title": "Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization",
      "authors": [
        "Maximos Kaliakatsos-Papakostas",
        "Dimos Makris",
        "Konstantinos Soiledis",
        "Konstantinos-Theodoros Tsamis",
        "Vassilis Katsouros",
        "Emilios Cambouropoulos"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody-harmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.",
      "url": "https://arxiv.org/abs/2601.16150",
      "pdfUrl": "https://arxiv.org/pdf/2601.16150.pdf",
      "titleJa": "メロディーに注意を払う（交差させる）：単一エンコーダーによるメロディーハーモニーのためのカリキュラムマスキング"
    },
    {
      "id": "2601.15872",
      "arxivId": "2601.15872",
      "title": "PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation",
      "authors": [
        "Jaekwon Im",
        "Natalia Polouliakh",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.",
      "url": "https://arxiv.org/abs/2601.15872",
      "pdfUrl": "https://arxiv.org/pdf/2601.15872.pdf",
      "titleJa": "PF-D2M: ユニバーサルなダンス・トゥ・ミュージック生成のためのポーズフリー拡散モデル"
    },
    {
      "id": "2601.15083",
      "arxivId": "2601.15083",
      "title": "Bangla Music Genre Classification Using Bidirectional LSTMS",
      "authors": [
        "Muntakimur Rahaman",
        "Md Mahmudul Hoque",
        "Md Mehedi Hassain"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Bangla music is enrich in its own music cultures. Now a days music genre classification is very significant because of the exponential increase in available music, both in digital and physical formats. It is necessary to index them accordingly to facilitate improved retrieval. Automatically classifying Bangla music by genre is essential for efficiently locating specific pieces within a vast and diverse music library. Prevailing methods for genre classification predominantly employ conventional machine learning or deep learning approaches. This work introduces a novel music dataset comprising ten distinct genres of Bangla music. For the task of audio classification, we utilize a recurrent neural network (RNN) architecture. Specifically, a Long Short-Term Memory (LSTM) network is implemented to train the model and perform the classification. Feature extraction represents a foundational stage in audio data processing. This study utilizes Mel-Frequency Cepstral Coefficients (MFCCs) to transform raw audio waveforms into a compact and representative set of features. The proposed framework facilitates music genre classification by leveraging these extracted features. Experimental results demonstrate a classification accuracy of 78%, indicating the system's strong potential to enhance and streamline the organization of Bangla music genres.",
      "url": "https://arxiv.org/abs/2601.15083",
      "pdfUrl": "https://arxiv.org/pdf/2601.15083.pdf",
      "titleJa": "双方向LSTMSを用いたバングラ音楽のジャンル分類"
    },
    {
      "id": "2601.14931",
      "arxivId": "2601.14931",
      "title": "Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali",
      "authors": [
        "Nouhoum Coulibaly",
        "Ousmane Ly",
        "Michael Leventhal",
        "Ousmane Goro"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "This study explores the capacity of generative artificial intelligence (Gen AI) to contribute to the construction of peace narratives and the revitalization of musical heritage in Mali. The study has been made in a political and social context where inter-community tensions and social fractures motivate a search for new symbolic frameworks for reconciliation. The study empirically explores three questions: (1) how Gen AI can be used as a tool for musical creation rooted in national languages and traditions; (2) to what extent Gen AI systems enable a balanced hybridization between technological innovation and cultural authenticity; and (3) how AI-assisted musical co-creation can strengthen social cohesion and cultural sovereignty. The experimental results suggest that Gen AI, embedded in a culturally conscious participatory framework, can act as a catalyst for symbolic diplomacy, amplifying local voices instead of standardizing them. However, challenges persist regarding the availability of linguistic corpora, algorithmic censorship, and the ethics of generating compositions derived from copyrighted sources.",
      "url": "https://arxiv.org/abs/2601.14931",
      "pdfUrl": "https://arxiv.org/pdf/2601.14931.pdf",
      "titleJa": "生成型人工知能、音楽遺産、そして平和物語の構築：マリにおける事例研究"
    },
    {
      "id": "2601.14786",
      "arxivId": "2601.14786",
      "title": "Training-Efficient Text-to-Music Generation with State-Space Modeling",
      "authors": [
        "Wei-Jaw Lee",
        "Fang-Chih Hsieh",
        "Xuanjun Chen",
        "Fang-Duo Tsai",
        "Yi-Hsuan Yang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in text-to-music generation (TTM) have yielded high-quality results, but often at the cost of extensive compute and the use of large proprietary internal data. To improve the affordability and openness of TTM training, an open-source generative model backbone that is more training- and data-efficient is needed. In this paper, we constrain the number of trainable parameters in the generative model to match that of the MusicGen-small benchmark (with about 300M parameters), and replace its Transformer backbone with the emerging class of state-space models (SSMs). Specifically, we explore different SSM variants for sequence modeling, and compare a single-stage SSM-based design with a decomposable two-stage SSM/diffusion hybrid design. All proposed models are trained from scratch on a purely public dataset comprising 457 hours of CC-licensed music, ensuring full openness. Our experimental findings are three-fold. First, we show that SSMs exhibit superior training efficiency compared to the Transformer counterpart. Second, despite using only 9% of the FLOPs and 2% of the training data size compared to the MusicGen-small benchmark, our model achieves competitive performance in both objective metrics and subjective listening tests based on MusicCaps captions. Finally, our scaling-down experiment demonstrates that SSMs can maintain competitive performance relative to the Transformer baseline even at the same training budget (measured in iterations), when the model size is reduced to four times smaller. To facilitate the democratization of TTM research, the processed captions, model checkpoints, and source code are available on GitHub via the project page: https://lonian6.github.io/ssmttm/.",
      "url": "https://arxiv.org/abs/2601.14786",
      "pdfUrl": "https://arxiv.org/pdf/2601.14786.pdf",
      "titleJa": "状態空間モデリングによる効率的なテキストから音楽への生成"
    },
    {
      "id": "2601.14684",
      "arxivId": "2601.14684",
      "title": "Dissecting Performance Degradation in Audio Source Separation under Sampling Frequency Mismatch",
      "authors": [
        "Kanami Imamura",
        "Tomohiko Nakamura",
        "Kohei Yatabe",
        "Hiroshi Saruwatari"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio processing methods based on deep neural networks are typically trained at a single sampling frequency (SF). To handle untrained SFs, signal resampling is commonly employed, but it can degrade performance, particularly when the input SF is lower than the trained SF. This paper investigates the causes of this degradation through two hypotheses: (i) the lack of high-frequency components introduced by up-sampling, and (ii) the greater importance of their presence than their precise representation. To examine these hypotheses, we compare conventional resampling with three alternatives: post-resampling noise addition, which adds Gaussian noise to the resampled signal; noisy-kernel resampling, which perturbs the kernel with Gaussian noise to enrich high-frequency components; and trainable-kernel resampling, which adapts the interpolation kernel through training. Experiments on music source separation show that noisy-kernel and trainable-kernel resampling alleviate the degradation observed with conventional resampling. We further demonstrate that noisy-kernel resampling is effective across diverse models, highlighting it as a simple yet practical option.",
      "url": "https://arxiv.org/abs/2601.14684",
      "pdfUrl": "https://arxiv.org/pdf/2601.14684.pdf",
      "titleJa": "サンプリング周波数の不一致による音源分離の性能劣化の解析"
    },
    {
      "id": "2601.21114",
      "arxivId": "2601.21114",
      "title": "DNN-Based Online Source Counting Based on Spatial Generalized Magnitude Squared Coherence",
      "authors": [
        "Henri Gode",
        "Simon Doclo"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "The number of active sound sources is a key parameter in many acoustic signal processing tasks, such as source localization, source separation, and multi-microphone speech enhancement. This paper proposes a novel method for online source counting by detecting changes in the number of active sources based on spatial coherence. The proposed method exploits the fact that a single coherent source in spatially white background noise yields high spatial coherence, whereas only noise results in low spatial coherence. By applying a spatial whitening operation, the source counting problem is reformulated as a change detection task, aiming to identify the time frames when the number of active sources changes. The method leverages the generalized magnitude-squared coherence as a measure to quantify spatial coherence, providing features for a compact neural network trained to detect source count changes framewise. Simulation results with binaural hearing aids in reverberant acoustic scenes with up to 4 speakers and background noise demonstrate the effectiveness of the proposed method for online source counting.",
      "url": "https://arxiv.org/abs/2601.21114",
      "pdfUrl": "https://arxiv.org/pdf/2601.21114.pdf",
      "titleJa": "空間一般化振幅二乗コヒーレンスに基づくDNNベースのオンラインソースカウント"
    },
    {
      "id": "2601.20432",
      "arxivId": "2601.20432",
      "title": "Self Voice Conversion as an Attack against Neural Audio Watermarking",
      "authors": [
        "Yigitcan Özer",
        "Wanying Ge",
        "Zhe Zhang",
        "Xin Wang",
        "Junichi Yamagishi"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Audio watermarking embeds auxiliary information into speech while maintaining speaker identity, linguistic content, and perceptual quality. Although recent advances in neural and digital signal processing-based watermarking methods have improved imperceptibility and embedding capacity, robustness is still primarily assessed against conventional distortions such as compression, additive noise, and resampling. However, the rise of deep learning-based attacks introduces novel and significant threats to watermark security. In this work, we investigate self voice conversion as a universal, content-preserving attack against audio watermarking systems. Self voice conversion remaps a speaker's voice to the same identity while altering acoustic characteristics through a voice conversion model. We demonstrate that this attack severely degrades the reliability of state-of-the-art watermarking approaches and highlight its implications for the security of modern audio watermarking techniques.",
      "url": "https://arxiv.org/abs/2601.20432",
      "pdfUrl": "https://arxiv.org/pdf/2601.20432.pdf",
      "titleJa": "ニューラルオーディオ透かしに対する攻撃としての自己音声変換"
    },
    {
      "id": "2601.20896",
      "arxivId": "2601.20896",
      "title": "A Study of Data Selection Strategies for Pre-training Self-Supervised Speech Models",
      "authors": [
        "Ryan Whetten",
        "Titouan Parcollet",
        "Marco Dinarelli",
        "Yannick Estève"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Self-supervised learning (SSL) has transformed speech processing, yet its reliance on massive pre-training datasets remains a bottleneck. While robustness is often attributed to scale and diversity, the role of the data distribution is less understood. We systematically examine how curated subsets of pre-training data influence Automatic Speech Recognition (ASR) performance. Surprisingly, optimizing for acoustic, speaker, or linguistic diversity yields no clear improvements over random sampling. Instead, we find that prioritizing the longest utterances achieves superior ASR results while using only half the original dataset, reducing pre-training time by 24% on a large corpora. These findings suggest that for pre-training speech SSL models, data length is a more critical factor than either data diversity or overall data quantity for performance and efficiency, offering a new perspective for data selection strategies in SSL speech processing.",
      "url": "https://arxiv.org/abs/2601.20896",
      "pdfUrl": "https://arxiv.org/pdf/2601.20896.pdf",
      "titleJa": "自己教師あり音声モデルの事前学習のためのデータ選択戦略の研究"
    },
    {
      "id": "2601.20094",
      "arxivId": "2601.20094",
      "title": "T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS",
      "authors": [
        "Haibin Wu",
        "Bach Viet Do",
        "Naveen Suda",
        "Julian Chan",
        "Madhavan C R",
        "Gene-Ping Yang",
        "Yi-Chiao Wu",
        "Naoyuki Kanda",
        "Yossef Adi",
        "Xin Lei",
        "Yue Liu",
        "Florian Metze",
        "Yuzong Liu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Neural audio codecs provide promising acoustic features for speech synthesis, with representative streaming codecs like Mimi providing high-quality acoustic features for real-time Text-to-Speech (TTS) applications. However, Mimi's decoder, which employs a hybrid transformer and convolution architecture, introduces significant latency bottlenecks on edge devices due to the the compute intensive nature of deconvolution layers which are not friendly for mobile-CPUs, such as the most representative framework XNNPACK. This paper introduces T-Mimi, a novel modification of the Mimi codec decoder that replaces its convolutional components with a purely transformer-based decoder, inspired by the TS3-Codec architecture. This change dramatically reduces on-device TTS latency from 42.1ms to just 4.4ms. Furthermore, we conduct quantization aware training and derive a crucial finding: the final two transformer layers and the concluding linear layers of the decoder, which are close to the waveform, are highly sensitive to quantization and must be preserved at full precision to maintain audio quality.",
      "url": "https://arxiv.org/abs/2601.20094",
      "pdfUrl": "https://arxiv.org/pdf/2601.20094.pdf",
      "titleJa": "T-Mimi: リアルタイムの電話音声合成のためのトランスフォーマーベースのMimiデコーダー"
    },
    {
      "id": "2601.19781",
      "arxivId": "2601.19781",
      "title": "Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means",
      "authors": [
        "Kentaro Onda",
        "Hayato Futami",
        "Yosuke Kashiwagi",
        "Emiru Tsunoo",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In recent years, there has been growing interest in representing speech with discrete tokens, which serve as pseudo-text for speech language models (speechLMs) and as efficient intermediate representations for downstream tasks. These tokens are typically categorized as acoustic and phonetic tokens: the former holds detailed acoustic information for reconstruction while the latter mainly captures linguistic content. In human speech communication, however, unnecessary acoustic details such as speaker information are abstracted, while both linguistic and prosodic information are utilized for speech comprehension and production. Given this, neither type of token seems an ideal representation for tasks sensitive to prosody, such as speechLMs. In this study, we propose the Phonological Tokenizer, a method that fine-tunes phonetic tokens via differentiable k-means with a multi-task objective of ASR and speech resynthesis. Experimental validation on diverse tasks confirms that our tokens retain phonological (both linguistic and prosodic) information while appropriately discarding speaker identity.",
      "url": "https://arxiv.org/abs/2601.19781",
      "pdfUrl": "https://arxiv.org/pdf/2601.19781.pdf",
      "titleJa": "音韻論トークナイザー: 微分可能K平均法を用いた多目的微調整による韻律を考慮した音声トークン"
    },
    {
      "id": "2601.19712",
      "arxivId": "2601.19712",
      "title": "Physics-Aware Novel-View Acoustic Synthesis with Vision-Language Priors and 3D Acoustic Environment Modeling",
      "authors": [
        "Congyi Fan",
        "Jian Guan",
        "Youtian Lin",
        "Dongli Xu",
        "Tong Ye",
        "Qiaoxi Zhu",
        "Pengming Feng",
        "Wenwu Wang"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.MM"
      ],
      "abstract": "Spatial audio is essential for immersive experiences, yet novel-view acoustic synthesis (NVAS) remains challenging due to complex physical phenomena such as reflection, diffraction, and material absorption. Existing methods based on single-view or panoramic inputs improve spatial fidelity but fail to capture global geometry and semantic cues such as object layout and material properties. To address this, we propose Phys-NVAS, the first physics-aware NVAS framework that integrates spatial geometry modeling with vision-language semantic priors. A global 3D acoustic environment is reconstructed from multi-view images and depth maps to estimate room size and shape, enhancing spatial awareness of sound propagation. Meanwhile, a vision-language model extracts physics-aware priors of objects, layouts, and materials, capturing absorption and reflection beyond geometry. An acoustic feature fusion adapter unifies these cues into a physics-aware representation for binaural generation. Experiments on RWAVS demonstrate that Phys-NVAS yields binaural audio with improved realism and physical consistency.",
      "url": "https://arxiv.org/abs/2601.19712",
      "pdfUrl": "https://arxiv.org/pdf/2601.19712.pdf",
      "titleJa": "視覚言語事前分布と3D音響環境モデリングを用いた物理を考慮した新規視点音響合成"
    },
    {
      "id": "2601.19491",
      "arxivId": "2601.19491",
      "title": "Permutation-Invariant Physics-Informed Neural Network for Region-to-Region Sound Field Reconstruction",
      "authors": [
        "Xingyu Chen",
        "Sipei Zhao",
        "Fei Ma",
        "Eva Cheng",
        "Ian S. Burnett"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Most existing sound field reconstruction methods target point-to-region reconstruction, interpolating the Acoustic Transfer Functions (ATFs) between a fixed-position sound source and a receiver region. The applicability of these methods is limited because real-world ATFs tend to varying continuously with respect to the positions of sound sources and receiver regions. This paper presents a permutation-invariant physics-informed neural network for region-to-region sound field reconstruction, which aims to interpolate the ATFs across continuously varying sound sources and measurement regions. The proposed method employs a deep set architecture to process the receiver and sound source positions as an unordered set, preserving acoustic reciprocity. Furthermore, it incorporates the Helmholtz equation as a physical constraint to guide network training, ensuring physically consistent predictions.",
      "url": "https://arxiv.org/abs/2601.19491",
      "pdfUrl": "https://arxiv.org/pdf/2601.19491.pdf",
      "titleJa": "領域間音場再構成のための順列不変な物理学に基づくニューラルネットワーク"
    }
  ],
  "lastUpdated": "2026-02-03T01:07:04.768717",
  "totalCount": 77
}