{
  "papers": [
    {
      "id": "2602.23333",
      "arxivId": "2602.23333",
      "title": "SemanticVocoder: Bridging Audio Generation and Audio Understanding via Semantic Latents",
      "authors": [
        "Zeyu Xie",
        "Chenxing Li",
        "Qiao Jin",
        "Xuenan Xu",
        "Guanrou Yang",
        "Wenfu Wang",
        "Mengyue Wu",
        "Dong Yu",
        "Yuexian Zou"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Recent audio generation models typically rely on Variational Autoencoders (VAEs) and perform generation within the VAE latent space. Although VAEs excel at compression and reconstruction, their latents inherently encode low-level acoustic details rather than semantically discriminative information, leading to entangled event semantics and complicating the training of generative models. To address these issues, we discard VAE acoustic latents and introduce semantic encoder latents, thereby proposing SemanticVocoder, a generative vocoder that directly synthesizes waveforms from semantic latents. Equipped with SemanticVocoder, our text-to-audio generation model achieves a Frechet Distance of 12.823 and a Frechet Audio Distance of 1.709 on the AudioCaps test set, as the introduced semantic latents exhibit superior discriminability compared to acoustic VAE latents. Beyond improved generation performance, it also serves as a promising attempt towards unifying audio understanding and generation within a shared semantic space. Generated samples are available at https://zeyuxie29.github.io/SemanticVocoder/.",
      "url": "https://arxiv.org/abs/2602.23333",
      "pdfUrl": "https://arxiv.org/pdf/2602.23333.pdf",
      "titleJa": "SemanticVocoder: 意味的潜在情報によるオーディオ生成とオーディオ理解の橋渡し"
    },
    {
      "id": "2602.23070",
      "arxivId": "2602.23070",
      "title": "Make It Hard to Hear, Easy to Learn: Long-Form Bengali ASR and Speaker Diarization via Extreme Augmentation and Perfect Alignment",
      "authors": [
        "Sanjid Hasan",
        "Risalat Labib",
        "A H M Fuad",
        "Bayazid Hasan"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Although Automatic Speech Recognition (ASR) in Bengali has seen significant progress, processing long-duration audio and performing robust speaker diarization remain critical research gaps. To address the severe scarcity of joint ASR and diarization resources for this language, we introduce Lipi-Ghor-882, a comprehensive 882-hour multi-speaker Bengali dataset. In this paper, detailing our submission to the DL Sprint 4.0 competition, we systematically evaluate various architectures and approaches for long-form Bengali speech. For ASR, we demonstrate that raw data scaling is ineffective; instead, targeted fine-tuning utilizing perfectly aligned annotations paired with synthetic acoustic degradation (noise and reverberation) emerges as the singular most effective approach. Conversely, for speaker diarization, we observed that global open-source state-of-the-art models (such as Diarizen) performed surprisingly poorly on this complex dataset. Extensive model retraining yielded negligible improvements; instead, strategic, heuristic post-processing of baseline model outputs proved to be the primary driver for increasing accuracy. Ultimately, this work outlines a highly optimized dual pipeline achieving a $\\sim$0.019 Real-Time Factor (RTF), establishing a practical, empirically backed benchmark for low-resource, long-form speech processing.",
      "url": "https://arxiv.org/abs/2602.23070",
      "pdfUrl": "https://arxiv.org/pdf/2602.23070.pdf",
      "titleJa": "聞き取りにくく、学びやすく：極度の拡張と完璧なアライメントによる長文ベンガル語ASRと話者ダイアライゼーション"
    },
    {
      "id": "2602.23068",
      "arxivId": "2602.23068",
      "title": "TADA: A Generative Framework for Speech Modeling via Text-Acoustic Dual Alignment",
      "authors": [
        "Trung Dang",
        "Sharath Rao",
        "Ananya Gupta",
        "Christopher Gagne",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Jakub Piotr Cłapa",
        "Peter Chin",
        "Alan Cowen"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Modern Text-to-Speech (TTS) systems increasingly leverage Large Language Model (LLM) architectures to achieve scalable, high-fidelity, zero-shot generation. However, these systems typically rely on fixed-frame-rate acoustic tokenization, resulting in speech sequences that are significantly longer than, and asynchronous with their corresponding text. Beyond computational inefficiency, this sequence length disparity often triggers hallucinations in TTS and amplifies the modality gap in spoken language modeling (SLM). In this paper, we propose a novel tokenization scheme that establishes one-to-one synchronization between continuous acoustic features and text tokens, enabling unified, single-stream modeling within an LLM. We demonstrate that these synchronous tokens maintain high-fidelity audio reconstruction and can be effectively modeled in a latent space by a large language model with a flow matching head. Moreover, the ability to seamlessly toggle speech modality within the context enables text-only guidance--a technique that blends logits from text-only and text-speech modes to flexibly bridge the gap toward text-only LLM intelligence. Experimental results indicate that our approach achieves performance competitive with state-of-the-art TTS and SLM systems while virtually eliminating content hallucinations and preserving linguistic integrity, all at a significantly reduced inference cost.",
      "url": "https://arxiv.org/abs/2602.23068",
      "pdfUrl": "https://arxiv.org/pdf/2602.23068.pdf",
      "titleJa": "TADA: テキストと音響の二重アライメントによる音声モデリングのための生成フレームワーク"
    },
    {
      "id": "2602.22935",
      "arxivId": "2602.22935",
      "title": "A Holistic Framework for Robust Bangla ASR and Speaker Diarization with Optimized VAD and CTC Alignment",
      "authors": [
        "Zarif Ishmam",
        "Zarif Mahir",
        "Shafnan Wasif",
        "Md. Ishtiak Moin"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Despite being one of the most widely spoken languages globally, Bangla remains a low-resource language in the field of Natural Language Processing (NLP). Mainstream Automatic Speech Recognition (ASR) and Speaker Diarization systems for Bangla struggles when processing longform audio exceeding 3060 seconds. This paper presents a robust framework specifically engineered for extended Bangla content by leveraging preexisting models enhanced with novel optimization pipelines for the DL Sprint 4.0 contest. Our approach utilizes Voice Activity Detection (VAD) optimization and Connectionist Temporal Classification (CTC) segmentation via forced word alignment to maintain temporal accuracy and transcription integrity over long durations. Additionally, we employed several finetuning techniques and preprocessed the data using augmentation techniques and noise removal. By bridging the performance gap in complex, multi-speaker environments, this work provides a scalable solution for real-world, longform Bangla speech applications.",
      "url": "https://arxiv.org/abs/2602.22935",
      "pdfUrl": "https://arxiv.org/pdf/2602.22935.pdf",
      "titleJa": "最適化されたVADとCTCアライメントを備えた堅牢なベンガル語ASRと話者ダイアライゼーションのための包括的なフレームワーク"
    },
    {
      "id": "2602.22710",
      "arxivId": "2602.22710",
      "title": "Same Words, Different Judgments: Modality Effects on Preference Alignment",
      "authors": [
        "Aaron Broukhim",
        "Nadir Weibel",
        "Eshin Jolly"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.HC"
      ],
      "abstract": "Preference-based reinforcement learning (PbRL) is the dominant framework for aligning AI systems to human preferences, but its application to speech remains underexplored. We present a controlled cross-modal study of human and synthetic preference annotations, comparing text and audio evaluations of identical semantic content across 100 prompts. Audio preferences prove as reliable as text, with inter-rater agreement reaching good levels (ICC(2,k) $\\approx$ .80) at $\\sim$9 raters -- the first ICC-based reliability characterization in the preference annotation literature for either modality. However, modality reshapes how people judge: audio raters exhibit narrower decision thresholds, reduced length bias, and more user-oriented evaluation criteria, with near-chance cross-modality agreement. Synthetic ratings further align with human judgments and predict inter-rater agreement, supporting their use both for triaging ambiguous pairs and as full replacements for human annotations.",
      "url": "https://arxiv.org/abs/2602.22710",
      "pdfUrl": "https://arxiv.org/pdf/2602.22710.pdf",
      "titleJa": "同じ言葉、異なる判断：モダリティが選好の整合に与える影響"
    },
    {
      "id": "2602.22597",
      "arxivId": "2602.22597",
      "title": "Relating the Neural Representations of Vocalized, Mimed, and Imagined Speech",
      "authors": [
        "Maryam Maghsoudi",
        "Rupesh Chillale",
        "Shihab A. Shamma"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "We investigated the relationship among neural representations of vocalized, mimed, and imagined speech recorded using publicly available stereotactic EEG recordings. Most prior studies have focused on decoding speech responses within each condition separately. Here, instead, we explore how responses across conditions relate by training linear spectrogram reconstruction models for each condition and evaluate their generalization across conditions. We demonstrate that linear decoders trained on one condition generally transfer successfully to others, implying shared speech representations. This commonality was assessed with stimulus-level discriminability by performing a rank-based analysis demonstrating preservation of stimulus-specific structure in both within- and across-conditions. Finally, we compared linear reconstructions to those from a nonlinear neural network. While both exhibited cross-condition transfer, linear models achieve superior stimulus-level discriminability.",
      "url": "https://arxiv.org/abs/2602.22597",
      "pdfUrl": "https://arxiv.org/pdf/2602.22597.pdf",
      "titleJa": "発声、身振り、想像による発話の神経表現の関連付け"
    },
    {
      "id": "2602.22522",
      "arxivId": "2602.22522",
      "title": "Efficient Dialect-Aware Modeling and Conditioning for Low-Resource Taiwanese Hakka Speech Processing",
      "authors": [
        "An-Ci Peng",
        "Kuan-Tang Huang",
        "Tien-Hong Lo",
        "Hung-Shin Lee",
        "Hsin-Min Wang",
        "Berlin Chen"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Taiwanese Hakka is a low-resource, endangered language that poses significant challenges for automatic speech recognition (ASR), including high dialectal variability and the presence of two distinct writing systems (Hanzi and Pinyin). Traditional ASR models often encounter difficulties in this context, as they tend to conflate essential linguistic content with dialect-specific variations across both phonological and lexical dimensions. To address these challenges, we propose a unified framework grounded in the Recurrent Neural Network Transducers (RNN-T). Central to our approach is the introduction of dialect-aware modeling strategies designed to disentangle dialectal \"style\" from linguistic \"content\", which enhances the model's capacity to learn robust and generalized representations. Additionally, the framework employs parameter-efficient prediction networks to concurrently model ASR (Hanzi and Pinyin). We demonstrate that these tasks create a powerful synergy, wherein the cross-script objective serves as a mutual regularizer to improve the primary ASR tasks. Experiments conducted on the HAT corpus reveal that our model achieves 57.00% and 40.41% relative error rate reduction on Hanzi and Pinyin ASR, respectively. To our knowledge, this is the first systematic investigation into the impact of Hakka dialectal variations on ASR and the first single model capable of jointly addressing these tasks.",
      "url": "https://arxiv.org/abs/2602.22522",
      "pdfUrl": "https://arxiv.org/pdf/2602.22522.pdf",
      "titleJa": "低リソース台湾客家語音声処理のための効率的な方言考慮モデリングとコンディショニング"
    },
    {
      "id": "2602.22487",
      "arxivId": "2602.22487",
      "title": "Moving Speaker Separation via Parallel Spectral-Spatial Processing",
      "authors": [
        "Yuzhu Wang",
        "Archontis Politis",
        "Konstantinos Drossos",
        "Tuomas Virtanen"
      ],
      "publishedDate": "2026-02-25",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Multi-channel speech separation in dynamic environments is challenging as time-varying spatial and spectral features evolve at different temporal scales. Existing methods typically employ sequential architectures, forcing a single network stream to simultaneously model both feature types, creating an inherent modeling conflict. In this paper, we propose a dual-branch parallel spectral-spatial (PS2) architecture that separately processes spectral and spatial features through parallel streams. The spectral branch uses a bi-directional long short-term memory (BLSTM)-based frequency module, a Mamba-based temporal module, and a self-attention module to model spectral features. The spatial branch employs bi-directional gated recurrent unit (BGRU) networks to process spatial features that encode the evolving geometric relationships between sources and microphones. Features from both branches are integrated through a cross-attention fusion mechanism that adaptively weights their contributions. Experimental results demonstrate that the PS2 outperforms existing state-of-the-art (SOTA) methods by 1.6-2.2 dB in scale-invariant signal-to-distortion ratio (SI-SDR) for moving speaker scenarios, with robust separation quality under different reverberation times (RT60), noise levels, and source movement speeds. Even with fast source movements, the proposed model maintains SI-SDR improvements of over 13 dB. These improvements are consistently observed across multiple datasets, including WHAMR! and our generated WSJ0-Demand-6ch-Move dataset.",
      "url": "https://arxiv.org/abs/2602.22487",
      "pdfUrl": "https://arxiv.org/pdf/2602.22487.pdf",
      "titleJa": "並列スペクトル空間処理による移動話者分離"
    },
    {
      "id": "2602.22431",
      "arxivId": "2602.22431",
      "title": "mmWave Radar Aware Dual-Conditioned GAN for Speech Reconstruction of Signals With Low SNR",
      "authors": [
        "Jash Karani",
        "Adithya Chittem",
        "Deepan Roy",
        "Sandeep Joshi"
      ],
      "publishedDate": "2026-02-25",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Millimeter-wave (mmWave) radar captures are band-limited and noisy, making for difficult reconstruction of intelligible full-bandwidth speech. In this work, we propose a two-stage speech reconstruction pipeline for mmWave using a Radar-Aware Dual-conditioned Generative Adversarial Network (RAD-GAN), which is capable of performing bandwidth extension on signals with low signal-to-noise ratios (-5 dB to -1 dB), captured through glass walls. We propose an mmWave-tailored Multi-Mel Discriminator (MMD) and a Residual Fusion Gate (RFG) to enhance the generator input to process multiple conditioning channels. The proposed two-stage pipeline involves pretraining the model on synthetically clipped clean speech and finetuning on fused mel spectrograms generated by the RFG. We empirically show that the proposed method, trained on a limited dataset, with no pre-trained modules, and no data augmentations, outperformed state-of-the-art approaches for this specific task. Audio examples of RAD-GAN are available online at https://rad-gan-demo-site.vercel.app/.",
      "url": "https://arxiv.org/abs/2602.22431",
      "pdfUrl": "https://arxiv.org/pdf/2602.22431.pdf",
      "titleJa": "低SNR信号の音声再構成のためのmmWaveレーダー対応デュアルコンディショニングGAN"
    },
    {
      "id": "2602.22417",
      "arxivId": "2602.22417",
      "title": "Absorbing Discrete Diffusion for Speech Enhancement",
      "authors": [
        "Philippe Gonzalez"
      ],
      "publishedDate": "2026-02-25",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Inspired by recent developments in neural speech coding and diffusion-based language modeling, we tackle speech enhancement by modeling the conditional distribution of clean speech codes given noisy speech codes using absorbing discrete diffusion. The proposed approach, which we call ADDSE, leverages both the expressive latent space of neural audio codecs and the non-autoregressive sampling procedure of diffusion models. To efficiently model the hierarchical structure of residual vector quantization codes, we propose RQDiT, which combines techniques from RQ-Transformer and diffusion Transformers for non-autoregressive modeling. Results show competitive performance in terms of non-intrusive objective metrics on two datasets, especially at low signal-to-noise ratios and with few sampling steps. Code and audio examples are available online.",
      "url": "https://arxiv.org/abs/2602.22417",
      "pdfUrl": "https://arxiv.org/pdf/2602.22417.pdf",
      "titleJa": "音声強調のための吸収離散拡散"
    },
    {
      "id": "2602.22039",
      "arxivId": "2602.22039",
      "title": "TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition",
      "authors": [
        "Cheng-Yeh Yang",
        "Chien-Chun Wang",
        "Li-Wei Chen",
        "Hung-Shin Lee",
        "Hsin-Min Wang",
        "Berlin Chen"
      ],
      "publishedDate": "2026-02-25",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "Low-resource automatic speech recognition (ASR) continues to pose significant challenges, primarily due to the limited availability of transcribed data for numerous languages. While a wealth of spoken content is accessible in television dramas and online videos, Taiwanese Hokkien exemplifies this issue, with transcriptions often being scarce and the majority of available subtitles provided only in Mandarin. To address this deficiency, we introduce TG-ASR for Taiwanese Hokkien drama speech recognition, a translation-guided ASR framework that utilizes multilingual translation embeddings to enhance recognition performance in low-resource environments. The framework is centered around the parallel gated cross-attention (PGCA) mechanism, which adaptively integrates embeddings from various auxiliary languages into the ASR decoder. This mechanism facilitates robust cross-linguistic semantic guidance while ensuring stable optimization and minimizing interference between languages. To support ongoing research initiatives, we present YT-THDC, a 30-hour corpus of Taiwanese Hokkien drama speech with aligned Mandarin subtitles and manually verified Taiwanese Hokkien transcriptions. Comprehensive experiments and analyses identify the auxiliary languages that most effectively enhance ASR performance, achieving a 14.77% relative reduction in character error rate and demonstrating the efficacy of translation-guided learning for underrepresented languages in practical applications.",
      "url": "https://arxiv.org/abs/2602.22039",
      "pdfUrl": "https://arxiv.org/pdf/2602.22039.pdf",
      "titleJa": "TG-ASR: 低リソース自動音声認識のための並列ゲートクロスアテンションを用いた翻訳誘導学習"
    },
    {
      "id": "2602.21900",
      "arxivId": "2602.21900",
      "title": "EmoOmni: Bridging Emotional Understanding and Expression in Omni-Modal LLMs",
      "authors": [
        "Wenjie Tian",
        "Zhixian Zhao",
        "Jingbin Hu",
        "Huakang Chen",
        "Haohe Liu",
        "Binshen Mu",
        "Lei Xie"
      ],
      "publishedDate": "2026-02-25",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The evolution of Omni-Modal Large Language Models~(Omni-LLMs) has revolutionized human--computer interaction, enabling unified audio-visual perception and speech response. However, existing Omni-LLMs struggle with complex real-world scenarios, often leading to superficial understanding and contextually mismatched emotional responses. This issue is further intensified by Omni-LLM's Thinker-Talker architectures, which are implicitly connected through hidden states, leading to the loss of emotional details. In this work, we present EmoOmni, a unified framework for accurate understanding and expression in multimodal emotional dialogue. At its core, we introduce the emotional Chain-of-Thought~(E-CoT), which enforces a reasoning from fine-grained multimodal perception to textual response. Moreover, we explicitly treat E-CoT as high-level emotional instructions that guide the talker, enabling accurate emotional expression. Complementing the model, we construct EmoOmniPipe to obtain the real-world annotated dialogue data and establish a benchmark, EmoOmniEval, to facilitate systematic assessment of multimodal emotional dialogue task. Experiments show that EmoOmni-7B achieves comparable performance with Qwen3Omni-30B-A3B-Thinking under the same talker.",
      "url": "https://arxiv.org/abs/2602.21900",
      "pdfUrl": "https://arxiv.org/pdf/2602.21900.pdf",
      "titleJa": "EmoOmni: オムニモーダルLLMにおける感情理解と表現の橋渡し"
    },
    {
      "id": "2602.21772",
      "arxivId": "2602.21772",
      "title": "UniWhisper: Efficient Continual Multi-task Training for Robust Universal Audio Representation",
      "authors": [
        "Yuxuan Chen",
        "Peize He",
        "Haoyuan Xu",
        "Junzi Zhang"
      ],
      "publishedDate": "2026-02-25",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "A universal audio representation should capture fine-grained speech cues and high-level semantics for environmental sounds and music in a single encoder. Existing encoders often excel in one domain but degrade in others. We propose UniWhisper, an efficient continual multi-task training framework that casts heterogeneous audio tasks into a unified instruction and answer format. This enables standard next-token training without task-specific heads and losses. We train it on 38k hours of public audio and assess the encoder using shallow MLP probes and k-nearest neighbors (kNN) on 20 tasks spanning speech, environmental sound, and music. UniWhisper reaches normalized weighted averages of 0.81 with MLP probes and 0.61 with kNN, compared to 0.64 and 0.46 for Whisper, while retaining strong speech performance.",
      "url": "https://arxiv.org/abs/2602.21772",
      "pdfUrl": "https://arxiv.org/pdf/2602.21772.pdf",
      "titleJa": "UniWhisper: 堅牢なユニバーサルオーディオ表現のための効率的な継続的マルチタスクトレーニング"
    },
    {
      "id": "2602.22279",
      "arxivId": "2602.22279",
      "title": "Learning to reconstruct from saturated data: audio declipping and high-dynamic range imaging",
      "authors": [
        "Victor Sechaud",
        "Laurent Jacques",
        "Patrice Abry",
        "Julián Tachella"
      ],
      "publishedDate": "2026-02-25",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Learning based methods are now ubiquitous for solving inverse problems, but their deployment in real-world applications is often hindered by the lack of ground truth references for training. Recent self-supervised learning strategies offer a promising alternative, avoiding the need for ground truth. However, most existing methods are limited to linear inverse problems. This work extends self-supervised learning to the non-linear problem of recovering audio and images from clipped measurements, by assuming that the signal distribution is approximately invariant to changes in amplitude. We provide sufficient conditions for learning to reconstruct from saturated signals alone and a self-supervised loss that can be used to train reconstruction networks. Experiments on both audio and image data show that the proposed approach is almost as effective as fully supervised approaches, despite relying solely on clipped measurements for training.",
      "url": "https://arxiv.org/abs/2602.22279",
      "pdfUrl": "https://arxiv.org/pdf/2602.22279.pdf",
      "titleJa": "飽和データからの再構成の学習：オーディオクリッピング除去と高ダイナミックレンジイメージング"
    },
    {
      "id": "2602.21741",
      "arxivId": "2602.21741",
      "title": "Robust Long-Form Bangla Speech Processing: Automatic Speech Recognition and Speaker Diarization",
      "authors": [
        "MD. Sagor Chowdhury",
        "Adiba Fairooz Chowdhury"
      ],
      "publishedDate": "2026-02-25",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "We describe our end-to-end system for Bengali long-form speech recognition (ASR) and speaker diarization submitted to the DL Sprint 4.0 competition on Kaggle. Bengali presents substantial challenges for both tasks: a large phoneme inventory, significant dialectal variation, frequent code-mixing with English, and a relative scarcity of large-scale labelled corpora. For ASR we achieve a best private Word Error Rate (WER) of 0.37738 and public WER of 0.36137, combining a BengaliAI fine-tuned Whisper medium model with Demucs source separation for vocal isolation, silence-boundary chunking, and carefully tuned generation hyperparameters. For speaker diarization we reach a best private Diarization Error Rate (DER) of 0.27671 and public DER of 0.20936 by replacing the default segmentation model inside the pyannote.audio pipeline with a Bengali-fine-tuned variant, pairing it with wespeaker-voxceleb-resnet34-LM embeddings and centroid-based agglomerative clustering. Our experiments demonstrate that domain-specific fine-tuning of the segmentation component, vocal source separation, and natural silence-aware chunking are the three most impactful design choices for low-resource Bengali speech processing.",
      "url": "https://arxiv.org/abs/2602.21741",
      "pdfUrl": "https://arxiv.org/pdf/2602.21741.pdf",
      "titleJa": "堅牢な長文ベンガル語音声処理：自動音声認識と話者ダイアライゼーション"
    },
    {
      "id": "2602.22266",
      "arxivId": "2602.22266",
      "title": "WaveSSM: Multiscale State-Space Models for Non-stationary Signal Attention",
      "authors": [
        "Ruben Solozabal",
        "Velibor Bojkovic",
        "Hilal Alquabeh",
        "Klea Ziu",
        "Kentaro Inui",
        "Martin Takac"
      ],
      "publishedDate": "2026-02-25",
      "categories": [
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "State-space models (SSMs) have emerged as a powerful foundation for long-range sequence modeling, with the HiPPO framework showing that continuous-time projection operators can be used to derive stable, memory-efficient dynamical systems that encode the past history of the input signal. However, existing projection-based SSMs often rely on polynomial bases with global temporal support, whose inductive biases are poorly matched to signals exhibiting localized or transient structure. In this work, we introduce \\emph{WaveSSM}, a collection of SSMs constructed over wavelet frames. Our key observation is that wavelet frames yield a localized support on the temporal dimension, useful for tasks requiring precise localization. Empirically, we show that on equal conditions, \\textit{WaveSSM} outperforms orthogonal counterparts as S4 on real-world datasets with transient dynamics, including physiological signals on the PTB-XL dataset and raw audio on Speech Commands.",
      "url": "https://arxiv.org/abs/2602.22266",
      "pdfUrl": "https://arxiv.org/pdf/2602.22266.pdf",
      "titleJa": "WaveSSM: 非定常信号アテンションのためのマルチスケール状態空間モデル"
    },
    {
      "id": "2602.22253",
      "arxivId": "2602.22253",
      "title": "AR&D: A Framework for Retrieving and Describing Concepts for Interpreting AudioLLMs",
      "authors": [
        "Townim Faisal Chowdhury",
        "Ta Duc Huy",
        "Siqi Pan",
        "Jeremy Stoddard",
        "Zhibin Liao"
      ],
      "publishedDate": "2026-02-24",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Despite strong performance in audio perception tasks, large audio-language models (AudioLLMs) remain opaque to interpretation. A major factor behind this lack of interpretability is that individual neurons in these models frequently activate in response to several unrelated concepts. We introduce the first mechanistic interpretability framework for AudioLLMs, leveraging sparse autoencoders (SAEs) to disentangle polysemantic activations into monosemantic features. Our pipeline identifies representative audio clips, assigns meaningful names via automated captioning, and validates concepts through human evaluation and steering. Experiments show that AudioLLMs encode structured and interpretable features, enhancing transparency and control. This work provides a foundation for trustworthy deployment in high-stakes domains and enables future extensions to larger models, multilingual audio, and more fine-grained paralinguistic features. Project URL: https://townim-faisal.github.io/AutoInterpret-AudioLLM/",
      "url": "https://arxiv.org/abs/2602.22253",
      "pdfUrl": "https://arxiv.org/pdf/2602.22253.pdf",
      "titleJa": "AR&D: AudioLLMを解釈するための概念の取得と記述のためのフレームワーク"
    },
    {
      "id": "2602.21183",
      "arxivId": "2602.21183",
      "title": "823-OLT @ BUET DL Sprint 4.0: Context-Aware Windowing for ASR and Fine-Tuned Speaker Diarization in Bengali Long Form Audio",
      "authors": [
        "Ratnajit Dhar",
        "Arpita Mallik"
      ],
      "publishedDate": "2026-02-24",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Bengali, despite being one of the most widely spoken languages globally, remains underrepresented in long form speech technology, particularly in systems addressing transcription and speaker attribution. We present frameworks for long form Bengali speech intelligence that address automatic speech recognition using a Whisper Medium based model and speaker diarization using a finetuned segmentation model. The ASR pipeline incorporates vocal separation, voice activity detection, and a gap aware windowing strategy to construct context preserving segments for stable decoding. For diarization, a pretrained speaker segmentation model is finetuned on the official competition dataset (provided as part of the DL Sprint 4.0 competition organized under BUET CSE Fest), to better capture Bengali conversational patterns. The resulting systems deliver both efficient transcription of long form audio and speaker aware transcription to provide scalable speech technology solutions for low resource languages.",
      "url": "https://arxiv.org/abs/2602.21183",
      "pdfUrl": "https://arxiv.org/pdf/2602.21183.pdf",
      "titleJa": "823-OLT @ BUET DL スプリント 4.0: ベンガル語の長文音声における音声認識のためのコンテキストアウェアウィンドウ処理と微調整された話者ダイアライゼーション"
    },
    {
      "id": "2602.20967",
      "arxivId": "2602.20967",
      "title": "Training-Free Intelligibility-Guided Observation Addition for Noisy ASR",
      "authors": [
        "Haoyang Li",
        "Changsong Liu",
        "Wei Rao",
        "Hao Shi",
        "Sakriani Sakti",
        "Eng Siong Chng"
      ],
      "publishedDate": "2026-02-24",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Automatic speech recognition (ASR) degrades severely in noisy environments. Although speech enhancement (SE) front-ends effectively suppress background noise, they often introduce artifacts that harm recognition. Observation addition (OA) addressed this issue by fusing noisy and SE enhanced speech, improving recognition without modifying the parameters of the SE or ASR models. This paper proposes an intelligibility-guided OA method, where fusion weights are derived from intelligibility estimates obtained directly from the backend ASR. Unlike prior OA methods based on trained neural predictors, the proposed method is training-free, reducing complexity and enhances generalization. Extensive experiments across diverse SE-ASR combinations and datasets demonstrate strong robustness and improvements over existing OA baselines. Additional analyses of intelligibility-guided switching-based alternatives and frame versus utterance-level OA further validate the proposed design.",
      "url": "https://arxiv.org/abs/2602.20967",
      "pdfUrl": "https://arxiv.org/pdf/2602.20967.pdf",
      "titleJa": "ノイズのある音声認識のための訓練不要の明瞭度ガイド付き観測追加"
    },
    {
      "id": "2602.20823",
      "arxivId": "2602.20823",
      "title": "Geometric Analysis of Speech Representation Spaces: Topological Disentanglement and Confound Detection",
      "authors": [
        "Bipasha Kashyap",
        "Pubudu N. Pathirana"
      ],
      "publishedDate": "2026-02-24",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Speech-based clinical tools are increasingly deployed in multilingual settings, yet whether pathological speech markers remain geometrically separable from accent variation remains unclear. Systems may misclassify healthy non-native speakers or miss pathology in multilingual patients. We propose a four-metric clustering framework to evaluate geometric disentanglement of emotional, linguistic, and pathological speech features across six corpora and eight dataset combinations. A consistent hierarchy emerges: emotional features form the tightest clusters (Silhouette 0.250), followed by pathological (0.141) and linguistic (0.077). Confound analysis shows pathological-linguistic overlap remains below 0.21, which is above the permutation null but bounded for clinical deployment. Trustworthiness analysis confirms embedding fidelity and robustness of the geometric conclusions. Our framework provides actionable guidelines for equitable and reliable speech health systems across diverse populations.",
      "url": "https://arxiv.org/abs/2602.20823",
      "pdfUrl": "https://arxiv.org/pdf/2602.20823.pdf",
      "titleJa": "音声表現空間の幾何学的分析：位相的分離と交絡検出"
    },
    {
      "id": "2602.23300",
      "arxivId": "2602.23300",
      "title": "A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations",
      "authors": [
        "Soumya Dutta",
        "Smruthi Balaji",
        "Sriram Ganapathy"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.",
      "url": "https://arxiv.org/abs/2602.23300",
      "pdfUrl": "https://arxiv.org/pdf/2602.23300.pdf",
      "titleJa": "会話におけるマルチモーダル感情認識のための専門家混合モデル"
    },
    {
      "id": "2602.23171",
      "arxivId": "2602.23171",
      "title": "Align-Consistency: Improving Non-autoregressive and Semi-supervised ASR with Consistency Regularization",
      "authors": [
        "Wanting Huang",
        "Weiran Wang"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Consistency regularization (CR) improves the robustness and accuracy of Connectionist Temporal Classification (CTC) by ensuring predictions remain stable across input perturbations. In this work, we propose Align-Consistency, an extension of CR designed for Align-Refine -- a non-autoregressive (non-AR) model that performs iterative refinement of frame-level hypotheses. This method leverages the speed of parallel inference while significantly boosting recognition performance. The effectiveness of Align-Consistency is demonstrated in two settings. First, in the fully supervised setting, our results indicate that applying CR to both the base CTC model and the subsequent refinement steps is critical, and the accuracy improvements from non-AR decoding and CR are mutually additive. Second, for semi-supervised ASR, we employ fast non-AR decoding to generate online pseudo-labels on unlabeled data, which are used to further refine the supervised model and lead to substantial gains.",
      "url": "https://arxiv.org/abs/2602.23171",
      "pdfUrl": "https://arxiv.org/pdf/2602.23171.pdf",
      "titleJa": "Align-Consistency: 一貫性正則化による非自己回帰および半教師あり ASR の改善"
    },
    {
      "id": "2602.23119",
      "arxivId": "2602.23119",
      "title": "A Directional-Derivative-Constrained Method for Continuously Steerable Differential Beamformers with Uniform Circular Arrays",
      "authors": [
        "Tiantian Xiong",
        "Yongyi Deng",
        "Kunlong Zhao",
        "Jilu Jin",
        "Xueqin Luo",
        "Gongping Huang",
        "Jingdong Chen",
        "Jacob Benesty"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Differential microphone arrays offer a promising solution for far-field acoustic signal acquisition due to their high spatial directivity and compact array structure. A key challenge lies in designing differential beamformers that are continuously steerable and capable of enhancing target signals arriving from arbitrary directions. This paper studies the design of differential beamformers for circular arrays and proposes a novel framework that incorporates directional derivative constraints. By constraining the first-order derivatives of the beampattern at the desired steering direction to zero and assigning suitable values to higher-order derivatives, the beamformer is ensured to achieve its maximum response in the target direction and provide sufficient beam steering. This approach not only improves steering flexibility but also enables a more intuitive and robust beampattern design. Simulation results demonstrate that the proposed method produces continuously steerable beampatterns.",
      "url": "https://arxiv.org/abs/2602.23119",
      "pdfUrl": "https://arxiv.org/pdf/2602.23119.pdf",
      "titleJa": "均一円形アレイを備えた連続的に制御可能な差動ビームフォーマのための方向微分制約法"
    },
    {
      "id": "2602.23003",
      "arxivId": "2602.23003",
      "title": "Scattering Transform for Auditory Attention Decoding",
      "authors": [
        "René Pallenberg",
        "Fabrice Katzberg",
        "Alfred Mertins",
        "Marco Maass"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "eess.SP",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "The use of hearing aids will increase in the coming years due to demographic change. One open problem that remains to be solved by a new generation of hearing aids is the cocktail party problem. A possible solution is electroencephalography-based auditory attention decoding. This has been the subject of several studies in recent years, which have in common that they use the same preprocessing methods in most cases. In this work, in order to achieve an advantage, the use of a scattering transform is proposed as an alternative to these preprocessing methods. The two-layer scattering transform is compared with a regular filterbank, the synchrosqueezing short-time Fourier transform and the common preprocessing. To demonstrate the performance, the known and the proposed preprocessing methods are compared for different classification tasks on two widely used datasets, provided by the KU Leuven (KUL) and the Technical University of Denmark (DTU). Both established and new neural-network-based models, CNNs, LSTMs, and recent Transformer/graph-based models are used for classification. Various evaluation strategies were compared, with a focus on the task of classifying speakers who are unknown from the training. We show that the two-layer scattering transform can significantly improve the performance for subject-related conditions, especially on the KUL dataset. However, on the DTU dataset, this only applies to some of the models, or when larger amounts of training data are provided, as in 10-fold cross-validation. This suggests that the scattering transform is capable of extracting additional relevant information.",
      "url": "https://arxiv.org/abs/2602.23003",
      "pdfUrl": "https://arxiv.org/pdf/2602.23003.pdf",
      "titleJa": "聴覚注意デコードのための散乱変換"
    },
    {
      "id": "2602.22658",
      "arxivId": "2602.22658",
      "title": "Deepfake Word Detection by Next-token Prediction using Fine-tuned Whisper",
      "authors": [
        "Hoan My Tran",
        "Xin Wang",
        "Wanying Ge",
        "Xuechen Liu",
        "Junichi Yamagishi"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "abstract": "Deepfake speech utterances can be forged by replacing one or more words in a bona fide utterance with semantically different words synthesized by speech generative models. While a dedicated synthetic word detector could be developed, we investigate a cost-effective method that fine-tunes a pre-trained Whisper model to detect synthetic words while transcribing the input utterance via next-token prediction. We further investigate using partially vocoded utterances as the fine-tuning data, thereby reducing the cost of data collection. Our experiments demonstrate that, on in-domain test data, the fine-tuned Whisper yields low synthetic-word detection error rates and transcription error rates. On out-of-domain test data with synthetic words produced by unseen speech generative models, the fine-tuned Whisper remains on par with a dedicated ResNet-based detection model; however, the overall performance degradation calls for strategies to improve its generalization capability.",
      "url": "https://arxiv.org/abs/2602.22658",
      "pdfUrl": "https://arxiv.org/pdf/2602.22658.pdf",
      "titleJa": "微調整されたWhisperを用いた次トークン予測によるディープフェイク単語検出"
    },
    {
      "id": "2602.21476",
      "arxivId": "2602.21476",
      "title": "A Knowledge-Driven Approach to Music Segmentation, Music Source Separation and Cinematic Audio Source Separation",
      "authors": [
        "Chun-wei Ho",
        "Sabato Marco Siniscalchi",
        "Kai Li",
        "Chin-Hui Lee"
      ],
      "publishedDate": "2026-02-25",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "abstract": "We propose a knowledge-driven, model-based approach to segmenting audio into single-category and mixed-category chunks with applications to source separation. \"Knowledge\" here denotes information associated with the data, such as music scores. \"Model\" here refers to tool that can be used for audio segmentation and recognition, such as hidden Markov models. In contrast to conventional learning that often relies on annotated data with given segment categories and their corresponding boundaries to guide the learning process, the proposed framework does not depend on any pre-segmented training data and learns directly from the input audio and its related knowledge sources to build all necessary models autonomously. Evaluation on simulation data shows that score-guided learning achieves very good music segmentation and separation results. Tested on movie track data for cinematic audio source separation also shows that utilizing sound category knowledge achieves better separation results than those obtained with data-driven techniques without using such information.",
      "url": "https://arxiv.org/abs/2602.21476",
      "pdfUrl": "https://arxiv.org/pdf/2602.21476.pdf",
      "titleJa": "音楽セグメンテーション、音楽ソース分離、映画音楽ソース分離への知識主導型アプローチ"
    },
    {
      "id": "2602.21464",
      "arxivId": "2602.21464",
      "title": "iMiGUE-Speech: A Spontaneous Speech Dataset for Affective Analysis",
      "authors": [
        "Sofoklis Kakouros",
        "Fang Kang",
        "Haoyu Chen"
      ],
      "publishedDate": "2026-02-25",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "abstract": "This work presents iMiGUE-Speech, an extension of the iMiGUE dataset that provides a spontaneous affective corpus for studying emotional and affective states. The new release focuses on speech and enriches the original dataset with additional metadata, including speech transcripts, speaker-role separation between interviewer and interviewee, and word-level forced alignments. Unlike existing emotional speech datasets that rely on acted or laboratory-elicited emotions, iMiGUE-Speech captures spontaneous affect arising naturally from real match outcomes. To demonstrate the utility of the dataset and establish initial benchmarks, we introduce two evaluation tasks for comparative assessment: speech emotion recognition and transcript-based sentiment analysis. These tasks leverage state-of-the-art pre-trained representations to assess the dataset's ability to capture spontaneous affective states from both acoustic and linguistic modalities. iMiGUE-Speech can also be synchronously paired with micro-gesture annotations from the original iMiGUE dataset, forming a uniquely multimodal resource for studying speech-gesture affective dynamics. The extended dataset is available at https://github.com/CV-AC/imigue-speech.",
      "url": "https://arxiv.org/abs/2602.21464",
      "pdfUrl": "https://arxiv.org/pdf/2602.21464.pdf",
      "titleJa": "iMiGUE-Speech: 感情分析のための自発音声データセット"
    },
    {
      "id": "2602.22029",
      "arxivId": "2602.22029",
      "title": "MIDI-Informed Singing Accompaniment Generation in a Compositional Song Pipeline",
      "authors": [
        "Fang-Duo Tsai",
        "Yi-An Lai",
        "Fei-Yueh Chen",
        "Hsueh-Wei Fu",
        "Li Chai",
        "Wei-Jaw Lee",
        "Hao-Chung Cheng",
        "Yi-Hsuan Yang"
      ],
      "publishedDate": "2026-02-24",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Song generation aims to produce full songs with vocals and accompaniment from lyrics and text descriptions, yet end-to-end models remain data- and compute-intensive and provide limited editability. We advocate a compositional alternative that decomposes the task into melody composition, singing voice synthesis, and singing accompaniment generation. Central to our approach is MIDI-informed singing accompaniment generation (MIDI-SAG), which conditions accompaniment on the symbolic vocal-melody MIDI to improve rhythmic and harmonic alignment between singing and instrumentation. Moreover, beyond conventional SAG settings that assume continuously sung vocals, compositional song generation features intermittent vocals; we address this by combining explicit rhythmic/harmonic controls with audio continuation to keep the backing track consistent across vocal and non-vocal regions. With lightweight newly trained components requiring only 2.5k hours of audio on a single RTX 3090, our pipeline approaches the perceptual quality of recent open-source end-to-end baselines in several metrics. We provide audio demos and will open-source our model at https://composerflow.github.io/web/.",
      "url": "https://arxiv.org/abs/2602.22029",
      "pdfUrl": "https://arxiv.org/pdf/2602.22029.pdf",
      "titleJa": "作曲パイプラインにおけるMIDI情報を利用した歌唱伴奏生成"
    },
    {
      "id": "2602.20592",
      "arxivId": "2602.20592",
      "title": "Quantifying Dimensional Independence in Speech: An Information-Theoretic Framework for Disentangled Representation Learning",
      "authors": [
        "Bipasha Kashyap",
        "Björn W. Schuller",
        "Pubudu N. Pathirana"
      ],
      "publishedDate": "2026-02-24",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Speech signals encode emotional, linguistic, and pathological information within a shared acoustic channel; however, disentanglement is typically assessed indirectly through downstream task performance. We introduce an information-theoretic framework to quantify cross-dimension statistical dependence in handcrafted acoustic features by integrating bounded neural mutual information (MI) estimation with non-parametric validation. Across six corpora, cross-dimension MI remains low, with tight estimation bounds ($< 0.15$ nats), indicating weak statistical coupling in the data considered, whereas Source--Filter MI is substantially higher (0.47 nats). Attribution analysis, defined as the proportion of total MI attributable to source versus filter components, reveals source dominance for emotional dimensions (80\\%) and filter dominance for linguistic and pathological dimensions (60\\% and 58\\%, respectively). These findings provide a principled framework for quantifying dimensional independence in speech.",
      "url": "https://arxiv.org/abs/2602.20592",
      "pdfUrl": "https://arxiv.org/pdf/2602.20592.pdf",
      "titleJa": "音声における次元独立性の定量化：分離表現学習のための情報理論的枠組み"
    },
    {
      "id": "2602.20530",
      "arxivId": "2602.20530",
      "title": "Memory-guided Prototypical Co-occurrence Learning for Mixed Emotion Recognition",
      "authors": [
        "Ming Li",
        "Yong-Jin Liu",
        "Fang Liu",
        "Huankun Sheng",
        "Yeying Fan",
        "Yixiang Wei",
        "Minnan Luo",
        "Weizhan Zhang",
        "Wenping Wang"
      ],
      "publishedDate": "2026-02-24",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Emotion recognition from multi-modal physiological and behavioral signals plays a pivotal role in affective computing, yet most existing models remain constrained to the prediction of singular emotions in controlled laboratory settings. Real-world human emotional experiences, by contrast, are often characterized by the simultaneous presence of multiple affective states, spurring recent interest in mixed emotion recognition as an emotion distribution learning problem. Current approaches, however, often neglect the valence consistency and structured correlations inherent among coexisting emotions. To address this limitation, we propose a Memory-guided Prototypical Co-occurrence Learning (MPCL) framework that explicitly models emotion co-occurrence patterns. Specifically, we first fuse multi-modal signals via a multi-scale associative memory mechanism. To capture cross-modal semantic relationships, we construct emotion-specific prototype memory banks, yielding rich physiological and behavioral representations, and employ prototype relation distillation to ensure cross-modal alignment in the latent prototype space. Furthermore, inspired by human cognitive memory systems, we introduce a memory retrieval strategy to extract semantic-level co-occurrence associations across emotion categories. Through this bottom-up hierarchical abstraction process, our model learns affectively informative representations for accurate emotion distribution prediction. Comprehensive experiments on two public datasets demonstrate that MPCL consistently outperforms state-of-the-art methods in mixed emotion recognition, both quantitatively and qualitatively.",
      "url": "https://arxiv.org/abs/2602.20530",
      "pdfUrl": "https://arxiv.org/pdf/2602.20530.pdf",
      "titleJa": "混合感情認識のための記憶誘導型プロトタイプ共起学習"
    },
    {
      "id": "2602.19825",
      "arxivId": "2602.19825",
      "title": "DTT-BSR: GAN-based DTTNet with RoPE Transformer Enhancement for Music Source Restoration",
      "authors": [
        "Shihong Tan",
        "Haoyu Wang",
        "Youran Ni",
        "Yingzhao Hou",
        "Jiayue Luo",
        "Zipei Hu",
        "Han Dou",
        "Zerui Han",
        "Ningning Pan",
        "Yuzhu Wang",
        "Gongping Huang"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Music source restoration (MSR) aims to recover unprocessed stems from mixed and mastered recordings. The challenge lies in both separating overlapping sources and reconstructing signals degraded by production effects such as compression and reverberation. We therefore propose DTT-BSR, a hybrid generative adversarial network (GAN) combining rotary positional embeddings (RoPE) transformer for long-term temporal modeling with dual-path band-split recurrent neural network (RNN) for multi-resolution spectral processing. Our model achieved 3rd place on the objective leaderboard and 4th place on the subjective leaderboard on the ICASSP 2026 MSR Challenge, demonstrating exceptional generation fidelity and semantic alignment with a compact size of 7.1M parameters.",
      "url": "https://arxiv.org/abs/2602.19825",
      "pdfUrl": "https://arxiv.org/pdf/2602.19825.pdf",
      "titleJa": "DTT-BSR: 音楽ソース復元のための RoPE トランスフォーマー強化を備えた GAN ベースの DTTNet"
    },
    {
      "id": "2602.23360",
      "arxivId": "2602.23360",
      "title": "Model Agreement via Anchoring",
      "authors": [
        "Eric Eaton",
        "Surbhi Goel",
        "Marcel Hussing",
        "Michael Kearns",
        "Aaron Roth",
        "Sikata Bela Sengupta",
        "Jessica Sorrell"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Numerous lines of aim to control $\\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies. We develop a simple general technique for proving bounds on independent model disagreement based on $\\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.",
      "url": "https://arxiv.org/abs/2602.23360",
      "pdfUrl": "https://arxiv.org/pdf/2602.23360.pdf",
      "titleJa": "アンカーによるモデル合意"
    },
    {
      "id": "2602.23359",
      "arxivId": "2602.23359",
      "title": "SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation",
      "authors": [
        "Vaibhav Agrawal",
        "Rishubh Parihar",
        "Pradhaan Bhat",
        "Ravi Kiran Sarvadevabhatla",
        "R. Venkatesh Babu"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.",
      "url": "https://arxiv.org/abs/2602.23359",
      "pdfUrl": "https://arxiv.org/pdf/2602.23359.pdf",
      "titleJa": "SeeThrough3D: テキストから画像への生成におけるオクルージョンを考慮した 3D 制御"
    },
    {
      "id": "2602.23353",
      "arxivId": "2602.23353",
      "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport",
      "authors": [
        "Simon Roschmann",
        "Paul Krzakala",
        "Sonia Mazelet",
        "Quentin Bouniot",
        "Zeynep Akata"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.",
      "url": "https://arxiv.org/abs/2602.23353",
      "pdfUrl": "https://arxiv.org/pdf/2602.23353.pdf",
      "titleJa": "SOTAlign: 最適輸送によるユニモーダル視覚・言語モデルの半教師ありアライメント"
    },
    {
      "id": "2602.23349",
      "arxivId": "2602.23349",
      "title": "FlashOptim: Optimizers for Memory Efficient Training",
      "authors": [
        "Jose Javier Gonzalez Ortiz",
        "Abhay Gupta",
        "Chris Renard",
        "Davis Blalock"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory. We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half. Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.",
      "url": "https://arxiv.org/abs/2602.23349",
      "pdfUrl": "https://arxiv.org/pdf/2602.23349.pdf",
      "titleJa": "FlashOptim: メモリ効率の高いトレーニングのためのオプティマイザー"
    },
    {
      "id": "2602.23335",
      "arxivId": "2602.23335",
      "title": "Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset",
      "authors": [
        "Dany Haddad",
        "Dan Bareket",
        "Joseph Chee Chang",
        "Jay DeYoung",
        "Jena D. Hwang",
        "Uri Katz",
        "Mark Polak",
        "Sangho Suh",
        "Harshit Surana",
        "Aryeh Tiktinsky",
        "Shriya Atmakuri",
        "Jonathan Bragg",
        "Mike D'Arcy",
        "Sergey Feldman",
        "Amal Hassan-Ali",
        "Rubén Lozano",
        "Bodhisattwa Prasad Majumder",
        "Charles McGrady",
        "Amanpreet Singh",
        "Brooke Vlahos",
        "Yoav Goldberg",
        "Doug Downey"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.IR"
      ],
      "abstract": "AI-powered scientific research tools are rapidly being integrated into research workflows, yet the field lacks a clear lens into how researchers use these systems in real-world settings. We present and analyze the Asta Interaction Dataset, a large-scale resource comprising over 200,000 user queries and interaction logs from two deployed tools (a literature discovery interface and a scientific question-answering interface) within an LLM-powered retrieval-augmented generation platform. Using this dataset, we characterize query patterns, engagement behaviors, and how usage evolves with experience. We find that users submit longer and more complex queries than in traditional search, and treat the system as a collaborative research partner, delegating tasks such as drafting content and identifying research gaps. Users treat generated responses as persistent artifacts, revisiting and navigating among outputs and cited evidence in non-linear ways. With experience, users issue more targeted queries and engage more deeply with supporting citations, although keyword-style queries persist even among experienced users. We release the anonymized dataset and analysis with a new query intent taxonomy to inform future designs of real-world AI research assistants and to support realistic evaluation.",
      "url": "https://arxiv.org/abs/2602.23335",
      "pdfUrl": "https://arxiv.org/pdf/2602.23335.pdf",
      "titleJa": "AIを活用した科学研究ツールの利用状況とエンゲージメントを理解する：Astaインタラクションデータセット"
    },
    {
      "id": "2602.23334",
      "arxivId": "2602.23334",
      "title": "Bitwise Systolic Array Architecture for Runtime-Reconfigurable Multi-precision Quantized Multiplication on Hardware Accelerators",
      "authors": [
        "Yuhao Liu",
        "Salim Ullah",
        "Akash Kumar"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "abstract": "Neural network accelerators have been widely applied to edge devices for complex tasks like object tracking, image recognition, etc. Previous works have explored the quantization technologies in related lightweight accelerator designs to reduce hardware resource consumption. However, low precision leads to high accuracy loss in inference. Therefore, mixed-precision quantization becomes an alternative solution by applying different precision in different layers to trade off resource consumption and accuracy. Because regular designs for multiplication on hardware cannot support the precision reconfiguration for a multi-precision Quantized Neural Network (QNN) model in runtime, we propose a runtime reconfigurable multi-precision multi-channel bitwise systolic array design for QNN accelerators. We have implemented and evaluated our work on the Ultra96 FPGA platform. Results show that our work can achieve 1.3185 to 3.5671 times speedup in inferring mixed-precision models and has less critical path delay, supporting a higher clock frequency (250MHz).",
      "url": "https://arxiv.org/abs/2602.23334",
      "pdfUrl": "https://arxiv.org/pdf/2602.23334.pdf",
      "titleJa": "ハードウェアアクセラレータ上で実行時に再構成可能な多精度量子化乗算を実現するビット単位のシストリックアレイアーキテクチャ"
    },
    {
      "id": "2602.23331",
      "arxivId": "2602.23331",
      "title": "Utilizing LLMs for Industrial Process Automation",
      "authors": [
        "Salim Fares"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "abstract": "A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.",
      "url": "https://arxiv.org/abs/2602.23331",
      "pdfUrl": "https://arxiv.org/pdf/2602.23331.pdf",
      "titleJa": "産業プロセスオートメーションにおける法学修士号の活用"
    },
    {
      "id": "2602.23330",
      "arxivId": "2602.23330",
      "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks",
      "authors": [
        "Kunihiro Miyazaki",
        "Takanobu Kawahara",
        "Stephen Roberts",
        "Stefan Zohren"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.AI",
        "q-fin.TR"
      ],
      "abstract": "The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.",
      "url": "https://arxiv.org/abs/2602.23330",
      "pdfUrl": "https://arxiv.org/pdf/2602.23330.pdf",
      "titleJa": "専門家投資チームに向けて：細粒度取引タスクを備えたマルチエージェントLLMシステム"
    },
    {
      "id": "2602.23329",
      "arxivId": "2602.23329",
      "title": "LLM Novice Uplift on Dual-Use, In Silico Biology Tasks",
      "authors": [
        "Chen Bo Calvin Zhang",
        "Christina Q. Knight",
        "Nicholas Kruus",
        "Jason Hausenloy",
        "Pedro Medeiros",
        "Nathaniel Li",
        "Aiden Kim",
        "Yury Orlovskiy",
        "Coleman Breen",
        "Bryce Cai",
        "Jasper Götting",
        "Andrew Bo Liu",
        "Samira Nedungadi",
        "Paula Rodriguez",
        "Yannis Yiming He",
        "Mohamed Shaaban",
        "Zifan Wang",
        "Seth Donoughe",
        "Julian Michael"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.CY",
        "cs.HC"
      ],
      "abstract": "Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.",
      "url": "https://arxiv.org/abs/2602.23329",
      "pdfUrl": "https://arxiv.org/pdf/2602.23329.pdf",
      "titleJa": "デュアルユース、インシリコ生物学タスクにおけるLLM初心者向け向上"
    },
    {
      "id": "2602.23318",
      "arxivId": "2602.23318",
      "title": "Generalized Rapid Action Value Estimation in Memory-Constrained Environments",
      "authors": [
        "Aloïs Rautureau",
        "Tristan Cazenave",
        "Éric Piette"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.",
      "url": "https://arxiv.org/abs/2602.23318",
      "pdfUrl": "https://arxiv.org/pdf/2602.23318.pdf",
      "titleJa": "メモリ制約環境における一般化高速行動価値推定"
    },
    {
      "id": "2602.23315",
      "arxivId": "2602.23315",
      "title": "Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction",
      "authors": [
        "Sha Hu"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.AI"
      ],
      "abstract": "An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a \"resampling\" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.",
      "url": "https://arxiv.org/abs/2602.23315",
      "pdfUrl": "https://arxiv.org/pdf/2602.23315.pdf",
      "titleJa": "不変変換と再サンプリングに基づく認識論的不確実性の削減"
    },
    {
      "id": "2602.23312",
      "arxivId": "2602.23312",
      "title": "Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction",
      "authors": [
        "Rafael R. Baptista",
        "André de Lima Salgado",
        "Ricardo V. Godoy",
        "Marcelo Becker",
        "Thiago Boaventura",
        "Gustavo J. G. Lahr"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "eess.SY"
      ],
      "abstract": "Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency limit on-device deployment. Small language models (SLMs) offer a potential alternative, but their effectiveness for role classification in HRI has not been systematically evaluated. In this paper, we present a benchmark of SLMs for leader-follower communication, introducing a novel dataset derived from a published database and augmented with synthetic samples to capture interaction-specific dynamics. We investigate two adaptation strategies: prompt engineering and fine-tuning, studied under zero-shot and one-shot interaction modes, compared with an untrained baseline. Experiments with Qwen2.5-0.5B reveal that zero-shot fine-tuning achieves robust classification performance (86.66% accuracy) while maintaining low latency (22.2 ms per sample), significantly outperforming baseline and prompt-engineered approaches. However, results also indicate a performance degradation in one-shot modes, where increased context length challenges the model's architectural capacity. These findings demonstrate that fine-tuned SLMs provide an effective solution for direct role assignment, while highlighting critical trade-offs between dialogue complexity and classification reliability on the edge.",
      "url": "https://arxiv.org/abs/2602.23312",
      "pdfUrl": "https://arxiv.org/pdf/2602.23312.pdf",
      "titleJa": "リーダー・フォロワー相互作用における小規模言語モデルのゼロショット適応とワンショット適応の評価"
    },
    {
      "id": "2602.23302",
      "arxivId": "2602.23302",
      "title": "The logic of KM belief update is contained in the logic of AGM belief revision",
      "authors": [
        "Giacomo Bonanno"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.AI",
        "cs.LO",
        "math.LO"
      ],
      "abstract": "For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\\mathcal L_{AGM}$ and the former by $\\mathcal L_{KM}$ we show that every axiom of $\\mathcal L_{KM}$ is a theorem of $\\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\\mathcal L_{KM}$ and $\\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.",
      "url": "https://arxiv.org/abs/2602.23302",
      "pdfUrl": "https://arxiv.org/pdf/2602.23302.pdf",
      "titleJa": "KMの信念更新のロジックは、AGMの信念修正のロジックに含まれている。"
    },
    {
      "id": "2602.23296",
      "arxivId": "2602.23296",
      "title": "Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity",
      "authors": [
        "Quang-Huy Nguyen",
        "Jiaqi Wang",
        "Wei-Shinn Ku"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents. Conformal prediction is a widely used distribution-free UQ framework, yet its applications in heterogeneous FL settings remains underexplored. We provide FedWQ-CP, a simple yet effective approach that balances empirical coverage performance with efficiency at both global and agent levels under the dual heterogeneity. FedWQ-CP performs agent-server calibration in a single communication round. On each agent, conformity scores are computed on calibration data and a local quantile threshold is derived. Each agent then transmits only its quantile threshold and calibration sample size to the server. The server simply aggregates these thresholds through a weighted average to produce a global threshold. Experimental results on seven public datasets for both classification and regression demonstrate that FedWQ-CP empirically maintains agent-wise and global coverage while producing the smallest prediction sets or intervals.",
      "url": "https://arxiv.org/abs/2602.23296",
      "pdfUrl": "https://arxiv.org/pdf/2602.23296.pdf",
      "titleJa": "二重異質性下における連合不確実性定量化のための適合ニューラルネットワーク"
    },
    {
      "id": "2602.23286",
      "arxivId": "2602.23286",
      "title": "SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables",
      "authors": [
        "Sungho Park",
        "Jueun Kim",
        "Wook-Shin Han"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.IR"
      ],
      "abstract": "Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntactically valid query that returns a non-empty result, and realistic-structure enforcement, which confines generation to post-order traversals of the query graph. The resulting pipeline produces thousands of high-fidelity question-answer pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. On SPARTA, state-of-the-art models that reach over 70 F1 on HybridQA or over 50 F1 on OTT-QA drop by more than 30 F1 points, exposing fundamental weaknesses in current cross-modal reasoning. Our benchmark, construction code, and baseline models are available at https://github.com/pshlego/SPARTA/tree/main.",
      "url": "https://arxiv.org/abs/2602.23286",
      "pdfUrl": "https://arxiv.org/pdf/2602.23286.pdf",
      "titleJa": "SPARTA: テキストとテーブルを対象とするツリー構造のマルチホップQAのスケーラブルかつ原理的なベンチマーク"
    },
    {
      "id": "2602.23285",
      "arxivId": "2602.23285",
      "title": "ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks",
      "authors": [
        "Haohui Jia",
        "Zheng Chen",
        "Lingwei Zhu",
        "Rikuto Kotoge",
        "Jathurshan Pradeepkumar",
        "Yasuko Matsubara",
        "Jimeng Sun",
        "Yasushi Sakurai",
        "Takashi Matsubara"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.",
      "url": "https://arxiv.org/abs/2602.23285",
      "pdfUrl": "https://arxiv.org/pdf/2602.23285.pdf",
      "titleJa": "ODEBrain: 動的脳ネットワークをモデル化するための連続時間EEGグラフ"
    },
    {
      "id": "2602.23276",
      "arxivId": "2602.23276",
      "title": "CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays",
      "authors": [
        "Hyungyung Lee",
        "Hangyul Yoon",
        "Edward Choi"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.",
      "url": "https://arxiv.org/abs/2602.23276",
      "pdfUrl": "https://arxiv.org/pdf/2602.23276.pdf",
      "titleJa": "CXReasonAgent: 胸部X線画像のためのエビデンスに基づいた診断推論エージェント"
    },
    {
      "id": "2602.23271",
      "arxivId": "2602.23271",
      "title": "Evaluating Stochasticity in Deep Research Agents",
      "authors": [
        "Haotian Zhai",
        "Elias Stengel-Eskin",
        "Pratik Patil",
        "Liu Leqi"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability in terms of research outcome, findings, and citations. In this paper, we formalize the study of stochasticity in DRAs by modeling them as information acquisition Markov Decision Processes. We introduce an evaluation framework that quantifies variance in the system and identify three sources of it: information acquisition, information compression, and inference. Through controlled experiments, we investigate how stochasticity from these modules across different decision steps influences the variance of DRA outputs. Our results show that reducing stochasticity can improve research output quality, with inference and early-stage stochasticity contributing the most to DRA output variance. Based on these findings, we propose strategies for mitigating stochasticity while maintaining output quality via structured output and ensemble-based query generation. Our experiments on DeepSearchQA show that our proposed mitigation methods reduce average stochasticity by 22% while maintaining high research quality.",
      "url": "https://arxiv.org/abs/2602.23271",
      "pdfUrl": "https://arxiv.org/pdf/2602.23271.pdf",
      "titleJa": "ディープリサーチエージェントにおける確率性の評価"
    },
    {
      "id": "2602.23259",
      "arxivId": "2602.23259",
      "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
      "authors": [
        "Jiangxin Sun",
        "Feng Xue",
        "Teng Long",
        "Chang Liu",
        "Jian-Fang Hu",
        "Wei-Shi Zheng",
        "Nicu Sebe"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "abstract": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.",
      "url": "https://arxiv.org/abs/2602.23259",
      "pdfUrl": "https://arxiv.org/pdf/2602.23259.pdf",
      "titleJa": "一般化可能なエンドツーエンド自動運転のためのリスクを考慮した世界モデル予測制御"
    },
    {
      "id": "2602.23258",
      "arxivId": "2602.23258",
      "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning",
      "authors": [
        "Yutong Wang",
        "Siyuan Xiong",
        "Xuebo Liu",
        "Wenkang Zhou",
        "Liang Ding",
        "Miao Zhang",
        "Min Zhang"
      ],
      "publishedDate": "2026-02-26",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.",
      "url": "https://arxiv.org/abs/2602.23258",
      "pdfUrl": "https://arxiv.org/pdf/2602.23258.pdf",
      "titleJa": "AgentDropoutV2: テスト時の Rectify-or-Reject プルーニングによるマルチエージェントシステムの情報フローの最適化"
    },
    {
      "id": "2602.20744",
      "arxivId": "2602.20744",
      "title": "Voices of the Mountains: Deep Learning-Based Vocal Error Detection System for Kurdish Maqams",
      "authors": [
        "Darvan Shvan Khairaldeen",
        "Hossein Hassani"
      ],
      "publishedDate": "2026-02-24",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Maqam, a singing type, is a significant component of Kurdish music. A maqam singer receives training in a traditional face-to-face or through self-training. Automatic Singing Assessment (ASA) uses machine learning (ML) to provide the accuracy of singing styles and can help learners to improve their performance through error detection. Currently, the available ASA tools follow Western music rules. The musical composition requires all notes to stay within their expected pitch range from start to finish. The system fails to detect micro-intervals and pitch bends, so it identifies Kurdish maqam singing as incorrect even though the singer performs according to traditional rules. Kurdish maqam requires recognizing performance errors within microtonal spaces, which is beyond Western equal temperament. This research is the first attempt to address the mentioned gap. While many error types happen during singing, our focus is on pitch, rhythm, and modal stability errors in the context of Bayati-Kurd. We collected 50 songs from 13 vocalists ( 2-3 hours) and annotated 221 error spans (150 fine pitch, 46 rhythm, 25 modal drift). The data was segmented into 15,199 overlapping windows and converted to log-mel spectrograms. We developed a two-headed CNN-BiLSTM with attention mode to decide whether a window contains an error and to classify it based on the chosen errors. Trained for 20 epochs with early stopping at epoch 10, the model reached a validation macro-F1 of 0.468. On the full 50-song evaluation at a 0.750 threshold, recall was 39.4% and precision 25.8% . Within detected windows, type macro-F1 was 0.387, with F1 of 0.492 (fine pitch), 0.536 (rhythm), and 0.133 (modal drift); modal drift recall was 8.0%. The better performance on common error types shows that the method works, while the poor modal-drift recall shows that more data and balancing are needed.",
      "url": "https://arxiv.org/abs/2602.20744",
      "pdfUrl": "https://arxiv.org/pdf/2602.20744.pdf",
      "titleJa": "山の声：クルドのマカームのためのディープラーニングベースの音声エラー検出システム"
    },
    {
      "id": "2602.19976",
      "arxivId": "2602.19976",
      "title": "SongEcho: Towards Cover Song Generation via Instance-Adaptive Element-wise Linear Modulation",
      "authors": [
        "Sifei Li",
        "Yang Li",
        "Zizhou Wang",
        "Yuxin Zhang",
        "Fuzhang Wu",
        "Oliver Deussen",
        "Tong-Yee Lee",
        "Weiming Dong"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Cover songs constitute a vital aspect of musical culture, preserving the core melody of an original composition while reinterpreting it to infuse novel emotional depth and thematic emphasis. Although prior research has explored the reinterpretation of instrumental music through melody-conditioned text-to-music models, the task of cover song generation remains largely unaddressed. In this work, we reformulate our cover song generation as a conditional generation, which simultaneously generates new vocals and accompaniment conditioned on the original vocal melody and text prompts. To this end, we present SongEcho, which leverages Instance-Adaptive Element-wise Linear Modulation (IA-EiLM), a framework that incorporates controllable generation by improving both conditioning injection mechanism and conditional representation. To enhance the conditioning injection mechanism, we extend Feature-wise Linear Modulation (FiLM) to an Element-wise Linear Modulation (EiLM), to facilitate precise temporal alignment in melody control. For conditional representations, we propose Instance-Adaptive Condition Refinement (IACR), which refines conditioning features by interacting with the hidden states of the generative model, yielding instance-adaptive conditioning. Additionally, to address the scarcity of large-scale, open-source full-song datasets, we construct Suno70k, a high-quality AI song dataset enriched with comprehensive annotations. Experimental results across multiple datasets demonstrate that our approach generates superior cover songs compared to existing methods, while requiring fewer than 30% of the trainable parameters. The code, dataset, and demos are available at https://github.com/lsfhuihuiff/SongEcho_ICLR2026.",
      "url": "https://arxiv.org/abs/2602.19976",
      "pdfUrl": "https://arxiv.org/pdf/2602.19976.pdf",
      "titleJa": "SongEcho: インスタンス適応型要素単位線形変調によるカバー曲生成に向けて"
    },
    {
      "id": "2602.19816",
      "arxivId": "2602.19816",
      "title": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling",
      "authors": [
        "Yungang Yi"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Long-context modeling is essential for symbolic music generation, since motif repetition and developmental variation can span thousands of musical events. However, practical composition and performance workflows frequently rely on resource-limited devices (e.g., electronic instruments and portable computers), making heavy memory and attention computation difficult to deploy. We introduce Depth-Structured Music Recurrence (DSMR), a recurrent long-context Transformer for full-piece symbolic music modeling that extends context beyond fixed-length excerpts via segment-level recurrence with detached cross-segment states, featuring a layer-wise memory-horizon schedule that budgets recurrent KV states across depth. DSMR is trained in a single left-to-right pass over each complete composition, akin to how a musician experiences it from beginning to end, while carrying recurrent cross-segment states forward. Within this recurrent framework, we systematically study how depth-wise horizon allocations affect optimization, best-checkpoint perplexity, and efficiency. By allocating different history-window lengths across layers while keeping the total recurrent-state budget fixed, DSMR creates depth-dependent temporal receptive fields within a recurrent attention stack without reducing compute depth. Our main instantiation is a two-scale DSMR schedule that allocates long history windows to lower layers and a uniform short window to the remaining layers. Experiments on the piano performance dataset MAESTRO demonstrate that two-scale DSMR provides a practical quality--efficiency recipe for full-length long-context symbolic music modeling with recurrent attention under limited computational resources.",
      "url": "https://arxiv.org/abs/2602.19816",
      "pdfUrl": "https://arxiv.org/pdf/2602.19816.pdf",
      "titleJa": "深度構造化音楽再帰：全曲記号音楽モデリングのための予算化された再帰的注意"
    },
    {
      "id": "2602.18635",
      "arxivId": "2602.18635",
      "title": "Musical Training, but not Mere Exposure to Music, Drives the Emergence of Chroma Equivalence in Artificial Neural Networks",
      "authors": [
        "Lukas Grasse",
        "Matthew S. Tata"
      ],
      "publishedDate": "2026-02-20",
      "categories": [
        "cs.SD",
        "cs.NE"
      ],
      "abstract": "Pitch is a fundamental aspect of auditory perception. Pitch perception is commonly described across two perceptual dimensions: pitch height is the sense that tones with varying frequencies seem to be higher or lower, and chroma equivalence is the cyclical similarity of notes octaves, corresponding to a doubling of fundamental frequency. Existing research is divided on whether chroma equivalence is a learned percept that varies according to musical experience and culture, or is an innate percept that develops automatically. Building on a recent framework that proposes to use ANNs to ask 'why' questions about the brain, we evaluated recent auditory ANNs using representational similarity analysis to test the emergence of pitch height and chroma equivalence in their learned representations. Additionally, we fine-tuned two models, Wav2Vec 2.0 and Data2Vec, on a self-supervised learning task using speech and music, and a supervised music transcription task. We found that all models exhibited varying degrees of pitch height representation, but that only models trained on the supervised music transcription task exhibited chroma equivalence. Mere exposure to music through self-supervised learning was not sufficient for chroma equivalence to emerge. This supports the view that chroma equivalence is a higher-order cognitive computation that emerges to support the specific task of music perception, distinct from other auditory perception such as speech listening. This work also highlights the usefulness of ANNs for probing the developmental conditions that give rise to perceptual representations in humans.",
      "url": "https://arxiv.org/abs/2602.18635",
      "pdfUrl": "https://arxiv.org/pdf/2602.18635.pdf",
      "titleJa": "音楽訓練は、単なる音楽への露出ではなく、人工ニューラルネットワークにおけるクロマ等価性の出現を促進する"
    },
    {
      "id": "2602.18030",
      "arxivId": "2602.18030",
      "title": "Methods for Pitch Analysis in Contemporary Popular Music: Multiphonic Tones Across Genres",
      "authors": [
        "Emmanuel Deruty",
        "David Meredith",
        "Yann Macé",
        "Luc Leroy",
        "Dima Tsypkin",
        "Pascal Arbez-Nicolas"
      ],
      "publishedDate": "2026-02-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This study argues that electronic tones routinely used in contemporary popular music - including 808-style bass and power chords - are structurally and perceptually equivalent to multiphonics in contemporary classical music. Using listening tests (n=10) and signal analysis, we show that both types of tones elicit multiple, listener-dependent pitch percepts arising from similar spectral and temporal features. These findings suggest that pitch ambiguity is not confined to experimental classical contexts but is also a feature of mainstream music production.",
      "url": "https://arxiv.org/abs/2602.18030",
      "pdfUrl": "https://arxiv.org/pdf/2602.18030.pdf",
      "titleJa": "現代ポピュラー音楽におけるピッチ分析の方法：ジャンルを超えた多重音"
    },
    {
      "id": "2602.17769",
      "arxivId": "2602.17769",
      "title": "MusicSem: A Semantically Rich Language--Audio Dataset of Natural Music Descriptions",
      "authors": [
        "Rebecca Salganik",
        "Teng Tu",
        "Fei-Yueh Chen",
        "Xiaohao Liu",
        "Keifeng Lu",
        "Ethan Luvisia",
        "Zhiyao Duan",
        "Guillaume Salha-Galvan",
        "Anson Kahng",
        "Yunshan Ma",
        "Jian Kang"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Music representation learning is central to music information retrieval and generation. While recent advances in multimodal learning have improved alignment between text and audio for tasks such as cross-modal music retrieval, text-to-music generation, and music-to-text generation, existing models often struggle to capture users' expressed intent in natural language descriptions of music. This observation suggests that the datasets used to train and evaluate these models do not fully reflect the broader and more natural forms of human discourse through which music is described. In this paper, we introduce MusicSem, a dataset of 32,493 language-audio pairs derived from organic music-related discussions on the social media platform Reddit. Compared to existing datasets, MusicSem captures a broader spectrum of musical semantics, reflecting how listeners naturally describe music in nuanced and human-centered ways. To structure these expressions, we propose a taxonomy of five semantic categories: descriptive, atmospheric, situational, metadata-related, and contextual. In addition to the construction, analysis, and release of MusicSem, we use the dataset to evaluate a wide range of multimodal models for retrieval and generation, highlighting the importance of modeling fine-grained semantics. Overall, MusicSem serves as a novel semantics-aware resource to support future research on human-aligned multimodal music representation learning.",
      "url": "https://arxiv.org/abs/2602.17769",
      "pdfUrl": "https://arxiv.org/pdf/2602.17769.pdf",
      "titleJa": "MusicSem: 意味的に豊かな言語 - 自然な音楽記述の音声データセット"
    },
    {
      "id": "2602.17599",
      "arxivId": "2602.17599",
      "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
      "authors": [
        "Ivan Rinaldi",
        "Matteo Mendula",
        "Nicola Fanelli",
        "Florence Levé",
        "Matteo Testi",
        "Giovanna Castellano",
        "Gennaro Vessio"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.",
      "url": "https://arxiv.org/abs/2602.17599",
      "pdfUrl": "https://arxiv.org/pdf/2602.17599.pdf",
      "titleJa": "Art2Mus: 視覚条件付けと大規模クロスモーダルアライメントによるアートワークから音楽への生成"
    },
    {
      "id": "2602.16008",
      "arxivId": "2602.16008",
      "title": "MAEB: Massive Audio Embedding Benchmark",
      "authors": [
        "Adnan El Assadi",
        "Isaac Chung",
        "Chenghao Xiao",
        "Roman Solomatin",
        "Animesh Jha",
        "Rahul Chand",
        "Silky Singh",
        "Kaitlyn Wang",
        "Ali Sartaz Khan",
        "Marc Moussa Nasser",
        "Sufen Fong",
        "Pengfei He",
        "Alan Xiao",
        "Ayush Sunil Munot",
        "Aditya Shrivastava",
        "Artem Gazizov",
        "Niklas Muennighoff",
        "Kenneth Enevoldsen"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.",
      "url": "https://arxiv.org/abs/2602.16008",
      "pdfUrl": "https://arxiv.org/pdf/2602.16008.pdf",
      "titleJa": "MAEB: 大規模オーディオ埋め込みベンチマーク"
    },
    {
      "id": "2602.15307",
      "arxivId": "2602.15307",
      "title": "What Do Neurons Listen To? A Neuron-level Dissection of a General-purpose Audio Model",
      "authors": [
        "Takao Kawamura",
        "Daisuke Niizumi",
        "Nobutaka Ono"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "In this paper, we analyze the internal representations of a general-purpose audio self-supervised learning (SSL) model from a neuron-level perspective. Despite their strong empirical performance as feature extractors, the internal mechanisms underlying the robust generalization of SSL audio models remain unclear. Drawing on the framework of mechanistic interpretability, we identify and examine class-specific neurons by analyzing conditional activation patterns across diverse tasks. Our analysis reveals that SSL models foster the emergence of class-specific neurons that provide extensive coverage across novel task classes. These neurons exhibit shared responses across different semantic categories and acoustic similarities, such as speech attributes and musical pitch. We also confirm that these neurons have a functional impact on classification performance. To our knowledge, this is the first systematic neuron-level analysis of a general-purpose audio SSL model, providing new insights into its internal representation.",
      "url": "https://arxiv.org/abs/2602.15307",
      "pdfUrl": "https://arxiv.org/pdf/2602.15307.pdf",
      "titleJa": "ニューロンは何を聞いているのか？汎用オーディオモデルのニューロンレベルの解析"
    },
    {
      "id": "2602.13928",
      "arxivId": "2602.13928",
      "title": "voice2mode: Phonation Mode Classification in Singing using Self-Supervised Speech Models",
      "authors": [
        "Aju Ani Justus",
        "Ruchit Agrawal",
        "Sudarsana Reddy Kadiri",
        "Shrikanth Narayanan"
      ],
      "publishedDate": "2026-02-14",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "We present voice2mode, a method for classification of four singing phonation modes (breathy, neutral (modal), flow, and pressed) using embeddings extracted from large self-supervised speech models. Prior work on singing phonation has relied on handcrafted signal features or task-specific neural nets; this work evaluates the transferability of speech foundation models to singing phonation classification. voice2mode extracts layer-wise representations from HuBERT and two wav2vec2 variants, applies global temporal pooling, and classifies the pooled embeddings with lightweight classifiers (SVM, XGBoost). Experiments on a publicly available soprano dataset (763 sustained vowel recordings, four labels) show that foundation-model features substantially outperform conventional spectral baselines (spectrogram, mel-spectrogram, MFCC). HuBERT embeddings obtained from early layers yield the best result (~95.7% accuracy with SVM), an absolute improvement of ~12-15% over the best traditional baseline. We also show layer-wise behaviour: lower layers, which retain acoustic/phonetic detail, are more effective than top layers specialized for Automatic Speech Recognition (ASR).",
      "url": "https://arxiv.org/abs/2602.13928",
      "pdfUrl": "https://arxiv.org/pdf/2602.13928.pdf",
      "titleJa": "voice2mode: 自己教師あり音声モデルを用いた歌唱における発声モードの分類"
    },
    {
      "id": "2602.11910",
      "arxivId": "2602.11910",
      "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
      "authors": [
        "Łukasz Staniszewski",
        "Katarzyna Zaleska",
        "Mateusz Modrzejewski",
        "Kamil Deja"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood.",
      "url": "https://arxiv.org/abs/2602.11910",
      "pdfUrl": "https://arxiv.org/pdf/2602.11910.pdf",
      "titleJa": "TADA! アクティベーションステアリングによるオーディオ拡散モデルのチューニング"
    },
    {
      "id": "2602.11896",
      "arxivId": "2602.11896",
      "title": "Musical Metamerism with Time--Frequency Scattering",
      "authors": [
        "Vincent Lostanlen",
        "Han Han"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The concept of metamerism originates from colorimetry, where it describes a sensation of visual similarity between two colored lights despite significant differences in spectral content. Likewise, we propose to call ``musical metamerism'' the sensation of auditory similarity which is elicited by two music fragments which differ in terms of underlying waveforms. In this technical report, we describe a method to generate musical metamers from any audio recording. Our method is based on joint time--frequency scattering in Kymatio, an open-source software in Python which enables GPU computing and automatic differentiation. The advantage of our method is that it does not require any manual preprocessing, such as transcription, beat tracking, or source separation. We provide a mathematical description of JTFS as well as some excerpts from the Kymatio source code. Lastly, we review the prior work on JTFS and draw connections with closely related algorithms, such as spectrotemporal receptive fields (STRF), modulation power spectra (MPS), and Gabor filterbank (GBFB).",
      "url": "https://arxiv.org/abs/2602.11896",
      "pdfUrl": "https://arxiv.org/pdf/2602.11896.pdf",
      "titleJa": "時間周波数散乱を伴う音楽的メタメリズム"
    },
    {
      "id": "2602.10934",
      "arxivId": "2602.10934",
      "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
      "authors": [
        "Yitian Gong",
        "Kuangwei Chen",
        "Zhaoye Fei",
        "Xiaogui Yang",
        "Ke Chen",
        "Yang Wang",
        "Kexin Huang",
        "Mingshu Chen",
        "Ruixiao Li",
        "Qingyuan Cheng",
        "Shimin Li",
        "Xipeng Qiu"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.",
      "url": "https://arxiv.org/abs/2602.10934",
      "pdfUrl": "https://arxiv.org/pdf/2602.10934.pdf",
      "titleJa": "MOSS-Audio-Tokenizer: 将来のオーディオ基盤モデルに向けたオーディオトークナイザーのスケーリング"
    },
    {
      "id": "2602.12301",
      "arxivId": "2602.12301",
      "title": "Beyond Musical Descriptors: Extracting Preference-Bearing Intent in Music Queries",
      "authors": [
        "Marion Baranes",
        "Romain Hennequin",
        "Elena V. Epure"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.IR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Although annotated music descriptor datasets for user queries are increasingly common, few consider the user's intent behind these descriptors, which is essential for effectively meeting their needs. We introduce MusicRecoIntent, a manually annotated corpus of 2,291 Reddit music requests, labeling musical descriptors across seven categories with positive, negative, or referential preference-bearing roles. We then investigate how reliably large language models (LLMs) can extract these music descriptors, finding that they do capture explicit descriptors but struggle with context-dependent ones. This work can further serve as a benchmark for fine-grained modeling of user intent and for gaining insights into improving LLM-based music understanding systems.",
      "url": "https://arxiv.org/abs/2602.12301",
      "pdfUrl": "https://arxiv.org/pdf/2602.12301.pdf",
      "titleJa": "音楽記述子を超えて：音楽検索クエリにおける嗜好意図の抽出"
    },
    {
      "id": "2602.10656",
      "arxivId": "2602.10656",
      "title": "AudioRAG: A Challenging Benchmark for Audio Reasoning and Information Retrieval",
      "authors": [
        "Jingru Lin",
        "Chen Zhang",
        "Tianrui Wang",
        "Haizhou Li"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Due to recent advancements in Large Audio-Language Models (LALMs) that demonstrate remarkable performance across a range of sound-, speech- and music-related tasks, there is a growing interest in proposing benchmarks to assess these models. Existing benchmarks generally focus only on reasoning with internal knowledge, neglecting real-world scenarios that require external information grounding. To bridge this gap, we introduce AudioRAG, a novel benchmark designed to evaluate audio-based reasoning augmented by information retrieval in realistic web environments. This benchmark comprises both LLM-generated and manually curated question-answer pairs. Our evaluations reveal that even the state-of-the-art LALMs struggle to answer these questions. We therefore propose an agentic pipeline that integrates audio reasoning with retrieval-augmented generation, providing a stronger baseline for future research.",
      "url": "https://arxiv.org/abs/2602.10656",
      "pdfUrl": "https://arxiv.org/pdf/2602.10656.pdf",
      "titleJa": "AudioRAG: オーディオ推論と情報検索のための挑戦的なベンチマーク"
    },
    {
      "id": "2602.10058",
      "arxivId": "2602.10058",
      "title": "Evaluating Disentangled Representations for Controllable Music Generation",
      "authors": [
        "Laura Ibáñez-Martínez",
        "Chukwuemeka Nkama",
        "Andrea Poltronieri",
        "Xavier Serra",
        "Martín Rocamora"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Recent approaches in music generation rely on disentangled representations, often labeled as structure and timbre or local and global, to enable controllable synthesis. Yet the underlying properties of these embeddings remain underexplored. In this work, we evaluate such disentangled representations in a set of music audio models for controllable generation using a probing-based framework that goes beyond standard downstream tasks. The selected models reflect diverse unsupervised disentanglement strategies, including inductive biases, data augmentations, adversarial objectives, and staged training procedures. We further isolate specific strategies to analyze their effect. Our analysis spans four key axes: informativeness, equivariance, invariance, and disentanglement, which are assessed across datasets, tasks, and controlled transformations. Our findings reveal inconsistencies between intended and actual semantics of the embeddings, suggesting that current strategies fall short of producing truly disentangled representations, and prompting a re-examination of how controllability is approached in music generation.",
      "url": "https://arxiv.org/abs/2602.10058",
      "pdfUrl": "https://arxiv.org/pdf/2602.10058.pdf",
      "titleJa": "制御可能な音楽生成のための分離表現の評価"
    },
    {
      "id": "2602.09891",
      "arxivId": "2602.09891",
      "title": "Stemphonic: All-at-once Flexible Multi-stem Music Generation",
      "authors": [
        "Shih-Lun Wu",
        "Ge Zhu",
        "Juan-Pablo Caceres",
        "Cheng-Zhi Anna Huang",
        "Nicholas J. Bryan"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM"
      ],
      "abstract": "Music stem generation, the task of producing musically-synchronized and isolated instrument audio clips, offers the potential of greater user control and better alignment with musician workflows compared to conventional text-to-music models. Existing stem generation approaches, however, either rely on fixed architectures that output a predefined set of stems in parallel, or generate only one stem at a time, resulting in slow inference despite flexibility in stem combination. We propose Stemphonic, a diffusion-/flow-based framework that overcomes this trade-off and generates a variable set of synchronized stems in one inference pass. During training, we treat each stem as a batch element, group synchronized stems in a batch, and apply a shared noise latent to each group. At inference-time, we use a shared initial noise latent and stem-specific text inputs to generate synchronized multi-stem outputs in one pass. We further expand our approach to enable one-pass conditional multi-stem generation and stem-wise activity controls to empower users to iteratively generate and orchestrate the temporal layering of a mix. We benchmark our results on multiple open-source stem evaluation sets and show that Stemphonic produces higher-quality outputs while accelerating the full mix generation process by 25 to 50%. Demos at: https://stemphonic-demo.vercel.app.",
      "url": "https://arxiv.org/abs/2602.09891",
      "pdfUrl": "https://arxiv.org/pdf/2602.09891.pdf",
      "titleJa": "Stemphonic: 一度に柔軟なマルチステム音楽生成"
    },
    {
      "id": "2602.19674",
      "arxivId": "2602.19674",
      "title": "Continuous Telemonitoring of Heart Failure using Personalised Speech Dynamics",
      "authors": [
        "Yue Pan",
        "Xingyao Wang",
        "Hanyue Zhang",
        "Liwei Liu",
        "Changxin Li",
        "Gang Yang",
        "Rong Sheng",
        "Yili Xia",
        "Ming Chu"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Remote monitoring of heart failure (HF) via speech signals provides a non-invasive and cost-effective solution for long-term patient management. However, substantial inter-individual heterogeneity in vocal characteristics often limits the accuracy of traditional cross-sectional classification models. To address this, we propose a Longitudinal Intra-Patient Tracking (LIPT) scheme designed to capture the trajectory of relative symptomatic changes within individuals. Central to this framework is a Personalised Sequential Encoder (PSE), which transforms longitudinal speech recordings into context-aware latent representations. By incorporating historical data at each timestamp, the PSE facilitates a holistic assessment of the clinical trajectory rather than modelling discrete visits independently. Experimental results from a cohort of 225 patients demonstrate that the LIPT paradigm significantly outperforms the classic cross-sectional approaches, achieving a recognition accuracy of 99.7% for clinical status transitions. The model's high sensitivity was further corroborated by additional follow-up data, confirming its efficacy in predicting HF deterioration and its potential to secure patient safety in remote, home-based settings. Furthermore, this work addresses the gap in existing literature by providing a comprehensive analysis of different speech task designs and acoustic features. Taken together, the superior performance of the LIPT framework and PSE architecture validates their readiness for integration into long-term telemonitoring systems, offering a scalable solution for remote heart failure management.",
      "url": "https://arxiv.org/abs/2602.19674",
      "pdfUrl": "https://arxiv.org/pdf/2602.19674.pdf",
      "titleJa": "パーソナライズされた音声ダイナミクスを用いた心不全の継続的な遠隔モニタリング"
    },
    {
      "id": "2602.19409",
      "arxivId": "2602.19409",
      "title": "AuditoryHuM: Auditory Scene Label Generation and Clustering using Human-MLLM Collaboration",
      "authors": [
        "Henry Zhong",
        "Jörg M. Buchholz",
        "Julian Maclaren",
        "Simon Carlile",
        "Richard F. Lyon"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Manual annotation of audio datasets is labour intensive, and it is challenging to balance label granularity with acoustic separability. We introduce AuditoryHuM, a novel framework for the unsupervised discovery and clustering of auditory scene labels using a collaborative Human-Multimodal Large Language Model (MLLM) approach. By leveraging MLLMs (Gemma and Qwen) the framework generates contextually relevant labels for audio data. To ensure label quality and mitigate hallucinations, we employ zero-shot learning techniques (Human-CLAP) to quantify the alignment between generated text labels and raw audio content. A strategically targeted human-in-the-loop intervention is then used to refine the least aligned pairs. The discovered labels are grouped into thematically cohesive clusters using an adjusted silhouette score that incorporates a penalty parameter to balance cluster cohesion and thematic granularity. Evaluated across three diverse auditory scene datasets (ADVANCE, AHEAD-DS, and TAU 2019), AuditoryHuM provides a scalable, low-cost solution for creating standardised taxonomies. This solution facilitates the training of lightweight scene recognition models deployable to edge devices, such as hearing aids and smart home assistants. The project page and code: https://github.com/Australian-Future-Hearing-Initiative",
      "url": "https://arxiv.org/abs/2602.19409",
      "pdfUrl": "https://arxiv.org/pdf/2602.19409.pdf",
      "titleJa": "AuditoryHuM: 人間とMLLMのコラボレーションによる聴覚シーンラベル生成とクラスタリング"
    },
    {
      "id": "2602.18952",
      "arxivId": "2602.18952",
      "title": "MDM-ASR: Bridging Accuracy and Efficiency in ASR with Diffusion-Based Non-Autoregressive Decoding",
      "authors": [
        "Hao Yen",
        "Pin-Jui Ku",
        "Ante Jukić",
        "Sabato Marco Siniscalchi"
      ],
      "publishedDate": "2026-02-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "In sequence-to-sequence Transformer ASR, autoregressive (AR) models achieve strong accuracy but suffer from slow decoding, while non-autoregressive (NAR) models enable parallel decoding at the cost of degraded performance. We propose a principled NAR ASR framework based on Masked Diffusion Models to reduce this gap. A pre-trained speech encoder is coupled with a Transformer diffusion decoder conditioned on acoustic features and partially masked transcripts for parallel token prediction. To mitigate the training-inference mismatch, we introduce Iterative Self-Correction Training that exposes the model to its own intermediate predictions. We also design a Position-Biased Entropy-Bounded Confidence-based sampler with positional bias to further boost results. Experiments across multiple benchmarks demonstrate consistent gains over prior NAR models and competitive performance with strong AR baselines, while retaining parallel decoding efficiency.",
      "url": "https://arxiv.org/abs/2602.18952",
      "pdfUrl": "https://arxiv.org/pdf/2602.18952.pdf",
      "titleJa": "MDM-ASR: 拡散ベースの非自己回帰デコードによるASRの精度と効率の両立"
    },
    {
      "id": "2602.18899",
      "arxivId": "2602.18899",
      "title": "[b]=[d]-[t]+[p]: Self-supervised Speech Models Discover Phonological Vector Arithmetic",
      "authors": [
        "Kwanghee Choi",
        "Eunjung Yeo",
        "Cheol Jun Cho",
        "David Harwath",
        "David R. Mortensen"
      ],
      "publishedDate": "2026-02-21",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Self-supervised speech models (S3Ms) are known to encode rich phonetic information, yet how this information is structured remains underexplored. We conduct a comprehensive study across 96 languages to analyze the underlying structure of S3M representations, with particular attention to phonological vectors. We first show that there exist linear directions within the model's representation space that correspond to phonological features. We further demonstrate that the scale of these phonological vectors correlate to the degree of acoustic realization of their corresponding phonological features in a continuous manner. For example, the difference between [d] and [t] yields a voicing vector: adding this vector to [p] produces [b], while scaling it results in a continuum of voicing. Together, these findings indicate that S3Ms encode speech using phonologically interpretable and compositional vectors, demonstrating phonological vector arithmetic. All code and interactive demos are available at https://github.com/juice500ml/phonetic-arithmetic .",
      "url": "https://arxiv.org/abs/2602.18899",
      "pdfUrl": "https://arxiv.org/pdf/2602.18899.pdf",
      "titleJa": "[b]=[d]-[t]+[p]: 自己教師あり音声モデルが音韻ベクトル算術を発見"
    },
    {
      "id": "2602.18010",
      "arxivId": "2602.18010",
      "title": "Scaling Audio-Text Retrieval with Multimodal Large Language Models",
      "authors": [
        "Jilan Xu",
        "Carl Thomé",
        "Danijela Horak",
        "Weidi Xie",
        "Andrew Zisserman"
      ],
      "publishedDate": "2026-02-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio-text retrieval is crucial for bridging acoustic signals and natural language. While contrastive dual-encoder architectures like CLAP have shown promise, they are fundamentally limited by the capacity of small-scale encoders. Specifically, the text encoders struggle to understand complex queries that require reasoning or world knowledge. In this paper, we propose AuroLA, a novel contrastive language-audio pre-training framework that re-purposes Multimodal Large Language Models (MLLMs) as a unified backbone for retrieval. Specifically, we make three contributions: (i) we construct a scalable data pipeline that curates diverse audio from multiple sources and generates multi-granular captions, ranging from long descriptions to structured tags, via automated annotation; (ii) we adapt an MLLM for retrieval by prompting it to summarize the audio/text input and using the hidden state of a special token as audio/text embeddings. For model training, we devise a novel Hybrid-NCE loss, which employs multi-granular supervision and hard-negative reweighting to robustly align audio with diverse textual supervision; and (iii) we design an MLLM-based bidirectional re-ranking module that refines retrieval candidates through deep cross-modal interaction. Extensive experiments demonstrate that AuroLA consistently outperforms state-of-the-art models, including the recent PE-AV, while utilizing only approximately 1% of PE-AV's training data. Lastly, we observe clear scaling trends regarding dataset size and model capacity, validating the effectiveness of MLLM as a unified backbone for audio-text retrieval. Code is available at https://github.com/Jazzcharles/AuroLA.",
      "url": "https://arxiv.org/abs/2602.18010",
      "pdfUrl": "https://arxiv.org/pdf/2602.18010.pdf",
      "titleJa": "マルチモーダル大規模言語モデルによる音声テキスト検索のスケーリング"
    },
    {
      "id": "2602.18527",
      "arxivId": "2602.18527",
      "title": "JAEGER: Joint 3D Audio-Visual Grounding and Reasoning in Simulated Physical Environments",
      "authors": [
        "Zhan Liu",
        "Changli Tang",
        "Yuxin Wang",
        "Zhiyuan Zhu",
        "Youjun Chen",
        "Yiwen Shao",
        "Tianzi Wang",
        "Lei Ke",
        "Zengrui Jin",
        "Chao Zhang"
      ],
      "publishedDate": "2026-02-20",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D perception, relying on RGB video and monaural audio. This design choice introduces a fundamental dimensionality mismatch that precludes reliable source localization and spatial reasoning in complex 3D environments. We address this limitation by presenting JAEGER, a framework that extends AV-LLMs to 3D space, to enable joint spatial grounding and reasoning through the integration of RGB-D observations and multi-channel first-order ambisonics. A core contribution of our work is the neural intensity vector (Neural IV), a learned spatial audio representation that encodes robust directional cues to enhance direction-of-arrival estimation, even in adverse acoustic scenarios with overlapping sources. To facilitate large-scale training and systematic evaluation, we propose SpatialSceneQA, a benchmark of 61k instruction-tuning samples curated from simulated physical environments. Extensive experiments demonstrate that our approach consistently surpasses 2D-centric baselines across diverse spatial perception and reasoning tasks, underscoring the necessity of explicit 3D modelling for advancing AI in physical environments. Our source code, pre-trained model checkpoints and datasets will be released upon acceptance.",
      "url": "https://arxiv.org/abs/2602.18527",
      "pdfUrl": "https://arxiv.org/pdf/2602.18527.pdf",
      "titleJa": "JAEGER: シミュレーションされた物理環境における共同3Dオーディオビジュアルグラウンディングと推論"
    },
    {
      "id": "2602.17818",
      "arxivId": "2602.17818",
      "title": "Lend me an Ear: Speech Enhancement Using a Robotic Arm with a Microphone Array",
      "authors": [
        "Zachary Turcotte",
        "François Grondin"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.RO",
        "cs.SD"
      ],
      "abstract": "Speech enhancement performance degrades significantly in noisy environments, limiting the deployment of speech-controlled technologies in industrial settings, such as manufacturing plants. Existing speech enhancement solutions primarly rely on advanced digital signal processing techniques, deep learning methods, or complex software optimization techniques. This paper introduces a novel enhancement strategy that incorporates a physical optimization stage by dynamically modifying the geometry of a microphone array to adapt to changing acoustic conditions. A sixteen-microphone array is mounted on a robotic arm manipulator with seven degrees of freedom, with microphones divided into four groups of four, including one group positioned near the end-effector. The system reconfigures the array by adjusting the manipulator joint angles to place the end-effector microphones closer to the target speaker, thereby improving the reference signal quality. This proposed method integrates sound source localization techniques, computer vision, inverse kinematics, minimum variance distortionless response beamformer and time-frequency masking using a deep neural network. Experimental results demonstrate that this approach outperforms other traditional recording configruations, achieving higher scale-invariant signal-to-distortion ratio and lower word error rate accross multiple input signal-to-noise ratio conditions.",
      "url": "https://arxiv.org/abs/2602.17818",
      "pdfUrl": "https://arxiv.org/pdf/2602.17818.pdf",
      "titleJa": "耳を貸してください：マイクアレイを備えたロボットアームによる音声強調"
    },
    {
      "id": "2602.17097",
      "arxivId": "2602.17097",
      "title": "AudioChat: Unified Audio Storytelling, Editing, and Understanding with Transfusion Forcing",
      "authors": [
        "William Chen",
        "Prem Seetharaman",
        "Rithesh Kumar",
        "Oriol Nieto",
        "Shinji Watanabe",
        "Justin Salamon",
        "Zeyu Jin"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Despite recent breakthroughs, audio foundation models struggle in processing complex multi-source acoustic scenes. We refer to this challenging domain as audio stories, which can have multiple speakers and background/foreground sound effects. Compared to traditional audio processing tasks, audio stories introduce new layers of semantic, temporal, and physical complexity. To address this challenge, we propose AudioChat, a framework for developing audio foundation models that can generate, edit, and understand audio stories. AudioChat introduces a new paradigm in which LLM-based toolcalling agents simulate interactions between users and the system, and these simulated dialogues are used as training data. We also introduce a novel Audio Transfusion Forcing objective to train the AudioChat model, allowing it to simultaneously decompose high-level instructions via structured chain-of-thought reasoning and perform interactive multi-turn audio understanding/generation. To evaluate generation and editing performance, we develop three new metrics that directly measure task performance instead of relying upon distribution-based scoring. We highly encourage readers to visit our demo to better understand the capabilities of AudioChat: https://wanchichen.github.io/audiochat/.",
      "url": "https://arxiv.org/abs/2602.17097",
      "pdfUrl": "https://arxiv.org/pdf/2602.17097.pdf",
      "titleJa": "AudioChat: Transfusion Forcingによる統合オーディオストーリーテリング、編集、理解"
    },
    {
      "id": "2602.16687",
      "arxivId": "2602.16687",
      "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
      "authors": [
        "Potsawee Manakul",
        "Woody Haosheng Gan",
        "Martijn Bartelds",
        "Guangzhi Sun",
        "William Held",
        "Diyi Yang"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.",
      "url": "https://arxiv.org/abs/2602.16687",
      "pdfUrl": "https://arxiv.org/pdf/2602.16687.pdf",
      "titleJa": "インターリーブされたセマンティック、音響、テキストトークンによるオープン離散オーディオ基盤モデルのスケーリング"
    },
    {
      "id": "2602.17732",
      "arxivId": "2602.17732",
      "title": "SIRUP: A diffusion-based virtual upmixer of steering vectors for highly-directive spatialization with first-order ambisonics",
      "authors": [
        "Emilio Picard",
        "Diego Di Carlo",
        "Aditya Arie Nugraha",
        "Mathieu Fontaine",
        "Kazuyoshi Yoshii"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "This paper presents virtual upmixing of steering vectors captured by a fewer-channel spherical microphone array. This challenge has conventionally been addressed by recovering the directions and signals of sound sources from first-order ambisonics (FOA) data, and then rendering the higher-order ambisonics (HOA) data using a physics-based acoustic simulator. This approach, however, struggles to handle the mutual dependency between the spatial directivity of source estimation and the spatial resolution of FOA ambisonics data. Our method, named SIRUP, employs a latent diffusion model architecture. Specifically, a variational autoencoder (VAE) is used to learn a compact encoding of the HOA data in a latent space and a diffusion model is then trained to generate the HOA embeddings, conditioned by the FOA data. Experimental results showed that SIRUP achieved a significant improvement compared to FOA systems for steering vector upmixing, source localization, and speech denoising.",
      "url": "https://arxiv.org/abs/2602.17732",
      "pdfUrl": "https://arxiv.org/pdf/2602.17732.pdf",
      "titleJa": "SIRUP: 一次アンビソニックスによる高指向性空間化を実現するステアリングベクトルの拡散ベース仮想アップミキサー"
    },
    {
      "id": "2602.16416",
      "arxivId": "2602.16416",
      "title": "Online Single-Channel Audio-Based Sound Speed Estimation for Robust Multi-Channel Audio Control",
      "authors": [
        "Andreas Jonas Fuglsig",
        "Mads Græsbøll Christensen",
        "Jesper Rindom Jensen"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Robust spatial audio control relies on accurate acoustic propagation models, yet environmental variations, especially changes in the speed of sound, cause systematic mismatches that degrade performance. Existing methods either assume known sound speed, require multiple microphones, or rely on separate calibration, making them impractical for systems with minimal sensing. We propose an online sound speed estimator that operates during general multichannel audio playback and requires only a single observation microphone. The method exploits the structured effect of sound speed on the reproduced signal and estimates it by minimizing the mismatch between the measured audio and a parametric acoustic model. Simulations show accurate tracking of sound speed for diverse input signals and improved spatial control performance when the estimates are used to compensate propagation errors in a sound zone control framework.",
      "url": "https://arxiv.org/abs/2602.16416",
      "pdfUrl": "https://arxiv.org/pdf/2602.16416.pdf",
      "titleJa": "堅牢なマルチチャンネルオーディオ制御のためのオンラインシングルチャンネルオーディオベースの音速推定"
    },
    {
      "id": "2602.16399",
      "arxivId": "2602.16399",
      "title": "Multi-Channel Replay Speech Detection using Acoustic Maps",
      "authors": [
        "Michael Neri",
        "Tuomas Virtanen"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Replay attacks remain a critical vulnerability for automatic speaker verification systems, particularly in real-time voice assistant applications. In this work, we propose acoustic maps as a novel spatial feature representation for replay speech detection from multi-channel recordings. Derived from classical beamforming over discrete azimuth and elevation grids, acoustic maps encode directional energy distributions that reflect physical differences between human speech radiation and loudspeaker-based replay. A lightweight convolutional neural network is designed to operate on this representation, achieving competitive performance on the ReMASC dataset with approximately 6k trainable parameters. Experimental results show that acoustic maps provide a compact and physically interpretable feature space for replay attack detection across different devices and acoustic environments.",
      "url": "https://arxiv.org/abs/2602.16399",
      "pdfUrl": "https://arxiv.org/pdf/2602.16399.pdf",
      "titleJa": "音響マップを用いたマルチチャンネル再生音声検出"
    },
    {
      "id": "2602.16118",
      "arxivId": "2602.16118",
      "title": "Real time fault detection in 3D printers using Convolutional Neural Networks and acoustic signals",
      "authors": [
        "Muhammad Fasih Waheed",
        "Shonda Bernadin"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The reliability and quality of 3D printing processes are critically dependent on the timely detection of mechanical faults. Traditional monitoring methods often rely on visual inspection and hardware sensors, which can be both costly and limited in scope. This paper explores a scalable and contactless method for the use of real-time audio signal analysis for detecting mechanical faults in 3D printers. By capturing and classifying acoustic emissions during the printing process, we aim to identify common faults such as nozzle clogging, filament breakage, pully skipping and various other mechanical faults. Utilizing Convolutional neural networks, we implement algorithms capable of real-time audio classification to detect these faults promptly. Our methodology involves conducting a series of controlled experiments to gather audio data, followed by the application of advanced machine learning models for fault detection. Additionally, we review existing literature on audio-based fault detection in manufacturing and 3D printing to contextualize our research within the broader field. Preliminary results demonstrate that audio signals, when analyzed with machine learning techniques, provide a reliable and cost-effective means of enhancing real-time fault detection.",
      "url": "https://arxiv.org/abs/2602.16118",
      "pdfUrl": "https://arxiv.org/pdf/2602.16118.pdf",
      "titleJa": "畳み込みニューラルネットワークと音響信号を用いた3Dプリンターのリアルタイム故障検出"
    }
  ],
  "lastUpdated": "2026-03-01T01:12:15.287625",
  "totalCount": 81
}