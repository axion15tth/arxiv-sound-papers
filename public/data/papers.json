{
  "papers": [
    {
      "id": "2602.12986",
      "arxivId": "2602.12986",
      "title": "A two-step approach for speech enhancement in low-SNR scenarios using cyclostationary beamforming and DNNs",
      "authors": [
        "Giovanni Bologni",
        "Nicolás Arrieta Larraza",
        "Richard Heusdens",
        "Richard C. Hendriks"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Deep Neural Networks (DNNs) often struggle to suppress noise at low signal-to-noise ratios (SNRs). This paper addresses speech enhancement in scenarios dominated by harmonic noise and proposes a framework that integrates cyclostationarity-aware preprocessing with lightweight DNN-based denoising. A cyclic minimum power distortionless response (cMPDR) spectral beamformer is used as a preprocessing block. It exploits the spectral correlations of cyclostationary noise to suppress harmonic components prior to learning-based enhancement and does not require modifications to the DNN architecture. The proposed pipeline is evaluated in a single-channel setting using two DNN architectures: a simple and lightweight convolutional recurrent neural network (CRNN), and a state-of-the-art model, namely ultra-low complexity network (ULCNet). Experiments on synthetic data and real-world recordings dominated by rotating machinery noise demonstrate consistent improvements over end-to-end DNN baselines, particularly at low SNRs. Remarkably, a parameter-efficient CRNN with cMPDR preprocessing surpasses the performance of the larger ULCNet operating on raw or Wiener-filtered inputs. These results indicate that explicitly incorporating cyclostationarity as a signal prior is more effective than increasing model capacity alone for suppressing harmonic interference.",
      "url": "https://arxiv.org/abs/2602.12986",
      "pdfUrl": "https://arxiv.org/pdf/2602.12986.pdf",
      "titleJa": "サイクロステーションビームフォーミングとDNNを用いた低SNRシナリオにおける音声強調のための2段階アプローチ"
    },
    {
      "id": "2602.12746",
      "arxivId": "2602.12746",
      "title": "Lamer-SSL: Layer-aware Mixture of LoRA Experts for Continual Multilingual Expansion of Self-supervised Models without Forgetting",
      "authors": [
        "Jing Xu",
        "Minglin Wu",
        "Xueyuan Chen",
        "Xixin Wu",
        "Helen Meng"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Despite their impressive performance, self-supervised speech models often struggle to generalize to new languages and tend to forget previously acquired knowledge during continual training. To address this, we propose Lamer-SSL, a parameter-efficient framework that integrates a Layer-Aware MixturE of LoRA Experts (Lamer) module with a replay strategy. The Lamer module enables flexible balancing between shared and language-specific representations, while layer-aware expert allocation assigns more experts to deeper layers where semantic information is richer. Meanwhile, the replay strategy retains prior knowledge using minimal data, mitigating forgetting during continual training. Experiments on automatic speech recognition (ASR) and language identification (LID) demonstrate that Lamer-SSL extends self-supervised models to new languages effectively while maintaining strong performance on previously learned languages with only 2.14% parameters being trainable.",
      "url": "https://arxiv.org/abs/2602.12746",
      "pdfUrl": "https://arxiv.org/pdf/2602.12746.pdf",
      "titleJa": "Lamer-SSL: 忘れることなく自己教師モデルの継続的な多言語拡張のためのLoRAエキスパートの層を考慮した混合"
    },
    {
      "id": "2602.12546",
      "arxivId": "2602.12546",
      "title": "Decoder-only Conformer with Modality-aware Sparse Mixtures of Experts for ASR",
      "authors": [
        "Jaeyoung Lee",
        "Masato Mimura"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "We present a decoder-only Conformer for automatic speech recognition (ASR) that processes speech and text in a single stack without external speech encoders or pretrained large language models (LLM). The model uses a modality-aware sparse mixture of experts (MoE): disjoint expert pools for speech and text with hard routing and top-1 selection, embedded in hybrid-causality Conformer blocks (bidirectional for speech, causal for text). Training combines CTC on speech positions with label-smoothed cross-entropy for text generation. Our 113M-parameter model consistently improves WER over a 139M AED baseline on Librispeech (2.8% vs. 3.2% test-clean; 5.6% vs. 6.0% test-other). On Common Voice 16.1 with a single multilingual model across five languages, our approach reduces average WER from 12.2% to 10.6%. To our knowledge, this is the first randomly initialized decoder-only ASR that surpasses strong AED baselines via modality-aware routing and sparse MoE, achieving better accuracy with fewer active parameters and without alignment/adaptation modules.",
      "url": "https://arxiv.org/abs/2602.12546",
      "pdfUrl": "https://arxiv.org/pdf/2602.12546.pdf",
      "titleJa": "ASRのためのモダリティを考慮した専門家のスパース混合を備えたデコーダのみのコンフォーマー"
    },
    {
      "id": "2602.11896",
      "arxivId": "2602.11896",
      "title": "Musical Metamerism with Time--Frequency Scattering",
      "authors": [
        "Vincent Lostanlen",
        "Han Han"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The concept of metamerism originates from colorimetry, where it describes a sensation of visual similarity between two colored lights despite significant differences in spectral content. Likewise, we propose to call ``musical metamerism'' the sensation of auditory similarity which is elicited by two music fragments which differ in terms of underlying waveforms. In this technical report, we describe a method to generate musical metamers from any audio recording. Our method is based on joint time--frequency scattering in Kymatio, an open-source software in Python which enables GPU computing and automatic differentiation. The advantage of our method is that it does not require any manual preprocessing, such as transcription, beat tracking, or source separation. We provide a mathematical description of JTFS as well as some excerpts from the Kymatio source code. Lastly, we review the prior work on JTFS and draw connections with closely related algorithms, such as spectrotemporal receptive fields (STRF), modulation power spectra (MPS), and Gabor filterbank (GBFB).",
      "url": "https://arxiv.org/abs/2602.11896",
      "pdfUrl": "https://arxiv.org/pdf/2602.11896.pdf",
      "titleJa": "時間周波数散乱を伴う音楽的メタメリズム"
    },
    {
      "id": "2602.11670",
      "arxivId": "2602.11670",
      "title": "Exploring Frequency-Domain Feature Modeling for HRTF Magnitude Upsampling",
      "authors": [
        "Xingyu Chen",
        "Hanwen Bi",
        "Fei Ma",
        "Sipei Zhao",
        "Eva Cheng",
        "Ian S. Burnett"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Accurate upsampling of Head-Related Transfer Functions (HRTFs) from sparse measurements is crucial for personalized spatial audio rendering. Traditional interpolation methods, such as kernel-based weighting or basis function expansions, rely on measurements from a single subject and are limited by the spatial sampling theorem, resulting in significant performance degradation under sparse sampling. Recent learning-based methods alleviate this limitation by leveraging cross-subject information, yet most existing neural architectures primarily focus on modeling spatial relationships across directions, while spectral dependencies along the frequency dimension are often modeled implicitly or treated independently. However, HRTF magnitude responses exhibit strong local continuity and long-range structure in the frequency domain, which are not fully exploited. This work investigates frequency-domain feature modeling by examining how different architectural choices, ranging from per-frequency multilayer perceptrons to convolutional, dilated convolutional, and attention-based models, affect performance under varying sparsity levels, showing that explicit spectral modeling consistently improves reconstruction accuracy, particularly under severe sparsity. Motivated by this observation, a frequency-domain Conformer-based architecture is adopted to jointly capture local spectral continuity and long-range frequency correlations. Experimental results on the SONICOM and HUTUBS datasets demonstrate that the proposed method achieves state-of-the-art performance in terms of interaural level difference and log-spectral distortion.",
      "url": "https://arxiv.org/abs/2602.11670",
      "pdfUrl": "https://arxiv.org/pdf/2602.11670.pdf",
      "titleJa": "HRTF振幅アップサンプリングのための周波数領域特徴モデリングの検討"
    },
    {
      "id": "2602.11546",
      "arxivId": "2602.11546",
      "title": "TC-BiMamba: Trans-Chunk bidirectionally within BiMamba for unified streaming and non-streaming ASR",
      "authors": [
        "Qingshun She",
        "Jing Peng",
        "Yangui Fang",
        "Yu Xi",
        "Kai Yu"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This work investigates bidirectional Mamba (BiMamba) for unified streaming and non-streaming automatic speech recognition (ASR). Dynamic chunk size training enables a single model for offline decoding and streaming decoding with various latency settings. In contrast, existing BiMamba based streaming method is limited to fixed chunk size decoding. When dynamic chunk size training is applied, training overhead increases substantially. To tackle this issue, we propose the Trans-Chunk BiMamba (TC-BiMamba) for dynamic chunk size training. Trans-Chunk mechanism trains both bidirectional sequences in an offline style with dynamic chunk size. On the one hand, compared to traditional chunk-wise processing, TC-BiMamba simultaneously achieves 1.3 times training speedup, reduces training memory by 50%, and improves model performance since it can capture bidirectional context. On the other hand, experimental results show that TC-BiMamba outperforms U2++ and matches LC-BiMmaba with smaller model size.",
      "url": "https://arxiv.org/abs/2602.11546",
      "pdfUrl": "https://arxiv.org/pdf/2602.11546.pdf",
      "titleJa": "TC-BiMamba: BiMamba 内で双方向にチャンクを転送し、ストリーミングと非ストリーミングの ASR を統合"
    },
    {
      "id": "2602.12304",
      "arxivId": "2602.12304",
      "title": "OmniCustom: Sync Audio-Video Customization Via Joint Audio-Video Generation Model",
      "authors": [
        "Maomao Li",
        "Zhen Li",
        "Kaipeng Zhang",
        "Guosheng Yin",
        "Zhifeng Li",
        "Dong Xu"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Existing mainstream video customization methods focus on generating identity-consistent videos based on given reference images and textual prompts. Benefiting from the rapid advancement of joint audio-video generation, this paper proposes a more compelling new task: sync audio-video customization, which aims to synchronously customize both video identity and audio timbre. Specifically, given a reference image $I^{r}$ and a reference audio $A^{r}$, this novel task requires generating videos that maintain the identity of the reference image while imitating the timbre of the reference audio, with spoken content freely specifiable through user-provided textual prompts. To this end, we propose OmniCustom, a powerful DiT-based audio-video customization framework that can synthesize a video following reference image identity, audio timbre, and text prompts all at once in a zero-shot manner. Our framework is built on three key contributions. First, identity and audio timbre control are achieved through separate reference identity and audio LoRA modules that operate through self-attention layers within the base audio-video generation model. Second, we introduce a contrastive learning objective alongside the standard flow matching objective. It uses predicted flows conditioned on reference inputs as positive examples and those without reference conditions as negative examples, thereby enhancing the model ability to preserve identity and timbre. Third, we train OmniCustom on our constructed large-scale, high-quality audio-visual human dataset. Extensive experiments demonstrate that OmniCustom outperforms existing methods in generating audio-video content with consistent identity and timbre fidelity.",
      "url": "https://arxiv.org/abs/2602.12304",
      "pdfUrl": "https://arxiv.org/pdf/2602.12304.pdf",
      "titleJa": "OmniCustom: オーディオとビデオの共同生成モデルによるオーディオとビデオのカスタマイズの同期"
    },
    {
      "id": "2602.11488",
      "arxivId": "2602.11488",
      "title": "When Audio-LLMs Don't Listen: A Cross-Linguistic Study of Modality Arbitration",
      "authors": [
        "Jayadev Billa"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "When audio and text conflict, speech-enabled language models follow the text 10 times more often than when arbitrating between two text sources, even when explicitly instructed to trust the audio. Using ALME, a benchmark of 57,602 controlled audio-text conflict stimuli across 8 languages, we find that Gemini 2.0 Flash exhibits 16.6\\% text dominance under audio-text conflict versus 1.6\\% under text-text conflict with identical reliability cues. This gap is not explained by audio quality: audio-only accuracy (97.2\\%) exceeds cascade accuracy (93.9\\%), indicating audio embeddings preserve more information than text transcripts. We propose that text dominance reflects an asymmetry not in information content but in arbitration accessibility: how easily the model can reason over competing representations. This framework explains otherwise puzzling findings. Forcing transcription before answering increases text dominance (19\\% to 33\\%), sacrificing audio's information advantage without improving accessibility. Framing text as ``deliberately corrupted'' reduces text dominance by 80\\%. A fine-tuning ablation provides interventional evidence: training only the audio projection layer increases text dominance (+26.5\\%), while LoRA on the language model halves it ($-$23.9\\%), localizing text dominance to the LLM's reasoning rather than the audio encoder. Experiments across four state-of-the-art audio-LLMs and 8 languages show consistent trends with substantial cross-linguistic and cross-model variation, establishing modality arbitration as a distinct reliability dimension not captured by standard speech benchmarks.",
      "url": "https://arxiv.org/abs/2602.11488",
      "pdfUrl": "https://arxiv.org/pdf/2602.11488.pdf",
      "titleJa": "オーディオLLMが聞き取れない時：モダリティ仲裁に関する言語間研究"
    },
    {
      "id": "2602.11477",
      "arxivId": "2602.11477",
      "title": "SLD-L2S: Hierarchical Subspace Latent Diffusion for High-Fidelity Lip to Speech Synthesis",
      "authors": [
        "Yifan Liang",
        "Andong Li",
        "Kang Yang",
        "Guochen Yu",
        "Fangkun Liu",
        "Lingling Dai",
        "Xiaodong Li",
        "Chengshi Zheng"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "eess.AS",
        "cs.CE"
      ],
      "abstract": "Although lip-to-speech synthesis (L2S) has achieved significant progress in recent years, current state-of-the-art methods typically rely on intermediate representations such as mel-spectrograms or discrete self-supervised learning (SSL) tokens. The potential of latent diffusion models (LDMs) in this task remains largely unexplored. In this paper, we introduce SLD-L2S, a novel L2S framework built upon a hierarchical subspace latent diffusion model. Our method aims to directly map visual lip movements to the continuous latent space of a pre-trained neural audio codec, thereby avoiding the information loss inherent in traditional intermediate representations. The core of our method is a hierarchical architecture that processes visual representations through multiple parallel subspaces, initiated by a subspace decomposition module. To efficiently enhance interactions within and between these subspaces, we design the diffusion convolution block (DiCB) as our network backbone. Furthermore, we employ a reparameterized flow matching technique to directly generate the target latent vectors. This enables a principled inclusion of speech language model (SLM) and semantic losses during training, moving beyond conventional flow matching objectives and improving synthesized speech quality. Our experiments show that SLD-L2S achieves state-of-the-art generation quality on multiple benchmark datasets, surpassing existing methods in both objective and subjective evaluations.",
      "url": "https://arxiv.org/abs/2602.11477",
      "pdfUrl": "https://arxiv.org/pdf/2602.11477.pdf",
      "titleJa": "SLD-L2S: 階層的部分空間潜在拡散法による高忠実度口唇音声合成"
    },
    {
      "id": "2602.11145",
      "arxivId": "2602.11145",
      "title": "SCRAPL: Scattering Transform with Random Paths for Machine Learning",
      "authors": [
        "Christopher Mitcheltree",
        "Vincent Lostanlen",
        "Emmanouil Benetos",
        "Mathieu Lagrange"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "The Euclidean distance between wavelet scattering transform coefficients (known as paths) provides informative gradients for perceptual quality assessment of deep inverse problems in computer vision, speech, and audio processing. However, these transforms are computationally expensive when employed as differentiable loss functions for stochastic gradient descent due to their numerous paths, which significantly limits their use in neural network training. Against this problem, we propose \"Scattering transform with Random Paths for machine Learning\" (SCRAPL): a stochastic optimization scheme for efficient evaluation of multivariable scattering transforms. We implement SCRAPL for the joint time-frequency scattering transform (JTFS) which demodulates spectrotemporal patterns at multiple scales and rates, allowing a fine characterization of intermittent auditory textures. We apply SCRAPL to differentiable digital signal processing (DDSP), specifically, unsupervised sound matching of a granular synthesizer and the Roland TR-808 drum machine. We also propose an initialization heuristic based on importance sampling, which adapts SCRAPL to the perceptual content of the dataset, improving neural network convergence and evaluation performance. We make our code and audio samples available and provide SCRAPL as a Python package.",
      "url": "https://arxiv.org/abs/2602.11145",
      "pdfUrl": "https://arxiv.org/pdf/2602.11145.pdf",
      "titleJa": "SCRAPL: 機械学習のためのランダムパスを用いた散乱変換"
    },
    {
      "id": "2602.11072",
      "arxivId": "2602.11072",
      "title": "Simultaneous Speech-to-Speech Translation Without Aligned Data",
      "authors": [
        "Tom Labiausse",
        "Romain Fabre",
        "Yannick Estève",
        "Alexandre Défossez",
        "Neil Zeghidour"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Simultaneous speech translation requires translating source speech into a target language in real-time while handling non-monotonic word dependencies. Traditional approaches rely on supervised training with word-level aligned data, which is difficult to collect at scale and thus depends on synthetic alignments using language-specific heuristics that are suboptimal. We propose Hibiki-Zero, which eliminates the need for word-level alignments entirely. This fundamentally simplifies the training pipeline and enables seamless scaling to diverse languages with varying grammatical structures, removing the bottleneck of designing language-specific alignment heuristics. We first train on sentence-level aligned data to learn speech translation at high latency, then apply a novel reinforcement learning strategy using GRPO to optimize latency while preserving translation quality. Hibiki-Zero achieves state-of-the-art performance in translation accuracy, latency, voice transfer, and naturalness across five X-to-English tasks. Moreover, we demonstrate that our model can be adapted to support a new input language with less than 1000h of speech. We provide examples, model weights, inference code and we release a benchmark containing 45h of multilingual data for speech translation evaluation.",
      "url": "https://arxiv.org/abs/2602.11072",
      "pdfUrl": "https://arxiv.org/pdf/2602.11072.pdf",
      "titleJa": "整合データなしの音声同時翻訳"
    },
    {
      "id": "2602.10934",
      "arxivId": "2602.10934",
      "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
      "authors": [
        "Yitian Gong",
        "Kuangwei Chen",
        "Zhaoye Fei",
        "Xiaogui Yang",
        "Ke Chen",
        "Yang Wang",
        "Kexin Huang",
        "Mingshu Chen",
        "Ruixiao Li",
        "Qingyuan Cheng",
        "Shimin Li",
        "Xipeng Qiu"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.",
      "url": "https://arxiv.org/abs/2602.10934",
      "pdfUrl": "https://arxiv.org/pdf/2602.10934.pdf",
      "titleJa": "MOSS-Audio-Tokenizer: 将来のオーディオ基盤モデルに向けたオーディオトークナイザーのスケーリング"
    },
    {
      "id": "2602.10829",
      "arxivId": "2602.10829",
      "title": "Self-Supervised Learning for Speaker Recognition: A study and review",
      "authors": [
        "Theo Lepage",
        "Reda Dehak"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Deep learning models trained in a supervised setting have revolutionized audio and speech processing. However, their performance inherently depends on the quantity of human-annotated data, making them costly to scale and prone to poor generalization under unseen conditions. To address these challenges, Self-Supervised Learning (SSL) has emerged as a promising paradigm, leveraging vast amounts of unlabeled data to learn relevant representations. The application of SSL for Automatic Speech Recognition (ASR) has been extensively studied, but research on other downstream tasks, notably Speaker Recognition (SR), remains in its early stages. This work describes major SSL instance-invariance frameworks (e.g., SimCLR, MoCo, and DINO), initially developed for computer vision, along with their adaptation to SR. Various SSL methods for SR, proposed in the literature and built upon these frameworks, are also presented. An extensive review of these approaches is then conducted: (1) the effect of the main hyperparameters of SSL frameworks is investigated; (2) the role of SSL components is studied (e.g., data-augmentation, projector, positive sampling); and (3) SSL frameworks are evaluated on SR with in-domain and out-of-domain data, using a consistent experimental setup, and a comprehensive comparison of SSL methods from the literature is provided. Specifically, DINO achieves the best downstream performance and effectively models intra-speaker variability, although it is highly sensitive to hyperparameters and training conditions, while SimCLR and MoCo provide robust alternatives that effectively capture inter-speaker variability and are less prone to collapse. This work aims to highlight recent trends and advancements, identifying current challenges in the field.",
      "url": "https://arxiv.org/abs/2602.10829",
      "pdfUrl": "https://arxiv.org/pdf/2602.10829.pdf",
      "titleJa": "話者認識のための自己教師学習：研究とレビュー"
    },
    {
      "id": "2602.12301",
      "arxivId": "2602.12301",
      "title": "Beyond Musical Descriptors: Extracting Preference-Bearing Intent in Music Queries",
      "authors": [
        "Marion Baranes",
        "Romain Hennequin",
        "Elena V. Epure"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.IR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Although annotated music descriptor datasets for user queries are increasingly common, few consider the user's intent behind these descriptors, which is essential for effectively meeting their needs. We introduce MusicRecoIntent, a manually annotated corpus of 2,291 Reddit music requests, labeling musical descriptors across seven categories with positive, negative, or referential preference-bearing roles. We then investigate how reliably large language models (LLMs) can extract these music descriptors, finding that they do capture explicit descriptors but struggle with context-dependent ones. This work can further serve as a benchmark for fine-grained modeling of user intent and for gaining insights into improving LLM-based music understanding systems.",
      "url": "https://arxiv.org/abs/2602.12301",
      "pdfUrl": "https://arxiv.org/pdf/2602.12301.pdf",
      "titleJa": "音楽記述子を超えて：音楽検索クエリにおける嗜好意図の抽出"
    },
    {
      "id": "2602.10716",
      "arxivId": "2602.10716",
      "title": "RE-LLM: Refining Empathetic Speech-LLM Responses by Integrating Emotion Nuance",
      "authors": [
        "Jing-Han Chen",
        "Bo-Hao Su",
        "Ya-Tse Wu",
        "Chi-Chun Lee"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "With generative AI advancing, empathy in human-AI interaction is essential. While prior work focuses on emotional reflection, emotional exploration, key to deeper engagement, remains overlooked. Existing LLMs rely on text which captures limited emotion nuances. To address this, we propose RE-LLM, a speech-LLM integrating dimensional emotion embeddings and auxiliary learning. Experiments show statistically significant gains in empathy metrics across three datasets. RE-LLM relatively improves the Emotional Reaction score by 14.79% and 6.76% compared to text-only and speech-LLM baselines on ESD. Notably, it raises the Exploration score by 35.42% and 3.91% on IEMOCAP, 139.28% and 9.83% on ESD, and 60.95% and 22.64% on MSP-PODCAST. It also boosts unweighted accuracy by 5.4% on IEMOCAP, 2.3% on ESD, and 6.9% on MSP-PODCAST in speech emotion recognition. These results highlight the enriched emotional understanding and improved empathetic response generation of RE-LLM.",
      "url": "https://arxiv.org/abs/2602.10716",
      "pdfUrl": "https://arxiv.org/pdf/2602.10716.pdf",
      "titleJa": "RE-LLM: 感情のニュアンスを統合することで共感的なスピーチLLM応答を洗練させる"
    },
    {
      "id": "2602.10666",
      "arxivId": "2602.10666",
      "title": "From Diet to Free Lunch: Estimating Auxiliary Signal Properties using Dynamic Pruning Masks in Speech Enhancement Networks",
      "authors": [
        "Riccardo Miccini",
        "Clément Laroche",
        "Tobias Piechowiak",
        "Xenofon Fafoutis",
        "Luca Pezzarossa"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Speech Enhancement (SE) in audio devices is often supported by auxiliary modules for Voice Activity Detection (VAD), SNR estimation, or Acoustic Scene Classification to ensure robust context-aware behavior and seamless user experience. Just like SE, these tasks often employ deep learning; however, deploying additional models on-device is computationally impractical, whereas cloud-based inference would introduce additional latency and compromise privacy. Prior work on SE employed Dynamic Channel Pruning (DynCP) to reduce computation by adaptively disabling specific channels based on the current input. In this work, we investigate whether useful signal properties can be estimated from these internal pruning masks, thus removing the need for separate models. We show that simple, interpretable predictors achieve up to 93% accuracy on VAD, 84% on noise classification, and an R2 of 0.86 on F0 estimation. With binary masks, predictions reduce to weighted sums, inducing negligible overhead. Our contribution is twofold: on one hand, we examine the emergent behavior of DynCP models through the lens of downstream prediction tasks, to reveal what they are learning; on the other, we repurpose and re-propose DynCP as a holistic solution for efficient SE and simultaneous estimation of signal properties.",
      "url": "https://arxiv.org/abs/2602.10666",
      "pdfUrl": "https://arxiv.org/pdf/2602.10666.pdf",
      "titleJa": "ダイエットから無料ランチへ：音声強調ネットワークにおける動的プルーニングマスクを用いた補助信号特性の推定"
    },
    {
      "id": "2602.10656",
      "arxivId": "2602.10656",
      "title": "AudioRAG: A Challenging Benchmark for Audio Reasoning and Information Retrieval",
      "authors": [
        "Jingru Lin",
        "Chen Zhang",
        "Tianrui Wang",
        "Haizhou Li"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Due to recent advancements in Large Audio-Language Models (LALMs) that demonstrate remarkable performance across a range of sound-, speech- and music-related tasks, there is a growing interest in proposing benchmarks to assess these models. Existing benchmarks generally focus only on reasoning with internal knowledge, neglecting real-world scenarios that require external information grounding. To bridge this gap, we introduce AudioRAG, a novel benchmark designed to evaluate audio-based reasoning augmented by information retrieval in realistic web environments. This benchmark comprises both LLM-generated and manually curated question-answer pairs. Our evaluations reveal that even the state-of-the-art LALMs struggle to answer these questions. We therefore propose an agentic pipeline that integrates audio reasoning with retrieval-augmented generation, providing a stronger baseline for future research.",
      "url": "https://arxiv.org/abs/2602.10656",
      "pdfUrl": "https://arxiv.org/pdf/2602.10656.pdf",
      "titleJa": "AudioRAG: オーディオ推論と情報検索のための挑戦的なベンチマーク"
    },
    {
      "id": "2602.12299",
      "arxivId": "2602.12299",
      "title": "Acoustivision Pro: An Open-Source Interactive Platform for Room Impulse Response Analysis and Acoustic Characterization",
      "authors": [
        "Mandip Goswami"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "Room acoustics analysis plays a central role in architectural design, audio engineering, speech intelligibility assessment, and hearing research. Despite the availability of standardized metrics such as reverberation time, clarity, and speech transmission index, accessible tools that combine rigorous signal processing with intuitive visualization remain scarce. This paper presents AcoustiVision Pro, an open-source web-based platform for comprehensive room impulse response (RIR) analysis. The system computes twelve distinct acoustic parameters from uploaded or dataset-sourced RIRs, provides interactive 3D visualizations of early reflections, generates frequency-dependent decay characteristics through waterfall plots, and checks compliance against international standards including ANSI S12.60 and ISO 3382. We introduce the accompanying RIRMega and RIRMega Speech datasets hosted on Hugging Face, containing thousands of simulated room impulse responses with full metadata. The platform supports real-time auralization through FFT-based convolution, exports detailed PDF reports suitable for engineering documentation, and provides CSV data export for further analysis. We describe the mathematical foundations underlying each acoustic metric, detail the system architecture, and present preliminary case studies demonstrating the platform's utility across diverse application domains including classroom acoustics, healthcare facility design, and recording studio evaluation.",
      "url": "https://arxiv.org/abs/2602.12299",
      "pdfUrl": "https://arxiv.org/pdf/2602.12299.pdf",
      "titleJa": "Acoustivision Pro: 室内インパルス応答分析と音響特性評価のためのオープンソースのインタラクティブプラットフォーム"
    },
    {
      "id": "2602.10439",
      "arxivId": "2602.10439",
      "title": "AudioRouter: Data Efficient Audio Understanding via RL based Dual Reasoning",
      "authors": [
        "Liyang Chen",
        "Hongkai Chen",
        "Yujun Cai",
        "Sifan Li",
        "Qingwen Ye",
        "Yiwei Wang"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Large Audio Language Models (LALMs) have demonstrated strong capabilities in audio understanding and reasoning. However, their performance on fine grained auditory perception remains unreliable, and existing approaches largely rely on data intensive training to internalize perceptual abilities. We propose AudioRouter, a reinforcement learning framework that enables LALMs to improve audio understanding by learning when and how to use external audio tools. Rather than tightly coupling tool usage with audio reasoning, AudioRouter formulates tool use as an explicit decision making problem and optimizes a lightweight routing policy while keeping the underlying reasoning model frozen. Experimental results show that AudioRouter achieves substantial improvements on standard audio understanding benchmarks while requiring up to 600x less training data to learn tool usage compared with conventional training paradigms. These findings suggest that learning effective tool usage offers a data efficient and scalable alternative to internalizing perceptual abilities in LALMs.",
      "url": "https://arxiv.org/abs/2602.10439",
      "pdfUrl": "https://arxiv.org/pdf/2602.10439.pdf",
      "titleJa": "AudioRouter: RL ベースの二重推論によるデータ効率の高いオーディオ理解"
    },
    {
      "id": "2602.10230",
      "arxivId": "2602.10230",
      "title": "Frame-Level Internal Tool Use for Temporal Grounding in Audio LMs",
      "authors": [
        "Joesph An",
        "Phillip Keung",
        "Jiaqi Wang",
        "Orevaoghene Ahia",
        "Noah A. Smith"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Large audio language models are increasingly used for complex audio understanding tasks, but they struggle with temporal tasks that require precise temporal grounding, such as word alignment and speaker diarization. The standard approach, where we generate timestamps as sequences of text tokens, is computationally expensive and prone to hallucination, especially when processing audio lengths outside the model's training distribution. In this work, we propose frame-level internal tool use, a method that trains audio LMs to use their own internal audio representations to perform temporal grounding directly. We introduce a lightweight prediction mechanism trained via two objectives: a binary frame classifier and a novel inhomogeneous Poisson process (IHP) loss that models temporal event intensity. Across word localization, speaker diarization, and event localization tasks, our approach outperforms token-based baselines. Most notably, it achieves a >50x inference speedup and demonstrates robust length generalization, maintaining high accuracy on out-of-distribution audio durations where standard token-based models collapse completely.",
      "url": "https://arxiv.org/abs/2602.10230",
      "pdfUrl": "https://arxiv.org/pdf/2602.10230.pdf",
      "titleJa": "オーディオLMにおける時間的グラウンディングのためのフレームレベル内部ツールの使用"
    },
    {
      "id": "2602.13194",
      "arxivId": "2602.13194",
      "title": "Semantic Chunking and the Entropy of Natural Language",
      "authors": [
        "Weishun Zhong",
        "Doron Sivan",
        "Tankut Can",
        "Mikhail Katkov",
        "Misha Tsodyks"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.CL",
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI"
      ],
      "abstract": "The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.",
      "url": "https://arxiv.org/abs/2602.13194",
      "pdfUrl": "https://arxiv.org/pdf/2602.13194.pdf",
      "titleJa": "セマンティックチャンキングと自然言語のエントロピー"
    },
    {
      "id": "2602.13191",
      "arxivId": "2602.13191",
      "title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models",
      "authors": [
        "Sayan Deb Sarkar",
        "Rémi Pautrat",
        "Ondrej Miksik",
        "Marc Pollefeys",
        "Iro Armeni",
        "Mahdi Rad",
        "Mihai Dusmanu"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\\%$ and token usage by up to $93\\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.",
      "url": "https://arxiv.org/abs/2602.13191",
      "pdfUrl": "https://arxiv.org/pdf/2602.13191.pdf",
      "titleJa": "CoPE-VideoLM: 効率的なビデオ言語モデルのためのコーデックプリミティブ"
    },
    {
      "id": "2602.13166",
      "arxivId": "2602.13166",
      "title": "Optimal Take-off under Fuzzy Clearances",
      "authors": [
        "Hugo Henry",
        "Arthur Tsai",
        "Kelly Cohen"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.AI"
      ],
      "abstract": "This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.",
      "url": "https://arxiv.org/abs/2602.13166",
      "pdfUrl": "https://arxiv.org/pdf/2602.13166.pdf",
      "titleJa": "あいまいなクリアランスでの最適な離陸"
    },
    {
      "id": "2602.13165",
      "arxivId": "2602.13165",
      "title": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures",
      "authors": [
        "Asmit Kumar Singh",
        "Haozhe Wang",
        "Laxmi Naga Santosh Attaluri",
        "Tak Chiam",
        "Weihua Zhu"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "abstract": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \\textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency.",
      "url": "https://arxiv.org/abs/2602.13165",
      "pdfUrl": "https://arxiv.org/pdf/2602.13165.pdf",
      "titleJa": "階層型LLMアーキテクチャのための非同期検証済みセマンティックキャッシング"
    },
    {
      "id": "2602.13156",
      "arxivId": "2602.13156",
      "title": "In-Context Autonomous Network Incident Response: An End-to-End Large Language Model Agent Approach",
      "authors": [
        "Yiran Gao",
        "Kim Hammar",
        "Tao Li"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "abstract": "Rapidly evolving cyberattacks demand incident response systems that can autonomously learn and adapt to changing threats. Prior work has extensively explored the reinforcement learning approach, which involves learning response strategies through extensive simulation of the incident. While this approach can be effective, it requires handcrafted modeling of the simulator and suppresses useful semantics from raw system logs and alerts. To address these limitations, we propose to leverage large language models' (LLM) pre-trained security knowledge and in-context learning to create an end-to-end agentic solution for incident response planning. Specifically, our agent integrates four functionalities, perception, reasoning, planning, and action, into one lightweight LLM (14b model). Through fine-tuning and chain-of-thought reasoning, our LLM agent is capable of processing system logs and inferring the underlying network state (perception), updating its conjecture of attack models (reasoning), simulating consequences under different response strategies (planning), and generating an effective response (action). By comparing LLM-simulated outcomes with actual observations, the LLM agent repeatedly refines its attack conjecture and corresponding response, thereby demonstrating in-context adaptation. Our agentic approach is free of modeling and can run on commodity hardware. When evaluated on incident logs reported in the literature, our agent achieves recovery up to 23% faster than those of frontier LLMs.",
      "url": "https://arxiv.org/abs/2602.13156",
      "pdfUrl": "https://arxiv.org/pdf/2602.13156.pdf",
      "titleJa": "コンテキスト内自律ネットワークインシデント対応：エンドツーエンド大規模言語モデルエージェントアプローチ"
    },
    {
      "id": "2602.13135",
      "arxivId": "2602.13135",
      "title": "Constrained Assumption-Based Argumentation Frameworks",
      "authors": [
        "Emanuele De Angelis",
        "Fabio Fioravanti",
        "Maria Chiara Meo",
        "Alberto Pettorossi",
        "Maurizio Proietti",
        "Francesca Toni"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "abstract": "Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.",
      "url": "https://arxiv.org/abs/2602.13135",
      "pdfUrl": "https://arxiv.org/pdf/2602.13135.pdf",
      "titleJa": "制約付き仮定に基づく議論のフレームワーク"
    },
    {
      "id": "2602.13110",
      "arxivId": "2602.13110",
      "title": "SCOPE: Selective Conformal Optimized Pairwise LLM Judging",
      "authors": [
        "Sher Badshah",
        "Ali Emami",
        "Hassan Sajjad"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $α$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies, providing a stronger selection signal that enables SCOPE to consistently meet the target risk level while retaining good coverage across judge scales. In particular, at $α= 0.10$, \\textsc{Scope} consistently satisfies the risk bound across all benchmarks and judge scales (empirical risk $\\approx 0.097$ to $0.099$), while retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and $0.98$ on RewardBench with Qwen-32B. Compared to naïve baselines, \\textsc{Scope} accepts up to $2.4\\times$ more judgments on MT-Bench with Qwen-7B under the same target risk constraint, demonstrating that BPE enables reliable and high-coverage LLM-based evaluation.",
      "url": "https://arxiv.org/abs/2602.13110",
      "pdfUrl": "https://arxiv.org/pdf/2602.13110.pdf",
      "titleJa": "範囲: 選択的共形最適化ペアワイズLLM判定"
    },
    {
      "id": "2602.13106",
      "arxivId": "2602.13106",
      "title": "Which Algorithms Can Graph Neural Networks Learn?",
      "authors": [
        "Solveig Wittig",
        "Antonis Vasileiou",
        "Robert R. Nerem",
        "Timo Stoll",
        "Floris Geerts",
        "Yusu Wang",
        "Christopher Morris"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS",
        "cs.NE"
      ],
      "abstract": "In recent years, there has been growing interest in understanding neural architectures' ability to learn to execute discrete algorithms, a line of work often referred to as neural algorithmic reasoning. The goal is to integrate algorithmic reasoning capabilities into larger neural pipelines. Many such architectures are based on (message-passing) graph neural networks (MPNNs), owing to their permutation equivariance and ability to deal with sparsity and variable-sized inputs. However, existing work is either largely empirical and lacks formal guarantees or it focuses solely on expressivity, leaving open the question of when and how such architectures generalize beyond a finite training set. In this work, we propose a general theoretical framework that characterizes the sufficient conditions under which MPNNs can learn an algorithm from a training set of small instances and provably approximate its behavior on inputs of arbitrary size. Our framework applies to a broad class of algorithms, including single-source shortest paths, minimum spanning trees, and general dynamic programming problems, such as the $0$-$1$ knapsack problem. In addition, we establish impossibility results for a wide range of algorithmic tasks, showing that standard MPNNs cannot learn them, and we derive more expressive MPNN-like architectures that overcome these limitations. Finally, we refine our analysis for the Bellman-Ford algorithm, yielding a substantially smaller required training set and significantly extending the recent work of Nerem et al. [2025] by allowing for a differentiable regularization loss. Empirical results largely support our theoretical findings.",
      "url": "https://arxiv.org/abs/2602.13106",
      "pdfUrl": "https://arxiv.org/pdf/2602.13106.pdf",
      "titleJa": "グラフ ニューラル ネットワークはどのようなアルゴリズムを学習できますか?"
    },
    {
      "id": "2602.13093",
      "arxivId": "2602.13093",
      "title": "Consistency of Large Reasoning Models Under Multi-Turn Attacks",
      "authors": [
        "Yubo Li",
        "Ramayya Krishnan",
        "Rema Padman"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.",
      "url": "https://arxiv.org/abs/2602.13093",
      "pdfUrl": "https://arxiv.org/pdf/2602.13093.pdf",
      "titleJa": "大規模推論モデルのマルチターン攻撃に対する一貫性"
    },
    {
      "id": "2602.13088",
      "arxivId": "2602.13088",
      "title": "How cyborg propaganda reshapes collective action",
      "authors": [
        "Jonas R. Kunst",
        "Kinga Bierwiaczonek",
        "Meeyoung Cha",
        "Omid V. Ebrahimi",
        "Marc Fawcett-Atkinson",
        "Asbjørn Følstad",
        "Anton Gollwitzer",
        "Nils Köbis",
        "Gary Marcus",
        "Jon Roozenbeek",
        "Daniel Thilo Schroeder",
        "Jay J. Van Bavel",
        "Sander van der Linden",
        "Rory White",
        "Live Leonhardsen Wilhelmsen"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "abstract": "The distinction between genuine grassroots activism and automated influence operations is collapsing. While policy debates focus on bot farms, a distinct threat to democracy is emerging via partisan coordination apps and artificial intelligence-what we term 'cyborg propaganda.' This architecture combines large numbers of verified humans with adaptive algorithmic automation, enabling a closed-loop system. AI tools monitor online sentiment to optimize directives and generate personalized content for users to post online. Cyborg propaganda thereby exploits a critical legal shield: by relying on verified citizens to ratify and disseminate messages, these campaigns operate in a regulatory gray zone, evading liability frameworks designed for automated botnets. We explore the collective action paradox of this technology: does it democratize power by 'unionizing' influence (pooling the reach of dispersed citizens to overcome the algorithmic invisibility of isolated voices), or does it reduce citizens to 'cognitive proxies' of a central directive? We argue that cyborg propaganda fundamentally alters the digital public square, shifting political discourse from a democratic contest of individual ideas to a battle of algorithmic campaigns. We outline a research agenda to distinguish organic from coordinated information diffusion and propose governance frameworks to address the regulatory challenges of AI-assisted collective expression.",
      "url": "https://arxiv.org/abs/2602.13088",
      "pdfUrl": "https://arxiv.org/pdf/2602.13088.pdf",
      "titleJa": "サイボーグプロパガンダが集団行動をどのように変えるか"
    },
    {
      "id": "2602.13087",
      "arxivId": "2602.13087",
      "title": "EXCODER: EXplainable Classification Of DiscretE time series Representations",
      "authors": [
        "Yannik Hahn",
        "Antonin Königsfeld",
        "Hasan Tercan",
        "Tobias Meisen"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Deep learning has significantly improved time series classification, yet the lack of explainability in these models remains a major challenge. While Explainable AI (XAI) techniques aim to make model decisions more transparent, their effectiveness is often hindered by the high dimensionality and noise present in raw time series data. In this work, we investigate whether transforming time series into discrete latent representations-using methods such as Vector Quantized Variational Autoencoders (VQ-VAE) and Discrete Variational Autoencoders (DVAE)-not only preserves but enhances explainability by reducing redundancy and focusing on the most informative patterns. We show that applying XAI methods to these compressed representations leads to concise and structured explanations that maintain faithfulness without sacrificing classification performance. Additionally, we propose Similar Subsequence Accuracy (SSA), a novel metric that quantitatively assesses the alignment between XAI-identified salient subsequences and the label distribution in the training data. SSA provides a systematic way to validate whether the features highlighted by XAI methods are truly representative of the learned classification patterns. Our findings demonstrate that discrete latent representations not only retain the essential characteristics needed for classification but also offer a pathway to more compact, interpretable, and computationally efficient explanations in time series analysis.",
      "url": "https://arxiv.org/abs/2602.13087",
      "pdfUrl": "https://arxiv.org/pdf/2602.13087.pdf",
      "titleJa": "EXCODER: 離散時系列表現の説明可能な分類"
    },
    {
      "id": "2602.13071",
      "arxivId": "2602.13071",
      "title": "Bus-Conditioned Zero-Shot Trajectory Generation via Task Arithmetic",
      "authors": [
        "Shuai Liu",
        "Ning Cao",
        "Yile Chen",
        "Yue Jiang",
        "Gao Cong"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Mobility trajectory data provide essential support for smart city applications. However, such data are often difficult to obtain. Meanwhile, most existing trajectory generation methods implicitly assume that at least a subset of real mobility data from target city is available, which limits their applicability in data-inaccessible scenarios. In this work, we propose a new problem setting, called bus-conditioned zero-shot trajectory generation, where no mobility trajectories from a target city are accessible. The generation process relies solely on source city mobility data and publicly available bus timetables from both cities. Under this setting, we propose MobTA, the first approach to introduce task arithmetic into trajectory generation. MobTA models the parameter shift from bus-timetable-based trajectory generation to mobility trajectory generation in source city, and applies this shift to target city through arithmetic operations on task vectors. This enables trajectory generation that reflects target-city mobility patterns without requiring any real mobility data from it. Furthermore, we theoretically analyze MobTA's stability across base and instruction-tuned LLMs. Extensive experiments show that MobTA significantly outperforms existing methods, and achieves performance close to models finetuned using target city mobility trajectories.",
      "url": "https://arxiv.org/abs/2602.13071",
      "pdfUrl": "https://arxiv.org/pdf/2602.13071.pdf",
      "titleJa": "タスク演算によるバス条件付きゼロショット軌道生成"
    },
    {
      "id": "2602.13061",
      "arxivId": "2602.13061",
      "title": "Diverging Flows: Detecting Extrapolations in Conditional Generation",
      "authors": [
        "Constantinos Tsakonas",
        "Serena Ivaldi",
        "Jean-Baptiste Mouret"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "abstract": "The ability of Flow Matching (FM) to model complex conditional distributions has established it as the state-of-the-art for prediction tasks (e.g., robotics, weather forecasting). However, deployment in safety-critical settings is hindered by a critical extrapolation hazard: driven by smoothness biases, flow models yield plausible outputs even for off-manifold conditions, resulting in silent failures indistinguishable from valid predictions. In this work, we introduce Diverging Flows, a novel approach that enables a single model to simultaneously perform conditional generation and native extrapolation detection by structurally enforcing inefficient transport for off-manifold inputs. We evaluate our method on synthetic manifolds, cross-domain style transfer, and weather temperature forecasting, demonstrating that it achieves effective detection of extrapolations without compromising predictive fidelity or inference latency. These results establish Diverging Flows as a robust solution for trustworthy flow models, paving the way for reliable deployment in domains such as medicine, robotics, and climate science.",
      "url": "https://arxiv.org/abs/2602.13061",
      "pdfUrl": "https://arxiv.org/pdf/2602.13061.pdf",
      "titleJa": "発散フロー：条件付き生成における外挿の検出"
    },
    {
      "id": "2602.13055",
      "arxivId": "2602.13055",
      "title": "Curriculum-DPO++: Direct Preference Optimization via Data and Model Curricula for Text-to-Image Generation",
      "authors": [
        "Florinel-Alin Croitoru",
        "Vlad Hondru",
        "Radu Tudor Ionescu",
        "Nicu Sebe",
        "Mubarak Shah"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). However, neither RLHF nor DPO take into account the fact that learning certain preferences is more difficult than learning other preferences, rendering the optimization process suboptimal. To address this gap in text-to-image generation, we recently proposed Curriculum-DPO, a method that organizes image pairs by difficulty. In this paper, we introduce Curriculum-DPO++, an enhanced method that combines the original data-level curriculum with a novel model-level curriculum. More precisely, we propose to dynamically increase the learning capacity of the denoising network as training advances. We implement this capacity increase via two mechanisms. First, we initialize the model with only a subset of the trainable layers used in the original Curriculum-DPO. As training progresses, we sequentially unfreeze layers until the configuration matches the full baseline architecture. Second, as the fine-tuning is based on Low-Rank Adaptation (LoRA), we implement a progressive schedule for the dimension of the low-rank matrices. Instead of maintaining a fixed capacity, we initialize the low-rank matrices with a dimension significantly smaller than that of the baseline. As training proceeds, we incrementally increase their rank, allowing the capacity to grow until it converges to the same rank value as in Curriculum-DPO. Furthermore, we propose an alternative ranking strategy to the one employed by Curriculum-DPO. Finally, we compare Curriculum-DPO++ against Curriculum-DPO and other state-of-the-art preference optimization approaches on nine benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at https://github.com/CroitoruAlin/Curriculum-DPO.",
      "url": "https://arxiv.org/abs/2602.13055",
      "pdfUrl": "https://arxiv.org/pdf/2602.13055.pdf",
      "titleJa": "カリキュラム-DPO++: テキストから画像への生成のためのデータとモデルカリキュラムによる直接的な嗜好最適化"
    },
    {
      "id": "2602.13047",
      "arxivId": "2602.13047",
      "title": "Can we trust AI to detect healthy multilingual English speakers among the cognitively impaired cohort in the UK? An investigation using real-world conversational speech",
      "authors": [
        "Madhurananda Pahar",
        "Caitlin Illingworth",
        "Dorota Braun",
        "Bahman Mirheidari",
        "Lise Sproson",
        "Daniel Blackburn",
        "Heidi Christensen"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Conversational speech often reveals early signs of cognitive decline, such as dementia and MCI. In the UK, one in four people belongs to an ethnic minority, and dementia prevalence is expected to rise most rapidly among Black and Asian communities. This study examines the trustworthiness of AI models, specifically the presence of bias, in detecting healthy multilingual English speakers among the cognitively impaired cohort, to make these tools clinically beneficial. For experiments, monolingual participants were recruited nationally (UK), and multilingual speakers were enrolled from four community centres in Sheffield and Bradford. In addition to a non-native English accent, multilinguals spoke Somali, Chinese, or South Asian languages, who were further divided into two Yorkshire accents (West and South) to challenge the efficiency of the AI tools thoroughly. Although ASR systems showed no significant bias across groups, classification and regression models using acoustic and linguistic features exhibited bias against multilingual speakers, particularly in memory, fluency, and reading tasks. This bias was more pronounced when models were trained on the publicly available DementiaBank dataset. Moreover, multilinguals were more likely to be misclassified as having cognitive decline. This study is the first of its kind to discover that, despite their strong overall performance, current AI models show bias against multilingual individuals from ethnic minority backgrounds in the UK, and they are also more likely to misclassify speakers with a certain accent (South Yorkshire) as living with a more severe cognitive decline. In this pilot study, we conclude that the existing AI tools are therefore not yet reliable for diagnostic use in these populations, and we aim to address this in future work by developing more generalisable, bias-mitigated models.",
      "url": "https://arxiv.org/abs/2602.13047",
      "pdfUrl": "https://arxiv.org/pdf/2602.13047.pdf",
      "titleJa": "英国の認知障害群から健常な多言語英語話者をAIで検出できるか？実世界の会話音声を用いた調査"
    },
    {
      "id": "2602.13045",
      "arxivId": "2602.13045",
      "title": "Geometric Manifold Rectification for Imbalanced Learning",
      "authors": [
        "Xubin Wang",
        "Qing Li",
        "Weijia Jia"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Imbalanced classification presents a formidable challenge in machine learning, particularly when tabular datasets are plagued by noise and overlapping class boundaries. From a geometric perspective, the core difficulty lies in the topological intrusion of the majority class into the minority manifold, which obscures the true decision boundary. Traditional undersampling techniques, such as Edited Nearest Neighbours (ENN), typically employ symmetric cleaning rules and uniform voting, failing to capture the local manifold structure and often inadvertently removing informative minority samples. In this paper, we propose GMR (Geometric Manifold Rectification), a novel framework designed to robustly handle imbalanced structured data by exploiting local geometric priors. GMR makes two contributions: (1) Geometric confidence estimation that uses inverse-distance weighted kNN voting with an adaptive distance metric to capture local reliability; and (2) asymmetric cleaning that is strict on majority samples while conservatively protecting minority samples via a safe-guarding cap on minority removal. Extensive experiments on multiple benchmark datasets show that GMR is competitive with strong sampling baselines.",
      "url": "https://arxiv.org/abs/2602.13045",
      "pdfUrl": "https://arxiv.org/pdf/2602.13045.pdf",
      "titleJa": "不均衡学習のための幾何学的多様体修正"
    },
    {
      "id": "2602.13035",
      "arxivId": "2602.13035",
      "title": "Look Inward to Explore Outward: Learning Temperature Policy from LLM Internal States via Hierarchical RL",
      "authors": [
        "Yixiao Zhou",
        "Yang Li",
        "Dongzhou Cheng",
        "Hehe Fan",
        "Yu Cheng"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) trains large language models (LLMs) from sampled trajectories, making decoding strategy a core component of learning rather than a purely inference-time choice. Sampling temperature directly controls the exploration--exploitation trade-off by modulating policy entropy, yet existing methods rely on static values or heuristic adaptations that are decoupled from task-level rewards. We propose Introspective LLM, a hierarchical reinforcement learning framework that learns to control sampling temperature during generation. At each decoding step, the model selects a temperature based on its hidden state and samples the next token from the resulting distribution. Temperature and token policies are jointly optimized from downstream rewards using a coordinate ascent scheme. Experiments on mathematical reasoning benchmarks show that learned temperature policies outperform fixed and heuristic baselines, while exhibiting interpretable exploration behaviors aligned with reasoning uncertainty.",
      "url": "https://arxiv.org/abs/2602.13035",
      "pdfUrl": "https://arxiv.org/pdf/2602.13035.pdf",
      "titleJa": "内省から外への探究：階層的強化学習による LLM 内部状態からの温度ポリシーの学習"
    },
    {
      "id": "2602.13033",
      "arxivId": "2602.13033",
      "title": "Buy versus Build an LLM: A Decision Framework for Governments",
      "authors": [
        "Jiahao Lu",
        "Ziwei Xu",
        "William Tjhi",
        "Junnan Li",
        "Antoine Bosselut",
        "Pang Wei Koh",
        "Mohan Kankanhalli"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.SI"
      ],
      "abstract": "Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over whether to buy existing services, build domestic capabilities, or adopt hybrid approaches across different domains and use cases. These are critical decisions especially when leading model providers are often foreign corporations, and LLM outputs are increasingly treated as trusted inputs to public decision-making and public discourse. In practice, these decisions are not intended to mandate a single approach across all domains; instead, national AI strategies are typically pluralistic, with sovereign, commercial and open-source models coexisting to serve different purposes. Governments may rely on commercial models for non-sensitive or commodity tasks, while pursuing greater control for critical, high-risk or strategically important applications. This paper provides a strategic framework for making this decision by evaluating these options across dimensions including sovereignty, safety, cost, resource capability, cultural fit, and sustainability. Importantly, \"building\" does not imply that governments must act alone: domestic capabilities may be developed through public research institutions, universities, state-owned enterprises, joint ventures, or broader national ecosystems. By detailing the technical requirements and practical challenges of each pathway, this work aims to serve as a reference for policy-makers to determine whether a buy or build approach best aligns with their specific national needs and societal goals.",
      "url": "https://arxiv.org/abs/2602.13033",
      "pdfUrl": "https://arxiv.org/pdf/2602.13033.pdf",
      "titleJa": "LLMを購入するか構築するか：政府のための意思決定フレームワーク"
    },
    {
      "id": "2602.13021",
      "arxivId": "2602.13021",
      "title": "Prior-Guided Symbolic Regression: Towards Scientific Consistency in Equation Discovery",
      "authors": [
        "Jing Xiao",
        "Xinhai Chen",
        "Jiaming Peng",
        "Qinglin Wang",
        "Menghan Jia",
        "Zhiquan Lai",
        "Guangping Yu",
        "Dongsheng Li",
        "Tiejun Li",
        "Jie Liu"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Symbolic Regression (SR) aims to discover interpretable equations from observational data, with the potential to reveal underlying principles behind natural phenomena. However, existing approaches often fall into the Pseudo-Equation Trap: producing equations that fit observations well but remain inconsistent with fundamental scientific principles. A key reason is that these approaches are dominated by empirical risk minimization, lacking explicit constraints to ensure scientific consistency. To bridge this gap, we propose PG-SR, a prior-guided SR framework built upon a three-stage pipeline consisting of warm-up, evolution, and refinement. Throughout the pipeline, PG-SR introduces a prior constraint checker that explicitly encodes domain priors as executable constraint programs, and employs a Prior Annealing Constrained Evaluation (PACE) mechanism during the evolution stage to progressively steer discovery toward scientifically consistent regions. Theoretically, we prove that PG-SR reduces the Rademacher complexity of the hypothesis space, yielding tighter generalization bounds and establishing a guarantee against pseudo-equations. Experimentally, PG-SR outperforms state-of-the-art baselines across diverse domains, maintaining robustness to varying prior quality, noisy data, and data scarcity.",
      "url": "https://arxiv.org/abs/2602.13021",
      "pdfUrl": "https://arxiv.org/pdf/2602.13021.pdf",
      "titleJa": "事前誘導記号回帰：方程式発見における科学的一貫性に向けて"
    },
    {
      "id": "2602.13017",
      "arxivId": "2602.13017",
      "title": "Synaptic Activation and Dual Liquid Dynamics for Interpretable Bio-Inspired Models",
      "authors": [
        "Mónika Farsang",
        "Radu Grosu"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "In this paper, we present a unified framework for various bio-inspired models to better understand their structural and functional differences. We show that liquid-capacitance-extended models lead to interpretable behavior even in dense, all-to-all recurrent neural network (RNN) policies. We further demonstrate that incorporating chemical synapses improves interpretability and that combining chemical synapses with synaptic activation yields the most accurate and interpretable RNN models. To assess the accuracy and interpretability of these RNN policies, we consider the challenging lane-keeping control task and evaluate performance across multiple metrics, including turn-weighted validation loss, neural activity during driving, absolute correlation between neural activity and road trajectory, saliency maps of the networks' attention, and the robustness of their saliency maps measured by the structural similarity index.",
      "url": "https://arxiv.org/abs/2602.13017",
      "pdfUrl": "https://arxiv.org/pdf/2602.13017.pdf",
      "titleJa": "解釈可能な生物に着想を得たモデルのためのシナプス活性化と二重液体ダイナミクス"
    },
    {
      "id": "2602.12701",
      "arxivId": "2602.12701",
      "title": "DisSR: Disentangling Speech Representation for Degradation-Prior Guided Cross-Domain Speech Restoration",
      "authors": [
        "Ziqi Liang",
        "Zhijun Jia",
        "Chang Liu",
        "Minghui Yang",
        "Zhihong Lu",
        "Jian Wang"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Previous speech restoration (SR) primarily focuses on single-task speech restoration (SSR), which cannot address general speech restoration problems. Training specific SSR models for different distortions is time-consuming and lacks generality. In addition, most studies ignore the problem of model generalization across unseen domains. To overcome those limitations, we propose DisSR, a Disentangling Speech Representation based general speech restoration model with two properties: 1) Degradation-prior guidance, which extracts speaker-invariant degradation representation to guide the diffusion-based speech restoration model. 2) Domain adaptation, where we design cross-domain alignment training to enhance the model's adaptability and generalization on cross-domain data, respectively. Experimental results demonstrate that our method can produce high-quality restored speech under various distortion conditions. Audio samples can be found at https://itspsp.github.io/DisSR.",
      "url": "https://arxiv.org/abs/2602.12701",
      "pdfUrl": "https://arxiv.org/pdf/2602.12701.pdf",
      "titleJa": "DisSR: 劣化事前ガイド付きクロスドメイン音声復元のための音声表現の分離"
    },
    {
      "id": "2602.11910",
      "arxivId": "2602.11910",
      "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
      "authors": [
        "Łukasz Staniszewski",
        "Katarzyna Zaleska",
        "Mateusz Modrzejewski",
        "Kamil Deja"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood.",
      "url": "https://arxiv.org/abs/2602.11910",
      "pdfUrl": "https://arxiv.org/pdf/2602.11910.pdf",
      "titleJa": "TADA! アクティベーションステアリングによるオーディオ拡散モデルのチューニング"
    },
    {
      "id": "2602.11909",
      "arxivId": "2602.11909",
      "title": "Echo: Towards Advanced Audio Comprehension via Audio-Interleaved Reasoning",
      "authors": [
        "Daiqing Wu",
        "Xuan Zhang",
        "Dongbao Yang",
        "Jiashu Yao",
        "Longfei Chen",
        "Qingsong Liu",
        "Sicheng Zhao",
        "Can Ma",
        "Yangyang Kang",
        "Yu Zhou"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "The maturation of Large Audio Language Models (LALMs) has raised growing expectations for them to comprehend complex audio much like humans. Current efforts primarily replicate text-based reasoning by contextualizing audio content through a one-time encoding, which introduces a critical information bottleneck. Drawing inspiration from human cognition, we propose audio-interleaved reasoning to break through this bottleneck. It treats audio as an active reasoning component, enabling sustained audio engagement and perception-grounded analysis. To instantiate it, we introduce a two-stage training framework, first teaching LALMs to localize salient audio segments through supervised fine-tuning, and then incentivizing proficient re-listening via reinforcement learning. In parallel, a structured data generation pipeline is developed to produce high-quality training data. Consequently, we present Echo, a LALM capable of dynamically re-listening to audio in demand during reasoning. On audio comprehension benchmarks, Echo achieves overall superiority in both challenging expert-level and general-purpose tasks. Comprehensive analysis further confirms the efficiency and generalizability of audio-interleaved reasoning, establishing it as a promising direction for advancing audio comprehension. Project page: https://github.com/wdqqdw/Echo.",
      "url": "https://arxiv.org/abs/2602.11909",
      "pdfUrl": "https://arxiv.org/pdf/2602.11909.pdf",
      "titleJa": "Echo: 音声インターリーブ推論による高度な音声理解に向けて"
    },
    {
      "id": "2602.10735",
      "arxivId": "2602.10735",
      "title": "Calliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity",
      "authors": [
        "Hugo L. Hammer",
        "Vajira Thambawita",
        "Pål Halvorsen"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "A narrated e-book combines synchronized audio with digital text, highlighting the currently spoken word or sentence during playback. This format supports early literacy and assists individuals with reading challenges, while also allowing general readers to seamlessly switch between reading and listening. With the emergence of natural-sounding neural Text-to-Speech (TTS) technology, several commercial services have been developed to leverage these technology for converting standard text e-books into high-quality narrated e-books. However, no open-source solutions currently exist to perform this task. In this paper, we present Calliope, an open-source framework designed to fill this gap. Our method leverages state-of-the-art open-source TTS to convert a text e-book into a narrated e-book in the EPUB 3 Media Overlay format. The method offers several innovative steps: audio timestamps are captured directly during TTS, ensuring exact synchronization between narration and text highlighting; the publisher's original typography, styling, and embedded media are strictly preserved; and the entire pipeline operates offline. This offline capability eliminates recurring API costs, mitigates privacy concerns, and avoids copyright compliance issues associated with cloud-based services. The framework currently supports the state-of-the-art open-source TTS systems XTTS-v2 and Chatterbox. A potential alternative approach involves first generating narration via TTS and subsequently synchronizing it with the text using forced alignment. However, while our method ensures exact synchronization, our experiments show that forced alignment introduces drift between the audio and text highlighting significant enough to degrade the reading experience. Source code and usage instructions are available at https://github.com/hugohammer/TTS-Narrated-Ebook-Creator.git.",
      "url": "https://arxiv.org/abs/2602.10735",
      "pdfUrl": "https://arxiv.org/pdf/2602.10735.pdf",
      "titleJa": "Calliope: 正確な同期、プライバシー、レイアウトの忠実性を保証する TTS ベースのナレーション付き電子書籍作成ツール"
    },
    {
      "id": "2602.12723",
      "arxivId": "2602.12723",
      "title": "Towards explainable reference-free speech intelligibility evaluation of people with pathological speech",
      "authors": [
        "Bence Mark Halpern",
        "Thomas Tienkamp",
        "Defne Abur",
        "Thomas Tienkamp"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Objective assessment of speech that reflects meaningful changes in communication is crucial for clinical decision making and reproducible research. While existing objective assessments, particularly reference-based approaches, can capture intelligibility changes, they are often hindered by lack of explainability and the need for labor-intensive manual transcriptions. To address these issues, this work proposes the reference-free, explainable ASR Inconsistency Score. We evaluate this method on pathological speech in Dutch, Spanish and English, and compare its performance to a reference-based Word Error Rate (WER) baseline. Our results demonstrate that the ASR Inconsistency Score achieves a high correlation with expert perceptual ratings, with performance closely matching, and in one case exceeding, a standard reference-based Word Error Rate (WER) baseline.",
      "url": "https://arxiv.org/abs/2602.12723",
      "pdfUrl": "https://arxiv.org/pdf/2602.12723.pdf",
      "titleJa": "病的な発話を持つ人々の説明可能な参照フリーの発話明瞭度評価に向けて"
    },
    {
      "id": "2602.12241",
      "arxivId": "2602.12241",
      "title": "Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications",
      "authors": [
        "Manjunath Kudlur",
        "Evan King",
        "James Wang",
        "Pete Warden"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent \"encode-the-whole-utterance\" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.",
      "url": "https://arxiv.org/abs/2602.12241",
      "pdfUrl": "https://arxiv.org/pdf/2602.12241.pdf",
      "titleJa": "Moonshine v2: 遅延が重要な音声アプリケーション向けのエルゴディックストリーミングエンコーダASR"
    },
    {
      "id": "2602.12805",
      "arxivId": "2602.12805",
      "title": "A Wavefield Correlation Approach to Improve Sound Speed Estimation in Ultrasound Autofocusing",
      "authors": [
        "Louise Zhuang",
        "Samuel Beuret",
        "Ben Frey",
        "Saachi Munot",
        "Jeremy J. Dahl"
      ],
      "publishedDate": "2026-02-13",
      "categories": [
        "physics.med-ph",
        "cs.SD",
        "eess.IV"
      ],
      "abstract": "Aberration often degrades ultrasound image quality when beamforming does not account for wavefront distortions. In the past decade, local sound speed estimators have been developed for distributed aberration correction throughout a medium. Recently, iterative sound speed optimization approaches have achieved more accurate estimates than earlier approaches, but these newer methods still struggle with decreased accuracy for media with reverberation clutter and large sound speed changes. To address these challenges, we propose using a wavefield correlation (WFC) beamforming approach when performing sound speed optimization. WFC correlates simulated forward-propagated transmit wavefields and backwards-propagated receive wavefields in order to form images. This process more accurately models wave propagation in heterogeneous media and can decrease diffuse clutter due to its spatiotemporal matched filtering effect. This beamformer is implemented using auto-differentiation software to then perform gradient descent optimization, using a total-variation regularized common midpoint phase focus metric loss, on the local sound speed map used during beamforming. This approach is compared to using delay and sum (DAS) with straight-ray time delay calculations in the same sound speed optimization approach on a variety of simulated, phantom, and in vivo data with large sound speed changes and clutter. Results show that using WFC decreases sound speed estimation error, and using the estimates for aberration correction improves image resolution and contrast. These promising results have potential to improve pulse-echo imaging for challenging clinical scenarios.",
      "url": "https://arxiv.org/abs/2602.12805",
      "pdfUrl": "https://arxiv.org/pdf/2602.12805.pdf",
      "titleJa": "超音波オートフォーカスにおける音速推定精度向上のための波動場相関アプローチ"
    },
    {
      "id": "2602.11425",
      "arxivId": "2602.11425",
      "title": "Surface impedance inference via neural fields and sparse acoustic data obtained by a compact array",
      "authors": [
        "Yuanxin Xia",
        "Xinyan Li",
        "Matteo Calafà",
        "Allan P. Engsig-Karup",
        "Cheol-Ho Jeong"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Standardized laboratory characterizations for absorbing materials rely on idealized sound field assumptions, which deviate largely from real-life conditions. Consequently, \\emph{in-situ} acoustic characterization has become essential for accurate diagnosis and virtual prototyping. We propose a physics-informed neural field that reconstructs local, near-surface broadband sound fields from sparse pressure samples to directly infer complex surface impedance. A parallel, multi-frequency architecture enables a broadband impedance retrieval within runtimes on the order of seconds to minutes. To validate the method, we developed a compact microphone array with low hardware complexity. Numerical verifications and laboratory experiments demonstrate accurate impedance retrieval with a small number of sensors under realistic conditions. We further showcase the approach in a vehicle cabin to provide practical guidance on measurement locations that avoid strong interference. Here, we show that this approach offers a robust means of characterizing \\emph{in-situ} boundary conditions for architectural and automotive acoustics.",
      "url": "https://arxiv.org/abs/2602.11425",
      "pdfUrl": "https://arxiv.org/pdf/2602.11425.pdf",
      "titleJa": "コンパクトなアレイによって得られた神経場とスパース音響データによる表面インピーダンスの推定"
    },
    {
      "id": "2602.10058",
      "arxivId": "2602.10058",
      "title": "Evaluating Disentangled Representations for Controllable Music Generation",
      "authors": [
        "Laura Ibáñez-Martínez",
        "Chukwuemeka Nkama",
        "Andrea Poltronieri",
        "Xavier Serra",
        "Martín Rocamora"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Recent approaches in music generation rely on disentangled representations, often labeled as structure and timbre or local and global, to enable controllable synthesis. Yet the underlying properties of these embeddings remain underexplored. In this work, we evaluate such disentangled representations in a set of music audio models for controllable generation using a probing-based framework that goes beyond standard downstream tasks. The selected models reflect diverse unsupervised disentanglement strategies, including inductive biases, data augmentations, adversarial objectives, and staged training procedures. We further isolate specific strategies to analyze their effect. Our analysis spans four key axes: informativeness, equivariance, invariance, and disentanglement, which are assessed across datasets, tasks, and controlled transformations. Our findings reveal inconsistencies between intended and actual semantics of the embeddings, suggesting that current strategies fall short of producing truly disentangled representations, and prompting a re-examination of how controllability is approached in music generation.",
      "url": "https://arxiv.org/abs/2602.10058",
      "pdfUrl": "https://arxiv.org/pdf/2602.10058.pdf",
      "titleJa": "制御可能な音楽生成のための分離表現の評価"
    },
    {
      "id": "2602.09891",
      "arxivId": "2602.09891",
      "title": "Stemphonic: All-at-once Flexible Multi-stem Music Generation",
      "authors": [
        "Shih-Lun Wu",
        "Ge Zhu",
        "Juan-Pablo Caceres",
        "Cheng-Zhi Anna Huang",
        "Nicholas J. Bryan"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM"
      ],
      "abstract": "Music stem generation, the task of producing musically-synchronized and isolated instrument audio clips, offers the potential of greater user control and better alignment with musician workflows compared to conventional text-to-music models. Existing stem generation approaches, however, either rely on fixed architectures that output a predefined set of stems in parallel, or generate only one stem at a time, resulting in slow inference despite flexibility in stem combination. We propose Stemphonic, a diffusion-/flow-based framework that overcomes this trade-off and generates a variable set of synchronized stems in one inference pass. During training, we treat each stem as a batch element, group synchronized stems in a batch, and apply a shared noise latent to each group. At inference-time, we use a shared initial noise latent and stem-specific text inputs to generate synchronized multi-stem outputs in one pass. We further expand our approach to enable one-pass conditional multi-stem generation and stem-wise activity controls to empower users to iteratively generate and orchestrate the temporal layering of a mix. We benchmark our results on multiple open-source stem evaluation sets and show that Stemphonic produces higher-quality outputs while accelerating the full mix generation process by 25 to 50%. Demos at: https://stemphonic-demo.vercel.app.",
      "url": "https://arxiv.org/abs/2602.09891",
      "pdfUrl": "https://arxiv.org/pdf/2602.09891.pdf",
      "titleJa": "Stemphonic: 一度に柔軟なマルチステム音楽生成"
    },
    {
      "id": "2602.08794",
      "arxivId": "2602.08794",
      "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
      "authors": [
        "SII-OpenMOSS Team",
        " :",
        "Donghua Yu",
        "Mingshu Chen",
        "Qi Chen",
        "Qi Luo",
        "Qianyi Wu",
        "Qinyuan Cheng",
        "Ruixiao Li",
        "Tianyi Liang",
        "Wenbo Zhang",
        "Wenming Tu",
        "Xiangyu Peng",
        "Yang Gao",
        "Yanru Huo",
        "Ying Zhu",
        "Yinze Luo",
        "Yiyang Zhang",
        "Yuerong Song",
        "Zhe Xu",
        "Zhiyu Zhang",
        "Chenchen Yang",
        "Cheng Chang",
        "Chushu Zhou",
        "Hanfu Chen",
        "Hongnan Ma",
        "Jiaxi Li",
        "Jingqi Tong",
        "Junxi Liu",
        "Ke Chen",
        "Shimin Li",
        "Shiqi Jiang",
        "Songlin Wang",
        "Wei Jiang",
        "Zhaoye Fei",
        "Zhiyuan Ning",
        "Chunguo Li",
        "Chenhui Li",
        "Ziwei He",
        "Zengfeng Huang",
        "Xie Chen",
        "Xipeng Qiu"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.CV",
        "cs.SD"
      ],
      "abstract": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
      "url": "https://arxiv.org/abs/2602.08794",
      "pdfUrl": "https://arxiv.org/pdf/2602.08794.pdf",
      "titleJa": "MOVA: スケーラブルで同期したビデオ・オーディオ生成に向けて"
    },
    {
      "id": "2602.08671",
      "arxivId": "2602.08671",
      "title": "Input-Adaptive Spectral Feature Compression by Sequence Modeling for Source Separation",
      "authors": [
        "Kohei Saijo",
        "Yoshiaki Bando"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Time-frequency domain dual-path models have demonstrated strong performance and are widely used in source separation. Because their computational cost grows with the number of frequency bins, these models often use the band-split (BS) module in high-sampling-rate tasks such as music source separation (MSS) and cinematic audio source separation (CASS). The BS encoder compresses frequency information by encoding features for each predefined subband. It achieves effective compression by introducing an inductive bias that places greater emphasis on low-frequency parts. Despite its success, the BS module has two inherent limitations: (i) it is not input-adaptive, preventing the use of input-dependent information, and (ii) the parameter count is large, since each subband requires a dedicated module. To address these issues, we propose Spectral Feature Compression (SFC). SFC compresses the input using a single sequence modeling module, making it both input-adaptive and parameter-efficient. We investigate two variants of SFC, one based on cross-attention and the other on Mamba, and introduce inductive biases inspired by the BS module to make them suitable for frequency information compression. Experiments on MSS and CASS tasks demonstrate that the SFC module consistently outperforms the BS module across different separator sizes and compression ratios. We also provide an analysis showing that SFC adaptively captures frequency patterns from the input.",
      "url": "https://arxiv.org/abs/2602.08671",
      "pdfUrl": "https://arxiv.org/pdf/2602.08671.pdf",
      "titleJa": "音源分離のためのシーケンスモデリングによる入力適応型スペクトル特徴圧縮"
    },
    {
      "id": "2602.09070",
      "arxivId": "2602.09070",
      "title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
      "authors": [
        "Yufan Wen",
        "Zhaocheng Liu",
        "YeGuo Hua",
        "Ziyi Guo",
        "Lihua Zhang",
        "Chun Yuan",
        "Jian Wu"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a \\textit{Global Semantic Anchor} ensures stylistic stability, while a surgical \\textit{Token-Level Affective Adapter} modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation.",
      "url": "https://arxiv.org/abs/2602.09070",
      "pdfUrl": "https://arxiv.org/pdf/2602.09070.pdf",
      "titleJa": "NarraScore: 階層的感情制御による視覚的物語と音楽的ダイナミクスの橋渡し"
    },
    {
      "id": "2602.08233",
      "arxivId": "2602.08233",
      "title": "Tutti: Expressive Multi-Singer Synthesis via Structure-Level Timbre Control and Vocal Texture Modeling",
      "authors": [
        "Jiatao Chen",
        "Xing Tang",
        "Xiaoyue Duan",
        "Yutang Feng",
        "Jinchao Zhang",
        "Jie Zhou"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "While existing Singing Voice Synthesis systems achieve high-fidelity solo performances, they are constrained by global timbre control, failing to address dynamic multi-singer arrangement and vocal texture within a single song. To address this, we propose Tutti, a unified framework designed for structured multi-singer generation. Specifically, we introduce a Structure-Aware Singer Prompt to enable flexible singer scheduling evolving with musical structure, and propose Complementary Texture Learning via Condition-Guided VAE to capture implicit acoustic textures (e.g., spatial reverberation and spectral fusion) that are complementary to explicit controls. Experiments demonstrate that Tutti excels in precise multi-singer scheduling and significantly enhances the acoustic realism of choral generation, offering a novel paradigm for complex multi-singer arrangement. Audio samples are available at https://annoauth123-ctrl.github.io/Tutii_Demo/.",
      "url": "https://arxiv.org/abs/2602.08233",
      "pdfUrl": "https://arxiv.org/pdf/2602.08233.pdf",
      "titleJa": "Tutti: 構造レベルの音色制御とボーカルテクスチャモデリングによる表現力豊かなマルチシンガー合成"
    },
    {
      "id": "2602.08148",
      "arxivId": "2602.08148",
      "title": "SNC: A Stem-Native Codec for Efficient Lossless Audio Storage with Adaptive Playback Capabilities",
      "authors": [
        "Shaad Sufi"
      ],
      "publishedDate": "2026-02-08",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Current audio formats present a fundamental trade-off between file size and functionality: lossless formats like FLAC preserve quality but lack adaptability, while lossy formats reduce size at the cost of fidelity and offer no stem-level access.We introduce the Stem-Native Codec (SNC), a novel audio container format that stores music as independently encoded stems plus a low-energy mastering residual. By exploiting the lower information entropy of separated stems compared to mixed audio, SNC achieves a 38.2% file size reduction versus FLAC (7.76 MB vs. 12.55 MB for a 2:18 test track) while maintaining perceptual transparency (STOI = 0.996). Unlike existing formats, SNC enables context-aware adaptive playback, spatial audio rendering, and user-controlled remixing without requiring additional storage. Our experimental validation demonstrates that the stems-plus residual architecture successfully decouples the conflicting requirements of compression efficiency and feature richness, offering a practical path toward next-generation audio distribution systems.",
      "url": "https://arxiv.org/abs/2602.08148",
      "pdfUrl": "https://arxiv.org/pdf/2602.08148.pdf",
      "titleJa": "SNC: 適応型再生機能を備えた効率的なロスレスオーディオストレージのためのステムネイティブコーデック"
    },
    {
      "id": "2602.07803",
      "arxivId": "2602.07803",
      "title": "SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis",
      "authors": [
        "Jiale Qian",
        "Hao Meng",
        "Tian Zheng",
        "Pengcheng Zhu",
        "Haopeng Lin",
        "Yuhang Dai",
        "Hanke Xie",
        "Wenxiao Cao",
        "Ruixuan Shang",
        "Jun Wu",
        "Hongmei Liu",
        "Hanlin Wen",
        "Jian Zhao",
        "Zhonglin Jiang",
        "Yong Chen",
        "Shunshun Yin",
        "Ming Tao",
        "Jianguo Wei",
        "Lei Xie",
        "Xinsheng Wang"
      ],
      "publishedDate": "2026-02-08",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.",
      "url": "https://arxiv.org/abs/2602.07803",
      "pdfUrl": "https://arxiv.org/pdf/2602.07803.pdf",
      "titleJa": "SoulX-Singer: 高品質なゼロショット歌声合成に向けて"
    },
    {
      "id": "2602.06917",
      "arxivId": "2602.06917",
      "title": "Automatic Detection and Analysis of Singing Mistakes for Music Pedagogy",
      "authors": [
        "Sumit Kumar",
        "Suraj Jaiswal",
        "Parampreet Singh",
        "Vipul Arora"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "The advancement of machine learning in audio analysis has opened new possibilities for technology-enhanced music education. This paper introduces a framework for automatic singing mistake detection in the context of music pedagogy, supported by a newly curated dataset. The dataset comprises synchronized teacher learner vocal recordings, with annotations marking different types of mistakes made by learners. Using this dataset, we develop different deep learning models for mistake detection and benchmark them. To compare the efficacy of mistake detection systems, a new evaluation methodology is proposed. Experiments indicate that the proposed learning-based methods are superior to rule-based methods. A systematic study of errors and a cross-teacher study reveal insights into music pedagogy that can be utilised for various music applications. This work sets out new directions of research in music pedagogy. The codes and dataset are publicly available.",
      "url": "https://arxiv.org/abs/2602.06917",
      "pdfUrl": "https://arxiv.org/pdf/2602.06917.pdf",
      "titleJa": "音楽教育のための歌唱ミスの自動検出と分析"
    },
    {
      "id": "2602.06823",
      "arxivId": "2602.06823",
      "title": "AI-Generated Music Detection in Broadcast Monitoring",
      "authors": [
        "David Lopez-Ayala",
        "Asier Cabello",
        "Pablo Zinemanas",
        "Emilio Molina",
        "Martin Rocamora"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "AI music generators have advanced to the point where their outputs are often indistinguishable from human compositions. While detection methods have emerged, they are typically designed and validated in music streaming contexts with clean, full-length tracks. Broadcast audio, however, poses a different challenge: music appears as short excerpts, often masked by dominant speech, conditions under which existing detectors fail. In this work, we introduce AI-OpenBMAT, the first dataset tailored to broadcast-style AI-music detection. It contains 3,294 one-minute audio excerpts (54.9 hours) that follow the duration patterns and loudness relations of real television audio, combining human-made production music with stylistically matched continuations generated with Suno v3.5. We benchmark a CNN baseline and state-of-the-art SpectTTTra models to assess SNR and duration robustness, and evaluate on a full broadcast scenario. Across all settings, models that excel in streaming scenarios suffer substantial degradation, with F1-scores dropping below 60% when music is in the background or has a short duration. These results highlight speech masking and short music length as critical open challenges for AI music detection, and position AI-OpenBMAT as a benchmark for developing detectors capable of meeting industrial broadcast requirements.",
      "url": "https://arxiv.org/abs/2602.06823",
      "pdfUrl": "https://arxiv.org/pdf/2602.06823.pdf",
      "titleJa": "放送監視におけるAI生成音楽検出"
    },
    {
      "id": "2602.07063",
      "arxivId": "2602.07063",
      "title": "Video-based Music Generation",
      "authors": [
        "Serkan Sulun"
      ],
      "publishedDate": "2026-02-05",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called \"boundary offset encodings,\" aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.",
      "url": "https://arxiv.org/abs/2602.07063",
      "pdfUrl": "https://arxiv.org/pdf/2602.07063.pdf",
      "titleJa": "ビデオベースの音楽生成"
    },
    {
      "id": "2602.05220",
      "arxivId": "2602.05220",
      "title": "Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions",
      "authors": [
        "Jinchuan Tian",
        "Haoran Wang",
        "Bo-Hao Su",
        "Chien-yu Huang",
        "Qingzheng Wang",
        "Jiatong Shi",
        "William Chen",
        "Xun Gong",
        "Siddhant Arora",
        "Chin-Jou Li",
        "Masao Someki",
        "Takashi Maekaku",
        "Yusuke Shinohara",
        "Jin Sakuma",
        "Chao-Han Huck Yang",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-02-05",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.",
      "url": "https://arxiv.org/abs/2602.05220",
      "pdfUrl": "https://arxiv.org/pdf/2602.05220.pdf",
      "titleJa": "Bagpiper: 豊富なキャプションでオープンエンドの音声タスクを解決する"
    },
    {
      "id": "2602.04683",
      "arxivId": "2602.04683",
      "title": "UniAudio 2.0: A Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization",
      "authors": [
        "Dongchao Yang",
        "Yuanyuan Wang",
        "Dading Chong",
        "Songxiang Liu",
        "Xixin Wu",
        "Helen Meng"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.SD"
      ],
      "abstract": "We study two foundational problems in audio language models: (1) how to design an audio tokenizer that can serve as an intermediate representation for both understanding and generation; and (2) how to build an audio foundation model that generalizes in few-shot and zero-shot settings, analogous to large language models. To this end, we make the following two contributions. First, we propose ReasoningCodec, a discrete audio codec that factorizes audio into (i) reasoning tokens, which encode text-aligned, high-level analysis and planning representations for audio understanding and hierarchical generation, and (ii) reconstruction tokens, which encode semantic-rich acoustic cues for high-fidelity waveform reconstruction. This design achieves understanding performance comparable to strong continuous representations while improving generation quality and reconstruction fidelity over prior discrete tokenizers. Second, we introduce a unified autoregressive architecture for text and audio, together with multi-stage training and multi-task data construction. Using this framework, we train UniAudio 2.0 on 100B text tokens and 60B audio tokens. Across a wide range of speech, sound, and music tasks, UniAudio 2.0 performs competitively on in-domain evaluations and demonstrates strong few-shot and zero-shot generalization to unseen tasks. Demo, code, and checkpoints will be available at \\href{https://dongchaoyang.top/UniAudio2Demo/}{https://dongchaoyang.top/UniAudio2Demo/}.",
      "url": "https://arxiv.org/abs/2602.04683",
      "pdfUrl": "https://arxiv.org/pdf/2602.04683.pdf",
      "titleJa": "UniAudio 2.0: テキスト整合されたファクタライズされたオーディオトークン化を備えた統合オーディオ言語モデル"
    },
    {
      "id": "2602.09042",
      "arxivId": "2602.09042",
      "title": "The SJTU X-LANCE Lab System for MSR Challenge 2025",
      "authors": [
        "Jinxuan Zhu",
        "Hao Qiu",
        "Haina Zhu",
        "Jianwei Yu",
        "Kai Yu",
        "Xie Chen"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This report describes the system submitted to the music source restoration (MSR) Challenge 2025. Our approach is composed of sequential BS-RoFormers, each dealing with a single task including music source separation (MSS), denoise and dereverb. To support 8 instruments given in the task, we utilize pretrained checkpoints from MSS community and finetune the MSS model with several training schemes, including (1) mixing and cleaning of datasets; (2) random mixture of music pieces for data augmentation; (3) scale-up of audio length. Our system achieved the first rank in all three subjective and three objective evaluation metrics, including an MMSNR score of 4.4623 and an FAD score of 0.1988. We have open-sourced all the code and checkpoints at https://github.com/ModistAndrew/xlance-msr.",
      "url": "https://arxiv.org/abs/2602.09042",
      "pdfUrl": "https://arxiv.org/pdf/2602.09042.pdf",
      "titleJa": "MSRチャレンジ2025向けSJTU X-LANCEラボシステム"
    },
    {
      "id": "2602.04085",
      "arxivId": "2602.04085",
      "title": "BASS: Benchmarking Audio LMs for Musical Structure and Semantic Reasoning",
      "authors": [
        "Min Jang",
        "Orevaoghene Ahia",
        "Nazif Tamer",
        "Sachin Kumar",
        "Yulia Tsvetkov",
        "Noah A. Smith"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Music understanding is a complex task that often requires reasoning over both structural and semantic elements of audio. We introduce BASS, designed to evaluate music understanding and reasoning in audio language models across four broad categories: structural segmentation, lyric transcription, musicological analysis, and artist collaboration. BASS comprises 2658 questions spanning 12 tasks, 1993 unique songs and covering over 138 hours of music from a wide range of genres and tracks, crafted to assess musicological knowledge and reasoning in real-world scenarios. We evaluate 14 open-source and frontier multimodal LMs, finding that even state-of-the-art models struggle on higher-level reasoning tasks such as structural segmentation and artist collaboration, while performing best on lyric transcription. Our analysis reveals that current models leverage linguistic priors effectively but remain limited in reasoning over musical structure, vocal, and musicological attributes. BASS provides an evaluation framework with widespread applications in music recommendation and search and has the potential to guide the development of audio LMs.",
      "url": "https://arxiv.org/abs/2602.04085",
      "pdfUrl": "https://arxiv.org/pdf/2602.04085.pdf",
      "titleJa": "BASS: 音楽構造と意味論的推論のためのオーディオ LM のベンチマーク"
    },
    {
      "id": "2602.09970",
      "arxivId": "2602.09970",
      "title": "BioME: A Resource-Efficient Bioacoustic Foundational Model for IoT Applications",
      "authors": [
        "Heitor R. Guimarães",
        "Abhishek Tiwari",
        "Mahsa Abdollahi",
        "Anderson R. Avila",
        "Tiago H. Falk"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Passive acoustic monitoring has become a key strategy in biodiversity assessment, conservation, and behavioral ecology, especially as Internet-of-Things (IoT) devices enable continuous in situ audio collection at scale. While recent self-supervised learning (SSL)-based audio encoders, such as BEATs and AVES, have shown strong performance in bioacoustic tasks, their computational cost and limited robustness to unseen environments hinder deployment on resource-constrained platforms. In this work, we introduce BioME, a resource-efficient audio encoder designed for bioacoustic applications. BioME is trained via layer-to-layer distillation from a high-capacity teacher model, enabling strong representational transfer while reducing the parameter count by 75%. To further improve ecological generalization, the model is pretrained on multi-domain data spanning speech, environmental sounds, and animal vocalizations. A key contribution is the integration of modulation-aware acoustic features via FiLM conditioning, injecting a DSP-inspired inductive bias that enhances feature disentanglement in low-capacity regimes. Across multiple bioacoustic tasks, BioME matches or surpasses the performance of larger models, including its teacher, while being suitable for resource-constrained IoT deployments. For reproducibility, code and pretrained checkpoints are publicly available.",
      "url": "https://arxiv.org/abs/2602.09970",
      "pdfUrl": "https://arxiv.org/pdf/2602.09970.pdf",
      "titleJa": "BioME: IoTアプリケーションのためのリソース効率の高いバイオ音響基礎モデル"
    },
    {
      "id": "2602.09594",
      "arxivId": "2602.09594",
      "title": "Evaluation of acoustic Green's function in rectangular rooms with general surface impedance walls",
      "authors": [
        "Matteo Calafà",
        "Yuanxin Xia",
        "Jonas Brunskog",
        "Cheol-Ho Jeong"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "eess.AS",
        "cs.CE",
        "cs.SD"
      ],
      "abstract": "Acoustic room modes and the Green's function mode expansion are well-known for rectangular rooms with perfectly reflecting walls. First-order approximations also exist for nearly rigid boundaries; however, current analytical methods fail to accommodate more general boundary conditions, e.g., when wall absorption is significant. In this work, we present a comprehensive analysis that extends previous studies by including additional first-order asymptotics that account for soft-wall boundaries. In addition, we introduce a semi-analytical, efficient, and reliable method for computing the Green's function in rectangular rooms, which is described and validated through numerical tests. With a sufficiently large truncation order, the resulting error becomes negligible, making the method suitable as a benchmark for numerical simulations. Additional aspects regarding the spectral basis orthogonality and completeness are also addressed, providing a general framework for the validity of the proposed approach.",
      "url": "https://arxiv.org/abs/2602.09594",
      "pdfUrl": "https://arxiv.org/pdf/2602.09594.pdf",
      "titleJa": "一般的な表面インピーダンス壁を備えた長方形の部屋における音響グリーン関数の評価"
    },
    {
      "id": "2602.09321",
      "arxivId": "2602.09321",
      "title": "Performance Comparison of CNN and AST Models with Stacked Features for Environmental Sound Classification",
      "authors": [
        "Parinaz Binandeh Dehaghania",
        "Danilo Penab",
        "A. Pedro Aguiar"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Environmental sound classification (ESC) has gained significant attention due to its diverse applications in smart city monitoring, fault detection, acoustic surveillance, and manufacturing quality control. To enhance CNN performance, feature stacking techniques have been explored to aggregate complementary acoustic descriptors into richer input representations. In this paper, we investigate CNN-based models employing various stacked feature combinations, including Log-Mel Spectrogram (LM), Spectral Contrast (SPC), Chroma (CH), Tonnetz (TZ), Mel-Frequency Cepstral Coefficients (MFCCs), and Gammatone Cepstral Coefficients (GTCC). Experiments are conducted on the widely used ESC-50 and UrbanSound8K datasets under different training regimes, including pretraining on ESC-50, fine-tuning on UrbanSound8K, and comparison with Audio Spectrogram Transformer (AST) models pretrained on large-scale corpora such as AudioSet. This experimental design enables an analysis of how feature-stacked CNNs compare with transformer-based models under varying levels of training data and pretraining diversity. The results indicate that feature-stacked CNNs offer a more computationally and data-efficient alternative when large-scale pretraining or extensive training data are unavailable, making them particularly well suited for resource-constrained and edge-level sound classification scenarios.",
      "url": "https://arxiv.org/abs/2602.09321",
      "pdfUrl": "https://arxiv.org/pdf/2602.09321.pdf",
      "titleJa": "環境音分類におけるスタック特徴量を用いたCNNとASTモデルの性能比較"
    },
    {
      "id": "2602.09295",
      "arxivId": "2602.09295",
      "title": "Positive-Unlabelled Active Learning to Curate a Dataset for Orca Resident Interpretation",
      "authors": [
        "Bret Nestor",
        "Bohan Yao",
        "Jasmine Moore",
        "Jasper Kanes"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "This work presents the largest curation of Southern Resident Killer Whale (SRKW) acoustic data to date, also containing other marine mammals in their environment. We systematically search all available public archival hydrophone data within the SRKW habitat (over 30 years of audio data). The search consists of a weakly-supervised, positive-unlabelled, active learning strategy to identify all instances of marine mammals. The resulting transformer-based detectors outperform state-of-the-art detectors on the DEEPAL, DCLDE-2026, and two newly introduced expert-annotated datasets in terms of accuracy, energy efficiency, and speed. The detection model has a specificity of 0-28.8% at 95% sensitivity. Our multiclass species classifier obtains a top-1 accuracy of 42.1% (11 train classes, 4 test classes) and our ecotype classifier obtains a top-1 accuracy of 43.0% (4 train classes, 5 test classes) on the DCLDE-2026 dataset. We yield 919 hours of SRKW data, 230 hours of Bigg's orca data, 1374 hours of orca data from unlabelled ecotypes, 1501 hours of humpback data, 88 hours of sea lion data, 246 hours of pacific white-sided dolphin data, and over 784 hours of unspecified marine mammal data. This SRKW dataset is larger than DCLDE-2026, Ocean Networks Canada, and OrcaSound combined. The curated species labels are available under CC-BY 4.0 license, and the corresponding audio data are available under the licenses of the original owners. The comprehensive nature of this dataset makes it suitable for unsupervised machine translation, habitat usage surveys, and conservation endeavours for this critically endangered ecotype.",
      "url": "https://arxiv.org/abs/2602.09295",
      "pdfUrl": "https://arxiv.org/pdf/2602.09295.pdf",
      "titleJa": "シャチの生息環境解釈のためのデータセットをキュレーションするためのポジティブラベルなしアクティブラーニング"
    },
    {
      "id": "2602.09233",
      "arxivId": "2602.09233",
      "title": "Gencho: Room Impulse Response Generation from Reverberant Speech and Text via Diffusion Transformers",
      "authors": [
        "Jackie Lin",
        "Jiaqi Su",
        "Nishit Anand",
        "Zeyu Jin",
        "Minje Kim",
        "Paris Smaragdis"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Blind room impulse response (RIR) estimation is a core task for capturing and transferring acoustic properties; yet existing methods often suffer from limited modeling capability and degraded performance under unseen conditions. Moreover, emerging generative audio applications call for more flexible impulse response generation methods. We propose Gencho, a diffusion-transformer-based model that predicts complex spectrogram RIRs from reverberant speech. A structure-aware encoder leverages isolation between early and late reflections to encode the input audio into a robust representation for conditioning, while the diffusion decoder generates diverse and perceptually realistic impulse responses from it. Gencho integrates modularly with standard speech processing pipelines for acoustic matching. Results show richer generated RIRs than non-generative baselines while maintaining strong performance in standard RIR metrics. We further demonstrate its application to text-conditioned RIR generation, highlighting Gencho's versatility for controllable acoustic simulation and generative audio tasks.",
      "url": "https://arxiv.org/abs/2602.09233",
      "pdfUrl": "https://arxiv.org/pdf/2602.09233.pdf",
      "titleJa": "源長：拡散トランスフォーマーによる残響音声とテキストからの室内インパルス応答生成"
    },
    {
      "id": "2602.09210",
      "arxivId": "2602.09210",
      "title": "AI-Driven Cardiorespiratory Signal Processing: Separation, Clustering, and Anomaly Detection",
      "authors": [
        "Yasaman Torabi"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This research applies artificial intelligence (AI) to separate, cluster, and analyze cardiorespiratory sounds. We recorded a new dataset (HLS-CMDS) and developed several AI models, including generative AI methods based on large language models (LLMs) for guided separation, explainable AI (XAI) techniques to interpret latent representations, variational autoencoders (VAEs) for waveform separation, a chemistry-inspired non-negative matrix factorization (NMF) algorithm for clustering, and a quantum convolutional neural network (QCNN) designed to detect abnormal physiological patterns. The performance of these AI models depends on the quality of the recorded signals. Therefore, this thesis also reviews the biosensing technologies used to capture biomedical data. It summarizes developments in microelectromechanical systems (MEMS) acoustic sensors and quantum biosensors, such as quantum dots and nitrogen-vacancy centers. It further outlines the transition from electronic integrated circuits (EICs) to photonic integrated circuits (PICs) and early progress toward integrated quantum photonics (IQP) for chip-based biosensing. Together, these studies show how AI and next-generation sensors can support more intelligent diagnostic systems for future healthcare.",
      "url": "https://arxiv.org/abs/2602.09210",
      "pdfUrl": "https://arxiv.org/pdf/2602.09210.pdf",
      "titleJa": "AI駆動型心肺信号処理：分離、クラスタリング、異常検出"
    },
    {
      "id": "2602.08979",
      "arxivId": "2602.08979",
      "title": "Beyond Transcripts: A Renewed Perspective on Audio Chaptering",
      "authors": [
        "Fabian Retkowski",
        "Maike Züfle",
        "Thai Binh Nguyen",
        "Jan Niehues",
        "Alexander Waibel"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Audio chaptering, the task of automatically segmenting long-form audio into coherent sections, is increasingly important for navigating podcasts, lectures, and videos. Despite its relevance, research remains limited and text-based, leaving key questions unresolved about leveraging audio information, handling ASR errors, and transcript-free evaluation. We address these gaps through three contributions: (1) a systematic comparison between text-based models with acoustic features, a novel audio-only architecture (AudioSeg) operating on learned audio representations, and multimodal LLMs; (2) empirical analysis of factors affecting performance, including transcript quality, acoustic features, duration, and speaker composition; and (3) formalized evaluation protocols contrasting transcript-dependent text-space protocols with transcript-invariant time-space protocols. Our experiments on YTSeg reveal that AudioSeg substantially outperforms text-based approaches, pauses provide the largest acoustic gains, and MLLMs remain limited by context length and weak instruction following, yet MLLMs are promising on shorter audio.",
      "url": "https://arxiv.org/abs/2602.08979",
      "pdfUrl": "https://arxiv.org/pdf/2602.08979.pdf",
      "titleJa": "トランスクリプトを超えて：オーディオチャプターの新たな視点"
    },
    {
      "id": "2602.08293",
      "arxivId": "2602.08293",
      "title": "Cross-Modal Bottleneck Fusion For Noise Robust Audio-Visual Speech Recognition",
      "authors": [
        "Seaone Ok",
        "Min Jun Choi",
        "Eungbeom Kim",
        "Seungu Han",
        "Kyogu Lee"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Audio-Visual Speech Recognition (AVSR) leverages both acoustic and visual cues to improve speech recognition under noisy conditions. A central question is how to design a fusion mechanism that allows the model to effectively exploit visual information when the audio signal is degraded, while maintaining strong performance on clean speech. We propose CoBRA (Cross-modal Bottleneck for Robust AVSR), a bottleneck-based fusion framework that introduces a compact set of learnable tokens to mediate cross-modal exchange. By regulating information flow through these tokens, the audio stream can reliably access essential visual cues even under adverse or out-of-domain noise. Despite limited training data, our model surpasses comparable baselines and remains competitive with large-scale systems through noise-adaptive fusion, demonstrating both efficiency and robustness. Ablation studies highlight that the depth of fusion is the most critical factor, underscoring its importance in designing robust AVSR systems.",
      "url": "https://arxiv.org/abs/2602.08293",
      "pdfUrl": "https://arxiv.org/pdf/2602.08293.pdf",
      "titleJa": "ノイズ耐性のあるオーディオビジュアル音声認識のためのクロスモーダルボトルネック融合"
    },
    {
      "id": "2602.06937",
      "arxivId": "2602.06937",
      "title": "Reciprocal Latent Fields for Precomputed Sound Propagation",
      "authors": [
        "Hugo Seuté",
        "Pranai Vasudev",
        "Etienne Richan",
        "Louis-Xavier Buffoni"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Realistic sound propagation is essential for immersion in a virtual scene, yet physically accurate wave-based simulations remain computationally prohibitive for real-time applications. Wave coding methods address this limitation by precomputing and compressing impulse responses of a given scene into a set of scalar acoustic parameters, which can reach unmanageable sizes in large environments with many source-receiver pairs. We introduce Reciprocal Latent Fields (RLF), a memory-efficient framework for encoding and predicting these acoustic parameters. The RLF framework employs a volumetric grid of trainable latent embeddings decoded with a symmetric function, ensuring acoustic reciprocity. We study a variety of decoders and show that leveraging Riemannian metric learning leads to a better reproduction of acoustic phenomena in complex scenes. Experimental validation demonstrates that RLF maintains replication quality while reducing the memory footprint by several orders of magnitude. Furthermore, a MUSHRA-like subjective listening test indicates that sound rendered via RLF is perceptually indistinguishable from ground-truth simulations.",
      "url": "https://arxiv.org/abs/2602.06937",
      "pdfUrl": "https://arxiv.org/pdf/2602.06937.pdf",
      "titleJa": "事前計算による音の伝播のための逆潜在場"
    },
    {
      "id": "2602.06921",
      "arxivId": "2602.06921",
      "title": "The Combination of Several Decorrelation Methods to Improve Acoustic Feedback Cancellation",
      "authors": [
        "Klaus Linhard",
        "Philipp Bulling"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This paper extends an acoustic feedback cancellation system by incorporating multiple decorrelation methods. The baseline system is based on a frequency-domain Kalman filter implemented in a multi-delay structure. The proposed extensions include a variable time delay line, prediction, distortion compensation, and a simplified reverberation model. Each extension is analyzed, and a practical parameter range is defined. While existing literature often focuses on a single extension, such as prediction, to describe an optimal system, this work demonstrates that each individual extension contributes to performance improvements. Furthermore, the combination of all proposed extensions results in a superior system. The evaluation is conducted using publicly available datasets, with performance assessed through system distance metrics and the objective speech quality measure PSEQ.",
      "url": "https://arxiv.org/abs/2602.06921",
      "pdfUrl": "https://arxiv.org/pdf/2602.06921.pdf",
      "titleJa": "音響フィードバックキャンセルを改善するための複数の相関除去法の組み合わせ"
    },
    {
      "id": "2602.06846",
      "arxivId": "2602.06846",
      "title": "DynFOA: Generating First-Order Ambisonics with Conditional Diffusion for Dynamic and Acoustically Complex 360-Degree Videos",
      "authors": [
        "Ziyu Luo",
        "Lin Chen",
        "Qiang Qu",
        "Xiaoming Chen",
        "Yiran Shen"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Spatial audio is crucial for creating compelling immersive 360-degree video experiences. However, generating realistic spatial audio, such as first-order ambisonics (FOA), from 360-degree videos in complex acoustic scenes remains challenging. Existing methods often overlook the dynamic nature and acoustic complexity of 360-degree scenes, fail to fully account for dynamic sound sources, and neglect complex environmental effects such as occlusion, reflections, and reverberation, which are influenced by scene geometries and materials. We propose DynFOA, a framework based on dynamic acoustic perception and conditional diffusion, for generating high-fidelity FOA from 360-degree videos. DynFOA first performs visual processing via a video encoder, which detects and localizes multiple dynamic sound sources, estimates their depth and semantics, and reconstructs the scene geometry and materials using a 3D Gaussian Splatting. This reconstruction technique accurately models occlusion, reflections, and reverberation based on the geometries and materials of the reconstructed 3D scene and the listener's viewpoint. The audio encoder then captures the spatial motion and temporal 4D sound source trajectories to fine-tune the diffusion-based FOA generator. The fine-tuned FOA generator adjusts spatial cues in real time, ensuring consistent directional fidelity during listener head rotation and complex environmental changes. Extensive evaluations demonstrate that DynFOA consistently outperforms existing methods across metrics such as spatial accuracy, acoustic fidelity, and distribution matching, while also improving the user experience. Therefore, DynFOA provides a robust and scalable approach to rendering realistic dynamic spatial audio for VR and immersive media applications.",
      "url": "https://arxiv.org/abs/2602.06846",
      "pdfUrl": "https://arxiv.org/pdf/2602.06846.pdf",
      "titleJa": "DynFOA: 条件付き拡散を用いた一次アンビソニックスの生成による、動的かつ音響的に複雑な360度動画の制作"
    },
    {
      "id": "2602.06647",
      "arxivId": "2602.06647",
      "title": "Reading Between the Waves: Robust Topic Segmentation Using Inter-Sentence Audio Features",
      "authors": [
        "Steffen Freisinger",
        "Philipp Seeberger",
        "Tobias Bocklet",
        "Korbinian Riedhammer"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Spoken content, such as online videos and podcasts, often spans multiple topics, which makes automatic topic segmentation essential for user navigation and downstream applications. However, current methods do not fully leverage acoustic features, leaving room for improvement. We propose a multi-modal approach that fine-tunes both a text encoder and a Siamese audio encoder, capturing acoustic cues around sentence boundaries. Experiments on a large-scale dataset of YouTube videos show substantial gains over text-only and multi-modal baselines. Our model also proves more resilient to ASR noise and outperforms a larger text-only baseline on three additional datasets in Portuguese, German, and English, underscoring the value of learned acoustic features for robust topic segmentation.",
      "url": "https://arxiv.org/abs/2602.06647",
      "pdfUrl": "https://arxiv.org/pdf/2602.06647.pdf",
      "titleJa": "波間を読む：文間音声特徴を用いた堅牢なトピックセグメンテーション"
    },
    {
      "id": "2602.06602",
      "arxivId": "2602.06602",
      "title": "Scaling Speech Tokenizers with Diffusion Autoencoders",
      "authors": [
        "Yuancheng Wang",
        "Zhenyu Tang",
        "Yun Wang",
        "Arthur Hinsvark",
        "Yingru Liu",
        "Yinghao Li",
        "Kainan Peng",
        "Junyi Ao",
        "Mingbo Ma",
        "Mike Seltzer",
        "Qing He",
        "Xubo Liu"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Speech tokenizers are foundational to speech language models, yet existing approaches face two major challenges: (1) balancing trade-offs between encoding semantics for understanding and acoustics for reconstruction, and (2) achieving low bit rates and low token rates. We propose Speech Diffusion Tokenizer (SiTok), a diffusion autoencoder that jointly learns semantic-rich representations through supervised learning and enables high-fidelity audio reconstruction with diffusion. We scale SiTok to 1.6B parameters and train it on 2 million hours of speech. Experiments show that SiTok outperforms strong baselines on understanding, reconstruction and generation tasks, at an extremely low token rate of $12.5$ Hz and a bit-rate of 200 bits-per-second.",
      "url": "https://arxiv.org/abs/2602.06602",
      "pdfUrl": "https://arxiv.org/pdf/2602.06602.pdf",
      "titleJa": "拡散オートエンコーダを用いた音声トークナイザーのスケーリング"
    },
    {
      "id": "2602.06180",
      "arxivId": "2602.06180",
      "title": "STACodec: Semantic Token Assignment for Balancing Acoustic Fidelity and Semantic Information in Audio Codecs",
      "authors": [
        "Kaiyuan Zhang",
        "Mohan Shi",
        "Eray Eren",
        "Natarajan Balaji Shankar",
        "Zilai Wang",
        "Abeer Alwan"
      ],
      "publishedDate": "2026-02-05",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "abstract": "Neural audio codecs are widely used for audio compression and can be integrated into token-based language models. Traditional codecs preserve acoustic details well but lack semantic information. Recent hybrid codecs attempt to incorporate semantic information through distillation, but this often degrades reconstruction performance, making it difficult to achieve both. To address this limitation, we introduce STACodec, a unified codec that integrates semantic information from self-supervised learning (SSL) models into the first layer of residual vector quantization (RVQ-1) via semantic token assignment (STA). To further eliminate reliance on SSL-based semantic tokenizers and improve efficiency during inference, we propose a semantic pre-distillation (SPD) module, which predicts semantic tokens directly for assignment to the first RVQ layer during inference. Experimental results show that STACodec outperforms existing hybrid codecs in both audio reconstruction and downstream semantic tasks, demonstrating a better balance between acoustic fidelity and semantic capability.",
      "url": "https://arxiv.org/abs/2602.06180",
      "pdfUrl": "https://arxiv.org/pdf/2602.06180.pdf",
      "titleJa": "STACodec: オーディオコーデックにおける音響忠実度と意味情報のバランスをとるための意味トークン割り当て"
    },
    {
      "id": "2602.05770",
      "arxivId": "2602.05770",
      "title": "Zero-Shot TTS With Enhanced Audio Prompts: Bsc Submission For The 2026 Wildspoof Challenge TTS Track",
      "authors": [
        "Jose Giraldo",
        "Alex Peiró-Lilja",
        "Rodolfo Zevallos",
        "Cristina España-Bonet"
      ],
      "publishedDate": "2026-02-05",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We evaluate two non-autoregressive architectures, StyleTTS2 and F5-TTS, to address the spontaneous nature of in-the-wild speech. Our models utilize flexible duration modeling to improve prosodic naturalness. To handle acoustic noise, we implement a multi-stage enhancement pipeline using the Sidon model, which significantly outperforms standard Demucs in signal quality. Experimental results show that finetuning enhanced audios yields superior robustness, achieving up to 4.21 UTMOS and 3.47 DNSMOS. Furthermore, we analyze the impact of reference prompt quality and length on zero-shot synthesis performance, demonstrating the effectiveness of our approach for realistic speech generation.",
      "url": "https://arxiv.org/abs/2602.05770",
      "pdfUrl": "https://arxiv.org/pdf/2602.05770.pdf",
      "titleJa": "強化された音声プロンプトを備えたゼロショットTTS：2026年ワイルドスポフチャレンジTTSトラックへのBSC提出"
    }
  ],
  "lastUpdated": "2026-02-17T01:07:29.109893",
  "totalCount": 78
}