{
  "papers": [
    {
      "id": "2601.16158",
      "arxivId": "2601.16158",
      "title": "Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems",
      "authors": [
        "Prakash Dhungana",
        "Sayed Ahmad Salehi"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions. To address this, we propose a comprehensive framework for continual learning designed to adapt to new domains while maintaining computational efficiency. The proposed pipeline integrates a dual-input Convolutional Neural Network, utilizing both Mel Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features, supported by a multi-stage denoising process, involving discrete wavelet transform and spectral subtraction techniques, plus model and prototype update blocks. Unlike prior methods that restrict updates to specific layers, our approach updates the complete quantized model, made possible due to compact model architecture. A subset of input samples are selected during runtime using class prototypes and confidence-driven filtering, which are then pseudo-labeled and combined with rehearsal buffer for incremental model retraining. Experimental results on noisy test dataset demonstrate the framework's effectiveness, achieving 99.63\\% accuracy on clean data and maintaining robust performance (exceeding 94\\% accuracy) across diverse noisy environments, even at -10 dB Signal-to-Noise Ratio. The proposed framework work confirms that integrating efficient denoising with prototype-based continual learning enables KWS models to operate autonomously and robustly in resource-constrained, dynamic environments.",
      "url": "https://arxiv.org/abs/2601.16158",
      "pdfUrl": "https://arxiv.org/pdf/2601.16158.pdf",
      "titleJa": "リソース制約システムにおける堅牢かつ効率的なキーワードスポッティングのためのドメイン増分継続学習"
    },
    {
      "id": "2601.16150",
      "arxivId": "2601.16150",
      "title": "Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization",
      "authors": [
        "Maximos Kaliakatsos-Papakostas",
        "Dimos Makris",
        "Konstantinos Soiledis",
        "Konstantinos-Theodoros Tsamis",
        "Vassilis Katsouros",
        "Emilios Cambouropoulos"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody-harmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.",
      "url": "https://arxiv.org/abs/2601.16150",
      "pdfUrl": "https://arxiv.org/pdf/2601.16150.pdf",
      "titleJa": "メロディーに注意を払う（交差させる）：単一エンコーダーによるメロディーハーモニーのためのカリキュラムマスキング"
    },
    {
      "id": "2601.16117",
      "arxivId": "2601.16117",
      "title": "Distillation-based Layer Dropping (DLD) Effective End-to-end Framework for Dynamic Speech Networks",
      "authors": [
        "Abdul Hannan",
        "Daniele Falavigna",
        "Shah Nawaz",
        "Mubashir Noman",
        "Markus Schedl",
        "Alessio Brutti"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.CV"
      ],
      "abstract": "Edge devices operate in constrained and varying resource settings, requiring dynamic architectures that can adapt to limitations of the available resources. To meet such demands, layer dropping ($\\mathcal{LD}$) approach is typically used to transform static models into dynamic ones by skipping parts of the network along with reducing overall computational complexity. However, existing $\\mathcal{LD}$ methods greatly impact the dynamic model's performance for low and high dropping cases, deteriorating the performance-computation trade-off. To this end, we propose a distillation-based layer dropping (DLD) framework that effectively combines the capabilities of knowledge distillation and $\\mathcal{LD}$ in an end-to-end fashion, thereby achieving state-of-the-art performance for dynamic speech networks. Comprehensive experimentation utilizing well-known speech recognition methods, including conformer and WavLM, on three public benchmarks demonstrates the effectiveness of our framework, reducing the word error rate by $9.32\\%$ and $2.25\\%$ for high and no dropping cases with $33.3\\%$ reduction in training time.",
      "url": "https://arxiv.org/abs/2601.16117",
      "pdfUrl": "https://arxiv.org/pdf/2601.16117.pdf",
      "titleJa": "蒸留ベースのレイヤードロッピング（DLD）動的音声ネットワークのための効果的なエンドツーエンドフレームワーク"
    },
    {
      "id": "2601.15889",
      "arxivId": "2601.15889",
      "title": "A Stabilized Hybrid Active Noise Control Algorithm of GFANC and FxNLMS with Online Clustering",
      "authors": [
        "Zhengding Luo",
        "Haozhe Ma",
        "Boxiang Wang",
        "Ziyi Yang",
        "Dongyuan Shi",
        "Woon-Seng Gan"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "The Filtered-x Normalized Least Mean Square (FxNLMS) algorithm suffers from slow convergence and a risk of divergence, although it can achieve low steady-state errors after sufficient adaptation. In contrast, the Generative Fixed-Filter Active Noise Control (GFANC) method offers fast response speed, but its lack of adaptability may lead to large steady-state errors. This paper proposes a hybrid GFANC-FxNLMS algorithm to leverage the complementary advantages of both approaches. In the hybrid GFANC-FxNLMS algorithm, GFANC provides a frame-level control filter as an initialization for FxNLMS, while FxNLMS performs continuous adaptation at the sampling rate. Small variations in the GFANC-generated filter may repeatedly reinitialize FxNLMS, interrupting its adaptation process and destabilizing the system. An online clustering module is introduced to avoid unnecessary re-initializations and improve system stability. Simulation results show that the proposed algorithm achieves fast response, very low steady-state error, and high stability, requiring only one pre-trained broadband filter.",
      "url": "https://arxiv.org/abs/2601.15889",
      "pdfUrl": "https://arxiv.org/pdf/2601.15889.pdf",
      "titleJa": "オンラインクラスタリングによるGFANCとFxNLMSの安定化ハイブリッドアクティブノイズ制御アルゴリズム"
    },
    {
      "id": "2601.15872",
      "arxivId": "2601.15872",
      "title": "PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation",
      "authors": [
        "Jaekwon Im",
        "Natalia Polouliakh",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.",
      "url": "https://arxiv.org/abs/2601.15872",
      "pdfUrl": "https://arxiv.org/pdf/2601.15872.pdf",
      "titleJa": "PF-D2M: ユニバーサルなダンス・トゥ・ミュージック生成のためのポーズフリー拡散モデル"
    },
    {
      "id": "2601.15719",
      "arxivId": "2601.15719",
      "title": "U3-xi: Pushing the Boundaries of Speaker Recognition via Incorporating Uncertainty",
      "authors": [
        "Junjie Li",
        "Kong Aik Lee"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD"
      ],
      "abstract": "An utterance-level speaker embedding is typically obtained by aggregating a sequence of frame-level representations. However, in real-world scenarios, individual frames encode not only speaker-relevant information but also various nuisance factors. As a result, different frames contribute unequally to the final utterance-level speaker representation for Automatic Speaker Verification systems. To address this issue, we propose to estimate the inherent uncertainty of each frame and assign adaptive weights accordingly, where frames with higher uncertainty receive lower attention. Based on this idea, we present U3-xi, a comprehensive framework designed to produce more reliable and interpretable uncertainty estimates for speaker embeddings. Specifically, we introduce several strategies for uncertainty supervision. First, we propose speaker-level uncertainty supervision via a Stochastic Variance Loss, where the distance between an utterance embedding and its corresponding speaker centroid serves as a pseudo ground truth for uncertainty learning. Second, we incorporate global-level uncertainty supervision by injecting the predicted uncertainty into the sof tmax scale during training. This adaptive scaling mechanism adjusts the sharpness of the decision boundary according to sample difficulty, providing global guidance. Third, we redesign the uncertainty estimation module by integrating a Transformer encoder with multi-view self-attention, enabling the model to capture rich local and long-range temporal dependencies. Comprehensive experiments demonstrate that U3-xi is model-agnostic and can be seamlessly applied to various speaker encoders. In particular, when applied to ECAPA-TDNN, it achieves 21.1% and 15.57% relative improvements on the VoxCeleb1 test sets in terms of EER and minDCF, respectively.",
      "url": "https://arxiv.org/abs/2601.15719",
      "pdfUrl": "https://arxiv.org/pdf/2601.15719.pdf",
      "titleJa": "U3-xi: 不確実性を組み込むことで話者認識の限界を押し広げる"
    },
    {
      "id": "2601.15676",
      "arxivId": "2601.15676",
      "title": "Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for Edge Audio Systems",
      "authors": [
        "Hengfan Zhang",
        "Yueqian Lin",
        "Hai Helen Li",
        "Yiran Chen"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Deploying Audio-Language Models (Audio-LLMs) on edge infrastructure exposes a persistent tension between perception depth and computational efficiency. Lightweight local models tend to produce passive perception - generic summaries that miss the subtle evidence required for multi-step audio reasoning - while indiscriminate cloud offloading incurs unacceptable latency, bandwidth cost, and privacy risk. We propose CoFi-Agent (Tool-Augmented Coarse-to-Fine Agent), a hybrid architecture targeting edge servers and gateways. It performs fast local perception and triggers conditional forensic refinement only when uncertainty is detected. CoFi-Agent runs an initial single-pass on a local 7B Audio-LLM, then a cloud controller gates difficult cases and issues lightweight plans for on-device tools such as temporal re-listening and local ASR. On the MMAR benchmark, CoFi-Agent improves accuracy from 27.20% to 53.60%, while achieving a better accuracy-efficiency trade-off than an always-on investigation pipeline. Overall, CoFi-Agent bridges the perception gap via tool-enabled, conditional edge-cloud collaboration under practical system constraints.",
      "url": "https://arxiv.org/abs/2601.15676",
      "pdfUrl": "https://arxiv.org/pdf/2601.15676.pdf",
      "titleJa": "認識ギャップを埋める: エッジオーディオシステム向けの軽量な粗密アーキテクチャ"
    },
    {
      "id": "2601.15668",
      "arxivId": "2601.15668",
      "title": "EmotionThinker: Prosody-Aware Reinforcement Learning for Explainable Speech Emotion Reasoning",
      "authors": [
        "Dingdong Wang",
        "Shujie Liu",
        "Tianhua Zhang",
        "Youjun Chen",
        "Jinyu Li",
        "Helen Meng"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Emotional information in speech plays a unique role in multimodal perception. However, current Speech Large Language Models (SpeechLLMs), similar to conventional speech emotion recognition (SER) systems, still treat emotion understanding as a simple classification problem. This provides limited interpretability of predictions, while leaving the LLMs' expressive and reasoning capabilities underutilized. In this work, we take the first step to reformulate SER as a deep reasoning problem through reinforcement learning (RL). We propose EmotionThinker, which is designed to generate accurate emotion predictions with interpretable explanations grounded in fine-grained acoustic cues. To achieve this, we first construct EmotionCoT-35K, an emotional reasoning dataset with Chain-of-Thought annotations and detailed captions. Second, we observe that current SpeechLLMs exhibit weak prosody perception, whereas prosodic cues constitute fundamental signals for interpreting emotions. To address this, we develop the prosody-enhanced foundation model EmotionThinker-Base, and demonstrate that prosody enhancement improves emotion understanding. Third, we introduce Group-Relative-Policy-Optimization with Progressive-Trust-aware-Reasoning-Reward (GRPO-PTR) for RL. Different from standard GRPO, which relies only on rule-based outcome rewards, GRPO-PTR progressively introduces reasoning reward, dynamically adjusts it with a trustworthiness weight reflecting the alignment between reasoning and outcome, and evaluates the overall reasoning quality with a reward model based on multi-dimensional criteria. EmotionThinker outperforms previous state-of-the-art evaluation models both in emotion accuracy and explanation quality, advancing SER toward interpretable multimodal reasoning. Project page: https://github.com/dingdongwang/EmotionThinker",
      "url": "https://arxiv.org/abs/2601.15668",
      "pdfUrl": "https://arxiv.org/pdf/2601.15668.pdf",
      "titleJa": "EmotionThinker: 説明可能な音声感情推論のための韻律を考慮した強化学習"
    },
    {
      "id": "2601.15621",
      "arxivId": "2601.15621",
      "title": "Qwen3-TTS Technical Report",
      "authors": [
        "Hangrui Hu",
        "Xinfa Zhu",
        "Ting He",
        "Dake Guo",
        "Bin Zhang",
        "Xiong Wang",
        "Zhifang Guo",
        "Ziyue Jiang",
        "Hongkun Hao",
        "Zishan Guo",
        "Xinyu Zhang",
        "Pei Zhang",
        "Baosong Yang",
        "Jin Xu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission ($97\\,\\mathrm{ms}$) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.",
      "url": "https://arxiv.org/abs/2601.15621",
      "pdfUrl": "https://arxiv.org/pdf/2601.15621.pdf",
      "titleJa": "Qwen3-TTS 技術レポート"
    },
    {
      "id": "2601.15596",
      "arxivId": "2601.15596",
      "title": "DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice",
      "authors": [
        "Leying Zhang",
        "Tingxiao Zhou",
        "Haiyang Sun",
        "Mengxiao Bi",
        "Yanmin Qian"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "While modern Text-to-Speech (TTS) systems achieve high fidelity for read-style speech, they struggle to generate Autonomous Sensory Meridian Response (ASMR), a specialized, low-intensity speech style essential for relaxation. The inherent challenges include ASMR's subtle, often unvoiced characteristics and the demand for zero-shot speaker adaptation. In this paper, we introduce DeepASMR, the first framework designed for zero-shot ASMR generation. We demonstrate that a single short snippet of a speaker's ordinary, read-style speech is sufficient to synthesize high-fidelity ASMR in their voice, eliminating the need for whispered training data from the target speaker. Methodologically, we first identify that discrete speech tokens provide a soft factorization of ASMR style from speaker timbre. Leveraging this insight, we propose a two-stage pipeline incorporating a Large Language Model (LLM) for content-style encoding and a flow-matching acoustic decoder for timbre reconstruction. Furthermore, we contribute DeepASMR-DB, a comprehensive 670-hour English-Chinese multi-speaker ASMR speech corpus, and introduce a novel evaluation protocol integrating objective metrics, human listening tests, LLM-based scoring and unvoiced speech analysis. Extensive experiments confirm that DeepASMR achieves state-of-the-art naturalness and style fidelity in ASMR generation for anyone of any voice, while maintaining competitive performance on normal speech synthesis.",
      "url": "https://arxiv.org/abs/2601.15596",
      "pdfUrl": "https://arxiv.org/pdf/2601.15596.pdf",
      "titleJa": "DeepASMR: LLMベースのゼロショットASMR音声生成（あらゆる声質の人向け）"
    },
    {
      "id": "2601.15397",
      "arxivId": "2601.15397",
      "title": "Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)",
      "authors": [
        "Peidong Wang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the \"lost-in-the-middle\" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from \"over-correction\", introducing hallucinations of entities that were never spoken. In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.",
      "url": "https://arxiv.org/abs/2601.15397",
      "pdfUrl": "https://arxiv.org/pdf/2601.15397.pdf",
      "titleJa": "プロンプティングを超えて：ロジット空間統合（LOGIC）による音声LLMのための効率的かつ堅牢な文脈バイアス"
    },
    {
      "id": "2601.15240",
      "arxivId": "2601.15240",
      "title": "WeDefense: A Toolkit to Defend Against Fake Audio",
      "authors": [
        "Lin Zhang",
        "Johan Rohdin",
        "Xin Wang",
        "Junyi Peng",
        "Tianchi Liu",
        "You Zhang",
        "Hieu-Thi Luong",
        "Shuai Wang",
        "Chengdong Liang",
        "Anna Silnova",
        "Nicholas Evans"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The advances in generative AI have enabled the creation of synthetic audio which is perceptually indistinguishable from real, genuine audio. Although this stellar progress enables many positive applications, it also raises risks of misuse, such as for impersonation, disinformation and fraud. Despite a growing number of open-source fake audio detection codes released through numerous challenges and initiatives, most are tailored to specific competitions, datasets or models. A standardized and unified toolkit that supports the fair benchmarking and comparison of competing solutions with not just common databases, protocols, metrics, but also a shared codebase, is missing. To address this, we propose WeDefense, the first open-source toolkit to support both fake audio detection and localization. Beyond model training, WeDefense emphasizes critical yet often overlooked components: flexible input and augmentation, calibration, score fusion, standardized evaluation metrics, and analysis tools for deeper understanding and interpretation. The toolkit is publicly available at https://github.com/zlin0/wedefense with interactive demos for fake audio detection and localization.",
      "url": "https://arxiv.org/abs/2601.15240",
      "pdfUrl": "https://arxiv.org/pdf/2601.15240.pdf",
      "titleJa": "WeDefense: 偽の音声から身を守るツールキット"
    },
    {
      "id": "2601.15118",
      "arxivId": "2601.15118",
      "title": "WavLink: Compact Audio-Text Embeddings with a Global Whisper Token",
      "authors": [
        "Gokul Karthik Kumar",
        "Ludovick Lepauloux",
        "Hakim Hacid"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "Whisper has become the de-facto encoder for extracting general-purpose audio features in large audio-language models, where a 30-second clip is typically represented by 1500 frame features projected into an LLM. In contrast, audio-text embedding models like CLAP-based models have largely relied on alternative audio encoders (e.g., HTS-AT, PaSST), and have not leveraged Whisper effectively. We present WavLink, a compact audio-text embedding model that augments Whisper encoder with a learnable global token, trained jointly with a text encoder. Through a systematic study of design choices, including pretrained text encoders, loss functions, training modes, and data mixtures, we identify configurations that yield state-of-the-art retrieval performance. Our two-stage training recipe across three model sizes, combined with Matryoshka-style supervision, improves scalability, enabling 8x smaller embeddings with minimal performance drop. WavLink also demonstrates competitive performance on AIR-Bench with MCQs and zero-shot classification.",
      "url": "https://arxiv.org/abs/2601.15118",
      "pdfUrl": "https://arxiv.org/pdf/2601.15118.pdf",
      "titleJa": "WavLink: グローバルウィスパートークンによるコンパクトな音声テキスト埋め込み"
    },
    {
      "id": "2601.15097",
      "arxivId": "2601.15097",
      "title": "Neural Tracking of Sustained Attention, Attention Switching, and Natural Conversation in Audiovisual Environments using Mobile EEG",
      "authors": [
        "Johanna Wilroth",
        "Oskar Keding",
        "Martin A. Skoglund",
        "Maria Sandsten",
        "Martin Enqvist",
        "Emina Alickovic"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Everyday communication is dynamic and multisensory, often involving shifting attention, overlapping speech and visual cues. Yet, most neural attention tracking studies are still limited to highly controlled lab settings, using clean, often audio-only stimuli and requiring sustained attention to a single talker. This work addresses that gap by introducing a novel dataset from 24 normal-hearing participants. We used a mobile electroencephalography (EEG) system (44 scalp electrodes and 20 cEEGrid electrodes) in an audiovisual (AV) paradigm with three conditions: sustained attention to a single talker in a two-talker environment, attention switching between two talkers, and unscripted two-talker conversations with a competing single talker. Analysis included temporal response functions (TRFs) modeling, optimal lag analysis, selective attention classification with decision windows ranging from 1.1s to 35s, and comparisons of TRFs for attention to AV conversations versus side audio-only talkers. Key findings show significant differences in the attention-related P2-peak between attended and ignored speech across conditions for scalp EEG. No significant change in performance between switching and sustained attention suggests robustness for attention switches. Optimal lag analysis revealed narrower peak for conversation compared to single-talker AV stimuli, reflecting the additional complexity of multi-talker processing. Classification of selective attention was consistently above chance (55-70% accuracy) for scalp EEG, while cEEGrid data yielded lower correlations, highlighting the need for further methodological improvements. These results demonstrate that mobile EEG can reliably track selective attention in dynamic, multisensory listening scenarios and provide guidance for designing future AV paradigms and real-world attention tracking applications.",
      "url": "https://arxiv.org/abs/2601.15097",
      "pdfUrl": "https://arxiv.org/pdf/2601.15097.pdf",
      "titleJa": "モバイルEEGを用いた視聴覚環境における持続的注意、注意の切り替え、自然な会話の神経追跡"
    },
    {
      "id": "2601.15083",
      "arxivId": "2601.15083",
      "title": "Bangla Music Genre Classification Using Bidirectional LSTMS",
      "authors": [
        "Muntakimur Rahaman",
        "Md Mahmudul Hoque",
        "Md Mehedi Hassain"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Bangla music is enrich in its own music cultures. Now a days music genre classification is very significant because of the exponential increase in available music, both in digital and physical formats. It is necessary to index them accordingly to facilitate improved retrieval. Automatically classifying Bangla music by genre is essential for efficiently locating specific pieces within a vast and diverse music library. Prevailing methods for genre classification predominantly employ conventional machine learning or deep learning approaches. This work introduces a novel music dataset comprising ten distinct genres of Bangla music. For the task of audio classification, we utilize a recurrent neural network (RNN) architecture. Specifically, a Long Short-Term Memory (LSTM) network is implemented to train the model and perform the classification. Feature extraction represents a foundational stage in audio data processing. This study utilizes Mel-Frequency Cepstral Coefficients (MFCCs) to transform raw audio waveforms into a compact and representative set of features. The proposed framework facilitates music genre classification by leveraging these extracted features. Experimental results demonstrate a classification accuracy of 78%, indicating the system's strong potential to enhance and streamline the organization of Bangla music genres.",
      "url": "https://arxiv.org/abs/2601.15083",
      "pdfUrl": "https://arxiv.org/pdf/2601.15083.pdf",
      "titleJa": "双方向LSTMSを用いたバングラ音楽のジャンル分類"
    },
    {
      "id": "2601.14960",
      "arxivId": "2601.14960",
      "title": "VCNAC: A Variable-Channel Neural Audio Codec for Mono, Stereo, and Surround Sound",
      "authors": [
        "Florian Grötschla",
        "Arunasish Sen",
        "Alessandro Lombardi",
        "Guillermo Cámbara",
        "Andreas Schwarz"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We present VCNAC, a variable channel neural audio codec. Our approach features a single encoder and decoder parametrization that enables native inference for different channel setups, from mono speech to cinematic 5.1 channel surround audio. Channel compatibility objectives ensure that multi-channel content maintains perceptual quality when decoded to fewer channels. The shared representation enables training of generative language models on a single set of codebooks while supporting inference-time scalability across modalities and channel configurations. Evaluation using objective spatial audio metrics and subjective listening tests demonstrates that our unified approach maintains high reconstruction quality across mono, stereo, and surround audio configurations.",
      "url": "https://arxiv.org/abs/2601.14960",
      "pdfUrl": "https://arxiv.org/pdf/2601.14960.pdf",
      "titleJa": "VCNAC: モノラル、ステレオ、サラウンドサウンド用の可変チャネルニューラルオーディオコーデック"
    },
    {
      "id": "2601.14931",
      "arxivId": "2601.14931",
      "title": "Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali",
      "authors": [
        "Nouhoum Coulibaly",
        "Ousmane Ly",
        "Michael Leventhal",
        "Ousmane Goro"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "This study explores the capacity of generative artificial intelligence (Gen AI) to contribute to the construction of peace narratives and the revitalization of musical heritage in Mali. The study has been made in a political and social context where inter-community tensions and social fractures motivate a search for new symbolic frameworks for reconciliation. The study empirically explores three questions: (1) how Gen AI can be used as a tool for musical creation rooted in national languages and traditions; (2) to what extent Gen AI systems enable a balanced hybridization between technological innovation and cultural authenticity; and (3) how AI-assisted musical co-creation can strengthen social cohesion and cultural sovereignty. The experimental results suggest that Gen AI, embedded in a culturally conscious participatory framework, can act as a catalyst for symbolic diplomacy, amplifying local voices instead of standardizing them. However, challenges persist regarding the availability of linguistic corpora, algorithmic censorship, and the ethics of generating compositions derived from copyrighted sources.",
      "url": "https://arxiv.org/abs/2601.14931",
      "pdfUrl": "https://arxiv.org/pdf/2601.14931.pdf",
      "titleJa": "生成型人工知能、音楽遺産、そして平和物語の構築：マリにおける事例研究"
    },
    {
      "id": "2601.14850",
      "arxivId": "2601.14850",
      "title": "Multi-Task Transformer for Explainable Speech Deepfake Detection via Formant Modeling",
      "authors": [
        "Viola Negroni",
        "Luca Cuccovillo",
        "Paolo Bestagini",
        "Patrick Aichroth",
        "Stefano Tubaro"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In this work, we introduce a multi-task transformer for speech deepfake detection, capable of predicting formant trajectories and voicing patterns over time, ultimately classifying speech as real or fake, and highlighting whether its decisions rely more on voiced or unvoiced regions. Building on a prior speaker-formant transformer architecture, we streamline the model with an improved input segmentation strategy, redesign the decoding process, and integrate built-in explainability. Compared to the baseline, our model requires fewer parameters, trains faster, and provides better interpretability, without sacrificing prediction performance.",
      "url": "https://arxiv.org/abs/2601.14850",
      "pdfUrl": "https://arxiv.org/pdf/2601.14850.pdf",
      "titleJa": "フォルマントモデリングによる説明可能な音声ディープフェイク検出のためのマルチタスクトランスフォーマー"
    },
    {
      "id": "2601.14786",
      "arxivId": "2601.14786",
      "title": "Training-Efficient Text-to-Music Generation with State-Space Modeling",
      "authors": [
        "Wei-Jaw Lee",
        "Fang-Chih Hsieh",
        "Xuanjun Chen",
        "Fang-Duo Tsai",
        "Yi-Hsuan Yang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in text-to-music generation (TTM) have yielded high-quality results, but often at the cost of extensive compute and the use of large proprietary internal data. To improve the affordability and openness of TTM training, an open-source generative model backbone that is more training- and data-efficient is needed. In this paper, we constrain the number of trainable parameters in the generative model to match that of the MusicGen-small benchmark (with about 300M parameters), and replace its Transformer backbone with the emerging class of state-space models (SSMs). Specifically, we explore different SSM variants for sequence modeling, and compare a single-stage SSM-based design with a decomposable two-stage SSM/diffusion hybrid design. All proposed models are trained from scratch on a purely public dataset comprising 457 hours of CC-licensed music, ensuring full openness. Our experimental findings are three-fold. First, we show that SSMs exhibit superior training efficiency compared to the Transformer counterpart. Second, despite using only 9% of the FLOPs and 2% of the training data size compared to the MusicGen-small benchmark, our model achieves competitive performance in both objective metrics and subjective listening tests based on MusicCaps captions. Finally, our scaling-down experiment demonstrates that SSMs can maintain competitive performance relative to the Transformer baseline even at the same training budget (measured in iterations), when the model size is reduced to four times smaller. To facilitate the democratization of TTM research, the processed captions, model checkpoints, and source code are available on GitHub via the project page: https://lonian6.github.io/ssmttm/.",
      "url": "https://arxiv.org/abs/2601.14786",
      "pdfUrl": "https://arxiv.org/pdf/2601.14786.pdf",
      "titleJa": "状態空間モデリングによる効率的なテキストから音楽への生成"
    },
    {
      "id": "2601.14744",
      "arxivId": "2601.14744",
      "title": "Unlocking Large Audio-Language Models for Interactive Language Learning",
      "authors": [
        "Hongfu Liu",
        "Zhouying Cui",
        "Xiangming Gu",
        "Ye Wang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Achieving pronunciation proficiency in a second language (L2) remains a challenge, despite the development of Computer-Assisted Pronunciation Training (CAPT) systems. Traditional CAPT systems often provide unintuitive feedback that lacks actionable guidance, limiting its effectiveness. Recent advancements in audio-language models (ALMs) offer the potential to enhance these systems by providing more user-friendly feedback. In this work, we investigate ALMs for chat-based pronunciation training by introducing L2-Arctic-plus, an English dataset with detailed error explanations and actionable suggestions for improvement. We benchmark cascaded ASR+LLMs and existing ALMs on this dataset, specifically in detecting mispronunciation and generating actionable feedback. To improve the performance, we further propose to instruction-tune ALMs on L2-Arctic-plus. Experimental results demonstrate that our instruction-tuned models significantly outperform existing baselines on mispronunciation detection and suggestion generation in terms of both objective and human evaluation, highlighting the value of the proposed dataset.",
      "url": "https://arxiv.org/abs/2601.14744",
      "pdfUrl": "https://arxiv.org/pdf/2601.14744.pdf",
      "titleJa": "インタラクティブな言語学習のための大規模音声言語モデルの解放"
    },
    {
      "id": "2601.16077",
      "arxivId": "2601.16077",
      "title": "Loose coupling of spectral and spatial models for multi-channel diarization and enhancement of meetings in dynamic environments",
      "authors": [
        "Adrian Meise",
        "Tobias Cord-Landwehr",
        "Christoph Boeddeker",
        "Marc Delcroix",
        "Tomohiro Nakatani",
        "Reinhold Haeb-Umbach"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Sound capture by microphone arrays opens the possibility to exploit spatial, in addition to spectral, information for diarization and signal enhancement, two important tasks in meeting transcription. However, there is no one-to-one mapping of positions in space to speakers if speakers move. Here, we address this by proposing a novel joint spatial and spectral mixture model, whose two submodels are loosely coupled by modeling the relationship between speaker and position index probabilistically. Thus, spatial and spectral information can be jointly exploited, while at the same time allowing for speakers speaking from different positions. Experiments on the LibriCSS data set with simulated speaker position changes show great improvements over tightly coupled subsystems.",
      "url": "https://arxiv.org/abs/2601.16077",
      "pdfUrl": "https://arxiv.org/pdf/2601.16077.pdf",
      "titleJa": "動的環境におけるマルチチャネルダイアリゼーションと会議の強化のためのスペクトルモデルと空間モデルの疎結合"
    },
    {
      "id": "2601.16023",
      "arxivId": "2601.16023",
      "title": "Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs",
      "authors": [
        "Lalaram Arya",
        "Mrinmoy Bhattacharjee",
        "Adarsh C. R.",
        "S. R. Mahadeva Prasanna"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "cs.HC"
      ],
      "abstract": "Direct Speech-to-Speech Translation (S2ST) has gained increasing attention for its ability to translate speech from one language to another, while reducing error propagation and latency inherent in traditional cascaded pipelines. However, existing direct S2ST systems continue to face notable challenges, including instability in semantic-acoustic alignment when parallel speech data is scarce, difficulty in preserving speaker identity, and limited multilingual scalability. In this work, we introduce DS2ST-LM, a scalable, single-stage direct S2ST framework leveraging a multilingual Large Language Model (LLM). The architecture integrates a Whisper speech encoder, a learnable projection module, a Qwen2-0.5B LLM, and a timbre-controlled vocoder. We construct GigaS2S-1000, a 1000-hour bilingual corpus by extending the GigaST dataset with high-fidelity synthetic target speech, and show that this synthetic data alleviates data scarcity to some extent. We investigate two semantic token generation strategies: speech-derived S3 tokens and text-derived tokens generated by a pre-trained LLM, and analyze their impact on training stability and semantic consistency. We further evaluate three projection architectures (Linear, Conv1D-Linear, and Q-Former) and observe that while higher-capacity projectors converge faster, the simple Linear projector achieves higher performance. Extensive experiments demonstrate that DS2ST-LM outperforms traditional cascaded and ST (Qwen-Audio) + TTS baselines across both lexical (BLEU, METEOR) and semantic (BLEURT, COMET) metrics, while extending to multiple language pairs, including French, Spanish, German, Hindi, Bengali, and Urdu. Furthermore, we incorporate timbre-aware speech synthesis to preserve speaker information, enabling DS2ST-LM to surpass prior direct S2ST systems in both speaker similarity and perceptual naturalness.",
      "url": "https://arxiv.org/abs/2601.16023",
      "pdfUrl": "https://arxiv.org/pdf/2601.16023.pdf",
      "titleJa": "音色を考慮したLLMベースの直接音声翻訳は複数の言語ペアに拡張可能"
    },
    {
      "id": "2601.15653",
      "arxivId": "2601.15653",
      "title": "Distributed Multichannel Active Noise Control with Asynchronous Communication",
      "authors": [
        "Junwei Ji",
        "Dongyuan Shi",
        "Boxiang Wang",
        "Ziyi Yang",
        "Haowen Li",
        "Woon-Seng Gan"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "Distributed multichannel active noise control (DMCANC) offers effective noise reduction across large spatial areas by distributing the computational load of centralized control to multiple low-cost nodes. Conventional DMCANC methods, however, typically assume synchronous communication and require frequent data exchange, resulting in high communication overhead. To enhance efficiency and adaptability, this work proposes an asynchronous communication strategy where each node executes a weight-constrained filtered-x LMS (WCFxLMS) algorithm and independently requests communication only when its local noise reduction performance degrades. Upon request, other nodes transmit the weight difference between their local control filter and the center point in WCFxLMS, which are then integrated to update both the control filter and the center point. This design enables nodes to operate asynchronously while preserving cooperative behavior. Simulation results demonstrate that the proposed asynchronous communication DMCANC (ACDMCANC) system maintains effective noise reduction with significantly reduced communication load, offering improved scalability for heterogeneous networks.",
      "url": "https://arxiv.org/abs/2601.15653",
      "pdfUrl": "https://arxiv.org/pdf/2601.15653.pdf",
      "titleJa": "非同期通信による分散型マルチチャネルアクティブノイズコントロール"
    },
    {
      "id": "2601.15433",
      "arxivId": "2601.15433",
      "title": "DynamicSound simulator for simulating moving sources and microphone arrays",
      "authors": [
        "Luca Barbisan",
        "Marco Levorato",
        "Fabrizio Riente"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Developing algorithms for sound classification, detection, and localization requires large amounts of flexible and realistic audio data, especially when leveraging modern machine learning and beamforming techniques. However, most existing acoustic simulators are tailored for indoor environments and are limited to static sound sources, making them unsuitable for scenarios involving moving sources, moving microphones, or long-distance propagation. This paper presents DynamicSound an open-source acoustic simulation framework for generating multichannel audio from one or more sound sources with the possibility to move them continuously in three-dimensional space and recorded by arbitrarily configured microphone arrays. The proposed model explicitly accounts for finite sound propagation delays, Doppler effects, distance-dependent attenuation, air absorption, and first-order reflections from planar surfaces, yielding temporally consistent spatial audio signals. Unlike conventional mono or stereo simulators, the proposed system synthesizes audio for an arbitrary number of virtual microphones, accurately reproducing inter-microphone time delays, level differences, and spectral coloration induced by the environment. Comparative evaluations with existing open-source tools demonstrate that the generated signals preserve high spatial fidelity across varying source positions and acoustic conditions. By enabling the generation of realistic multichannel audio under controlled and repeatable conditions, the proposed open framework provides a flexible and reproducible tool for the development, training, and evaluation of modern spatial audio and sound-source localization algorithms.",
      "url": "https://arxiv.org/abs/2601.15433",
      "pdfUrl": "https://arxiv.org/pdf/2601.15433.pdf",
      "titleJa": "移動音源とマイクアレイをシミュレートするDynamicSoundシミュレータ"
    },
    {
      "id": "2601.14925",
      "arxivId": "2601.14925",
      "title": "Fast-ULCNet: A fast and ultra low complexity network for single-channel speech enhancement",
      "authors": [
        "Nicolás Arrieta Larraza",
        "Niels de Koeijer"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "Single-channel speech enhancement algorithms are often used in resource-constrained embedded devices, where low latency and low complexity designs gain more importance. In recent years, researchers have proposed a wide variety of novel solutions to this problem. In particular, a recent deep learning model named ULCNet is among the state-of-the-art approaches in this domain. This paper proposes an adaptation of ULCNet, by replacing its GRU layers with FastGRNNs, to reduce both computational latency and complexity. Furthermore, this paper shows empirical evidence on the performance decay of FastGRNNs in long audio signals during inference due to internal state drifting, and proposes a novel approach based on a trainable complementary filter to mitigate it. The resulting model, Fast-ULCNet, performs on par with the state-of-the-art original ULCNet architecture on a speech enhancement task, while reducing its model size by more than half and decreasing its latency by 34% on average.",
      "url": "https://arxiv.org/abs/2601.14925",
      "pdfUrl": "https://arxiv.org/pdf/2601.14925.pdf",
      "titleJa": "Fast-ULCNet: 単一チャネル音声強調のための高速かつ超低複雑性ネットワーク"
    },
    {
      "id": "2601.14770",
      "arxivId": "2601.14770",
      "title": "Test-Time Adaptation For Speech Enhancement Via Mask Polarization",
      "authors": [
        "Tobias Raichle",
        "Erfan Amini",
        "Bin Yang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Adapting speech enhancement (SE) models to unseen environments is crucial for practical deployments, yet test-time adaptation (TTA) for SE remains largely under-explored due to a lack of understanding of how SE models degrade under domain shifts. We observe that mask-based SE models lose confidence under domain shifts, with predicted masks becoming flattened and losing decisive speech preservation and noise suppression. Based on this insight, we propose mask polarization (MPol), a lightweight TTA method that restores mask bimodality through distribution comparison using the Wasserstein distance. MPol requires no additional parameters beyond the trained model, making it suitable for resource-constrained edge deployments. Experimental results across diverse domain shifts and architectures demonstrate that MPol achieves very consistent gains that are competitive with significantly more complex approaches.",
      "url": "https://arxiv.org/abs/2601.14770",
      "pdfUrl": "https://arxiv.org/pdf/2601.14770.pdf",
      "titleJa": "マスク偏光による音声強調のためのテスト時適応"
    },
    {
      "id": "2601.14751",
      "arxivId": "2601.14751",
      "title": "Inverse-Hessian Regularization for Continual Learning in ASR",
      "authors": [
        "Steven Vander Eeckt",
        "Hugo Van hamme"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Catastrophic forgetting remains a major challenge for continual learning (CL) in automatic speech recognition (ASR), where models must adapt to new domains without losing performance on previously learned conditions. Several CL methods have been proposed for ASR, and, recently, weight averaging - where models are averaged in a merging step after fine-tuning - has proven effective as a simple memory-free strategy. However, it is heuristic in nature and ignores the underlying loss landscapes of the tasks, hindering adaptability. In this work, we propose Inverse Hessian Regularization (IHR), a memory-free approach for CL in ASR that incorporates curvature information into the merging step. After fine-tuning on a new task, the adaptation is adjusted through a Kronecker-factored inverse Hessian approximation of the previous task, ensuring that the model moves primarily in directions less harmful to past performance, while keeping the method lightweight. We evaluate IHR on two CL benchmarks and show that it significantly outperforms state-of-the-art baselines, reducing forgetting while improving adaptability. Ablation studies and analyses further confirm its effectiveness.",
      "url": "https://arxiv.org/abs/2601.14751",
      "pdfUrl": "https://arxiv.org/pdf/2601.14751.pdf",
      "titleJa": "ASRにおける継続学習のための逆ヘッセ行列正則化"
    },
    {
      "id": "2601.14728",
      "arxivId": "2601.14728",
      "title": "AQAScore: Evaluating Semantic Alignment in Text-to-Audio Generation via Audio Question Answering",
      "authors": [
        "Chun-Yi Kuan",
        "Kai-Wei Chang",
        "Hung-yi Lee"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Although text-to-audio generation has made remarkable progress in realism and diversity, the development of evaluation metrics has not kept pace. Widely-adopted approaches, typically based on embedding similarity like CLAPScore, effectively measure general relevance but remain limited in fine-grained semantic alignment and compositional reasoning. To address this, we introduce AQAScore, a backbone-agnostic evaluation framework that leverages the reasoning capabilities of audio-aware large language models (ALLMs). AQAScore reformulates assessment as a probabilistic semantic verification task; rather than relying on open-ended text generation, it estimates alignment by computing the exact log-probability of a \"Yes\" answer to targeted semantic queries. We evaluate AQAScore across multiple benchmarks, including human-rated relevance, pairwise comparison, and compositional reasoning tasks. Experimental results show that AQAScore consistently achieves higher correlation with human judgments than similarity-based metrics and generative prompting baselines, showing its effectiveness in capturing subtle semantic inconsistencies and scaling with the capability of underlying ALLMs.",
      "url": "https://arxiv.org/abs/2601.14728",
      "pdfUrl": "https://arxiv.org/pdf/2601.14728.pdf",
      "titleJa": "AQAScore: 音声質問応答によるテキスト音声生成における意味的整合の評価"
    },
    {
      "id": "2601.14721",
      "arxivId": "2601.14721",
      "title": "NLP-Based Review for Toxic Comment Detection Tailored to the Chinese Cyberspace",
      "authors": [
        "Ruixing Ren",
        "Junhui Zhao",
        "Xiaoke Sun",
        "Qiuping Li"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "With the in-depth integration of mobile Internet and widespread adoption of social platforms, user-generated content in the Chinese cyberspace has witnessed explosive growth. Among this content, the proliferation of toxic comments poses severe challenges to individual mental health, community atmosphere and social trust. Owing to the strong context dependence, cultural specificity and rapid evolution of Chinese cyber language, toxic expressions are often conveyed through complex forms such as homophones and metaphors, imposing notable limitations on traditional detection methods. To address this issue, this review focuses on the core topic of natural language processing based toxic comment detection in the Chinese cyberspace, systematically collating and critically analyzing the research progress and key challenges in this field. This review first defines the connotation and characteristics of Chinese toxic comments, and analyzes the platform ecology and transmission mechanisms they rely on. It then comprehensively reviews the construction methods and limitations of existing public datasets, and proposes a novel fine-grained and scalable framework for toxic comment definition and classification, along with corresponding data annotation and quality assessment strategies. We systematically summarize the evolutionary path of detection models from traditional methods to deep learning, with special emphasis on the importance of interpretability in model design. Finally, we thoroughly discuss the open challenges faced by current research and provide forward-looking suggestions for future research directions.",
      "url": "https://arxiv.org/abs/2601.14721",
      "pdfUrl": "https://arxiv.org/pdf/2601.14721.pdf",
      "titleJa": "中国のサイバースペースに合わせたNLPベースの有害コメント検出レビュー"
    },
    {
      "id": "2601.14699",
      "arxivId": "2601.14699",
      "title": "Triage knowledge distillation for speaker verification",
      "authors": [
        "Ju-ho Kim",
        "Youngmoon Jung",
        "Joon-Young Yang",
        "Jaeyoung Roh",
        "Chang Woo Han",
        "Hoon-Young Cho"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Deploying speaker verification on resource-constrained devices remains challenging due to the computational cost of high-capacity models; knowledge distillation (KD) offers a remedy. Classical KD entangles target confidence with non-target structure in a Kullback-Leibler term, limiting the transfer of relational information. Decoupled KD separates these signals into target and non-target terms, yet treats non-targets uniformly and remains vulnerable to the long tail of low-probability classes in large-class settings. We introduce Triage KD (TRKD), a distillation scheme that operationalizes assess-prioritize-focus. TRKD introduces a cumulative-probability cutoff $τ$ to assess per-example difficulty and partition the teacher posterior into three groups: the target class, a high-probability non-target confusion-set, and a background-set. To prioritize informative signals, TRKD distills the confusion-set conditional distribution and discards the background. Concurrently, it transfers a three-mass (target/confusion/background) that capture sample difficulty and inter-class confusion. Finally, TRKD focuses learning via a curriculum on $τ$: training begins with a larger $τ$ to convey broad non-target context, then $τ$ is progressively decreased to shrink the confusion-set, concentrating supervision on the most confusable classes. In extensive experiments on VoxCeleb1 with both homogeneous and heterogeneous teacher-student pairs, TRKD was consistently superior to recent KD variants and attained the lowest EER across all protocols.",
      "url": "https://arxiv.org/abs/2601.14699",
      "pdfUrl": "https://arxiv.org/pdf/2601.14699.pdf",
      "titleJa": "話者認証のためのトリアージ知識抽出"
    },
    {
      "id": "2601.14620",
      "arxivId": "2601.14620",
      "title": "Scaling Ambiguity: Augmenting Human Annotation in Speech Emotion Recognition with Audio-Language Models",
      "authors": [
        "Wenda Zhang",
        "Hongyu Jin",
        "Siyi Wang",
        "Zhiqiang Wei",
        "Ting Dang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Speech Emotion Recognition models typically use single categorical labels, overlooking the inherent ambiguity of human emotions. Ambiguous Emotion Recognition addresses this by representing emotions as probability distributions, but progress is limited by unreliable ground-truth distributions inferred from sparse human annotations. This paper explores whether Large Audio-Language Models (ALMs) can mitigate the annotation bottleneck by generating high-quality synthetic annotations. We introduce a framework leveraging ALMs to create Synthetic Perceptual Proxies, augmenting human annotations to improve ground-truth distribution reliability. We validate these proxies through statistical analysis of their alignment with human distributions and evaluate their impact by fine-tuning ALMs with the augmented emotion distributions. Furthermore, to address class imbalance and enable unbiased evaluation, we propose DiME-Aug, a Distribution-aware Multimodal Emotion Augmentation strategy. Experiments on IEMOCAP and MSP-Podcast show that synthetic annotations enhance emotion distribution, especially in low-ambiguity regions where annotation agreement is high. However, benefits diminish for highly ambiguous emotions with greater human disagreement. This work provides the first evidence that ALMs could address annotation scarcity in ambiguous emotion recognition, but highlights the need for more advanced prompting or generation strategies to handle highly ambiguous cases.",
      "url": "https://arxiv.org/abs/2601.14620",
      "pdfUrl": "https://arxiv.org/pdf/2601.14620.pdf",
      "titleJa": "曖昧さのスケーリング：音声言語モデルによる音声感情認識における人間の注釈の強化"
    },
    {
      "id": "2601.16211",
      "arxivId": "2601.16211",
      "title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition",
      "authors": [
        "Geo Ahn",
        "Inwoong Lee",
        "Taeoh Kim",
        "Minho Shim",
        "Dongyoon Wee",
        "Jinwoo Choi"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.",
      "url": "https://arxiv.org/abs/2601.16211",
      "pdfUrl": "https://arxiv.org/pdf/2601.16211.pdf",
      "titleJa": "なぜ引き出しが開けられないのか？ゼロショット構成動作認識におけるオブジェクト駆動型ショートカットの緩和"
    },
    {
      "id": "2601.16210",
      "arxivId": "2601.16210",
      "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
      "authors": [
        "Onkar Susladkar",
        "Tushar Prakash",
        "Adheesh Juvekar",
        "Kiet A. Nguyen",
        "Dong-Hwan Jang",
        "Inderjit S Dhillon",
        "Ismini Lourentzou"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.",
      "url": "https://arxiv.org/abs/2601.16210",
      "pdfUrl": "https://arxiv.org/pdf/2601.16210.pdf",
      "titleJa": "PyraTok: 動画の理解と生成のための言語に合わせたピラミッド型トークナイザー"
    },
    {
      "id": "2601.16206",
      "arxivId": "2601.16206",
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "authors": [
        "Daixuan Cheng",
        "Shaohan Huang",
        "Yuxian Gu",
        "Huatong Song",
        "Guoxin Chen",
        "Li Dong",
        "Wayne Xin Zhao",
        "Ji-Rong Wen",
        "Furu Wei"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
      "url": "https://arxiv.org/abs/2601.16206",
      "pdfUrl": "https://arxiv.org/pdf/2601.16206.pdf",
      "titleJa": "LLM-in-Sandboxは汎用エージェントインテリジェンスを引き出します"
    },
    {
      "id": "2601.16205",
      "arxivId": "2601.16205",
      "title": "Counterfactual Training: Teaching Models Plausible and Actionable Explanations",
      "authors": [
        "Patrick Altmeyer",
        "Aleksander Buszydlik",
        "Arie van Deursen",
        "Cynthia C. S. Liem"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.",
      "url": "https://arxiv.org/abs/2601.16205",
      "pdfUrl": "https://arxiv.org/pdf/2601.16205.pdf",
      "titleJa": "反事実的トレーニング：もっともらしく実行可能な説明をモデル化する"
    },
    {
      "id": "2601.16175",
      "arxivId": "2601.16175",
      "title": "Learning to Discover at Test Time",
      "authors": [
        "Mert Yuksekgonul",
        "Daniel Koceja",
        "Xinhao Li",
        "Federico Bianchi",
        "Jed McCaleb",
        "Xiaolong Wang",
        "Jan Kautz",
        "Yejin Choi",
        "James Zou",
        "Carlos Guestrin",
        "Yu Sun"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
      "url": "https://arxiv.org/abs/2601.16175",
      "pdfUrl": "https://arxiv.org/pdf/2601.16175.pdf",
      "titleJa": "テスト時に発見することを学ぶ"
    },
    {
      "id": "2601.16172",
      "arxivId": "2601.16172",
      "title": "Structured Hints for Sample-Efficient Lean Theorem Proving",
      "authors": [
        "Zachary Burton"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.AI"
      ],
      "abstract": "State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.",
      "url": "https://arxiv.org/abs/2601.16172",
      "pdfUrl": "https://arxiv.org/pdf/2601.16172.pdf",
      "titleJa": "サンプル効率の高いリーン定理証明のための構造化ヒント"
    },
    {
      "id": "2601.16163",
      "arxivId": "2601.16163",
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "authors": [
        "Moo Jin Kim",
        "Yihuai Gao",
        "Tsung-Yi Lin",
        "Yen-Chen Lin",
        "Yunhao Ge",
        "Grace Lam",
        "Percy Liang",
        "Shuran Song",
        "Ming-Yu Liu",
        "Chelsea Finn",
        "Jinwei Gu"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "abstract": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
      "url": "https://arxiv.org/abs/2601.16163",
      "pdfUrl": "https://arxiv.org/pdf/2601.16163.pdf",
      "titleJa": "コスモスポリシー：視覚運動制御と計画のためのビデオモデルの微調整"
    },
    {
      "id": "2601.16152",
      "arxivId": "2601.16152",
      "title": "Substrate Stability Under Persistent Disagreement: Structural Constraints for Neutral Ontological Substrates",
      "authors": [
        "Denise M. Case"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "abstract": "Modern data systems increasingly operate under conditions of persistent legal, political, and analytic disagreement. In such settings, interoperability cannot rely on shared interpretation, negotiated semantics, or centralized authority. Instead, representations must function as neutral substrates that preserve stable reference across incompatible extensions. This paper investigates the structural constraints imposed on ontological design by this requirement. Building on a neutrality framework that treats interpretive non-commitment and stability under extension as explicit design constraints, we ask what minimal ontological structure is forced if accountability relationships are to remain referable and comparable under disagreement. Minimality here is not mere parsimony: a reduction is admissible only if it does not reintroduce stability-critical distinctions as hidden roles, flags, or contextual predicates. We establish a conditional lower-bound result: any ontology capable of supporting accountability under persistent disagreement must realize at least six distinct identity-and-persistence regimes. We further show that a construction with exactly six such regimes is sufficient to satisfy the stated requirements without embedding causal or normative commitments in the substrate. The result is not a proposal for a universal ontology, but a constraint on what is possible when neutrality and stable reference are treated as non-negotiable design goals.",
      "url": "https://arxiv.org/abs/2601.16152",
      "pdfUrl": "https://arxiv.org/pdf/2601.16152.pdf",
      "titleJa": "持続的不一致下における基質安定性：中立的オントロジー基質に対する構造的制約"
    },
    {
      "id": "2601.16140",
      "arxivId": "2601.16140",
      "title": "Learning to Watermark in the Latent Space of Generative Models",
      "authors": [
        "Sylvestre-Alvise Rebuffi",
        "Tuan Tran",
        "Valeriu Lacatusu",
        "Pierre Fernandez",
        "Tomáš Souček",
        "Nikola Jovanović",
        "Tom Sander",
        "Hady Elsahar",
        "Alexandre Mourachko"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "abstract": "Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.",
      "url": "https://arxiv.org/abs/2601.16140",
      "pdfUrl": "https://arxiv.org/pdf/2601.16140.pdf",
      "titleJa": "生成モデルの潜在空間における透かしの学習"
    },
    {
      "id": "2601.16134",
      "arxivId": "2601.16134",
      "title": "LLM Prompt Evaluation for Educational Applications",
      "authors": [
        "Langdon Holmes",
        "Adam Coscia",
        "Scott Crossley",
        "Joon Suh Choi",
        "Wesley Morris"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.",
      "url": "https://arxiv.org/abs/2601.16134",
      "pdfUrl": "https://arxiv.org/pdf/2601.16134.pdf",
      "titleJa": "教育アプリケーションのためのLLMプロンプト評価"
    },
    {
      "id": "2601.16130",
      "arxivId": "2601.16130",
      "title": "Replicating Human Motivated Reasoning Studies with LLMs",
      "authors": [
        "Neeley Pate",
        "Adiba Mahbub Proma",
        "Hangfeng He",
        "James N. Druckman",
        "Daniel Molden",
        "Gourab Ghoshal",
        "Ehsan Hoque"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "abstract": "Motivated reasoning -- the idea that individuals processing information may be motivated to reach a certain conclusion, whether it be accurate or predetermined -- has been well-explored as a human phenomenon. However, it is unclear whether base LLMs mimic these motivational changes. Replicating 4 prior political motivated reasoning studies, we find that base LLM behavior does not align with expected human behavior. Furthermore, base LLM behavior across models shares some similarities, such as smaller standard deviations and inaccurate argument strength assessments. We emphasize the importance of these findings for researchers using LLMs to automate tasks such as survey data collection and argument assessment.",
      "url": "https://arxiv.org/abs/2601.16130",
      "pdfUrl": "https://arxiv.org/pdf/2601.16130.pdf",
      "titleJa": "LLMによる人間の動機づけられた推論研究の再現"
    },
    {
      "id": "2601.16127",
      "arxivId": "2601.16127",
      "title": "Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging",
      "authors": [
        "Alphaeus Dmonte",
        "Vidhi Gupta",
        "Daniel J Perry",
        "Mark Arehart"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.",
      "url": "https://arxiv.org/abs/2601.16127",
      "pdfUrl": "https://arxiv.org/pdf/2601.16127.pdf",
      "titleJa": "言語固有のモデルのマージによるトレーニング効率の向上とメンテナンスコストの削減"
    },
    {
      "id": "2601.16108",
      "arxivId": "2601.16108",
      "title": "Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources",
      "authors": [
        "Marzieh Adeli Shamsabad",
        "Hamed Ghodrati"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.",
      "url": "https://arxiv.org/abs/2601.16108",
      "pdfUrl": "https://arxiv.org/pdf/2601.16108.pdf",
      "titleJa": "マルチモーダル気候偽情報検出：視覚言語モデルと外部知識ソースの統合"
    },
    {
      "id": "2601.16091",
      "arxivId": "2601.16091",
      "title": "Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals",
      "authors": [
        "Saar Cohen"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.",
      "url": "https://arxiv.org/abs/2601.16091",
      "pdfUrl": "https://arxiv.org/pdf/2601.16091.pdf",
      "titleJa": "確率的到着を伴うオンライン非重心クラスタリングにおける遅延割り当て"
    },
    {
      "id": "2601.16087",
      "arxivId": "2601.16087",
      "title": "Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics",
      "authors": [
        "Sukesh Subaharan"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.",
      "url": "https://arxiv.org/abs/2601.16087",
      "pdfUrl": "https://arxiv.org/pdf/2601.16087.pdf",
      "titleJa": "明示的な状態ダイナミクスを用いた言語モデルエージェントの長期的行動の制御"
    },
    {
      "id": "2601.16083",
      "arxivId": "2601.16083",
      "title": "Probably Approximately Correct Maximum A Posteriori Inference",
      "authors": [
        "Matthew Shorvon",
        "Frederik Mallmann-Trenn",
        "David S. Watson"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Computing the conditional mode of a distribution, better known as the $\\mathit{maximum\\ a\\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\\mathit{probably\\ approximately\\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.",
      "url": "https://arxiv.org/abs/2601.16083",
      "pdfUrl": "https://arxiv.org/pdf/2601.16083.pdf",
      "titleJa": "おそらく近似的に正しい最大事後推論"
    },
    {
      "id": "2601.16056",
      "arxivId": "2601.16056",
      "title": "Designing faster mixed integer linear programming algorithm via learning the optimal path",
      "authors": [
        "Ruizhi Liu",
        "Liming Xu",
        "Xulin Huang",
        "Jingyan Sui",
        "Shizhe Ding",
        "Boyang Xia",
        "Chungong Yu",
        "Dongbo Bu"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP problem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpredictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algorithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solution, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise training paradigm that enhances the model's ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flexible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.",
      "url": "https://arxiv.org/abs/2601.16056",
      "pdfUrl": "https://arxiv.org/pdf/2601.16056.pdf",
      "titleJa": "最適経路の学習による高速混合整数線形計画アルゴリズムの設計"
    },
    {
      "id": "2601.16045",
      "arxivId": "2601.16045",
      "title": "AgriPINN: A Process-Informed Neural Network for Interpretable and Scalable Crop Biomass Prediction Under Water Stress",
      "authors": [
        "Yue Shi",
        "Liangxiu Han",
        "Xin Zhang",
        "Tam Sobeih",
        "Thomas Gaiser",
        "Nguyen Huu Thuy",
        "Dominik Behrend",
        "Amit Kumar Srivastava",
        "Krishnagopal Halder",
        "Frank Ewert"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Accurate prediction of crop above-ground biomass (AGB) under water stress is critical for monitoring crop productivity, guiding irrigation, and supporting climate-resilient agriculture. Data-driven models scale well but often lack interpretability and degrade under distribution shift, whereas process-based crop models (e.g. DSSAT, APSIM, LINTUL5) require extensive calibration and are difficult to deploy over large spatial domains. To address these limitations, we propose AgriPINN, a process-informed neural network that integrates a biophysical crop-growth differential equation as a differentiable constraint within a deep learning backbone. This design encourages physiologically consistent biomass dynamics under water-stress conditions while preserving model scalability for spatially distributed AGB prediction. AgriPINN recovers latent physiological variables, including leaf area index (LAI), absorbed photosynthetically active radiation (PAR), radiation use efficiency (RUE), and water-stress factors, without requiring direct supervision. We pretrain AgriPINN on 60 years of historical data across 397 regions in Germany and fine-tune it on three years of field experiments under controlled water treatments. Results show that AgriPINN consistently outperforms state-of-the-art deep-learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer) and the process-based LINTUL5 model in terms of accuracy (RMSE reductions up to $43\\%$) and computational efficiency. By combining the scalability of deep learning with the biophysical rigor of process-based modeling, AgriPINN provides a robust and interpretable framework for spatio-temporal AGB prediction, offering practical value for planning of irrigation infrastructure, yield forecasting, and climate-adaptation planning.",
      "url": "https://arxiv.org/abs/2601.16045",
      "pdfUrl": "https://arxiv.org/pdf/2601.16045.pdf",
      "titleJa": "AgriPINN: 水ストレス下における解釈可能かつスケーラブルな作物バイオマス予測のためのプロセス情報に基づくニューラルネットワーク"
    },
    {
      "id": "2601.16038",
      "arxivId": "2601.16038",
      "title": "Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval",
      "authors": [
        "Olga Bunkova",
        "Lorenzo Di Fruscia",
        "Sophia Rupprecht",
        "Artur M. Schweidtmann",
        "Marcel J. T. Reinders",
        "Jana M. Weber"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.",
      "url": "https://arxiv.org/abs/2601.16038",
      "pdfUrl": "https://arxiv.org/pdf/2601.16038.pdf",
      "titleJa": "反応知識グラフに基づく大規模言語モデルの統合検索"
    },
    {
      "id": "2601.14684",
      "arxivId": "2601.14684",
      "title": "Dissecting Performance Degradation in Audio Source Separation under Sampling Frequency Mismatch",
      "authors": [
        "Kanami Imamura",
        "Tomohiko Nakamura",
        "Kohei Yatabe",
        "Hiroshi Saruwatari"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio processing methods based on deep neural networks are typically trained at a single sampling frequency (SF). To handle untrained SFs, signal resampling is commonly employed, but it can degrade performance, particularly when the input SF is lower than the trained SF. This paper investigates the causes of this degradation through two hypotheses: (i) the lack of high-frequency components introduced by up-sampling, and (ii) the greater importance of their presence than their precise representation. To examine these hypotheses, we compare conventional resampling with three alternatives: post-resampling noise addition, which adds Gaussian noise to the resampled signal; noisy-kernel resampling, which perturbs the kernel with Gaussian noise to enrich high-frequency components; and trainable-kernel resampling, which adapts the interpolation kernel through training. Experiments on music source separation show that noisy-kernel and trainable-kernel resampling alleviate the degradation observed with conventional resampling. We further demonstrate that noisy-kernel resampling is effective across diverse models, highlighting it as a simple yet practical option.",
      "url": "https://arxiv.org/abs/2601.14684",
      "pdfUrl": "https://arxiv.org/pdf/2601.14684.pdf",
      "titleJa": "サンプリング周波数の不一致による音源分離の性能劣化の解析"
    },
    {
      "id": "2601.15348",
      "arxivId": "2601.15348",
      "title": "Abusive music and song transformation using GenAI and LLMs",
      "authors": [
        "Jiyang Choi",
        "Rohitash Chandra"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Repeated exposure to violence and abusive content in music and song content can influence listeners' emotions and behaviours, potentially normalising aggression or reinforcing harmful stereotypes. In this study, we explore the use of generative artificial intelligence (GenAI) and Large Language Models (LLMs) to automatically transform abusive words (vocal delivery) and lyrical content in popular music. Rather than simply muting or replacing a single word, our approach transforms the tone, intensity, and sentiment, thus not altering just the lyrics, but how it is expressed. We present a comparative analysis of four selected English songs and their transformed counterparts, evaluating changes through both acoustic and sentiment-based lenses. Our findings indicate that Gen-AI significantly reduces vocal aggressiveness, with acoustic analysis showing improvements in Harmonic to Noise Ratio, Cepstral Peak Prominence, and Shimmer. Sentiment analysis reduced aggression by 63.3-85.6\\% across artists, with major improvements in chorus sections (up to 88.6\\% reduction). The transformed versions maintained musical coherence while mitigating harmful content, offering a promising alternative to traditional content moderation that avoids triggering the \"forbidden fruit\" effect, where the censored content becomes more appealing simply because it is restricted. This approach demonstrates the potential for GenAI to create safer listening experiences while preserving artistic expression.",
      "url": "https://arxiv.org/abs/2601.15348",
      "pdfUrl": "https://arxiv.org/pdf/2601.15348.pdf",
      "titleJa": "GenAIとLLMを使用した虐待的な音楽と歌の変換"
    },
    {
      "id": "2601.14356",
      "arxivId": "2601.14356",
      "title": "Single-step Controllable Music Bandwidth Extension With Flow Matching",
      "authors": [
        "Carlos Hernandez-Olivan",
        "Hendrik Vincent Koops",
        "Hao Hao Tan",
        "Elio Quinton"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio restoration consists in inverting degradations of a digital audio signal to recover what would have been the pristine quality signal before the degradation occurred. This is valuable in contexts such as archives of music recordings, particularly those of precious historical value, for which a clean version may have been lost or simply does not exist. Recent work applied generative models to audio restoration, showing promising improvement over previous methods, and opening the door to the ability to perform restoration operations that were not possible before. However, making these models finely controllable remains a challenge. In this paper, we propose an extension of FLowHigh and introduce the Dynamic Spectral Contour (DSC) as a control signal for bandwidth extension via classifier-free guidance. Our experiments show competitive model performance, and indicate that DSC is a promising feature to support fine-grained conditioning.",
      "url": "https://arxiv.org/abs/2601.14356",
      "pdfUrl": "https://arxiv.org/pdf/2601.14356.pdf",
      "titleJa": "フローマッチングによるシングルステップ制御可能な音楽帯域幅拡張"
    },
    {
      "id": "2601.14157",
      "arxivId": "2601.14157",
      "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models",
      "authors": [
        "Bruno Sienkiewicz",
        "Łukasz Neumann",
        "Mateusz Modrzejewski"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.",
      "url": "https://arxiv.org/abs/2601.14157",
      "pdfUrl": "https://arxiv.org/pdf/2601.14157.pdf",
      "titleJa": "ConceptCaps - 音楽モデルの解釈可能性のための蒸留概念データセット"
    },
    {
      "id": "2601.13931",
      "arxivId": "2601.13931",
      "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music",
      "authors": [
        "Yannis Vasilakis",
        "Rachel Bittner",
        "Johan Pauwels"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG"
      ],
      "abstract": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.",
      "url": "https://arxiv.org/abs/2601.13931",
      "pdfUrl": "https://arxiv.org/pdf/2601.13931.pdf",
      "titleJa": "音楽のための音声テキスト統合モデルにおける効果的な否定モデリングに向けて"
    },
    {
      "id": "2601.13647",
      "arxivId": "2601.13647",
      "title": "Fusion Segment Transformer: Bi-Directional Attention Guided Fusion Network for AI-Generated Music Detection",
      "authors": [
        "Yumin Kim",
        "Seonghyeon Go"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "With the rise of generative AI technology, anyone can now easily create and deploy AI-generated music, which has heightened the need for technical solutions to address copyright and ownership issues. While existing works mainly focused on short-audio, the challenge of full-audio detection, which requires modeling long-term structure and context, remains insufficiently explored. To address this, we propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. As in our previous work, we extract content embeddings from short music segments using diverse feature extractors. Furthermore, we enhance the architecture for full-audio AI-generated music detection by introducing a Gated Fusion Layer that effectively integrates content and structural information, enabling the capture of long-term context. Experiments on the SONICS and AIME datasets show that our approach outperforms the previous model and recent baselines, achieving state-of-the-art results in AI-generated music detection.",
      "url": "https://arxiv.org/abs/2601.13647",
      "pdfUrl": "https://arxiv.org/pdf/2601.13647.pdf",
      "titleJa": "Fusion Segment Transformer: AI生成音楽検出のための双方向アテンションガイド融合ネットワーク"
    },
    {
      "id": "2601.12961",
      "arxivId": "2601.12961",
      "title": "Supervised Learning for Game Music Segmentation",
      "authors": [
        "Shangxuan Luo",
        "Joshua Reiss"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD"
      ],
      "abstract": "At present, neural network-based models, including transformers, struggle to generate memorable and readily comprehensible music from unified and repetitive musical material due to a lack of understanding of musical structure. Consequently, these models are rarely employed by the games industry. It is hypothesised by many scholars that the modelling of musical structure may inform models at a higher level, thereby enhancing the quality of music generation. The aim of this study is to explore the performance of supervised learning methods in the task of structural segmentation, which is the initial step in music structure modelling. An audio game music dataset with 309 structural annotations was created to train the proposed method, which combines convolutional neural networks and recurrent neural networks, achieving performance comparable to the state-of-the-art unsupervised learning methods with fewer training resources.",
      "url": "https://arxiv.org/abs/2601.12961",
      "pdfUrl": "https://arxiv.org/pdf/2601.12961.pdf",
      "titleJa": "ゲーム音楽セグメンテーションのための教師あり学習"
    },
    {
      "id": "2601.12802",
      "arxivId": "2601.12802",
      "title": "UNMIXX: Untangling Highly Correlated Singing Voices Mixtures",
      "authors": [
        "Jihoo Jung",
        "Ji-Hoon Kim",
        "Doyeop Kwak",
        "Junwon Lee",
        "Juhan Nam",
        "Joon Son Chung"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We introduce UNMIXX, a novel framework for multiple singing voices separation (MSVS). While related to speech separation, MSVS faces unique challenges: data scarcity and the highly correlated nature of singing voices mixture. To address these issues, we propose UNMIXX with three key components: (1) musically informed mixing strategy to construct highly correlated, music-like mixtures, (2) cross-source attention that drives representations of two singers apart via reverse attention, and (3) magnitude penalty loss penalizing erroneously assigned interfering energy. UNMIXX not only addresses data scarcity by simulating realistic training data, but also excels at separating highly correlated mixtures through cross-source interactions at both the architectural and loss levels. Our extensive experiments demonstrate that UNMIXX greatly enhances performance, with SDRi gains exceeding 2.2 dB over prior work.",
      "url": "https://arxiv.org/abs/2601.12802",
      "pdfUrl": "https://arxiv.org/pdf/2601.12802.pdf",
      "titleJa": "UNMIXX: 相関性の高い歌声ミックスを解きほぐす"
    },
    {
      "id": "2601.12314",
      "arxivId": "2601.12314",
      "title": "A Similarity Network for Correlating Musical Structure to Military Strategy",
      "authors": [
        "Yiwen Zhang",
        "Hui Zhang",
        "Fanqin Meng"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Music perception, a multi-sensory process based on the synesthesia effect, is an essential component of music aesthetic education. Understanding music structure helps both perception and aesthetic education. Music structure incorporates a range of information, the coordination of which forms the melody, just as different military actions cooperate to produce a military strategy. However, there are a few ways for assessing music perception from the perspectives of system operation and information management. In this paper, we explore the similarities between music structure and military strategy while creating the Music Clips Correlation Network (MCCN) based on Mel-frequency Cepstral Coefficients (MFCCs). The inspiration comes from the comparison between a concert conductor's musical score and a military war commander's sand table exercise. Specifically, we create MCCNs for various kinds of war movie soundtracks, then relate military tactics (Sun Tzu's Art of War, etc.) and political institutions to military operations networks. Our primary findings suggest a few similarities, implying that music perception and aesthetic education can be approached from a military strategy and management perspective through this interdisciplinary research. Similarly, we can discover similarities between the art of military scheming and the art of musical structure based on network analysis in order to facilitate the understanding of the relationship between technology and art.",
      "url": "https://arxiv.org/abs/2601.12314",
      "pdfUrl": "https://arxiv.org/pdf/2601.12314.pdf",
      "titleJa": "音楽構造と軍事戦略を相関させる類似性ネットワーク"
    },
    {
      "id": "2601.12245",
      "arxivId": "2601.12245",
      "title": "Sound2Hap: Learning Audio-to-Vibrotactile Haptic Generation from Human Ratings",
      "authors": [
        "Yinan Li",
        "Hasti Seifi"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Environmental sounds like footsteps, keyboard typing, or dog barking carry rich information and emotional context, making them valuable for designing haptics in user applications. Existing audio-to-vibration methods, however, rely on signal-processing rules tuned for music or games and often fail to generalize across diverse sounds. To address this, we first investigated user perception of four existing audio-to-haptic algorithms, then created a data-driven model for environmental sounds. In Study 1, 34 participants rated vibrations generated by the four algorithms for 1,000 sounds, revealing no consistent algorithm preferences. Using this dataset, we trained Sound2Hap, a CNN-based autoencoder, to generate perceptually meaningful vibrations from diverse sounds with low latency. In Study 2, 15 participants rated its output higher than signal-processing baselines on both audio-vibration match and Haptic Experience Index (HXI), finding it more harmonious with diverse sounds. This work demonstrates a perceptually validated approach to audio-haptic translation, broadening the reach of sound-driven haptics.",
      "url": "https://arxiv.org/abs/2601.12245",
      "pdfUrl": "https://arxiv.org/pdf/2601.12245.pdf",
      "titleJa": "Sound2Hap: 人間の評価から音声から振動触覚への触覚生成を学習する"
    },
    {
      "id": "2601.12222",
      "arxivId": "2601.12222",
      "title": "Song Aesthetics Evaluation with Multi-Stem Attention and Hierarchical Uncertainty Modeling",
      "authors": [
        "Yishan Lv",
        "Jing Luo",
        "Boyuan Ju",
        "Yang Zhang",
        "Xinda Wu",
        "Bo Yuan",
        "Xinyu Yang"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Music generative artificial intelligence (AI) is rapidly expanding music content, necessitating automated song aesthetics evaluation. However, existing studies largely focus on speech, audio or singing quality, leaving song aesthetics underexplored. Moreover, conventional approaches often predict a precise Mean Opinion Score (MOS) value directly, which struggles to capture the nuances of human perception in song aesthetics evaluation. This paper proposes a song-oriented aesthetics evaluation framework, featuring two novel modules: 1) Multi-Stem Attention Fusion (MSAF) builds bidirectional cross-attention between mixture-vocal and mixture-accompaniment pairs, fusing them to capture complex musical features; 2) Hierarchical Granularity-Aware Interval Aggregation (HiGIA) learns multi-granularity score probability distributions, aggregates them into a score interval, and applies a regression within the interval to produce the final score. We evaluated on two datasets of full-length songs: SongEval dataset (AI-generated) and an internal aesthetics dataset (human-created), and compared with two state-of-the-art (SOTA) models. Results show that the proposed method achieves stronger performance for multi-dimensional song aesthetics evaluation.",
      "url": "https://arxiv.org/abs/2601.12222",
      "pdfUrl": "https://arxiv.org/pdf/2601.12222.pdf",
      "titleJa": "マルチステムアテンションと階層的不確実性モデリングによる歌の美的評価"
    },
    {
      "id": "2601.12205",
      "arxivId": "2601.12205",
      "title": "Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks",
      "authors": [
        "Shih-Heng Wang",
        "Jiatong Shi",
        "Jinchuan Tian",
        "Haibin Wu",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "This paper investigates three crucial yet underexplored aspects of the generalization capabilities of neural audio codecs (NACs): (i) whether NACs can generalize to unseen languages during pre-training, (ii) whether speech-only pre-trained NACs can effectively generalize to non-speech applications such as environmental sounds, music, and animal vocalizations, and (iii) whether incorporating non-speech data during pre-training can improve performance on both speech and non-speech tasks. Existing studies typically rely on off-the-shelf NACs for comparison, which limits insight due to variations in implementation. In this work, we train NACs from scratch using strictly controlled configurations and carefully curated pre-training data to enable fair comparisons. We conduct a comprehensive evaluation of NAC performance on both signal reconstruction quality and downstream applications using 11 metrics. Our results show that NACs can generalize to unseen languages during pre-training, speech-only pre-trained NACs exhibit degraded performance on non-speech tasks, and incorporating non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks.",
      "url": "https://arxiv.org/abs/2601.12205",
      "pdfUrl": "https://arxiv.org/pdf/2601.12205.pdf",
      "titleJa": "ニューラルコーデックは一般化するか？未知の言語と非音声タスクを対象とした対照研究"
    },
    {
      "id": "2601.12180",
      "arxivId": "2601.12180",
      "title": "VidTune: Creating Video Soundtracks with Generative Music and Contextual Thumbnails",
      "authors": [
        "Mina Huh",
        "Ailie C. Fraser",
        "Dingzeyu Li",
        "Mira Dontcheva",
        "Bryan Wang"
      ],
      "publishedDate": "2026-01-17",
      "categories": [
        "cs.HC",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Music shapes the tone of videos, yet creators often struggle to find soundtracks that match their video's mood and narrative. Recent text-to-music models let creators generate music from text prompts, but our formative study (N=8) shows creators struggle to construct diverse prompts, quickly review and compare tracks, and understand their impact on the video. We present VidTune, a system that supports soundtrack creation by generating diverse music options from a creator's prompt and producing contextual thumbnails for rapid review. VidTune extracts representative video subjects to ground thumbnails in context, maps each track's valence and energy onto visual cues like color and brightness, and depicts prominent genres and instruments. Creators can refine tracks through natural language edits, which VidTune expands into new generations. In a controlled user study (N=12) and an exploratory case study (N=6), participants found VidTune helpful for efficiently reviewing and comparing music options and described the process as playful and enriching.",
      "url": "https://arxiv.org/abs/2601.12180",
      "pdfUrl": "https://arxiv.org/pdf/2601.12180.pdf",
      "titleJa": "VidTune: ジェネレーティブミュージックとコンテキストサムネイルを使ったビデオサウンドトラックの作成"
    },
    {
      "id": "2601.11968",
      "arxivId": "2601.11968",
      "title": "MuseAgent-1: Interactive Grounded Multimodal Understanding of Music Scores and Performance Audio",
      "authors": [
        "Qihao Zhao",
        "Yunqi Cao",
        "Yangyu Huang",
        "Hui Yi Leong",
        "Fan Zhang",
        "Kim-Hui Yap",
        "Wei Hu"
      ],
      "publishedDate": "2026-01-17",
      "categories": [
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Despite recent advances in multimodal large language models (MLLMs), their ability to understand and interact with music remains limited. Music understanding requires grounded reasoning over symbolic scores and expressive performance audio, which general-purpose MLLMs often fail to handle due to insufficient perceptual grounding. We introduce MuseAgent, a music-centric multimodal agent that augments language models with structured symbolic representations derived from sheet music images and performance audio. By integrating optical music recognition and automatic music transcription modules, MuseAgent enables multi-step reasoning and interaction over fine-grained musical content. To systematically evaluate music understanding capabilities, we further propose MuseBench, a benchmark covering music theory reasoning, score interpretation, and performance-level analysis across text, image, and audio modalities. Experiments show that existing MLLMs perform poorly on these tasks, while MuseAgent achieves substantial improvements, highlighting the importance of structured multimodal grounding for interactive music understanding.",
      "url": "https://arxiv.org/abs/2601.11968",
      "pdfUrl": "https://arxiv.org/pdf/2601.11968.pdf",
      "titleJa": "MuseAgent-1: 楽譜と演奏音声のインタラクティブなマルチモーダル理解"
    },
    {
      "id": "2601.11768",
      "arxivId": "2601.11768",
      "title": "Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music",
      "authors": [
        "Venkat Suprabath Bitra",
        "Homayoon Beigi"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "Reliable fundamental frequency (F 0) and voicing estimation is essential for neural synthesis, yet many pitch extractors depend on large labeled corpora and degrade under realistic recording artifacts. We propose a lightweight, fully self-supervised framework for joint F 0 estimation and voicing inference, designed for rapid single-instrument training from limited audio. Using transposition-equivariant learning on CQT features, we introduce an EM-style iterative reweighting scheme that uses Shift Cross-Entropy (SCE) consistency as a reliability signal to suppress uninformative noisy/unvoiced frames. The resulting weights provide confidence scores that enable pseudo-labeling for a separate lightweight voicing classifier without manual annotations. Trained on MedleyDB and evaluated on MDB-stem-synth ground truth, our method achieves competitive cross-corpus performance (RPA 95.84, RCA 96.24) and demonstrates cross-instrument generalization.",
      "url": "https://arxiv.org/abs/2601.11768",
      "pdfUrl": "https://arxiv.org/pdf/2601.11768.pdf",
      "titleJa": "モノフォニック音楽における基本周波数と正確な有声音確率の軽量自己教師検出"
    },
    {
      "id": "2601.14227",
      "arxivId": "2601.14227",
      "title": "Transformer Architectures for Respiratory Sound Analysis and Multimodal Diagnosis",
      "authors": [
        "Theodore Aptekarev",
        "Vladimir Sokolovsky",
        "Gregory Furman"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Respiratory sound analysis is a crucial tool for screening asthma and other pulmonary pathologies, yet traditional auscultation remains subjective and experience-dependent. Our prior research established a CNN baseline using DenseNet201, which demonstrated high sensitivity in classifying respiratory sounds. In this work, we (i) adapt the Audio Spectrogram Transformer (AST) for respiratory sound analysis and (ii) evaluate a multimodal Vision-Language Model (VLM) that integrates spectrograms with structured patient metadata. AST is initialized from publicly available weights and fine-tuned on a medical dataset containing hundreds of recordings per diagnosis. The VLM experiment uses a compact Moondream-type model that processes spectrogram images alongside a structured text prompt (sex, age, recording site) to output a JSON-formatted diagnosis. Results indicate that AST achieves approximately 97% accuracy with an F1-score around 97% and ROC AUC of 0.98 for asthma detection, significantly outperforming both the internal CNN baseline and typical external benchmarks. The VLM reaches 86-87% accuracy, performing comparably to the CNN baseline while demonstrating the capability to integrate clinical context into the inference process. These results confirm the effectiveness of self-attention for acoustic screening and highlight the potential of multimodal architectures for holistic diagnostic tools.",
      "url": "https://arxiv.org/abs/2601.14227",
      "pdfUrl": "https://arxiv.org/pdf/2601.14227.pdf",
      "titleJa": "呼吸音解析とマルチモーダル診断のためのトランスフォーマーアーキテクチャ"
    },
    {
      "id": "2601.13849",
      "arxivId": "2601.13849",
      "title": "Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control",
      "authors": [
        "Ziyi Yang",
        "Li Rao",
        "Zhengding Luo",
        "Dongyuan Shi",
        "Qirui Huang",
        "Woon-Seng Gan"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.LG",
        "eess.SP"
      ],
      "abstract": "Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train.",
      "url": "https://arxiv.org/abs/2601.13849",
      "pdfUrl": "https://arxiv.org/pdf/2601.13849.pdf",
      "titleJa": "能動騒音制御のためのメタ学習による制御フィルタと二次パスの共初期化"
    },
    {
      "id": "2601.13847",
      "arxivId": "2601.13847",
      "title": "Emotion and Acoustics Should Agree: Cross-Level Inconsistency Analysis for Audio Deepfake Detection",
      "authors": [
        "Jinhua Zhang",
        "Zhenqi Jia",
        "Rui Liu"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio Deepfake Detection (ADD) aims to detect spoof speech from bonafide speech. Most prior studies assume that stronger correlations within or across acoustic and emotional features imply authenticity, and thus focus on enhancing or measuring such correlations. However, existing methods often treat acoustic and emotional features in isolation or rely on correlation metrics, which overlook subtle desynchronization between them and smooth out abrupt discontinuities. To address these issues, we propose EAI-ADD, which treats cross level emotion acoustic inconsistency as the primary detection signal. We first project emotional and acoustic representations into a comparable space. Then we progressively integrate frame level and utterance level emotion features with acoustic features to capture cross level emotion acoustic inconsistencies across different temporal granularities. Experimental results on the ASVspoof 2019LA and 2021LA datasets demonstrate that the proposed EAI-ADD outperforms baselines, providing a more effective solution for audio anti spoofing detection.",
      "url": "https://arxiv.org/abs/2601.13847",
      "pdfUrl": "https://arxiv.org/pdf/2601.13847.pdf",
      "titleJa": "感情と音響は一致するはず：オーディオディープフェイク検出のためのクロスレベル不整合分析"
    },
    {
      "id": "2601.13679",
      "arxivId": "2601.13679",
      "title": "Ultra-Lightweight Network for Ship-Radiated Sound Classification on Embedded Deployment",
      "authors": [
        "Sangwon Park",
        "Dongjun Kim",
        "Sung-Hoon Byun",
        "Sangwook Park"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This letter presents ShuffleFAC, a lightweight acoustic model for ship-radiated sound classification in resource-constrained maritime monitoring systems. ShuffleFAC integrates Frequency-Aware convolution into an efficiency-oriented backbone using separable convolution, point-wise group convolution, and channel shuffle, enabling frequency-sensitive feature extraction with low computational cost. Experiments on the DeepShip dataset show that ShuffleFAC achieves competitive performance with substantially reduced complexity. In particular, ShuffleFAC ($γ=16$) attains a macro F1-score of 71.45 $\\pm$ 1.18% using 39K parameters and 3.06M MACs, and achieves an inference latency of 6.05 $\\pm$ 0.95ms on a Raspberry Pi. Compared with MicroNet0, it improves macro F1-score by 1.82 % while reducing model size by 9.7x and latency by 2.5x. These results indicate that ShuffleFAC is suitable for real-time embedded UATR.",
      "url": "https://arxiv.org/abs/2601.13679",
      "pdfUrl": "https://arxiv.org/pdf/2601.13679.pdf",
      "titleJa": "組み込み型船舶放射音分類用超軽量ネットワーク"
    },
    {
      "id": "2601.13589",
      "arxivId": "2601.13589",
      "title": "Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification",
      "authors": [
        "HyeYoung Lee"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.",
      "url": "https://arxiv.org/abs/2601.13589",
      "pdfUrl": "https://arxiv.org/pdf/2601.13589.pdf",
      "titleJa": "リアルタイム安全性検証を備えたマルチエージェントAIシステムによる動作応答コンテンツ生成"
    },
    {
      "id": "2601.13513",
      "arxivId": "2601.13513",
      "title": "Event Classification by Physics-informed Inpainting for Distributed Multichannel Acoustic Sensor with Partially Degraded Channels",
      "authors": [
        "Noriyuki Tonami",
        "Wataru Kohno",
        "Yoshiyuki Yajima",
        "Sakiko Mishima",
        "Yumi Arai",
        "Reishi Kondo",
        "Tomoyuki Hino"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Distributed multichannel acoustic sensing (DMAS) enables large-scale sound event classification (SEC), but performance drops when many channels are degraded and when sensor layouts at test time differ from training layouts. We propose a learning-free, physics-informed inpainting frontend based on reverse time migration (RTM). In this approach, observed multichannel spectrograms are first back-propagated on a 3D grid using an analytic Green's function to form a scene-consistent image, and then forward-projected to reconstruct inpainted signals before log-mel feature extraction and Transformer-based classification. We evaluate the method on ESC-50 with 50 sensors and three layouts (circular, linear, right-angle), where per-channel SNRs are sampled from -30 to 0 dB. Compared with an AST baseline, scaling-sparsemax channel selection, and channel-swap augmentation, the proposed RTM frontend achieves the best or competitive accuracy across all layouts, improving accuracy by 13.1 points on the right-angle layout (from 9.7% to 22.8%). Correlation analyses show that spatial weights align more strongly with SNR than with channel--source distance, and that higher SNR--weight correlation corresponds to higher SEC accuracy. These results demonstrate that a reconstruct-then-project, physics-based preprocessing effectively complements learning-only methods for DMAS under layout-open configurations and severe channel degradation.",
      "url": "https://arxiv.org/abs/2601.13513",
      "pdfUrl": "https://arxiv.org/pdf/2601.13513.pdf",
      "titleJa": "部分的に劣化したチャネルを持つ分散型マルチチャネル音響センサーのための物理学に基づくインペインティングによるイベント分類"
    },
    {
      "id": "2601.12752",
      "arxivId": "2601.12752",
      "title": "SoundPlot: An Open-Source Framework for Birdsong Acoustic Analysis and Neural Synthesis with Interactive 3D Visualization",
      "authors": [
        "Naqcho Ali Mehdi",
        "Mohammad Adeel",
        "Aizaz Ali Larik"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "We present SoundPlot, an open-source framework for analyzing avian vocalizations through acoustic feature extraction, dimensionality reduction, and neural audio synthesis. The system transforms audio signals into a multi-dimensional acoustic feature space, enabling real-time visualization of temporal dynamics in 3D using web-based interactive graphics. Our framework implements a complete analysis-synthesis pipeline that extracts spectral features (centroid, bandwidth, contrast), pitch contours via probabilistic YIN (pYIN), and mel-frequency cepstral coefficients (MFCCs), mapping them to a unified timbre space for visualization. Audio reconstruction employs the Griffin-Lim phase estimation algorithm applied to mel spectrograms. The accompanying Three.js-based interface provides dual-viewport visualization comparing original and synthesized audio trajectories with independent playback controls. We demonstrate the framework's capabilities through comprehensive waveform analysis, spectrogram comparisons, and feature space evaluation using Principal Component Analysis (PCA). Quantitative evaluation shows mel spectrogram correlation scores exceeding 0.92, indicating high-fidelity preservation of perceptual acoustic structure. SoundPlot is released under the MIT License to facilitate research in bioacoustics, audio signal processing, and computational ethology.",
      "url": "https://arxiv.org/abs/2601.12752",
      "pdfUrl": "https://arxiv.org/pdf/2601.12752.pdf",
      "titleJa": "SoundPlot: インタラクティブな3D可視化による鳥のさえずりの音響分析とニューラルネットワーク合成のためのオープンソースフレームワーク"
    },
    {
      "id": "2601.12660",
      "arxivId": "2601.12660",
      "title": "Toward Faithful Explanations in Acoustic Anomaly Detection",
      "authors": [
        "Maab Elrashid",
        "Anthony Deschênes",
        "Cem Subakan",
        "Mirco Ravanelli",
        "Rémi Georges",
        "Michael Morin"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Interpretability is essential for user trust in real-world anomaly detection applications. However, deep learning models, despite their strong performance, often lack transparency. In this work, we study the interpretability of autoencoder-based models for audio anomaly detection, by comparing a standard autoencoder (AE) with a mask autoencoder (MAE) in terms of detection performance and interpretability. We applied several attribution methods, including error maps, saliency maps, SmoothGrad, Integrated Gradients, GradSHAP, and Grad-CAM. Although MAE shows a slightly lower detection, it consistently provides more faithful and temporally precise explanations, suggesting a better alignment with true anomalies. To assess the relevance of the regions highlighted by the explanation method, we propose a perturbation-based faithfulness metric that replaces them with their reconstructions to simulate normal input. Our findings, based on experiments in a real industrial scenario, highlight the importance of incorporating interpretability into anomaly detection pipelines and show that masked training improves explanation quality without compromising performance.",
      "url": "https://arxiv.org/abs/2601.12660",
      "pdfUrl": "https://arxiv.org/pdf/2601.12660.pdf",
      "titleJa": "音響異常検知における忠実な説明に向けて"
    },
    {
      "id": "2601.12600",
      "arxivId": "2601.12600",
      "title": "SSVD-O: Parameter-Efficient Fine-Tuning with Structured SVD for Speech Recognition",
      "authors": [
        "Pu Wang",
        "Shinji Watanabe",
        "Hugo Van hamme"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) is a scalable approach for adapting large speech foundation models to new domains. While methods such as LoRA and its state-of-the-art variants reduce adaptation costs, they typically allocate parameters uniformly across model subspaces, which limits their efficiency and scalability in speech applications. Building on our prior work, this paper introduces SSVD-Outer (SSVD-O), an extension of the structured SVD-guided (SSVD) fine-tuning method. SSVD-O combines input acoustic feature space-associated inner transformations with output semantic feature space-associated outer transformations to enable scalable and balanced adaptation. We conduct the first systematic analysis of parameter budget allocation across model subspaces in PEFT for automatic speech recognition (ASR), and investigate the trade-off between learning and forgetting under constrained resources. SSVD-O is benchmarked against LoRA, DoRA, PiSSA, and SSVD on domain-shifted ASR tasks, including child speech and regional accents, across model scales from 0.1B to 2B within the ESPnet framework. Experimental results show that SSVD-O consistently narrows the performance gap to full fine-tuning while improving generalization and mitigating catastrophic forgetting.",
      "url": "https://arxiv.org/abs/2601.12600",
      "pdfUrl": "https://arxiv.org/pdf/2601.12600.pdf",
      "titleJa": "SSVD-O: 音声認識のための構造化SVDによるパラメータ効率の高い微調整"
    },
    {
      "id": "2601.12494",
      "arxivId": "2601.12494",
      "title": "Harmonizing the Arabic Audio Space with Data Scheduling",
      "authors": [
        "Hunzalah Hassan Bhatti",
        "Firoj Alam",
        "Shammur Absar Chowdhury"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments.",
      "url": "https://arxiv.org/abs/2601.12494",
      "pdfUrl": "https://arxiv.org/pdf/2601.12494.pdf",
      "titleJa": "データスケジューリングによるアラビア語オーディオ空間の調和"
    },
    {
      "id": "2601.12480",
      "arxivId": "2601.12480",
      "title": "A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation",
      "authors": [
        "Hanchen Pei",
        "Shujie Liu",
        "Yanqing Liu",
        "Jianwei Yu",
        "Yuanhang Qian",
        "Gongping Huang",
        "Sheng Zhao",
        "Yan Lu"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Neural codec language models achieve impressive zero-shot Text-to-Speech (TTS) by fully imitating the acoustic characteristics of a short speech prompt, including timbre, prosody, and paralinguistic information. However, such holistic imitation limits their ability to isolate and control individual attributes. In this paper, we present a unified codec language model SpeechEdit that extends zero-shot TTS with a selective control mechanism. By default, SpeechEdit reproduces the complete acoustic profile inferred from the speech prompt, but it selectively overrides only the attributes specified by explicit control instructions. To enable controllable modeling, SpeechEdit is trained on our newly constructed LibriEdit dataset, which provides delta (difference-aware) training pairs derived from LibriHeavy. Experimental results show that our approach maintains naturalness and robustness while offering flexible and localized control over desired attributes. Audio samples are available at https://speech-editing.github.io/speech-editing/.",
      "url": "https://arxiv.org/abs/2601.12480",
      "pdfUrl": "https://arxiv.org/pdf/2601.12480.pdf",
      "titleJa": "選択的に編集可能なテキスト音声生成のための統合ニューラルコーデック言語モデル"
    },
    {
      "id": "2601.12354",
      "arxivId": "2601.12354",
      "title": "Bone-conduction Guided Multimodal Speech Enhancement with Conditional Diffusion Models",
      "authors": [
        "Sina Khanagha",
        "Bunlong Lay",
        "Timo Gerkmann"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Single-channel speech enhancement models face significant performance degradation in extremely noisy environments. While prior work has shown that complementary bone-conducted speech can guide enhancement, effective integration of this noise-immune modality remains a challenge. This paper introduces a novel multimodal speech enhancement framework that integrates bone-conduction sensors with air-conducted microphones using a conditional diffusion model. Our proposed model significantly outperforms previously established multimodal techniques and a powerful diffusion-based single-modal baseline across a wide range of acoustic conditions.",
      "url": "https://arxiv.org/abs/2601.12354",
      "pdfUrl": "https://arxiv.org/pdf/2601.12354.pdf",
      "titleJa": "条件付き拡散モデルを用いた骨伝導誘導マルチモーダル音声強調"
    },
    {
      "id": "2601.12345",
      "arxivId": "2601.12345",
      "title": "Adaptive Rotary Steering with Joint Autoregression for Robust Extraction of Closely Moving Speakers in Dynamic Scenarios",
      "authors": [
        "Jakob Kienegger",
        "Timo Gerkmann"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Latest advances in deep spatial filtering for Ambisonics demonstrate strong performance in stationary multi-speaker scenarios by rotating the sound field toward a target speaker prior to multi-channel enhancement. For applicability in dynamic acoustic conditions with moving speakers, we propose to automate this rotary steering using an interleaved tracking algorithm conditioned on the target's initial direction. However, for nearby or crossing speakers, robust tracking becomes difficult and spatial cues less effective for enhancement. By incorporating the processed recording as additional guide into both algorithms, our novel joint autoregressive framework leverages temporal-spectral correlations of speech to resolve spatially challenging speaker constellations. Consequently, our proposed method significantly improves tracking and enhancement of closely spaced speakers, consistently outperforming comparable non-autoregressive methods on a synthetic dataset. Real-world recordings complement these findings in complex scenarios with multiple speaker crossings and varying speaker-to-array distances.",
      "url": "https://arxiv.org/abs/2601.12345",
      "pdfUrl": "https://arxiv.org/pdf/2601.12345.pdf",
      "titleJa": "動的シナリオにおける近接して移動する話者のロバストな抽出のためのジョイント自己回帰を用いた適応型ロータリーステアリング"
    },
    {
      "id": "2601.12203",
      "arxivId": "2601.12203",
      "title": "Embryonic Exposure to VPA Influences Chick Vocalisations: A Computational Study",
      "authors": [
        "Antonella M. C. Torrisi",
        "Inês Nolasco",
        "Paola Sgadò",
        "Elisabetta Versace",
        "Emmanouil Benetos"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In young animals like poultry chicks (Gallus gallus), vocalisations convey information about affective and behavioural states. Traditional approaches to vocalisation analysis, relying on manual annotation and predefined categories, introduce biases, limit scalability, and fail to capture the full complexity of vocal repertoires. We introduce a computational framework for the automated detection, acoustic feature extraction, and unsupervised learning of chick vocalisations. Applying this framework to a dataset of newly hatched chicks, we identified two primary vocal clusters. We then tested our computational framework on an independent dataset of chicks exposed during embryonic development to vehicle or Valproic Acid (VPA), a compound that disrupts neural development and is linked to autistic-like symptoms. Clustering analysis on the experimental dataset confirmed two primary vocal clusters and revealed systematic differences between groups. VPA-exposed chicks showed an altered repertoire, with a relative increase in softer calls. VPA differentially affected call clusters, modulating temporal, frequency, and energy domain features. Overall, VPA-exposed chicks produced vocalisations with shorter duration, reduced pitch variability, and modified energy profiles, with the strongest alterations observed in louder calls. This study provides a computational framework for analysing animal vocalisations, advancing knowledge of early-life communication in typical and atypical vocal development.",
      "url": "https://arxiv.org/abs/2601.12203",
      "pdfUrl": "https://arxiv.org/pdf/2601.12203.pdf",
      "titleJa": "胎児期のVPA曝露がニワトリの発声に影響を与える：計算論的研究"
    },
    {
      "id": "2601.12153",
      "arxivId": "2601.12153",
      "title": "A Survey on 30+ Years of Automatic Singing Assessment and Singing Information Processing",
      "authors": [
        "Arthur N. dos Santos",
        "Bruno S. Masiero"
      ],
      "publishedDate": "2026-01-17",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Automatic Singing Assessment and Singing Information Processing have evolved over the past three decades to support singing pedagogy, performance analysis, and vocal training. While the first approach objectively evaluates a singer's performance through computational metrics ranging from real-time visual feedback and acoustical biofeedback to sophisticated pitch tracking and spectral analysis, the latter method compares a predictor vocal signal with a target reference to capture nuanced data embedded in the singing voice. Notable advancements include the development of interactive systems that have significantly improved real-time visual feedback, and the integration of machine learning and deep neural network architectures that enhance the precision of vocal signal processing. This survey critically examines the literature to map the historical evolution of these technologies, while identifying and discussing key gaps. The analysis reveals persistent challenges, such as the lack of standardized evaluation frameworks, difficulties in reliably separating vocal signals from various noise sources, and the underutilization of advanced digital signal processing and artificial intelligence methodologies for capturing artistic expressivity. By detailing these limitations and the corresponding technological advances, this review demonstrates how addressing these issues can bridge the gap between objective computational assessments and subjective human-like evaluations of singing performance, ultimately enhancing both the technical accuracy and pedagogical relevance of automated singing evaluation systems.",
      "url": "https://arxiv.org/abs/2601.12153",
      "pdfUrl": "https://arxiv.org/pdf/2601.12153.pdf",
      "titleJa": "30年以上にわたる自動歌唱評価と歌唱情報処理に関する調査"
    }
  ],
  "lastUpdated": "2026-01-24T00:53:02.077490",
  "totalCount": 80
}