{
  "papers": [
    {
      "id": "2602.03817",
      "arxivId": "2602.03817",
      "title": "Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion",
      "authors": [
        "Oscar Ovanger",
        "Levi Harris",
        "Timothy H. Keitt"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce \\textbf{F}usion under \\textbf{IN}dependent \\textbf{C}onditional \\textbf{H}ypotheses (\\textbf{FINCH}), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family \\emph{contains} the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \\texttt{\\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md}{anonymous-repository}}",
      "url": "https://arxiv.org/abs/2602.03817",
      "pdfUrl": "https://arxiv.org/pdf/2602.03817.pdf",
      "titleJa": "音声時空間融合のための適応的証拠重み付け"
    },
    {
      "id": "2602.03624",
      "arxivId": "2602.03624",
      "title": "A Multi-decoder Neural Tracking Method for Accurately Predicting Speech Intelligibility",
      "authors": [
        "Rien Sonck",
        "Bernd Accou",
        "Tom Francart",
        "Jonas Vanthornhout"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "eess.SP",
        "cs.SD"
      ],
      "abstract": "Objective: EEG-based methods can predict speech intelligibility, but their accuracy and robustness lag behind behavioral tests, which typically show test-retest differences under 1 dB. We introduce the multi-decoder method to predict speech reception thresholds (SRTs) from EEG recordings, enabling objective assessment for populations unable to perform behavioral tests; such as those with disorders of consciousness or during hearing aid fitting. Approach: The method aggregates data from hundreds of decoders, each trained on different speech features and EEG preprocessing setups to quantify neural tracking (NT) of speech signals. Using data from 39 participants (ages 18-24), we recorded 29 minutes of EEG per person while they listened to speech at six signal-to-noise ratios and a quiet story. NT values were combined into a high-dimensional feature vector per subject, and a support vector regression model was trained to predict SRTs from these vectors. Main Result: Predictions correlated significantly with behavioral SRTs (r = 0.647, p < 0.001; NRMSE = 0.19), with all differences under 1 dB. SHAP analysis showed theta/delta bands and early lags had slightly greater influence. Using pretrained subject-independent decoders reduced required EEG data collection to 15 minutes (3 minutes of story, 12 minutes across six SNR conditions) without losing accuracy.",
      "url": "https://arxiv.org/abs/2602.03624",
      "pdfUrl": "https://arxiv.org/pdf/2602.03624.pdf",
      "titleJa": "音声明瞭度を正確に予測するマルチデコーダーニューラルトラッキング法"
    },
    {
      "id": "2602.03549",
      "arxivId": "2602.03549",
      "title": "EarResp-ANS : Audio-Based On-Device Respiration Rate Estimation on Earphones with Adaptive Noise Suppression",
      "authors": [
        "Michael Küttner",
        "Valeria Zitz",
        "Supraja Ramesh",
        "Michael Beigl",
        "Tobias Röddiger"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.HC"
      ],
      "abstract": "Respiratory rate (RR) is a key vital sign for clinical assessment and mental well-being, yet it is rarely monitored in everyday life due to the lack of unobtrusive sensing technologies. In-ear audio sensing is promising due to its high social acceptance and the amplification of physiological sounds caused by the occlusion effect; however, existing approaches often fail under real-world noise or rely on computationally expensive models. We present EarResp-ANS, the first system enabling fully on-device, real-time RR estimation on commercial earphones. The system employs LMS-based adaptive noise suppression (ANS) to attenuate ambient noise while preserving respiration-related acoustic components, without requiring neural networks or audio streaming, thereby explicitly addressing the energy and privacy constraints of wearable devices. We evaluate EarResp-ANS in a study with 18 participants under realistic acoustic conditions, including music, cafeteria noise, and white noise up to 80 dB SPL. EarResp-ANS achieves robust performance with a global MAE of 0.84 CPM , reduced to 0.47 CPM via automatic outlier rejection, while operating with less than 2% processor load directly on the earphone.",
      "url": "https://arxiv.org/abs/2602.03549",
      "pdfUrl": "https://arxiv.org/pdf/2602.03549.pdf",
      "titleJa": "EarResp-ANS：適応型ノイズ抑制機能を備えたイヤホンにおける音声ベースのデバイス内呼吸数推定"
    },
    {
      "id": "2602.03523",
      "arxivId": "2602.03523",
      "title": "D3PIA: A Discrete Denoising Diffusion Model for Piano Accompaniment Generation From Lead sheet",
      "authors": [
        "Eunjin Choi",
        "Hounsu Kim",
        "Hayeon Bang",
        "Taegyun Kwon",
        "Juhan Nam"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM"
      ],
      "abstract": "Generating piano accompaniments in the symbolic music domain is a challenging task that requires producing a complete piece of piano music from given melody and chord constraints, such as those provided by a lead sheet. In this paper, we propose a discrete diffusion-based piano accompaniment generation model, D3PIA, leveraging local alignment between lead sheet and accompaniment in piano-roll representation. D3PIA incorporates Neighborhood Attention (NA) to both encode the lead sheet and condition it for predicting note states in the piano accompaniment. This design enhances local contextual modeling by efficiently attending to nearby melody and chord conditions. We evaluate our model using the POP909 dataset, a widely used benchmark for piano accompaniment generation. Objective evaluation results demonstrate that D3PIA preserves chord conditions more faithfully compared to continuous diffusion-based and Transformer-based baselines. Furthermore, a subjective listening test indicates that D3PIA generates more musically coherent accompaniments than the comparison models.",
      "url": "https://arxiv.org/abs/2602.03523",
      "pdfUrl": "https://arxiv.org/pdf/2602.03523.pdf",
      "titleJa": "D3PIA: リードシートからのピアノ伴奏生成のための離散ノイズ除去拡散モデル"
    },
    {
      "id": "2602.03420",
      "arxivId": "2602.03420",
      "title": "CoCoEmo: Composable and Controllable Human-Like Emotional TTS via Activation Steering",
      "authors": [
        "Siyi Wang",
        "Shihong Tan",
        "Siyi Liu",
        "Hong Jia",
        "Gongping Huang",
        "James Bailey",
        "Ting Dang"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Emotional expression in human speech is nuanced and compositional, often involving multiple, sometimes conflicting, affective cues that may diverge from linguistic content. In contrast, most expressive text-to-speech systems enforce a single utterance-level emotion, collapsing affective diversity and suppressing mixed or text-emotion-misaligned expression. While activation steering via latent direction vectors offers a promising solution, it remains unclear whether emotion representations are linearly steerable in TTS, where steering should be applied within hybrid TTS architectures, and how such complex emotion behaviors should be evaluated. This paper presents the first systematic analysis of activation steering for emotional control in hybrid TTS models, introducing a quantitative, controllable steering framework, and multi-rater evaluation protocols that enable composable mixed-emotion synthesis and reliable text-emotion mismatch synthesis. Our results demonstrate, for the first time, that emotional prosody and expressive variability are primarily synthesized by the TTS language module instead of the flow-matching module, and also provide a lightweight steering approach for generating natural, human-like emotional speech.",
      "url": "https://arxiv.org/abs/2602.03420",
      "pdfUrl": "https://arxiv.org/pdf/2602.03420.pdf",
      "titleJa": "CoCoEmo: アクティベーションステアリングによる構成可能かつ制御可能な人間のような感情表現TTS"
    },
    {
      "id": "2602.03355",
      "arxivId": "2602.03355",
      "title": "PACE: Pretrained Audio Continual Learning",
      "authors": [
        "Chang Li",
        "Kanglei Zhou",
        "Liyuan Wang"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Audio is a fundamental modality for analyzing speech, music, and environmental sounds. Although pretrained audio models have significantly advanced audio understanding, they remain fragile in real-world settings where data distributions shift over time. In this work, we present the first systematic benchmark for audio continual learning (CL) with pretrained models (PTMs), together with a comprehensive analysis of its unique challenges. Unlike in vision, where parameter-efficient fine-tuning (PEFT) has proven effective for CL, directly transferring such strategies to audio leads to poor performance. This stems from a fundamental property of audio backbones: they focus on low-level spectral details rather than structured semantics, causing severe upstream-downstream misalignment. Through extensive empirical study, we identify analytic classifiers with first-session adaptation (FSA) as a promising direction, but also reveal two major limitations: representation saturation in coarse-grained scenarios and representation drift in fine-grained scenarios. To address these challenges, we propose PACE, a novel method that enhances FSA via a regularized analytic classifier and enables multi-session adaptation through adaptive subspace-orthogonal PEFT for improved semantic alignment. In addition, we introduce spectrogram-based boundary-aware perturbations to mitigate representation overlap and improve stability. Experiments on six diverse audio CL benchmarks demonstrate that PACE substantially outperforms state-of-the-art baselines, marking an important step toward robust and scalable audio continual learning with PTMs.",
      "url": "https://arxiv.org/abs/2602.03355",
      "pdfUrl": "https://arxiv.org/pdf/2602.03355.pdf",
      "titleJa": "PACE: 事前学習済み音声継続学習"
    },
    {
      "id": "2602.03307",
      "arxivId": "2602.03307",
      "title": "GRAM: Spatial general-purpose audio representations for real-world environments",
      "authors": [
        "Goksenin Yuksel",
        "Marcel van Gerven",
        "Kiki van der Heijden"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio foundation models learn general-purpose audio representations that facilitate a wide range of downstream tasks. While the performance of these models has greatly increased for conventional single-channel, dry audio clips, their success in real-world acoustic environments with reverberation and noise is limited. Furthermore, most audio foundation models ignore the spatial dimension of real-world acoustic environments, ruling out tasks involving sound localization. To address these limitations, we propose GRAM: a general-purpose real-world audio model that employs a multi-channel masked autoencoder to efficiently learn spatial audio representations. We evaluated GRAM and other audio foundation models in a standardized manner on high-quality simulations of naturalistic, spatial acoustic environments as well as recordings of real-world environments and release these two complementary benchmark task suites: NatHEAR and RealSELD. Our results demonstrate that GRAM outperforms all state-of-the-art self-supervised audio foundation models on NatHEAR and the clean, single-channel version HEAR, while using only a fraction of the training data. GRAM also shows state-of-the-art localization performance in simulated environments and generalizes efficiently to real-world recordings in RealSELD. Taken together, GRAM presents a significant advance toward robust spatial audio foundation models for real-world environments.",
      "url": "https://arxiv.org/abs/2602.03307",
      "pdfUrl": "https://arxiv.org/pdf/2602.03307.pdf",
      "titleJa": "GRAM: 現実世界環境のための空間汎用オーディオ表現"
    },
    {
      "id": "2602.03023",
      "arxivId": "2602.03023",
      "title": "Rethinking Music Captioning with Music Metadata LLMs",
      "authors": [
        "Irmak Bukey",
        "Zhepei Wang",
        "Chris Donahue",
        "Nicholas J. Bryan"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Music captioning, or the task of generating a natural language description of music, is useful for both music understanding and controllable music generation. Training captioning models, however, typically requires high-quality music caption data which is scarce compared to metadata (e.g., genre, mood, etc.). As a result, it is common to use large language models (LLMs) to synthesize captions from metadata to generate training data for captioning models, though this process imposes a fixed stylization and entangles factual information with natural language style. As a more direct approach, we propose metadata-based captioning. We train a metadata prediction model to infer detailed music metadata from audio and then convert it into expressive captions via pre-trained LLMs at inference time. Compared to a strong end-to-end baseline trained on LLM-generated captions derived from metadata, our method: (1) achieves comparable performance in less training time over end-to-end captioners, (2) offers flexibility to easily change stylization post-training, enabling output captions to be tailored to specific stylistic and quality requirements, and (3) can be prompted with audio and partial metadata to enable powerful metadata imputation or in-filling--a common task for organizing music data.",
      "url": "https://arxiv.org/abs/2602.03023",
      "pdfUrl": "https://arxiv.org/pdf/2602.03023.pdf",
      "titleJa": "音楽メタデータLLMによる音楽キャプションの再考"
    },
    {
      "id": "2602.02955",
      "arxivId": "2602.02955",
      "title": "Synthetic Data Augmentation for Medical Audio Classification: A Preliminary Evaluation",
      "authors": [
        "David McShannon",
        "Anthony Mella",
        "Nicholas Dietrich"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Medical audio classification remains challenging due to low signal-to-noise ratios, subtle discriminative features, and substantial intra-class variability, often compounded by class imbalance and limited training data. Synthetic data augmentation has been proposed as a potential strategy to mitigate these constraints; however, prior studies report inconsistent methodological approaches and mixed empirical results. In this preliminary study, we explore the impact of synthetic augmentation on respiratory sound classification using a baseline deep convolutional neural network trained on a moderately imbalanced dataset (73%:27%). Three generative augmentation strategies (variational autoencoders, generative adversarial networks, and diffusion models) were assessed under controlled experimental conditions. The baseline model without augmentation achieved an F1-score of 0.645. Across individual augmentation strategies, performance gains were not observed, with several configurations demonstrating neutral or degraded classification performance. Only an ensemble of augmented models yielded a modest improvement in F1-score (0.664). These findings suggest that, for medical audio classification, synthetic augmentation may not consistently enhance performance when applied to a standard CNN classifier. Future work should focus on delineating task-specific data characteristics, model-augmentation compatibility, and evaluation frameworks necessary for synthetic augmentation to be effective in medical audio applications.",
      "url": "https://arxiv.org/abs/2602.02955",
      "pdfUrl": "https://arxiv.org/pdf/2602.02955.pdf",
      "titleJa": "医療用音声分類のための合成データ拡張：予備評価"
    },
    {
      "id": "2602.02738",
      "arxivId": "2602.02738",
      "title": "When Noise Lowers The Loss: Rethinking Likelihood-Based Evaluation in Music Large Language Models",
      "authors": [
        "Xiaosha Li",
        "Chun Liu",
        "Ziyu Wang"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "The rise of music large language models (LLMs) demands robust methods of evaluating output quality, especially in distinguishing high-quality compositions from \"garbage music\". Curiously, we observe that the standard cross-entropy loss -- a core training metric -- often decrease when models encounter systematically corrupted music, undermining its validity as a standalone quality indicator. To investigate this paradox, we introduce noise injection experiment, where controlled noise signal of varying lengths are injected into musical contexts. We hypothesize that a model's loss reacting positively to these perturbations, specifically a sharp increase (\"Peak\" area) for short injection, can serve as a proxy for its ability to discern musical integrity. Experiments with MusicGen models in the audio waveform domain confirm that Music LLMs respond more strongly to local, texture-level disruptions than to global semantic corruption. Beyond exposing this bias, our results highlight a new principle: the shape of the loss curve -- rather than its absolute value -- encodes critical information about the quality of the generated content (i.e., model behavior). We envision this profile-based evaluation as a label-free, model-intrinsic framework for assessing musical quality -- opening the door to more principled training objectives and sharper benchmarks.",
      "url": "https://arxiv.org/abs/2602.02738",
      "pdfUrl": "https://arxiv.org/pdf/2602.02738.pdf",
      "titleJa": "ノイズが損失を低下させるとき：音楽大規模言語モデルにおける尤度ベースの評価の再考"
    },
    {
      "id": "2602.02725",
      "arxivId": "2602.02725",
      "title": "Automated Dysphagia Screening Using Noninvasive Neck Acoustic Sensing",
      "authors": [
        "Jade Chng",
        "Rong Xing",
        "Yunfei Luo",
        "Kristen Linnemeyer-Risser",
        "Tauhidur Rahman",
        "Andrew Yousef",
        "Philip A Weissbrod"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "Pharyngeal health plays a vital role in essential human functions such as breathing, swallowing, and vocalization. Early detection of swallowing abnormalities, also known as dysphagia, is crucial for timely intervention. However, current diagnostic methods often rely on radiographic imaging or invasive procedures. In this study, we propose an automated framework for detecting dysphagia using portable and noninvasive acoustic sensing coupled with applied machine learning. By capturing subtle acoustic signals from the neck during swallowing tasks, we aim to identify patterns associated with abnormal physiological conditions. Our approach achieves promising test-time abnormality detection performance, with an AUC-ROC of 0.904 under 5 independent train-test splits. This work demonstrates the feasibility of using noninvasive acoustic sensing as a practical and scalable tool for pharyngeal health monitoring.",
      "url": "https://arxiv.org/abs/2602.02725",
      "pdfUrl": "https://arxiv.org/pdf/2602.02725.pdf",
      "titleJa": "非侵襲性頸部音響センシングを用いた自動嚥下障害スクリーニング"
    },
    {
      "id": "2602.02413",
      "arxivId": "2602.02413",
      "title": "Masked Autoencoders as Universal Speech Enhancer",
      "authors": [
        "Rajalaxmi Rajagopalan",
        "Ritwik Giri",
        "Zhiqiang Tang",
        "Kyu Han"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Supervised speech enhancement methods have been very successful. However, in practical scenarios, there is a lack of clean speech, and self-supervised learning-based (SSL) speech enhancement methods that offer comparable enhancement performance and can be applied to other speech-related downstream applications are desired. In this work, we develop a masked autoencoder based universal speech enhancer that is agnostic to the type of distortion affecting speech, can handle multiple distortions simultaneously, and is trained in a self-supervised manner. An augmentation stack adds further distortions to the noisy input data. The masked autoencoder model learns to remove the added distortions along with reconstructing the masked regions of the spectrogram during pre-training. The pre-trained embeddings are then used by fine-tuning models trained on a small amount of paired data for specific downstream tasks. We evaluate the pre-trained features for denoising and dereverberation downstream tasks. We explore different augmentations (like single or multi-speaker) in the pre-training augmentation stack and the effect of different noisy input feature representations (like $log1p$ compression) on pre-trained embeddings and downstream fine-tuning enhancement performance. We show that the proposed method not only outperforms the baseline but also achieves state-of-the-art performance for both in-domain and out-of-domain evaluation datasets.",
      "url": "https://arxiv.org/abs/2602.02413",
      "pdfUrl": "https://arxiv.org/pdf/2602.02413.pdf",
      "titleJa": "ユニバーサルスピーチエンハンサーとしてのマスクされたオートエンコーダー"
    },
    {
      "id": "2602.02286",
      "arxivId": "2602.02286",
      "title": "DFKI-Speech System for WildSpoof Challenge: A robust framework for SASV In-the-Wild",
      "authors": [
        "Arnab Das",
        "Yassine El Kheir",
        "Enes Erdem Erdogan",
        "Feidi Kallel",
        "Tim Polzehl",
        "Sebastian Moeller"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "This paper presents the DFKI-Speech system developed for the WildSpoof Challenge under the Spoofing aware Automatic Speaker Verification (SASV) track. We propose a robust SASV framework in which a spoofing detector and a speaker verification (SV) network operate in tandem. The spoofing detector employs a self-supervised speech embedding extractor as the frontend, combined with a state-of-the-art graph neural network backend. In addition, a top-3 layer based mixture-of-experts (MoE) is used to fuse high-level and low-level features for effective spoofed utterance detection. For speaker verification, we adapt a low-complexity convolutional neural network that fuses 2D and 1D features at multiple scales, trained with the SphereFace loss. Additionally, contrastive circle loss is applied to adaptively weight positive and negative pairs within each training batch, enabling the network to better distinguish between hard and easy sample pairs. Finally, fixed imposter cohort based AS Norm score normalization and model ensembling are used to further enhance the discriminative capability of the speaker verification system.",
      "url": "https://arxiv.org/abs/2602.02286",
      "pdfUrl": "https://arxiv.org/pdf/2602.02286.pdf",
      "titleJa": "WildSpoofチャレンジのためのDFKI音声システム：SASV In-the-Wildのための堅牢なフレームワーク"
    },
    {
      "id": "2602.02249",
      "arxivId": "2602.02249",
      "title": "Evaluating Acoustic Data Transmission Schemes for Ad-Hoc Communication Between Nearby Smart Devices",
      "authors": [
        "Florentin Putz",
        "Philipp Fortmann",
        "Jan Frank",
        "Christoph Haugwitz",
        "Mario Kupnik",
        "Matthias Hollick"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.NI",
        "cs.SD"
      ],
      "abstract": "Acoustic data transmission offers a compelling alternative to Bluetooth and NFC by leveraging the ubiquitous speakers and microphones in smartphones and IoT devices. However, most research in this field relies on simulations or limited on-device testing, which makes the real-world reliability of proposed schemes difficult to assess. We systematically reviewed 31 acoustic communication studies for commodity devices and found that none provided accessible source code. After contacting authors and re-implementing three promising schemes, we assembled a testbed of eight representative acoustic communication systems. Using over 11000 smartphone transmissions in both realistic indoor environments and an anechoic chamber, we provide a systematic and repeatable methodology for evaluating the reliability and generalizability of these schemes under real-world conditions. Our results show that many existing schemes face challenges in practical usage, largely due to severe multipath propagation indoors and varying audio characteristics across device models. To support future research and foster more robust evaluations, we release our re-implementations alongside the first comprehensive dataset of real-world acoustic transmissions. Overall, our findings highlight the importance of rigorous on-device testing and underscore the need for robust design strategies to bridge the gap between simulation results and reliable IoT deployments.",
      "url": "https://arxiv.org/abs/2602.02249",
      "pdfUrl": "https://arxiv.org/pdf/2602.02249.pdf",
      "titleJa": "近くのスマートデバイス間のアドホック通信のための音響データ伝送方式の評価"
    },
    {
      "id": "2602.01908",
      "arxivId": "2602.01908",
      "title": "LipSody: Lip-to-Speech Synthesis with Enhanced Prosody Consistency",
      "authors": [
        "Jaejun Lee",
        "Yoori Oh",
        "Kyogu Lee"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Lip-to-speech synthesis aims to generate speech audio directly from silent facial video by reconstructing linguistic content from lip movements, providing valuable applications in situations where audio signals are unavailable or degraded. While recent diffusion-based models such as LipVoicer have demonstrated impressive performance in reconstructing linguistic content, they often lack prosodic consistency. In this work, we propose LipSody, a lip-to-speech framework enhanced for prosody consistency. LipSody introduces a prosody-guiding strategy that leverages three complementary cues: speaker identity extracted from facial images, linguistic content derived from lip movements, and emotional context inferred from face video. Experimental results demonstrate that LipSody substantially improves prosody-related metrics, including global and local pitch deviations, energy consistency, and speaker similarity, compared to prior approaches.",
      "url": "https://arxiv.org/abs/2602.01908",
      "pdfUrl": "https://arxiv.org/pdf/2602.01908.pdf",
      "titleJa": "LipSody: 韻律の一貫性を強化した唇音声合成"
    },
    {
      "id": "2602.01879",
      "arxivId": "2602.01879",
      "title": "Speaking Without Sound: Multi-speaker Silent Speech Voicing with Facial Inputs Only",
      "authors": [
        "Jaejun Lee",
        "Yoori Oh",
        "Kyogu Lee"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In this paper, we introduce a novel framework for generating multi-speaker speech without relying on any audible inputs. Our approach leverages silent electromyography (EMG) signals to capture linguistic content, while facial images are used to match with the vocal identity of the target speaker. Notably, we present a pitch-disentangled content embedding that enhances the extraction of linguistic content from EMG signals. Extensive analysis demonstrates that our method can generate multi-speaker speech without any audible inputs and confirms the effectiveness of the proposed pitch-disentanglement approach.",
      "url": "https://arxiv.org/abs/2602.01879",
      "pdfUrl": "https://arxiv.org/pdf/2602.01879.pdf",
      "titleJa": "音なしで話す：顔入力のみによるマルチスピーカーサイレントスピーチ音声化"
    },
    {
      "id": "2602.01793",
      "arxivId": "2602.01793",
      "title": "ParaGSE: Parallel Generative Speech Enhancement with Group-Vector-Quantization-based Neural Speech Codec",
      "authors": [
        "Fei Liu",
        "Yang Ai"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Recently, generative speech enhancement has garnered considerable interest; however, existing approaches are hindered by excessive complexity, limited efficiency, and suboptimal speech quality. To overcome these challenges, this paper proposes a novel parallel generative speech enhancement (ParaGSE) framework that leverages a group vector quantization (GVQ)-based neural speech codec. The GVQ-based codec adopts separate VQs to produce mutually independent tokens, enabling efficient parallel token prediction in ParaGSE. Specifically, ParaGSE leverages the GVQ-based codec to encode degraded speech into distinct tokens, predicts the corresponding clean tokens through parallel branches conditioned on degraded spectral features, and ultimately reconstructs clean speech via the codec decoder. Experimental results demonstrate that ParaGSE consistently produces superior enhanced speech compared to both discriminative and generative baselines, under a wide range of distortions including noise, reverberation, band-limiting, and their mixtures. Furthermore, empowered by parallel computation in token prediction, ParaGSE attains about a 1.5-fold improvement in generation efficiency on CPU compared with serial generative speech enhancement approaches.",
      "url": "https://arxiv.org/abs/2602.01793",
      "pdfUrl": "https://arxiv.org/pdf/2602.01793.pdf",
      "titleJa": "ParaGSE: グループベクトル量子化ベースのニューラル音声コーデックによる並列生成音声拡張"
    },
    {
      "id": "2602.01727",
      "arxivId": "2602.01727",
      "title": "Voting-based Pitch Estimation with Temporal and Frequential Alignment and Correlation Aware Selection",
      "authors": [
        "Junya Koguchi",
        "Tomoki Koriyama"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD"
      ],
      "abstract": "The voting method, an ensemble approach for fundamental frequency estimation, is empirically known for its robustness but lacks thorough investigation. This paper provides a principled analysis and improvement of this technique. First, we offer a theoretical basis for its effectiveness, explaining the error variance reduction for fundamental frequency estimation and invoking Condorcet's jury theorem for voiced/unvoiced detection accuracy. To address its practical limitations, we propose two key improvements: 1) a pre-voting alignment procedure to correct temporal and frequential biases among estimators, and 2) a greedy algorithm to select a compact yet effective subset of estimators based on error correlation. Experiments on a diverse dataset of speech, singing, and music show that our proposed method with alignment outperforms individual state-of-the-art estimators in clean conditions and maintains robust voiced/unvoiced detection in noisy environments.",
      "url": "https://arxiv.org/abs/2602.01727",
      "pdfUrl": "https://arxiv.org/pdf/2602.01727.pdf",
      "titleJa": "時間的・頻度的アライメントと相関を考慮した選択による投票ベースのピッチ推定"
    },
    {
      "id": "2602.01645",
      "arxivId": "2602.01645",
      "title": "Membership Inference Attack Against Music Diffusion Models via Generative Manifold Perturbation",
      "authors": [
        "Yuxuan Liu",
        "Peihong Zhang",
        "Rui Sang",
        "Zhixin Li",
        "Yizhou Tan",
        "Yiqiang Cai",
        "Shengchen Li"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Membership inference attacks (MIAs) test whether a specific audio clip was used to train a model, making them a key tool for auditing generative music models for copyright compliance. However, loss-based signals (e.g., reconstruction error) are weakly aligned with human perception in practice, yielding poor separability at the low false-positive rates (FPRs) required for forensics. We propose the Latent Stability Adversarial Probe (LSA-Probe), a white-box method that measures a geometric property of the reverse diffusion: the minimal time-normalized perturbation budget needed to cross a fixed perceptual degradation threshold at an intermediate diffusion state. We show that training members, residing in more stable regions, exhibit a significantly higher degradation cost.",
      "url": "https://arxiv.org/abs/2602.01645",
      "pdfUrl": "https://arxiv.org/pdf/2602.01645.pdf",
      "titleJa": "生成多様体摂動法による音楽拡散モデルに対するメンバーシップ推論攻撃"
    },
    {
      "id": "2602.01547",
      "arxivId": "2602.01547",
      "title": "Attention-weighted Centered Kernel Alignment for Knowledge Distillation in Large Audio-Language Models Applied to Speech Emotion Recognition",
      "authors": [
        "Qingran Yang",
        "Botao Zhao",
        "Zuheng Kang",
        "Xue Li",
        "Yayun He",
        "Chuhang Liu",
        "Xulong Zhang",
        "Xiaoyang Qu",
        "Junqing Peng",
        "Jianzong Wang"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The emergence of Large Audio-Language Models (LALMs) has advanced Speech Emotion Recognition (SER), but their size limits deployment in resource-constrained environments. While Knowledge Distillation is effective for LALM compression, existing methods remain underexplored in distilling the cross-modal projection module (Projector), and often struggle with alignment due to differences in feature dimensions. We propose PL-Distill, a KD framework that combines Projector-Level Distillation (PDist) to align audio embeddings and Logits-Level Distillation (LDist) to align output logits. PDist introduces Attention-weighted Centered Kernel Alignment, a novel approach we propose to highlight important time steps and address dimension mismatches. Meanwhile, LDist minimizes the Kullback-Leibler divergence between teacher and student logits from audio and text modalities. On IEMOCAP, RAVDESS, and SAVEE, PL-Distill compresses an 8.4B-parameter teacher to a compact 1.1B-parameter student, consistently outperforming the teacher, state-of-the-art pretrained models, and other KD baselines across all metrics.",
      "url": "https://arxiv.org/abs/2602.01547",
      "pdfUrl": "https://arxiv.org/pdf/2602.01547.pdf",
      "titleJa": "音声感情認識に適用される大規模音声言語モデルにおける知識蒸留のための注目度重み付け中心カーネルアライメント"
    },
    {
      "id": "2602.03762",
      "arxivId": "2602.03762",
      "title": "Conditional Flow Matching for Visually-Guided Acoustic Highlighting",
      "authors": [
        "Hugo Malard",
        "Gael Le Lan",
        "Daniel Wong",
        "David Lou Alon",
        "Yi-Chiao Wu",
        "Sanjeel Parekh"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Visually-guided acoustic highlighting seeks to rebalance audio in alignment with the accompanying video, creating a coherent audio-visual experience. While visual saliency and enhancement have been widely studied, acoustic highlighting remains underexplored, often leading to misalignment between visual and auditory focus. Existing approaches use discriminative models, which struggle with the inherent ambiguity in audio remixing, where no natural one-to-one mapping exists between poorly-balanced and well-balanced audio mixes. To address this limitation, we reframe this task as a generative problem and introduce a Conditional Flow Matching (CFM) framework. A key challenge in iterative flow-based generation is that early prediction errors -- in selecting the correct source to enhance -- compound over steps and push trajectories off-manifold. To address this, we introduce a rollout loss that penalizes drift at the final step, encouraging self-correcting trajectories and stabilizing long-range flow integration. We further propose a conditioning module that fuses audio and visual cues before vector field regression, enabling explicit cross-modal source selection. Extensive quantitative and qualitative evaluations show that our method consistently surpasses the previous state-of-the-art discriminative approach, establishing that visually-guided audio remixing is best addressed through generative modeling.",
      "url": "https://arxiv.org/abs/2602.03762",
      "pdfUrl": "https://arxiv.org/pdf/2602.03762.pdf",
      "titleJa": "視覚誘導音響強調表示のための条件付きフローマッチング"
    },
    {
      "id": "2602.03398",
      "arxivId": "2602.03398",
      "title": "A Unified SVD-Modal Solution for Sparse Sound Field Reconstruction with Hybrid Spherical-Linear Microphone Arrays",
      "authors": [
        "Shunxi Xu",
        "Thushara Abhayapala",
        "Craig T. Jin"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We propose a data-driven sparse recovery framework for hybrid spherical linear microphone arrays using singular value decomposition (SVD) of the transfer operator. The SVD yields orthogonal microphone and field modes, reducing to spherical harmonics (SH) in the SMA-only case, while incorporating LMAs introduces complementary modes beyond SH. Modal analysis reveals consistent divergence from SH across frequency, confirming the improved spatial selectivity. Experiments in reverberant conditions show reduced energy-map mismatch and angular error across frequency, distance, and source count, outperforming SMA-only and direct concatenation. The results demonstrate that SVD-modal processing provides a principled and unified treatment of hybrid arrays for robust sparse sound-field reconstruction.",
      "url": "https://arxiv.org/abs/2602.03398",
      "pdfUrl": "https://arxiv.org/pdf/2602.03398.pdf",
      "titleJa": "ハイブリッド球面線形マイクロホンアレイを用いたスパース音場再構成のための統合SVD-モーダルソリューション"
    },
    {
      "id": "2602.03245",
      "arxivId": "2602.03245",
      "title": "Mići Princ -- A Little Boy Teaching Speech Technologies the Chakavian Dialect",
      "authors": [
        "Nikola Ljubešić",
        "Peter Rupnik",
        "Tea Perinčić"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "abstract": "This paper documents our efforts in releasing the printed and audio book of the translation of the famous novel The Little Prince into the Chakavian dialect, as a computer-readable, AI-ready dataset, with the textual and the audio components of the two releases now aligned on the level of each written and spoken word. Our motivation for working on this release is multiple. The first one is our wish to preserve the highly valuable and specific content beyond the small editions of the printed and the audio book. With the dataset published in the CLARIN.SI repository, this content is from now on at the fingertips of any interested individual. The second motivation is to make the data available for various artificial-intelligence-related usage scenarios, such as the one we follow upon inside this paper already -- adapting the Whisper-large-v3 open automatic speech recognition model, with decent performance on standard Croatian, to Chakavian dialectal speech. We can happily report that with adapting the model, the word error rate on the selected test data has being reduced to a half, while we managed to remove up to two thirds of the error on character level. We envision many more usages of this dataset beyond the set of experiments we have already performed, both on tasks of artificial intelligence research and application, as well as dialectal research. The third motivation for this release is our hope that this, now highly structured dataset, will be transformed into a digital online edition of this work, allowing individuals beyond the research and technology communities to enjoy the beauty of the message of the little boy in the desert, told through the spectacular prism of the Chakavian dialect.",
      "url": "https://arxiv.org/abs/2602.03245",
      "pdfUrl": "https://arxiv.org/pdf/2602.03245.pdf",
      "titleJa": "ミチ・プリンチ ― チャカビ語方言で音声技術を教える少年"
    },
    {
      "id": "2602.02980",
      "arxivId": "2602.02980",
      "title": "WST-X Series: Wavelet Scattering Transform for Interpretable Speech Deepfake Detection",
      "authors": [
        "Xi Xuan",
        "Davide Carbone",
        "Ruchi Pandey",
        "Wenxin Zhang",
        "Tomi H. Kinnunen"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "eess.AS",
        "cs.CL",
        "eess.SP"
      ],
      "abstract": "Designing front-ends for speech deepfake detectors primarily focuses on two categories. Hand-crafted filterbank features are transparent but are limited in capturing high-level semantic details, often resulting in performance gaps compared to self-supervised (SSL) features. SSL features, in turn, lack interpretability and may overlook fine-grained spectral anomalies. We propose the WST-X series, a novel family of feature extractors that combines the best of both worlds via the wavelet scattering transform (WST), integrating wavelets with nonlinearities analogous to deep convolutional networks. We investigate 1D and 2D WSTs to extract acoustic details and higher-order structural anomalies, respectively. Experimental results on the recent and challenging Deepfake-Eval-2024 dataset indicate that WST-X outperforms existing front-ends by a wide margin. Our analysis reveals that a small averaging scale ($J$), combined with high-frequency and directional resolutions ($Q, L$), is critical for capturing subtle artifacts. This underscores the value of translation-invariant and deformation-stable features for robust and interpretable speech deepfake detection.",
      "url": "https://arxiv.org/abs/2602.02980",
      "pdfUrl": "https://arxiv.org/pdf/2602.02980.pdf",
      "titleJa": "WST-Xシリーズ: 解釈可能な音声ディープフェイク検出のためのウェーブレット散乱変換"
    },
    {
      "id": "2602.02734",
      "arxivId": "2602.02734",
      "title": "WAXAL: A Large-Scale Multilingual African Language Speech Corpus",
      "authors": [
        "Abdoulaye Diack",
        "Perry Nelson",
        "Kwaku Agbesi",
        "Angela Nakalembe",
        "MohamedElfatih MohamedKhair",
        "Vusumuzi Dube",
        "Tavonga Siyavora",
        "Subhashini Venugopalan",
        "Jason Hickey",
        "Uche Okonkwo",
        "Abhishek Bapna",
        "Isaac Wiafe",
        "Raynard Dodzi Helegah",
        "Elikem Doe Atsakpo",
        "Charles Nutrokpor",
        "Fiifi Baffoe Payin Winful",
        "Kafui Kwashie Solaga",
        "Jamal-Deen Abdulai",
        "Akon Obu Ekpezu",
        "Audace Niyonkuru",
        "Samuel Rutunda",
        "Boris Ishimwe",
        "Michael Melese",
        "Engineer Bainomugisha",
        "Joyce Nakatumba-Nabende",
        "Andrew Katumba",
        "Claire Babirye",
        "Jonathan Mukiibi",
        "Vincent Kimani",
        "Samuel Kibacia",
        "James Maina",
        "Fridah Emmah",
        "Ahmed Ibrahim Shekarau",
        "Ibrahim Shehu Adamu",
        "Yusuf Abdullahi",
        "Howard Lakougna",
        "Bob MacDonald",
        "Hadar Shemtov",
        "Aisha Walcott-Bryant",
        "Moustapha Cisse",
        "Avinatan Hassidim",
        "Jeff Dean",
        "Yossi Matias"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "The advancement of speech technology has predominantly favored high-resource languages, creating a significant digital divide for speakers of most Sub-Saharan African languages. To address this gap, we introduce WAXAL, a large-scale, openly accessible speech dataset for 21 languages representing over 100 million speakers. The collection consists of two main components: an Automated Speech Recognition (ASR) dataset containing approximately 1,250 hours of transcribed, natural speech from a diverse range of speakers, and a Text-to-Speech (TTS) dataset with over 180 hours of high-quality, single-speaker recordings reading phonetically balanced scripts. This paper details our methodology for data collection, annotation, and quality control, which involved partnerships with four African academic and community organizations. We provide a detailed statistical overview of the dataset and discuss its potential limitations and ethical considerations. The WAXAL datasets are released at https://huggingface.co/datasets/google/WaxalNLP under the permissive CC-BY-4.0 license to catalyze research, enable the development of inclusive technologies, and serve as a vital resource for the digital preservation of these languages.",
      "url": "https://arxiv.org/abs/2602.02734",
      "pdfUrl": "https://arxiv.org/pdf/2602.02734.pdf",
      "titleJa": "WAXAL: 大規模多言語アフリカ言語音声コーパス"
    },
    {
      "id": "2602.02198",
      "arxivId": "2602.02198",
      "title": "QuietPrint: Protecting 3D Printers Against Acoustic Side-Channel Attacks",
      "authors": [
        "Seyed Ali Ghazi Asgar",
        "Narasimha Reddy"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.CR",
        "eess.AS"
      ],
      "abstract": "The 3D printing market has experienced significant growth in recent years, with an estimated revenue of 15 billion USD for 2025. Cyber-attacks targeting the 3D printing process whether through the machine itself, the supply chain, or the fabricated components are becoming increasingly common. One major concern is intellectual property (IP) theft, where a malicious attacker gains access to the design file. One method for carrying out such theft is through side-channel attacks. In this work, we investigate the possibility of IP theft via acoustic side channels and propose a novel method to protect 3D printers against such attacks. The primary advantage of our approach is that it requires no additional hardware, such as large speakers or noise-canceling devices. Instead, it secures printed parts by minimal modifications to the G-code.",
      "url": "https://arxiv.org/abs/2602.02198",
      "pdfUrl": "https://arxiv.org/pdf/2602.02198.pdf",
      "titleJa": "QuietPrint: 音響サイドチャネル攻撃から3Dプリンターを保護する"
    },
    {
      "id": "2602.01861",
      "arxivId": "2602.01861",
      "title": "RIR-Former: Coordinate-Guided Transformer for Continuous Reconstruction of Room Impulse Responses",
      "authors": [
        "Shaoheng Xu",
        "Chunyi Sun",
        "Jihui Zhang",
        "Prasanga N. Samarasinghe",
        "Thushara D. Abhayapala"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Room impulse responses (RIRs) are essential for many acoustic signal processing tasks, yet measuring them densely across space is often impractical. In this work, we propose RIR-Former, a grid-free, one-step feed-forward model for RIR reconstruction. By introducing a sinusoidal encoding module into a transformer backbone, our method effectively incorporates microphone position information, enabling interpolation at arbitrary array locations. Furthermore, a segmented multi-branch decoder is designed to separately handle early reflections and late reverberation, improving reconstruction across the entire RIR. Experiments on diverse simulated acoustic environments demonstrate that RIR-Former consistently outperforms state-of-the-art baselines in terms of normalized mean square error (NMSE) and cosine distance (CD), under varying missing rates and array configurations. These results highlight the potential of our approach for practical deployment and motivate future work on scaling from randomly spaced linear arrays to complex array geometries, dynamic acoustic scenes, and real-world environments.",
      "url": "https://arxiv.org/abs/2602.01861",
      "pdfUrl": "https://arxiv.org/pdf/2602.01861.pdf",
      "titleJa": "RIR-Former: 室内インパルス応答の連続再構成のための座標誘導型変換器"
    },
    {
      "id": "2602.01758",
      "arxivId": "2602.01758",
      "title": "Short-wave admittance correction for a time-domain cochlear transmission line model",
      "authors": [
        "François Deloche",
        "Morgan Thienpont",
        "Sarah Verhulst"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS",
        "physics.bio-ph"
      ],
      "abstract": "Transmission line (TL) models implemented in the time domain can efficiently simulate basilar-membrane (BM) displacement in response to transient or non-stationary sounds. By design, a TL model is well-suited for an one-dimensional (1-D) characterization of the traveling wave, but the real configuration of the cochlea also introduces higher-dimensional effects. Such effects include the focusing of the pressure around the BM and transverse viscous damping, both of which are magnified in the short-wave region. The two effects depend on the wavelength and are more readily expressed in the frequency domain. In this paper, we introduce a numerical correction for the BM admittance to account for 2-D effects in the time domain using autoregressive filtering and regression techniques. The correction was required for the implementation of a TL model tailored to the gerbil cochlear physiology. The model, which includes instantaneous nonlinearities in the form of variable damping, initially presented insufficient compression with increasing sound levels. This limitation was explained by the strong coupling between gain and frequency selectivity assumed in the 1-D nonlinear TL model, whereas cochlear frequency selectivity shows only a moderate dependence on sound level in small mammals. The correction factor was implemented in the gerbil model and made level-dependent using a feedback loop. The updated model achieved some decoupling between frequency selectivity and gain, providing 5 dB of additional gain and extending the range of sound levels of the compressive regime by 10 dB. We discuss the relevance of this work through two key features: the integration of both analytical and regression methods for characterizing BM admittance, and the combination of instantaneous and non-instantaneous nonlinearities.",
      "url": "https://arxiv.org/abs/2602.01758",
      "pdfUrl": "https://arxiv.org/pdf/2602.01758.pdf",
      "titleJa": "時間領域蝸牛伝送線路モデルの短波アドミタンス補正"
    },
    {
      "id": "2602.01722",
      "arxivId": "2602.01722",
      "title": "Joint Optimization of ASV and CM tasks: BTUEF Team's Submission for WildSpoof Challenge",
      "authors": [
        "Oguzhan Kurnaz",
        "Jagabandhu Mishra",
        "Tomi Kinnunen",
        "Cemal Hanilci"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Spoofing-aware speaker verification (SASV) jointly addresses automatic speaker verification and spoofing countermeasures to improve robustness against adversarial attacks. In this paper, we investigate our recently proposed modular SASV framework that enables effective reuse of publicly available ASV and CM systems through non-linear fusion, explicitly modeling their interaction, and optimization with an operating-condition-dependent trainable a-DCF loss. The framework is evaluated using ECAPA-TDNN and ReDimNet as ASV embedding extractors and SSL-AASIST as the CM model, with experiments conducted both with and without fine-tuning on the WildSpoof SASV training data. Results show that the best performance is achieved by combining ReDimNet-based ASV embeddings with fine-tuned SSL-AASIST representations, yielding an a-DCF of 0.0515 on the progress evaluation set and 0.2163 on the final evaluation set.",
      "url": "https://arxiv.org/abs/2602.01722",
      "pdfUrl": "https://arxiv.org/pdf/2602.01722.pdf",
      "titleJa": "ASVとCMタスクの共同最適化：BTUEFチームのWildSpoofチャレンジへの応募"
    },
    {
      "id": "2602.01634",
      "arxivId": "2602.01634",
      "title": "HuPER: A Human-Inspired Framework for Phonetic Perception",
      "authors": [
        "Chenxu Guo",
        "Jiachen Lian",
        "Yisi Liu",
        "Baihe Huang",
        "Shriyaa Narayanan",
        "Cheol Jun Cho",
        "Gopala Anumanchipalli"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "We propose HuPER, a human-inspired framework that models phonetic perception as adaptive inference over acoustic-phonetics evidence and linguistic knowledge. With only 100 hours of training data, HuPER achieves state-of-the-art phonetic error rates on five English benchmarks and strong zero-shot transfer to 95 unseen languages. HuPER is also the first framework to enable adaptive, multi-path phonetic perception under diverse acoustic conditions. All training data, models, and code are open-sourced. Code and demo avaliable at https://github.com/HuPER29/HuPER.",
      "url": "https://arxiv.org/abs/2602.01634",
      "pdfUrl": "https://arxiv.org/pdf/2602.01634.pdf",
      "titleJa": "HuPER: 人間に着想を得た音声知覚フレームワーク"
    },
    {
      "id": "2602.01394",
      "arxivId": "2602.01394",
      "title": "SSNAPS: Audio-Visual Separation of Speech and Background Noise with Diffusion Inverse Sampling",
      "authors": [
        "Yochai Yemini",
        "Yoav Ellinson",
        "Rami Ben-Ari",
        "Sharon Gannot",
        "Ethan Fetaya"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "This paper addresses the challenge of audio-visual single-microphone speech separation and enhancement in the presence of real-world environmental noise. Our approach is based on generative inverse sampling, where we model clean speech and ambient noise with dedicated diffusion priors and jointly leverage them to recover all underlying sources. To achieve this, we reformulate a recent inverse sampler to match our setting. We evaluate on mixtures of 1, 2, and 3 speakers with noise and show that, despite being entirely unsupervised, our method consistently outperforms leading supervised baselines in \\ac{WER} across all conditions. We further extend our framework to handle off-screen speaker separation. Moreover, the high fidelity of the separated noise component makes it suitable for downstream acoustic scene detection. Demo page: https://ssnapsicml.github.io/ssnapsicml2026/",
      "url": "https://arxiv.org/abs/2602.01394",
      "pdfUrl": "https://arxiv.org/pdf/2602.01394.pdf",
      "titleJa": "SSNAPS: 拡散逆サンプリングによる音声と背景雑音のオーディオビジュアル分離"
    },
    {
      "id": "2602.01363",
      "arxivId": "2602.01363",
      "title": "Causally Disentangled Contrastive Learning for Multilingual Speaker Embeddings",
      "authors": [
        "Mariëtte Olijslager",
        "Seyed Sahand Mohammadi Ziabari",
        "Ali Mohammed Mansoor Alsahag"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Self-supervised speaker embeddings are widely used in speaker verification systems, but prior work has shown that they often encode sensitive demographic attributes, raising fairness and privacy concerns. This paper investigates the extent to which demographic information, specifically gender, age, and accent, is present in SimCLR-trained speaker embeddings and whether such leakage can be mitigated without severely degrading speaker verification performance. We study two debiasing strategies: adversarial training through gradient reversal and a causal bottleneck architecture that explicitly separates demographic and residual information. Demographic leakage is quantified using both linear and nonlinear probing classifiers, while speaker verification performance is evaluated using ROC-AUC and EER. Our results show that gender information is strongly and linearly encoded in baseline embeddings, whereas age and accent are weaker and primarily nonlinearly represented. Adversarial debiasing reduces gender leakage but has limited effect on age and accent and introduces a clear trade-off with verification accuracy. The causal bottleneck further suppresses demographic information, particularly in the residual representation, but incurs substantial performance degradation. These findings highlight fundamental limitations in mitigating demographic leakage in self-supervised speaker embeddings and clarify the trade-offs inherent in current debiasing approaches.",
      "url": "https://arxiv.org/abs/2602.01363",
      "pdfUrl": "https://arxiv.org/pdf/2602.01363.pdf",
      "titleJa": "多言語話者埋め込みのための因果的に分離した対照学習"
    },
    {
      "id": "2602.01249",
      "arxivId": "2602.01249",
      "title": "Generative AI in Signal Processing Education: An Audio Foundation Model Based Approach",
      "authors": [
        "Muhammad Salman Khan",
        "Ahmad Ullah",
        "Siddique Latif",
        "Junaid Qadir"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "eess.SP",
        "eess.AS"
      ],
      "abstract": "Audio Foundation Models (AFMs), a specialized category of Generative AI (GenAI), have the potential to transform signal processing (SP) education by integrating core applications such as speech and audio enhancement, denoising, source separation, feature extraction, automatic classification, and real-time signal analysis into learning and research. This paper introduces SPEduAFM, a conceptual AFM tailored for SP education, bridging traditional SP principles with GenAI-driven innovations. Through an envisioned case study, we outline how AFMs can enable a range of applications, including automated lecture transcription, interactive demonstrations, and inclusive learning tools, showcasing their potential to transform abstract concepts into engaging, practical experiences. This paper also addresses challenges such as ethics, explainability, and customization by highlighting dynamic, real-time auditory interactions that foster experiential and authentic learning. By presenting SPEduAFM as a forward-looking vision, we aim to inspire broader adoption of GenAI in engineering education, enhancing accessibility, engagement, and innovation in the classroom and beyond.",
      "url": "https://arxiv.org/abs/2602.01249",
      "pdfUrl": "https://arxiv.org/pdf/2602.01249.pdf",
      "titleJa": "信号処理教育における生成AI：オーディオ基盤モデルに基づくアプローチ"
    },
    {
      "id": "2602.01060",
      "arxivId": "2602.01060",
      "title": "TLDiffGAN: A Latent Diffusion-GAN Framework with Temporal Information Fusion for Anomalous Sound Detection",
      "authors": [
        "Chengyuan Ma",
        "Peng Jia",
        "Hongyue Guo",
        "Wenming Yang"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Existing generative models for unsupervised anomalous sound detection are limited by their inability to fully capture the complex feature distribution of normal sounds, while the potential of powerful diffusion models in this domain remains largely unexplored. To address this challenge, we propose a novel framework, TLDiffGAN, which consists of two complementary branches. One branch incorporates a latent diffusion model into the GAN generator for adversarial training, thereby making the discriminator's task more challenging and improving the quality of generated samples. The other branch leverages pretrained audio model encoders to extract features directly from raw audio waveforms for auxiliary discrimination. This framework effectively captures feature representations of normal sounds from both raw audio and Mel spectrograms. Moreover, we introduce a TMixup spectrogram augmentation technique to enhance sensitivity to subtle and localized temporal patterns that are often overlooked. Extensive experiments on the DCASE 2020 Challenge Task 2 dataset demonstrate the superior detection performance of TLDiffGAN, as well as its strong capability in anomalous time-frequency localization.",
      "url": "https://arxiv.org/abs/2602.01060",
      "pdfUrl": "https://arxiv.org/pdf/2602.01060.pdf",
      "titleJa": "TLDiffGAN: 異常音検出のための時間情報融合機能を備えた潜在拡散GANフレームワーク"
    },
    {
      "id": "2602.01032",
      "arxivId": "2602.01032",
      "title": "HierCon: Hierarchical Contrastive Attention for Audio Deepfake Detection",
      "authors": [
        "Zhili Nicholas Liang",
        "Soyeon Caren Han",
        "Qizhou Wang",
        "Christopher Leckie"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Audio deepfakes generated by modern TTS and voice conversion systems are increasingly difficult to distinguish from real speech, raising serious risks for security and online trust. While state-of-the-art self-supervised models provide rich multi-layer representations, existing detectors treat layers independently and overlook temporal and hierarchical dependencies critical for identifying synthetic artefacts. We propose HierCon, a hierarchical layer attention framework combined with margin-based contrastive learning that models dependencies across temporal frames, neighbouring layers, and layer groups, while encouraging domain-invariant embeddings. Evaluated on ASVspoof 2021 DF and In-the-Wild datasets, our method achieves state-of-the-art performance (1.93% and 6.87% EER), improving over independent layer weighting by 36.6% and 22.5% respectively. The results and attention visualisations confirm that hierarchical modelling enhances generalisation to cross-domain generation techniques and recording conditions.",
      "url": "https://arxiv.org/abs/2602.01032",
      "pdfUrl": "https://arxiv.org/pdf/2602.01032.pdf",
      "titleJa": "HierCon: オーディオディープフェイク検出のための階層的コントラストアテンション"
    },
    {
      "id": "2602.01030",
      "arxivId": "2602.01030",
      "title": "Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations",
      "authors": [
        "Sheng-Lun Wei",
        "Yu-Ling Liao",
        "Yen-Hua Chang",
        "Hen-Hsen Huang",
        "Hsin-Hsi Chen"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This work presents the first systematic investigation of speech bias in multilingual MLLMs. We construct and release the BiasInEar dataset, a speech-augmented benchmark based on Global MMLU Lite, spanning English, Chinese, and Korean, balanced by gender and accent, and totaling 70.8 hours ($\\approx$4,249 minutes) of speech with 11,200 questions. Using four complementary metrics (accuracy, entropy, APES, and Fleiss' $κ$), we evaluate nine representative models under linguistic (language and accent), demographic (gender), and structural (option order) perturbations. Our findings reveal that MLLMs are relatively robust to demographic factors but highly sensitive to language and option order, suggesting that speech can amplify existing structural biases. Moreover, architectural design and reasoning strategy substantially affect robustness across languages. Overall, this study establishes a unified framework for assessing fairness and robustness in speech-integrated LLMs, bridging the gap between text- and speech-based evaluation. The resources can be found at https://github.com/ntunlplab/BiasInEar.",
      "url": "https://arxiv.org/abs/2602.01030",
      "pdfUrl": "https://arxiv.org/pdf/2602.01030.pdf",
      "titleJa": "聞き手の耳のバイアス：言語的、人口統計学的、および位置的差異における音声言語モデルの感度評価"
    },
    {
      "id": "2602.01008",
      "arxivId": "2602.01008",
      "title": "Adapting Where It Matters: Depth-Aware Adaptation for Efficient Multilingual Speech Recognition in Low-Resource Languages",
      "authors": [
        "Yang Xiao",
        "Eun-Jung Holden",
        "Ting Dang"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "Recent speech foundation models excel at multilingual automatic speech recognition (ASR) for high-resource languages, but adapting them to low-resource languages remains challenging due to data scarcity and efficiency constraints. Full-model fine-tuning is computationally expensive and prone to overfitting, while parameter-efficient methods like LoRA apply adaptation uniformly across layers, overlooking internal representations thus compromising effectiveness and efficiency. We analyze multilingual ASR models and reveal a U-shaped adaptability pattern: early and late layers are language-specific and require more adaptation, while intermediate layers retain shared semantics and need less. Building on this observation, we propose DAMA, a Depth-Aware Model Adaptation framework that allocates adaptation capacity according to each layer's role. DAMA also introduces Singular Value Decomposition (SVD)-based initialization to constrain adaptation and preserve the U-shaped pattern, as well as a frozen middle-layer basis for further efficiency. Evaluated on 18 low-resource languages across two benchmark datasets, DAMA matches or surpasses state-of-the-art accuracy with 80% fewer trainable parameters, achieves a 29% error reduction under extreme data scarcity, and significantly improves memory, training time, and computational efficiency over baselines. These results highlight the benefits of structure-aware adaptation for efficient, scalable multilingual ASR.",
      "url": "https://arxiv.org/abs/2602.01008",
      "pdfUrl": "https://arxiv.org/pdf/2602.01008.pdf",
      "titleJa": "重要な箇所への適応：低リソース言語における効率的な多言語音声認識のための深度を考慮した適応"
    },
    {
      "id": "2602.00914",
      "arxivId": "2602.00914",
      "title": "A Baseline Multimodal Approach to Emotion Recognition in Conversations",
      "authors": [
        "Víctor Yeste",
        "Rodrigo Rivas-Arévalo"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (i) a transformer-based text classifier and (ii) a self-supervised speech representation model, with a simple late-fusion ensemble. We report the baseline setup and empirical results obtained under a limited training protocol, highlighting when multimodal fusion improves over unimodal models. This preprint is provided for transparency and to support future, more rigorous comparisons.",
      "url": "https://arxiv.org/abs/2602.00914",
      "pdfUrl": "https://arxiv.org/pdf/2602.00914.pdf",
      "titleJa": "会話における感情認識のためのベースラインマルチモーダルアプローチ"
    },
    {
      "id": "2602.03846",
      "arxivId": "2602.03846",
      "title": "PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning",
      "authors": [
        "Romain Cosentino"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "We develop a continual learning method for pretrained models that \\emph{requires no access to old-task data}, addressing a practical barrier in foundation model adaptation where pretraining distributions are often unavailable. Our key observation is that pretrained networks exhibit substantial \\emph{geometric redundancy}, and that this redundancy can be exploited in two complementary ways. First, redundant neurons provide a proxy for dominant pretraining-era feature directions, enabling the construction of approximately protected update subspaces directly from pretrained weights. Second, redundancy offers a natural bias for \\emph{where} to place plasticity: by restricting updates to a subset of redundant neurons and constraining the remaining degrees of freedom, we obtain update families with reduced functional drift on the old-data distribution and improved worst-case retention guarantees. These insights lead to \\textsc{PLATE} (\\textbf{Pla}sticity-\\textbf{T}unable \\textbf{E}fficient Adapters), a continual learning method requiring no past-task data that provides explicit control over the plasticity-retention trade-off. PLATE parameterizes each layer with a structured low-rank update $ΔW = B A Q^\\top$, where $B$ and $Q$ are computed once from pretrained weights and kept frozen, and only $A$ is trained on the new task. The code is available at https://github.com/SalesforceAIResearch/PLATE.",
      "url": "https://arxiv.org/abs/2602.03846",
      "pdfUrl": "https://arxiv.org/pdf/2602.03846.pdf",
      "titleJa": "PLATE: 形状を考慮した継続学習のための可塑性調整可能な効率的なアダプター"
    },
    {
      "id": "2602.03838",
      "arxivId": "2602.03838",
      "title": "PrevizWhiz: Combining Rough 3D Scenes and 2D Video to Guide Generative Video Previsualization",
      "authors": [
        "Erzhen Hu",
        "Frederik Brudy",
        "David Ledo",
        "George Fitzmaurice",
        "Fraser Anderson"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "abstract": "In pre-production, filmmakers and 3D animation experts must rapidly prototype ideas to explore a film's possibilities before fullscale production, yet conventional approaches involve trade-offs in efficiency and expressiveness. Hand-drawn storyboards often lack spatial precision needed for complex cinematography, while 3D previsualization demands expertise and high-quality rigged assets. To address this gap, we present PrevizWhiz, a system that leverages rough 3D scenes in combination with generative image and video models to create stylized video previews. The workflow integrates frame-level image restyling with adjustable resemblance, time-based editing through motion paths or external video inputs, and refinement into high-fidelity video clips. A study with filmmakers demonstrates that our system lowers technical barriers for film-makers, accelerates creative iteration, and effectively bridges the communication gap, while also surfacing challenges of continuity, authorship, and ethical consideration in AI-assisted filmmaking.",
      "url": "https://arxiv.org/abs/2602.03838",
      "pdfUrl": "https://arxiv.org/pdf/2602.03838.pdf",
      "titleJa": "PrevizWhiz: 粗い 3D シーンと 2D ビデオを組み合わせてジェネレーティブ ビデオのプレビジュアライゼーションをガイドする"
    },
    {
      "id": "2602.03837",
      "arxivId": "2602.03837",
      "title": "Accelerating Scientific Research with Gemini: Case Studies and Common Techniques",
      "authors": [
        "David P. Woodruff",
        "Vincent Cohen-Addad",
        "Lalit Jain",
        "Jieming Mao",
        "Song Zuo",
        "MohammadHossein Bateni",
        "Simina Branzei",
        "Michael P. Brenner",
        "Lin Chen",
        "Ying Feng",
        "Lance Fortnow",
        "Gang Fu",
        "Ziyi Guan",
        "Zahra Hadizadeh",
        "Mohammad T. Hajiaghayi",
        "Mahdi JafariRaviz",
        "Adel Javanmard",
        "Karthik C. S.",
        "Ken-ichi Kawarabayashi",
        "Ravi Kumar",
        "Silvio Lattanzi",
        "Euiwoong Lee",
        "Yi Li",
        "Ioannis Panageas",
        "Dimitris Paparas",
        "Benjamin Przybocki",
        "Bernardo Subercaseaux",
        "Ola Svensson",
        "Shayan Taherijam",
        "Xuan Wu",
        "Eylon Yogev",
        "Morteza Zadimoghaddam",
        "Samson Zhou",
        "Vahab Mirrokni"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a \"neuro-symbolic\" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.",
      "url": "https://arxiv.org/abs/2602.03837",
      "pdfUrl": "https://arxiv.org/pdf/2602.03837.pdf",
      "titleJa": "Geminiによる科学研究の加速：ケーススタディと一般的な手法"
    },
    {
      "id": "2602.03828",
      "arxivId": "2602.03828",
      "title": "AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations",
      "authors": [
        "Minjun Zhu",
        "Zhen Lin",
        "Yixuan Weng",
        "Panzhong Lu",
        "Qiujie Xie",
        "Yifan Wei",
        "Sifan Liu",
        "Qiyao Sun",
        "Yue Zhang"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.DL"
      ],
      "abstract": "High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.",
      "url": "https://arxiv.org/abs/2602.03828",
      "pdfUrl": "https://arxiv.org/pdf/2602.03828.pdf",
      "titleJa": "AutoFigure: 出版に適した科学イラストの作成と改良"
    },
    {
      "id": "2602.03814",
      "arxivId": "2602.03814",
      "title": "Conformal Thinking: Risk Control for Reasoning on a Compute Budget",
      "authors": [
        "Xi Wang",
        "Anushri Suresh",
        "Alvin Zhang",
        "Rishi More",
        "William Jurayj",
        "Benjamin Van Durme",
        "Mehrdad Farajtabar",
        "Daniel Khashabi",
        "Eric Nalisnick"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.",
      "url": "https://arxiv.org/abs/2602.03814",
      "pdfUrl": "https://arxiv.org/pdf/2602.03814.pdf",
      "titleJa": "コンフォーマル思考：コンピューティング予算内での推論のためのリスク管理"
    },
    {
      "id": "2602.03812",
      "arxivId": "2602.03812",
      "title": "Antidistillation Fingerprinting",
      "authors": [
        "Yixuan Even Xu",
        "John Kirchenbauer",
        "Yash Savani",
        "Asher Trockman",
        "Alexander Robey",
        "Tom Goldstein",
        "Fei Fang",
        "J. Zico Kolter"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Model distillation enables efficient emulation of frontier large language models (LLMs), creating a need for robust mechanisms to detect when a third-party student model has trained on a teacher model's outputs. However, existing fingerprinting techniques that could be used to detect such distillation rely on heuristic perturbations that impose a steep trade-off between generation quality and fingerprinting strength, often requiring significant degradation of utility to ensure the fingerprint is effectively internalized by the student. We introduce antidistillation fingerprinting (ADFP), a principled approach that aligns the fingerprinting objective with the student's learning dynamics. Building upon the gradient-based framework of antidistillation sampling, ADFP utilizes a proxy model to identify and sample tokens that directly maximize the expected detectability of the fingerprint in the student after fine-tuning, rather than relying on the incidental absorption of the un-targeted biases of a more naive watermark. Experiments on GSM8K and OASST1 benchmarks demonstrate that ADFP achieves a significant Pareto improvement over state-of-the-art baselines, yielding stronger detection confidence with minimal impact on utility, even when the student model's architecture is unknown.",
      "url": "https://arxiv.org/abs/2602.03812",
      "pdfUrl": "https://arxiv.org/pdf/2602.03812.pdf",
      "titleJa": "蒸留防止指紋採取"
    },
    {
      "id": "2602.03808",
      "arxivId": "2602.03808",
      "title": "Enhancing Imbalanced Node Classification via Curriculum-Guided Feature Learning and Three-Stage Attention Network",
      "authors": [
        "Abdul Joseph Fofanah",
        "Lian Wen",
        "David Chen",
        "Shaoyang Zhang"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Imbalanced node classification in graph neural networks (GNNs) happens when some labels are much more common than others, which causes the model to learn unfairly and perform badly on the less common classes. To solve this problem, we propose a Curriculum-Guided Feature Learning and Three-Stage Attention Network (CL3AN-GNN), a learning network that uses a three-step attention system (Engage, Enact, Embed) similar to how humans learn. The model begins by engaging with structurally simpler features, defined as (1) local neighbourhood patterns (1-hop), (2) low-degree node attributes, and (3) class-separable node pairs identified via initial graph convolutional networks and graph attention networks (GCN and GAT) embeddings. This foundation enables stable early learning despite label skew. The Enact stage then addresses complicated aspects: (1) connections that require multiple steps, (2) edges that connect different types of nodes, and (3) nodes at the edges of minority classes by using adjustable attention weights. Finally, Embed consolidates these features via iterative message passing and curriculum-aligned loss weighting. We evaluate CL3AN-GNN on eight Open Graph Benchmark datasets spanning social, biological, and citation networks. Experiments show consistent improvements across all datasets in accuracy, F1-score, and AUC over recent state-of-the-art methods. The model's step-by-step method works well with different types of graph datasets, showing quicker results than training everything at once, better performance on new, imbalanced graphs, and clear explanations of each step using gradient stability and attention correlation learning curves. This work provides both a theoretically grounded framework for curriculum learning in GNNs and practical evidence of its effectiveness against imbalances, validated through metrics, convergence speeds, and generalisation tests.",
      "url": "https://arxiv.org/abs/2602.03808",
      "pdfUrl": "https://arxiv.org/pdf/2602.03808.pdf",
      "titleJa": "カリキュラム誘導型特徴学習と3段階アテンションネットワークによる不均衡ノード分類の強化"
    },
    {
      "id": "2602.03806",
      "arxivId": "2602.03806",
      "title": "Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation",
      "authors": [
        "Ziru Chen",
        "Dongdong Chen",
        "Ruinan Jin",
        "Yingbin Liang",
        "Yujia Xie",
        "Huan Sun"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "abstract": "Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.",
      "url": "https://arxiv.org/abs/2602.03806",
      "pdfUrl": "https://arxiv.org/pdf/2602.03806.pdf",
      "titleJa": "オンラインとオフラインの強化学習の橋渡し：マルチターンコード生成のためのコンテキストバンディット学習"
    },
    {
      "id": "2602.03802",
      "arxivId": "2602.03802",
      "title": "Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods",
      "authors": [
        "Grigory Begunov",
        "Alexander Tyurin"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.DC",
        "cs.AI",
        "math.NA",
        "math.OC"
      ],
      "abstract": "Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios.",
      "url": "https://arxiv.org/abs/2602.03802",
      "pdfUrl": "https://arxiv.org/pdf/2602.03802.pdf",
      "titleJa": "非同期SGDは必要か？同期メソッドの近似最適性について"
    },
    {
      "id": "2602.03799",
      "arxivId": "2602.03799",
      "title": "Conformal Reachability for Safe Control in Unknown Environments",
      "authors": [
        "Xinhang Ma",
        "Junlin Wu",
        "Yiannis Kantaros",
        "Yevgeniy Vorobeychik"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "abstract": "Designing provably safe control is a core problem in trustworthy autonomy. However, most prior work in this regard assumes either that the system dynamics are known or deterministic, or that the state and action space are finite, significantly limiting application scope. We address this limitation by developing a probabilistic verification framework for unknown dynamical systems which combines conformal prediction with reachability analysis. In particular, we use conformal prediction to obtain valid uncertainty intervals for the unknown dynamics at each time step, with reachability then verifying whether safety is maintained within the conformal uncertainty bounds. Next, we develop an algorithmic approach for training control policies that optimize nominal reward while also maximizing the planning horizon with sound probabilistic safety guarantees. We evaluate the proposed approach in seven safe control settings spanning four domains -- cartpole, lane following, drone control, and safe navigation -- for both affine and nonlinear safety specifications. Our experiments show that the policies we learn achieve the strongest provable safety guarantees while still maintaining high average reward.",
      "url": "https://arxiv.org/abs/2602.03799",
      "pdfUrl": "https://arxiv.org/pdf/2602.03799.pdf",
      "titleJa": "未知の環境における安全な制御のための適合到達可能性"
    },
    {
      "id": "2602.03794",
      "arxivId": "2602.03794",
      "title": "Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity",
      "authors": [
        "Yingxuan Yang",
        "Chengrui Qu",
        "Muning Wen",
        "Laixi Shi",
        "Ying Wen",
        "Weinan Zhang",
        "Adam Wierman",
        "Shangding Gu"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.",
      "url": "https://arxiv.org/abs/2602.03794",
      "pdfUrl": "https://arxiv.org/pdf/2602.03794.pdf",
      "titleJa": "LLMベースのマルチエージェントシステムにおけるエージェントのスケーリングを多様性の観点から理解する"
    },
    {
      "id": "2602.03792",
      "arxivId": "2602.03792",
      "title": "WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents",
      "authors": [
        "Xilong Wang",
        "Yinuo Liu",
        "Zhun Wang",
        "Dawn Song",
        "Neil Gong"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Prompt injection attacks manipulate webpage content to cause web agents to execute attacker-specified tasks instead of the user's intended ones. Existing methods for detecting and localizing such attacks achieve limited effectiveness, as their underlying assumptions often do not hold in the web-agent setting. In this work, we propose WebSentinel, a two-step approach for detecting and localizing prompt injection attacks in webpages. Given a webpage, Step I extracts \\emph{segments of interest} that may be contaminated, and Step II evaluates each segment by checking its consistency with the webpage content as context. We show that WebSentinel is highly effective, substantially outperforming baseline methods across multiple datasets of both contaminated and clean webpages that we collected. Our code is available at: https://github.com/wxl-lxw/WebSentinel.",
      "url": "https://arxiv.org/abs/2602.03792",
      "pdfUrl": "https://arxiv.org/pdf/2602.03792.pdf",
      "titleJa": "WebSentinel: Webエージェントに対するプロンプトインジェクション攻撃の検出と局所化"
    },
    {
      "id": "2602.03789",
      "arxivId": "2602.03789",
      "title": "Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants",
      "authors": [
        "Gabriel Damsholt",
        "Jes Frellsen",
        "Susanne Ditlevsen"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Stochastic interpolants unify flows and diffusions, popular generative modeling frameworks. A primary hyperparameter in these methods is the interpolation schedule that determines how to bridge a standard Gaussian base measure to an arbitrary target measure. We prove how to convert a sample path of a stochastic differential equation (SDE) with arbitrary diffusion coefficient under any schedule into the unique sample path under another arbitrary schedule and diffusion coefficient. We then extend the stochastic interpolant framework to admit a larger class of point mass schedules in which the Gaussian base measure collapses to a point mass measure. Under the assumption of Gaussian data, we identify lazy schedule families that make the drift identically zero and show that with deterministic sampling one gets a variance-preserving schedule commonly used in diffusion models, whereas with statistically optimal SDE sampling one gets our point mass schedule. Finally, to demonstrate the usefulness of our theoretical results on realistic highly non-Gaussian data, we apply our lazy schedule conversion to a state-of-the-art pretrained flow model and show that this allows for generating images in fewer steps without retraining the model.",
      "url": "https://arxiv.org/abs/2602.03789",
      "pdfUrl": "https://arxiv.org/pdf/2602.03789.pdf",
      "titleJa": "遅延および点質量確率補間による流れと拡散の高速サンプリング"
    },
    {
      "id": "2602.03786",
      "arxivId": "2602.03786",
      "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration",
      "authors": [
        "Jianhao Ruan",
        "Zhihao Xu",
        "Yiran Peng",
        "Fashen Ren",
        "Zhaoyang Yu",
        "Xinbing Liang",
        "Jinyu Xiang",
        "Bang Liu",
        "Chenglin Wu",
        "Yuyu Luo",
        "Jiayi Zhang"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra",
      "url": "https://arxiv.org/abs/2602.03786",
      "pdfUrl": "https://arxiv.org/pdf/2602.03786.pdf",
      "titleJa": "AOrchestra: エージェントオーケストレーションのためのサブエージェント作成の自動化"
    },
    {
      "id": "2602.03783",
      "arxivId": "2602.03783",
      "title": "Efficient Estimation of Kernel Surrogate Models for Task Attribution",
      "authors": [
        "Zhenshuo Zhang",
        "Minxuan Duan",
        "Hongyang R. Zhang"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.",
      "url": "https://arxiv.org/abs/2602.03783",
      "pdfUrl": "https://arxiv.org/pdf/2602.03783.pdf",
      "titleJa": "タスク帰属のためのカーネルサロゲートモデルの効率的な推定"
    },
    {
      "id": "2602.03778",
      "arxivId": "2602.03778",
      "title": "Reward Redistribution for CVaR MDPs using a Bellman Operator on L-infinity",
      "authors": [
        "Aneri Muni",
        "Vincent Taboga",
        "Esther Derman",
        "Pierre-Luc Bacon",
        "Erick Delage"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Tail-end risk measures such as static conditional value-at-risk (CVaR) are used in safety-critical applications to prevent rare, yet catastrophic events. Unlike risk-neutral objectives, the static CVaR of the return depends on entire trajectories without admitting a recursive Bellman decomposition in the underlying Markov decision process. A classical resolution relies on state augmentation with a continuous variable. However, unless restricted to a specialized class of admissible value functions, this formulation induces sparse rewards and degenerate fixed points. In this work, we propose a novel formulation of the static CVaR objective based on augmentation. Our alternative approach leads to a Bellman operator with: (1) dense per-step rewards; (2) contracting properties on the full space of bounded value functions. Building on this theoretical foundation, we develop risk-averse value iteration and model-free Q-learning algorithms that rely on discretized augmented states. We further provide convergence guarantees and approximation error bounds due to discretization. Empirical results demonstrate that our algorithms successfully learn CVaR-sensitive policies and achieve effective performance-safety trade-offs.",
      "url": "https://arxiv.org/abs/2602.03778",
      "pdfUrl": "https://arxiv.org/pdf/2602.03778.pdf",
      "titleJa": "L無限大上のベルマン演算子を用いたCVaR MDPの報酬再分配"
    },
    {
      "id": "2602.03776",
      "arxivId": "2602.03776",
      "title": "DiffLOB: Diffusion Models for Counterfactual Generation in Limit Order Books",
      "authors": [
        "Zhuohan Wang",
        "Carmine Ventre"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "q-fin.CP",
        "cs.AI"
      ],
      "abstract": "Modern generative models for limit order books (LOBs) can reproduce realistic market dynamics, but remain fundamentally passive: they either model what typically happens without accounting for hypothetical future market conditions, or they require interaction with another agent to explore alternative outcomes. This limits their usefulness for stress testing, scenario analysis, and decision-making. We propose \\textbf{DiffLOB}, a regime-conditioned \\textbf{Diff}usion model for controllable and counterfactual generation of \\textbf{LOB} trajectories. DiffLOB explicitly conditions the generative process on future market regimes--including trend, volatility, liquidity, and order-flow imbalance, which enables the model to answer counterfactual queries of the form: ``If the future market regime were X instead of Y, how would the limit order book evolve?'' Our systematic evaluation framework for counterfactual LOB generation consists of three criteria: (1) \\textit{Controllable Realism}, measuring how well generated trajectories can reproduce marginal distributions, temporal dependence structure and regime variables; (2) \\textit{Counterfactual validity}, testing whether interventions on future regimes induce consistent changes in the generated LOB dynamics; (3) \\textit{Counterfactual usefulness}, assessing whether synthetic counterfactual trajectories improve downstream prediction of future market regimes.",
      "url": "https://arxiv.org/abs/2602.03776",
      "pdfUrl": "https://arxiv.org/pdf/2602.03776.pdf",
      "titleJa": "DiffLOB: 指値注文帳における反事実生成のための拡散モデル"
    },
    {
      "id": "2602.03775",
      "arxivId": "2602.03775",
      "title": "An Empirical Study of Collective Behaviors and Social Dynamics in Large Language Model Agents",
      "authors": [
        "Farnoosh Hashemi",
        "Michael W. Macy"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "abstract": "Large Language Models (LLMs) increasingly mediate our social, cultural, and political interactions. While they can simulate some aspects of human behavior and decision-making, it is still underexplored whether repeated interactions with other agents amplify their biases or lead to exclusionary behaviors. To this end, we study Chirper.ai-an LLM-driven social media platform-analyzing 7M posts and interactions among 32K LLM agents over a year. We start with homophily and social influence among LLMs, learning that similar to humans', their social networks exhibit these fundamental phenomena. Next, we study the toxic language of LLMs, its linguistic features, and their interaction patterns, finding that LLMs show different structural patterns in toxic posting than humans. After studying the ideological leaning in LLMs posts, and the polarization in their community, we focus on how to prevent their potential harmful activities. We present a simple yet effective method, called Chain of Social Thought (CoST), that reminds LLM agents to avoid harmful posting.",
      "url": "https://arxiv.org/abs/2602.03775",
      "pdfUrl": "https://arxiv.org/pdf/2602.03775.pdf",
      "titleJa": "大規模言語モデルエージェントにおける集団行動と社会ダイナミクスの実証的研究"
    },
    {
      "id": "2602.03772",
      "arxivId": "2602.03772",
      "title": "UniGeM: Unifying Data Mixing and Selection via Geometric Exploration and Mining",
      "authors": [
        "Changhao Wang",
        "Yunfei Yu",
        "Xinhao Yao",
        "Jiaolong Yang",
        "Riccardo Cantoro",
        "Chaobo Li",
        "Qing Cui",
        "Jun Zhou"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "The scaling of Large Language Models (LLMs) is increasingly limited by data quality. Most methods handle data mixing and sample selection separately, which can break the structure in code corpora. We introduce \\textbf{UniGeM}, a framework that unifies mixing and selection by treating data curation as a \\textit{manifold approximation} problem without training proxy models or relying on external reference datasets. UniGeM operates hierarchically: \\textbf{Macro-Exploration} learns mixing weights with stability-based clustering; \\textbf{Micro-Mining} filters high-quality instances by their geometric distribution to ensure logical consistency. Validated by training 8B and 16B MoE models on 100B tokens, UniGeM achieves \\textbf{2.0$\\times$ data efficiency} over a random baseline and further improves overall performance compared to SOTA methods in reasoning-heavy evaluations and multilingual generalization.",
      "url": "https://arxiv.org/abs/2602.03772",
      "pdfUrl": "https://arxiv.org/pdf/2602.03772.pdf",
      "titleJa": "UniGeM: 幾何学的探索とマイニングによるデータの混合と選択の統合"
    },
    {
      "id": "2602.00744",
      "arxivId": "2602.00744",
      "title": "ACE-Step 1.5: Pushing the Boundaries of Open-Source Music Generation",
      "authors": [
        "Junmin Gong",
        "Yulin Song",
        "Wenxiao Zhao",
        "Sen Wang",
        "Shengyuan Xu",
        "Jing Guo"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.SD"
      ],
      "abstract": "We present ACE-Step v1.5, a highly efficient open-source music foundation model that brings commercial-grade generation to consumer hardware. On commonly used evaluation metrics, ACE-Step v1.5 achieves quality beyond most commercial music models while remaining extremely fast -- under 2 seconds per full song on an A100 and under 10 seconds on an RTX 3090. The model runs locally with less than 4GB of VRAM, and supports lightweight personalization: users can train a LoRA from just a few songs to capture their own style. At its core lies a novel hybrid architecture where the Language Model (LM) functions as an omni-capable planner: it transforms simple user queries into comprehensive song blueprints -- scaling from short loops to 10-minute compositions -- while synthesizing metadata, lyrics, and captions via Chain-of-Thought to guide the Diffusion Transformer (DiT). Uniquely, this alignment is achieved through intrinsic reinforcement learning relying solely on the model's internal mechanisms, thereby eliminating the biases inherent in external reward models or human preferences. Beyond standard synthesis, ACE-Step v1.5 unifies precise stylistic control with versatile editing capabilities -- such as cover generation, repainting, and vocal-to-BGM conversion -- while maintaining strict adherence to prompts across 50+ languages. This paves the way for powerful tools that seamlessly integrate into the creative workflows of music artists, producers, and content creators. The code, the model weights and the demo are available at: https://ace-step.github.io/ace-step-v1.5.github.io/",
      "url": "https://arxiv.org/abs/2602.00744",
      "pdfUrl": "https://arxiv.org/pdf/2602.00744.pdf",
      "titleJa": "ACE-ステップ1.5: オープンソース音楽生成の限界を押し広げる"
    },
    {
      "id": "2601.22764",
      "arxivId": "2601.22764",
      "title": "How Far Can Pretrained LLMs Go in Symbolic Music? Controlled Comparisons of Supervised and Preference-based Adaptation",
      "authors": [
        "Deepak Kumar",
        "Emmanouil Karystinaios",
        "Gerhard Widmer",
        "Markus Schedl"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Music often shares notable parallels with language, motivating the use of pretrained large language models (LLMs) for symbolic music understanding and generation. Despite growing interest, the practical effectiveness of adapting instruction-tuned LLMs to symbolic music remains insufficiently characterized. We present a controlled comparative study of finetuning strategies for ABC-based generation and understanding, comparing an off-the-shelf instruction-tuned backbone to domain-adapted variants and a music-specialized LLM baseline. Across multiple symbolic music corpora and evaluation signals, we provide some insights into adaptation choices for symbolic music applications. We highlight the domain adaptation vs.~preserving prior information tradeoff as well as the distinct behaviour of metrics used to measure the domain adaptation for symbolic music.",
      "url": "https://arxiv.org/abs/2601.22764",
      "pdfUrl": "https://arxiv.org/pdf/2601.22764.pdf",
      "titleJa": "事前学習済みLLMは象徴音楽においてどこまで到達できるか？教師あり適応と選好に基づく適応の制御比較"
    },
    {
      "id": "2601.21740",
      "arxivId": "2601.21740",
      "title": "MIDI-LLaMA: An Instruction-Following Multimodal LLM for Symbolic Music Understanding",
      "authors": [
        "Meng Yang",
        "Jon McCormack",
        "Maria Teresa Llano",
        "Wanchao Su",
        "Chao Lei"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLM) for audio music have demonstrated strong capabilities in music understanding, yet symbolic music, a fundamental representation of musical structure, remains unexplored. In this work, we introduce MIDI-LLaMA, the first instruction-following MLLM for symbolic music understanding. Our approach aligns the MIDI encoder MusicBERT and Llama-3-8B via a two-stage pipeline comprising feature alignment and instruction tuning. To support training, we design a scalable annotation pipeline that annotates GiantMIDI-Piano with fine-grained metadata, resulting in a MIDI-text dataset. Compared with the baseline trained on converting MIDI into ABC notation under the same instruction-tuning procedure, MIDI-LLaMA substantially outperforms in captioning and semantic alignment in question answering. Human evaluation further confirms the advantages of MIDI-LLaMA in music understanding, emotion recognition, creativity, and overall preference. These findings demonstrate that incorporating symbolic music into large language models enhances their capacity for musical understanding.",
      "url": "https://arxiv.org/abs/2601.21740",
      "pdfUrl": "https://arxiv.org/pdf/2601.21740.pdf",
      "titleJa": "MIDI-LLaMA: 記号的音楽理解のための指示追従型マルチモーダルLLM"
    },
    {
      "id": "2601.21260",
      "arxivId": "2601.21260",
      "title": "Music Plagiarism Detection: Problem Formulation and a Segment-based Solution",
      "authors": [
        "Seonghyeon Go",
        "Yumin Kim"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Recently, the problem of music plagiarism has emerged as an even more pressing social issue. As music information retrieval research advances, there is a growing effort to address issues related to music plagiarism. However, many studies, including our previous work, have conducted research without clearly defining what the music plagiarism detection task actually involves. This lack of a clear definition has slowed research progress and made it hard to apply results to real-world scenarios. To fix this situation, we defined how Music Plagiarism Detection is different from other MIR tasks and explained what problems need to be solved. We introduce the Similar Music Pair dataset to support this newly defined task. In addition, we propose a method based on segment transcription as one way to solve the task. Our demo and dataset are available at https://github.com/Mippia/ICASSP2026-MPD.",
      "url": "https://arxiv.org/abs/2601.21260",
      "pdfUrl": "https://arxiv.org/pdf/2601.21260.pdf",
      "titleJa": "音楽盗作検出：問題の定式化とセグメントベースのソリューション"
    },
    {
      "id": "2601.20478",
      "arxivId": "2601.20478",
      "title": "On Every Note a Griff: Looking for a Useful Representation of Basso Continuo Performance Style",
      "authors": [
        "Adam Štefunko",
        "Carlos Eduardo Cancino-Chacón",
        "Jan Hajič"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.SD",
        "cs.IR"
      ],
      "abstract": "Basso continuo is a baroque improvisatory accompaniment style which involves improvising multiple parts above a given bass line in a musical score on a harpsichord or organ. Basso continuo is not merely a matter of history; moreover, it is a historically inspired living practice, and The Aligned Continuo Dataset (ACoRD) records the first sample of modern-day basso continuo playing in the symbolic domain. This dataset, containing 175 MIDI recordings of 5 basso continuo scores performed by 7 players, allows us to start observing and analyzing the variety that basso continuo improvisation brings. A recently proposed basso continuo performance-to-score alignment system provides a way of mapping improvised performance notes to score notes. In order to study aligned basso continuo performances, we need an appropriate feature representation. We propose griff, a representation inspired by historical basso continuo treatises. It enables us to encode both pitch content and structure of a basso continuo realization in a transposition-invariant way. Griffs are directly extracted from aligned basso continuo performances by grouping together performance notes aligned to the same score note in a onset-time ordered way, and they provide meaningful tokens that form a feature space in which we can analyze basso continuo performance styles. We statistically describe griffs extracted from the ACoRD dataset recordings, and show in two experiments how griffs can be used for statistical analysis of individuality of different players' basso continuo performance styles. We finally present an argument why it is desirable to preserve the structure of a basso continuo improvisation in order to conduct a refined analysis of personal performance styles of individual basso continuo practitioners, and why griffs can provide a meaningful historically informed feature space worthy of a more robust empirical validation.",
      "url": "https://arxiv.org/abs/2601.20478",
      "pdfUrl": "https://arxiv.org/pdf/2601.20478.pdf",
      "titleJa": "すべての音符にグリフ：通奏低音演奏スタイルの有用な表現を求めて"
    },
    {
      "id": "2601.20883",
      "arxivId": "2601.20883",
      "title": "VoxMorph: Scalable Zero-shot Voice Identity Morphing via Disentangled Embeddings",
      "authors": [
        "Bharath Krishnamurthy",
        "Ajita Rattani"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.CR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Morphing techniques generate artificial biometric samples that combine features from multiple individuals, allowing each contributor to be verified against a single enrolled template. While extensively studied in face recognition, this vulnerability remains largely unexplored in voice biometrics. Prior work on voice morphing is computationally expensive, non-scalable, and limited to acoustically similar identity pairs, constraining practical deployment. Moreover, existing sound-morphing methods target audio textures, music, or environmental sounds and are not transferable to voice identity manipulation. We propose VoxMorph, a zero-shot framework that produces high-fidelity voice morphs from as little as five seconds of audio per subject without model retraining. Our method disentangles vocal traits into prosody and timbre embeddings, enabling fine-grained interpolation of speaking style and identity. These embeddings are fused via Spherical Linear Interpolation (Slerp) and synthesized using an autoregressive language model coupled with a Conditional Flow Matching network. VoxMorph achieves state-of-the-art performance, delivering a 2.6x gain in audio quality, a 73% reduction in intelligibility errors, and a 67.8% morphing attack success rate on automated speaker verification systems under strict security thresholds. This work establishes a practical and scalable paradigm for voice morphing with significant implications for biometric security. The code and dataset are available on our project page: https://vcbsl.github.io/VoxMorph/",
      "url": "https://arxiv.org/abs/2601.20883",
      "pdfUrl": "https://arxiv.org/pdf/2601.20883.pdf",
      "titleJa": "VoxMorph: 分離埋め込みによるスケーラブルなゼロショット音声アイデンティティモーフィング"
    },
    {
      "id": "2601.19702",
      "arxivId": "2601.19702",
      "title": "SAM Audio Judge: A Unified Multimodal Framework for Perceptual Evaluation of Audio Separation",
      "authors": [
        "Helin Wang",
        "Bowen Shi",
        "Andros Tjandra",
        "John Hoffman",
        "Yi-Chiao Wu",
        "Apoorv Vyas",
        "Najim Dehak",
        "Ann Lee",
        "Wei-Ning Hsu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "The performance evaluation remains a complex challenge in audio separation, and existing evaluation metrics are often misaligned with human perception, course-grained, relying on ground truth signals. On the other hand, subjective listening tests remain the gold standard for real-world evaluation, but they are expensive, time-consuming, and difficult to scale. This paper addresses the growing need for automated systems capable of evaluating audio separation without human intervention. The proposed evaluation metric, SAM Audio Judge (SAJ), is a multimodal fine-grained reference-free objective metric, which shows highly alignment with human perceptions. SAJ supports three audio domains (speech, music and general sound events) and three prompt inputs (text, visual and span), covering four different dimensions of evaluation (recall, percision, faithfulness, and overall). SAM Audio Judge also shows potential applications in data filtering, pseudo-labeling large datasets and reranking in audio separation models. We release our code and pre-trained models at: https://github.com/facebookresearch/sam-audio.",
      "url": "https://arxiv.org/abs/2601.19702",
      "pdfUrl": "https://arxiv.org/pdf/2601.19702.pdf",
      "titleJa": "SAM Audio Judge: 音声分離の知覚評価のための統合マルチモーダルフレームワーク"
    },
    {
      "id": "2601.19109",
      "arxivId": "2601.19109",
      "title": "Interpretable and Perceptually-Aligned Music Similarity with Pretrained Embeddings",
      "authors": [
        "Arhan Vohra",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Perceptual similarity representations enable music retrieval systems to determine which songs sound most similar to listeners. State-of-the-art approaches based on task-specific training via self-supervised metric learning show promising alignment with human judgment, but are difficult to interpret or generalize due to limited dataset availability. We show that pretrained text-audio embeddings (CLAP and MuQ-MuLan) offer comparable perceptual alignment on similarity tasks without any additional fine-tuning. To surpass this baseline, we introduce a novel method to perceptually align pretrained embeddings with source separation and linear optimization on ABX preference data from listening tests. Our model provides interpretable and controllable instrument-wise weights, allowing music producers to retrieve stem-level loops and samples based on mixed reference songs.",
      "url": "https://arxiv.org/abs/2601.19109",
      "pdfUrl": "https://arxiv.org/pdf/2601.19109.pdf",
      "titleJa": "事前学習済みの埋め込みによる解釈可能かつ知覚的に整合された音楽類似性"
    },
    {
      "id": "2601.18766",
      "arxivId": "2601.18766",
      "title": "Learning to Discover: A Generalized Framework for Raga Identification without Forgetting",
      "authors": [
        "Parampreet Singh",
        "Somya Kumar",
        "Chaitanya Shailendra Nitawe",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.",
      "url": "https://arxiv.org/abs/2601.18766",
      "pdfUrl": "https://arxiv.org/pdf/2601.18766.pdf",
      "titleJa": "発見することを学ぶ：忘れずにラーガを識別するための一般化された枠組み"
    },
    {
      "id": "2601.18339",
      "arxivId": "2601.18339",
      "title": "A Dataset for Automatic Vocal Mode Classification",
      "authors": [
        "Reemt Hinrichs",
        "Sonja Stephan",
        "Alexander Lange",
        "Jörn Ostermann"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\\,\\% was achieved with a ResNet18. The dataset can be downloaded under https://zenodo.org/records/14276415.",
      "url": "https://arxiv.org/abs/2601.18339",
      "pdfUrl": "https://arxiv.org/pdf/2601.18339.pdf",
      "titleJa": "自動音声モード分類のためのデータセット"
    },
    {
      "id": "2601.19951",
      "arxivId": "2601.19951",
      "title": "Pianoroll-Event: A Novel Score Representation for Symbolic Music",
      "authors": [
        "Lekai Qian",
        "Haoyu Gu",
        "Dehan Li",
        "Boyu Cao",
        "Qi Liu"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Symbolic music representation is a fundamental challenge in computational musicology. While grid-based representations effectively preserve pitch-time spatial correspondence, their inherent data sparsity leads to low encoding efficiency. Discrete-event representations achieve compact encoding but fail to adequately capture structural invariance and spatial locality. To address these complementary limitations, we propose Pianoroll-Event, a novel encoding scheme that describes pianoroll representations through events, combining structural properties with encoding efficiency while maintaining temporal dependencies and local spatial patterns. Specifically, we design four complementary event types: Frame Events for temporal boundaries, Gap Events for sparse regions, Pattern Events for note patterns, and Musical Structure Events for musical metadata. Pianoroll-Event strikes an effective balance between sequence length and vocabulary size, improving encoding efficiency by 1.36\\times to 7.16\\times over representative discrete sequence methods. Experiments across multiple autoregressive architectures show models using our representation consistently outperform baselines in both quantitative and human evaluations.",
      "url": "https://arxiv.org/abs/2601.19951",
      "pdfUrl": "https://arxiv.org/pdf/2601.19951.pdf",
      "titleJa": "ピアノロールイベント：象徴音楽のための新しい楽譜表現"
    },
    {
      "id": "2601.17645",
      "arxivId": "2601.17645",
      "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
      "authors": [
        "Xilin Jiang",
        "Qiaolin Wang",
        "Junkai Wu",
        "Xiaomin He",
        "Zhongweiyang Xu",
        "Yinghao Ma",
        "Minshuo Piao",
        "Kaiyi Yang",
        "Xiuwen Zheng",
        "Riki Shimizu",
        "Yicong Chen",
        "Arsalan Firoozi",
        "Gavin Mischler",
        "Sukru Samet Dindar",
        "Richard Antonello",
        "Linyang He",
        "Tsun-An Hsieh",
        "Xulin Fan",
        "Yulun Wu",
        "Yuesheng Ma",
        "Chaitanya Amballa",
        "Weixiong Chen",
        "Jiarui Hai",
        "Ruisi Li",
        "Vishal Choudhari",
        "Cong Han",
        "Yinghao Aaron Li",
        "Adeen Flinker",
        "Mounya Elhilali",
        "Emmanouil Benetos",
        "Mark Hasegawa-Johnson",
        "Romit Roy Choudhury",
        "Nima Mesgarani"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public",
      "url": "https://arxiv.org/abs/2601.17645",
      "pdfUrl": "https://arxiv.org/pdf/2601.17645.pdf",
      "titleJa": "AVMeme試験：法学修士（LLM）の文脈的・文化的知識と思考力を評価するマルチモーダル・多言語・多文化ベンチマーク"
    },
    {
      "id": "2601.17517",
      "arxivId": "2601.17517",
      "title": "EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding",
      "authors": [
        "Luca Cerovaz",
        "Michele Mancusi",
        "Emanuele Rodolà"
      ],
      "publishedDate": "2026-01-24",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Audio codecs power discrete music generative modelling, music streaming and immersive media by shrinking PCM audio to bandwidth-friendly bit-rates. Recent works have gravitated towards processing in the spectral domain; however, spectrogram-domains typically struggle with phase modeling which is naturally complex-valued. Most frequency-domain neural codecs either disregard phase information or encode it as two separate real-valued channels, limiting spatial fidelity. This entails the need to introduce adversarial discriminators at the expense of convergence speed and training stability to compensate for the inadequate representation power of the audio signal. In this work we introduce an end-to-end complex-valued RVQ-VAE audio codec that preserves magnitude-phase coupling across the entire analysis-quantization-synthesis pipeline and removes adversarial discriminators and diffusion post-filters. Without GANs or diffusion we match or surpass much longer-trained baselines in-domain and reach SOTA out-of-domain performance. Compared to standard baselines that train for hundreds of thousands of steps, our model reducing training budget by an order of magnitude is markedly more compute-efficient while preserving high perceptual quality.",
      "url": "https://arxiv.org/abs/2601.17517",
      "pdfUrl": "https://arxiv.org/pdf/2601.17517.pdf",
      "titleJa": "EuleroDec: 効率的かつ堅牢なオーディオコーディングのための複素値RVQ-VAE"
    },
    {
      "id": "2602.02591",
      "arxivId": "2602.02591",
      "title": "VividVoice: A Unified Framework for Scene-Aware Visually-Driven Speech Synthesis",
      "authors": [
        "Chengyuan Ma",
        "Jiawei Jin",
        "Ruijie Xiong",
        "Chunxiang Jin",
        "Canxiang Yan",
        "Wenming Yang"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "We introduce and define a novel task-Scene-Aware Visually-Driven Speech Synthesis, aimed at addressing the limitations of existing speech generation models in creating immersive auditory experiences that align with the real physical world. To tackle the two core challenges of data scarcity and modality decoupling, we propose VividVoice, a unified generative framework. First, we constructed a large-scale, high-quality hybrid multimodal dataset, Vivid-210K, which, through an innovative programmatic pipeline, establishes a strong correlation between visual scenes, speaker identity, and audio for the first time. Second, we designed a core alignment module, D-MSVA, which leverages a decoupled memory bank architecture and a cross-modal hybrid supervision strategy to achieve fine-grained alignment from visual scenes to timbre and environmental acoustic features. Both subjective and objective experimental results provide strong evidence that VividVoice significantly outperforms existing baseline models in terms of audio fidelity, content clarity, and multimodal consistency. Our demo is available at https://chengyuann.github.io/VividVoice/.",
      "url": "https://arxiv.org/abs/2602.02591",
      "pdfUrl": "https://arxiv.org/pdf/2602.02591.pdf",
      "titleJa": "VividVoice: シーン認識型視覚駆動型音声合成のための統合フレームワーク"
    },
    {
      "id": "2602.00648",
      "arxivId": "2602.00648",
      "title": "High-Fidelity Generative Audio Compression at 0.275kbps",
      "authors": [
        "Hao Ma",
        "Ruihao Jing",
        "Shansong Liu",
        "Cheng Gong",
        "Chi Zhang",
        "Xiao-Lei Zhang",
        "Xuelong Li"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "High-fidelity general audio compression at ultra-low bitrates is crucial for applications ranging from low-bandwidth communication to generative audio-language modeling. Traditional audio compression methods and contemporary neural codecs are fundamentally designed for waveform reconstruction. As a result, when operating at ultra-low bitrates, these methods degrade rapidly and often fail to preserve essential information, leading to severe acoustic artifacts and pronounced semantic distortion. To overcome these limitations, we introduce Generative Audio Compression (GAC), a novel paradigm shift from signal fidelity to task-oriented effectiveness. Implemented within the AI Flow framework, GAC is theoretically grounded in the Law of Information Capacity. These foundations posit that abundant computational power can be leveraged at the receiver to offset extreme communication bottlenecks--exemplifying the More Computation, Less Bandwidth philosophy. By integrating semantic understanding at the transmitter with scalable generative synthesis at the receiver, GAC offloads the information burden to powerful model priors. Our 1.8B-parameter model achieves high-fidelity reconstruction of 32kHz general audio at an unprecedented bitrate of 0.275kbps. Even at 0.175kbps, it still preserves a strong intelligible audio transmission capability, which represents an about 3000x compression ratio, significantly outperforming current state-of-the-art neural codecs in maintaining both perceptual quality and semantic consistency.",
      "url": "https://arxiv.org/abs/2602.00648",
      "pdfUrl": "https://arxiv.org/pdf/2602.00648.pdf",
      "titleJa": "0.275kbpsの高忠実度生成オーディオ圧縮"
    },
    {
      "id": "2602.00594",
      "arxivId": "2602.00594",
      "title": "Kanade: A Simple Disentangled Tokenizer for Spoken Language Modeling",
      "authors": [
        "Zhijie Huang",
        "Stephen McIntosh",
        "Daisuke Saito",
        "Nobuaki Minematsu"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "A good language model starts with a good tokenizer. Tokenization is especially important for speech modeling, which must handle continuous signals that mix linguistic and non-linguistic information. A speech tokenizer should extract phonetics and prosody, suppress linguistically irrelevant information like speaker identity, and enable high-quality synthesis. We present Kanade, a single-layer disentangled speech tokenizer that realizes this ideal. Kanade separates out acoustic constants to create a single stream of tokens that captures rich phonetics and prosody. It does so without the need for auxiliary methods that existing disentangled codecs often rely on. Experiments show that Kanade achieves state-of-the-art speaker disentanglement and lexical availability, while maintaining excellent reconstruction quality.",
      "url": "https://arxiv.org/abs/2602.00594",
      "pdfUrl": "https://arxiv.org/pdf/2602.00594.pdf",
      "titleJa": "Kanade: 音声言語モデルのためのシンプルな分離トークナイザー"
    },
    {
      "id": "2602.00560",
      "arxivId": "2602.00560",
      "title": "Edit Content, Preserve Acoustics: Imperceptible Text-Based Speech Editing via Self-Consistency Rewards",
      "authors": [
        "Yong Ren",
        "Jiangyan Yi",
        "Jianhua Tao",
        "Zhengqi Wen",
        "Tao Wang"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Imperceptible text-based speech editing allows users to modify spoken content by altering the transcript. It demands that modified segments fuse seamlessly with the surrounding context. Prevalent methods operating in the acoustic space suffer from inherent content-style entanglement, leading to generation instability and boundary artifacts. In this paper, we propose a novel framework grounded in the principle of \"Edit Content, Preserve Acoustics\". Our approach relies on two core components: (1) Structural Foundations, which decouples editing into a stable semantic space while delegating acoustic reconstruction to a Flow Matching decoder; and (2) Perceptual Alignment, which employs a novel Self-Consistency Rewards Group Relative Policy Optimization. By leveraging a pre-trained Text-to-Speech model as an implicit critic -- complemented by strict intelligibility and duration constraints -- we effectively align the edited semantic token sequence with the original context. Empirical evaluations demonstrate that our method significantly outperforms state-of-the-art autoregressive and non-autoregressive baselines, achieving superior intelligibility, robustness, and perceptual quality.",
      "url": "https://arxiv.org/abs/2602.00560",
      "pdfUrl": "https://arxiv.org/pdf/2602.00560.pdf",
      "titleJa": "コンテンツを編集し、音響を維持する：自己一貫性報酬による知覚できないテキストベースの音声編集"
    },
    {
      "id": "2601.23161",
      "arxivId": "2601.23161",
      "title": "DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding",
      "authors": [
        "Jiaming Zhou",
        "Xuxin Cheng",
        "Shiwan Zhao",
        "Yuhang Jia",
        "Cao Liu",
        "Ke Zeng",
        "Xunliang Cai",
        "Yong Qin"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.",
      "url": "https://arxiv.org/abs/2601.23161",
      "pdfUrl": "https://arxiv.org/pdf/2601.23161.pdf",
      "titleJa": "DIFFA-2: 一般的な音声理解のための実用的な拡散大規模言語モデル"
    },
    {
      "id": "2601.23149",
      "arxivId": "2601.23149",
      "title": "Hearing is Believing? Evaluating and Analyzing Audio Language Model Sycophancy with SYAUDIO",
      "authors": [
        "Junchi Yao",
        "Lokranjan Lakshmikanthan",
        "Annie Zhao",
        "Danielle Zhao",
        "Shu Yang",
        "Zikang Ding",
        "Di Wang",
        "Lijie Hu"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio Language Models (ALMs) have recently shown strong capabilities in unified reasoning over speech, sound, and natural language; yet they inherit behavioral issues observed in Large Language Models, including sycophancy--the tendency to agree with user assertions even when they contradict objective evidence. While sycophancy has been extensively studied in text and vision-language models, its manifestation in audio-conditioned reasoning remains largely unexplored, despite the need for ALMs to rely on auditory cues such as acoustic events, speaker characteristics, and speech rate. To address this gap, we introduce SYAUDIO, the first benchmark dedicated to evaluating sycophancy in ALMs, consisting of 4,319 audio questions spanning Audio Perception, Audio Reasoning, Audio Math, and Audio Ethics. Built upon established audio benchmarks and augmented with TTS-generated arithmetic and moral reasoning tasks, SYAUDIO enables systematic evaluation across multiple domains and sycophancy types with carefully verified data quality. Furthermore, we analyze audio-specific sycophancy under realistic conditions involving noise and rate, and demonstrate that supervised fine-tuning with chain-of-thought data is an effective mitigation strategy for reducing sycophantic behavior in ALMs.",
      "url": "https://arxiv.org/abs/2601.23149",
      "pdfUrl": "https://arxiv.org/pdf/2601.23149.pdf",
      "titleJa": "聞くことは信じること？SYAUDIOによる音声言語モデルの追従性の評価と分析"
    },
    {
      "id": "2601.23066",
      "arxivId": "2601.23066",
      "title": "Towards Explicit Acoustic Evidence Perception in Audio LLMs for Speech Deepfake Detection",
      "authors": [
        "Xiaoxuan Guo",
        "Yuankun Xie",
        "Haonan Cheng",
        "Jiayi Zhou",
        "Jian Liu",
        "Hengyan Huang",
        "Long Ye",
        "Qin Zhang"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Speech deepfake detection (SDD) focuses on identifying whether a given speech signal is genuine or has been synthetically generated. Existing audio large language model (LLM)-based methods excel in content understanding; however, their predictions are often biased toward semantically correlated cues, which results in fine-grained acoustic artifacts being overlooked during the decisionmaking process. Consequently, fake speech with natural semantics can bypass detectors despite harboring subtle acoustic anomalies; this suggests that the challenge stems not from the absence of acoustic data, but from its inadequate accessibility when semantic-dominant reasoning prevails. To address this issue, we investigate SDD within the audio LLM paradigm and introduce SDD with Auditory Perception-enhanced Audio Large Language Model (SDD-APALLM), an acoustically enhanced framework designed to explicitly expose fine-grained time-frequency evidence as accessible acoustic cues. By combining raw audio with structured spectrograms, the proposed framework empowers audio LLMs to more effectively capture subtle acoustic inconsistencies without compromising their semantic understanding. Experimental results indicate consistent gains in detection accuracy and robustness, especially in cases where semantic cues are misleading. Further analysis reveals that these improvements stem from a coordinated utilization of semantic and acoustic information, as opposed to simple modality aggregation.",
      "url": "https://arxiv.org/abs/2601.23066",
      "pdfUrl": "https://arxiv.org/pdf/2601.23066.pdf",
      "titleJa": "音声ディープフェイク検出のためのオーディオLLMにおける明示的な音響証拠認識に向けて"
    },
    {
      "id": "2601.23004",
      "arxivId": "2601.23004",
      "title": "Layer-Aware Early Fusion of Acoustic and Linguistic Embeddings for Cognitive Status Classification",
      "authors": [
        "Krystof Novotny",
        "Laureano Moro-Velázquez",
        "Jiri Mekyska"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Speech contains both acoustic and linguistic patterns that reflect cognitive decline, and therefore models describing only one domain cannot fully capture such complexity. This study investigates how early fusion (EF) of speech and its corresponding transcription text embeddings, with attention to encoder layer depth, can improve cognitive status classification. Using a DementiaBank-derived collection of recordings (1,629 speakers; cognitively normal controls$\\unicode{x2013}$CN, Mild Cognitive Impairment$\\unicode{x2013}$MCI, and Alzheimer's Disease and Related Dementias$\\unicode{x2013}$ADRD), we extracted frame-aligned embeddings from different internal layers of wav2vec 2.0 or Whisper combined with DistilBERT or RoBERTa. Unimodal, EF and late fusion (LF) models were trained with a transformer classifier, optimized, and then evaluated across 10 seeds. Performance consistently peaked in mid encoder layers ($\\sim$8$\\unicode{x2013}$10), with the single best F1 at Whisper + RoBERTa layer 9 and the best log loss at Whisper + DistilBERT layer 10. Acoustic-only models consistently outperformed text-only variants. EF boosts discrimination for genuinely acoustic embeddings, whereas LF improves probability calibration. Layer choice critically shapes clinical multimodal synergy.",
      "url": "https://arxiv.org/abs/2601.23004",
      "pdfUrl": "https://arxiv.org/pdf/2601.23004.pdf",
      "titleJa": "認知状態分類のための音響と言語埋め込みのレイヤー認識型早期融合"
    },
    {
      "id": "2601.22792",
      "arxivId": "2601.22792",
      "title": "CALM: Joint Contextual Acoustic-Linguistic Modeling for Personalization of Multi-Speaker ASR",
      "authors": [
        "Muhammad Shakeel",
        "Yosuke Fukumoto",
        "Chikara Maeda",
        "Chyi-Jiunn Lin",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "We present CALM, a joint Contextual Acoustic-Linguistic Modeling framework for multi-speaker automatic speech recognition (ASR). In personalized AI scenarios, the joint availability of acoustic and linguistic cues naturally motivates the integration of target-speaker conditioning with contextual biasing in overlapping conversations. CALM implements this integration in an end-to-end framework through speaker embedding-driven target-speaker extraction and dynamic vocabulary-based contextual biasing. We evaluate CALM on simulated English (LibriSpeechMix) and Japanese (Corpus of Spontaneous Japanese mixtures, CSJMix). On two-speaker mixtures, CALM reduces biased word error rate (B-WER) from 12.7 to 4.7 on LibriSpeech2Mix and biased character error rate (B-CER) from 16.6 to 8.4 on CSJMix2 (eval3), demonstrating the effectiveness of joint acoustic-linguistic modeling across languages. We additionally report results on the AMI corpus (IHM-mix condition) to validate performance on standardized speech mixtures.",
      "url": "https://arxiv.org/abs/2601.22792",
      "pdfUrl": "https://arxiv.org/pdf/2601.22792.pdf",
      "titleJa": "CALM: 複数話者ASRのパーソナライゼーションのためのコンテキスト音響言語モデリング"
    }
  ],
  "lastUpdated": "2026-02-05T01:04:10.135818",
  "totalCount": 79
}