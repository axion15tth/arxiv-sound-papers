{
  "papers": [
    {
      "id": "2601.20573",
      "arxivId": "2601.20573",
      "title": "Gen-SER: When the generative model meets speech emotion recognition",
      "authors": [
        "Taihui Wang",
        "Jinzheng Zhao",
        "Rilin Chen",
        "Tong Lei",
        "Wenwu Wang",
        "Dong Yu"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Speech emotion recognition (SER) is crucial in speech understanding and generation. Most approaches are based on either classification models or large language models. Different from previous methods, we propose Gen-SER, a novel approach that reformulates SER as a distribution shift problem via generative models. We propose to project discrete class labels into a continuous space, and obtain the terminal distribution via sinusoidal taxonomy encoding. The target-matching-based generative model is adopted to transform the initial distribution into the terminal distribution efficiently. The classification is achieved by calculating the similarity of the generated terminal distribution and ground truth terminal distribution. The experimental results confirm the efficacy of the proposed method, demonstrating its extensibility to various speech-understanding tasks and suggesting its potential applicability to a broader range of classification tasks.",
      "url": "https://arxiv.org/abs/2601.20573",
      "pdfUrl": "https://arxiv.org/pdf/2601.20573.pdf",
      "titleJa": "Gen-SER: 生成モデルと音声感情認識が出会うとき"
    },
    {
      "id": "2601.20510",
      "arxivId": "2601.20510",
      "title": "Audio Deepfake Detection in the Age of Advanced Text-to-Speech models",
      "authors": [
        "Robin Singh",
        "Aditya Yogesh Nair",
        "Fabio Palumbo",
        "Florian Barbaro",
        "Anna Dyka",
        "Lohith Rachakonda"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Recent advances in Text-to-Speech (TTS) systems have substantially increased the realism of synthetic speech, raising new challenges for audio deepfake detection. This work presents a comparative evaluation of three state-of-the-art TTS models--Dia2, Maya1, and MeloTTS--representing streaming, LLM-based, and non-autoregressive architectures. A corpus of 12,000 synthetic audio samples was generated using the Daily-Dialog dataset and evaluated against four detection frameworks, including semantic, structural, and signal-level approaches. The results reveal significant variability in detector performance across generative mechanisms: models effective against one TTS architecture may fail against others, particularly LLM-based synthesis. In contrast, a multi-view detection approach combining complementary analysis levels demonstrates robust performance across all evaluated models. These findings highlight the limitations of single-paradigm detectors and emphasize the necessity of integrated detection strategies to address the evolving landscape of audio deepfake threats.",
      "url": "https://arxiv.org/abs/2601.20510",
      "pdfUrl": "https://arxiv.org/pdf/2601.20510.pdf",
      "titleJa": "高度な音声合成モデル時代の音声ディープフェイク検出"
    },
    {
      "id": "2601.20481",
      "arxivId": "2601.20481",
      "title": "Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech",
      "authors": [
        "Myungjin Lee",
        "Eunji Shin",
        "Jiyoung Lee"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Modern zero-shot text-to-speech (TTS) models offer unprecedented expressivity but also pose serious crime risks, as they can synthesize voices of individuals who never consented. In this context, speaker unlearning aims to prevent the generation of specific speaker identities upon request. Existing approaches, reliant on retraining, are costly and limited to speakers seen in the training set. We present TruS, a training-free speaker unlearning framework that shifts the paradigm from data deletion to inference-time control. TruS steers identity-specific hidden activations to suppress target speakers while preserving other attributes (e.g., prosody and emotion). Experimental results show that TruS effectively prevents voice generation on both seen and unseen opt-out speakers, establishing a scalable safeguard for speech synthesis. The demo and code are available on http://mmai.ewha.ac.kr/trus.",
      "url": "https://arxiv.org/abs/2601.20481",
      "pdfUrl": "https://arxiv.org/pdf/2601.20481.pdf",
      "titleJa": "聞かれる前に声を消す：ゼロショット音声合成のためのトレーニング不要のスピーカーアンラーニング"
    },
    {
      "id": "2601.20478",
      "arxivId": "2601.20478",
      "title": "On Every Note a Griff: Looking for a Useful Representation of Basso Continuo Performance Style",
      "authors": [
        "Adam Štefunko",
        "Carlos Eduardo Cancino-Chacón",
        "Jan Hajič"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.SD",
        "cs.IR"
      ],
      "abstract": "Basso continuo is a baroque improvisatory accompaniment style which involves improvising multiple parts above a given bass line in a musical score on a harpsichord or organ. Basso continuo is not merely a matter of history; moreover, it is a historically inspired living practice, and The Aligned Continuo Dataset (ACoRD) records the first sample of modern-day basso continuo playing in the symbolic domain. This dataset, containing 175 MIDI recordings of 5 basso continuo scores performed by 7 players, allows us to start observing and analyzing the variety that basso continuo improvisation brings. A recently proposed basso continuo performance-to-score alignment system provides a way of mapping improvised performance notes to score notes. In order to study aligned basso continuo performances, we need an appropriate feature representation. We propose griff, a representation inspired by historical basso continuo treatises. It enables us to encode both pitch content and structure of a basso continuo realization in a transposition-invariant way. Griffs are directly extracted from aligned basso continuo performances by grouping together performance notes aligned to the same score note in a onset-time ordered way, and they provide meaningful tokens that form a feature space in which we can analyze basso continuo performance styles. We statistically describe griffs extracted from the ACoRD dataset recordings, and show in two experiments how griffs can be used for statistical analysis of individuality of different players' basso continuo performance styles. We finally present an argument why it is desirable to preserve the structure of a basso continuo improvisation in order to conduct a refined analysis of personal performance styles of individual basso continuo practitioners, and why griffs can provide a meaningful historically informed feature space worthy of a more robust empirical validation.",
      "url": "https://arxiv.org/abs/2601.20478",
      "pdfUrl": "https://arxiv.org/pdf/2601.20478.pdf",
      "titleJa": "すべての音符にグリフ：通奏低音演奏スタイルの有用な表現を求めて"
    },
    {
      "id": "2601.20432",
      "arxivId": "2601.20432",
      "title": "Self Voice Conversion as an Attack against Neural Audio Watermarking",
      "authors": [
        "Yigitcan Özer",
        "Wanying Ge",
        "Zhe Zhang",
        "Xin Wang",
        "Junichi Yamagishi"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Audio watermarking embeds auxiliary information into speech while maintaining speaker identity, linguistic content, and perceptual quality. Although recent advances in neural and digital signal processing-based watermarking methods have improved imperceptibility and embedding capacity, robustness is still primarily assessed against conventional distortions such as compression, additive noise, and resampling. However, the rise of deep learning-based attacks introduces novel and significant threats to watermark security. In this work, we investigate self voice conversion as a universal, content-preserving attack against audio watermarking systems. Self voice conversion remaps a speaker's voice to the same identity while altering acoustic characteristics through a voice conversion model. We demonstrate that this attack severely degrades the reliability of state-of-the-art watermarking approaches and highlight its implications for the security of modern audio watermarking techniques.",
      "url": "https://arxiv.org/abs/2601.20432",
      "pdfUrl": "https://arxiv.org/pdf/2601.20432.pdf",
      "titleJa": "ニューラルオーディオ透かしに対する攻撃としての自己音声変換"
    },
    {
      "id": "2601.20426",
      "arxivId": "2601.20426",
      "title": "Mix2Morph: Learning Sound Morphing from Noisy Mixes",
      "authors": [
        "Annie Chu",
        "Hugo Flores García",
        "Oriol Nieto",
        "Justin Salamon",
        "Bryan Pardo",
        "Prem Seetharaman"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.SD"
      ],
      "abstract": "We introduce Mix2Morph, a text-to-audio diffusion model fine-tuned to perform sound morphing without a dedicated dataset of morphs. By finetuning on noisy surrogate mixes at higher diffusion timesteps, Mix2Morph yields stable, perceptually coherent morphs that convincingly integrate qualities of both sources. We specifically target sound infusions, a practically and perceptually motivated subclass of morphing in which one sound acts as the dominant primary source, providing overall temporal and structural behavior, while a secondary sound is infused throughout, enriching its timbral and textural qualities. Objective evaluations and listening tests show that Mix2Morph outperforms prior baselines and produces high-quality sound infusions across diverse categories, representing a step toward more controllable and concept-driven tools for sound design. Sound examples are available at https://anniejchu.github.io/mix2morph .",
      "url": "https://arxiv.org/abs/2601.20426",
      "pdfUrl": "https://arxiv.org/pdf/2601.20426.pdf",
      "titleJa": "Mix2Morph: ノイズの多いミックスからサウンドモーフィングを学ぶ"
    },
    {
      "id": "2601.20362",
      "arxivId": "2601.20362",
      "title": "Switchcodec: Adaptive residual-expert sparse quantization for high-fidelity neural audio coding",
      "authors": [
        "Xiangbo Wang",
        "Wenbin Jiang",
        "Jin Wang",
        "Yubo You",
        "Sheng Fang",
        "Fei Wen"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent neural audio compression models often rely on residual vector quantization for high-fidelity coding, but using a fixed number of per-frame codebooks is suboptimal for the wide variability of audio content-especially for signals that are either very simple or highly complex. To address this limitation, we propose SwitchCodec, a neural audio codec based on Residual Experts Vector Quantization (REVQ). REVQ combines a shared quantizer with dynamically routed expert quantizers that are activated according to the input audio, decoupling bitrate from codebook capacity and improving compression efficiency. This design ensures full training and utilization of each quantizer. In addition, a variable-bitrate mechanism adjusts the number of active expert quantizers at inference, enabling multi-bitrate operation without retraining. Experiments demonstrate that SwitchCodec surpasses existing baselines on both objective metrics and subjective listening tests.",
      "url": "https://arxiv.org/abs/2601.20362",
      "pdfUrl": "https://arxiv.org/pdf/2601.20362.pdf",
      "titleJa": "Switchcodec: 高忠実度ニューラルオーディオコーディングのための適応型残差エキスパートスパース量子化"
    },
    {
      "id": "2601.20185",
      "arxivId": "2601.20185",
      "title": "Improving X-Codec-2.0 for Multi-Lingual Speech: 25 Hz Latent Rate and 24 kHz Sampling",
      "authors": [
        "Husein Zolkepli"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "X-Codec-2.0 has shown strong performance in neural audio compression and multilingual speech modeling, operating at a 50 Hz latent rate and a 16 kHz sampling rate using frozen HuBERT features. While effective, this configuration limits temporal efficiency and audio fidelity. In this work, we explore a simple and effective modification by introducing additional pooling and increasing the decoder hop size. This reduces the latent rate from 50 Hz to 25 Hz and simultaneously raises the output sampling rate from 16 kHz to 24 kHz, improving efficiency and perceptual quality without altering the core architecture. Evaluated on the multilingual Common Voice 17 test set, the proposed configuration achieves a 0.29 MOS improvement over the original X-Codec-2.0 baseline based on UTMOSv2, and attains the best reported performance among all codecs operating at 25 Hz. The source code, checkpoints, and generation comparisons are released at \\href{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}.",
      "url": "https://arxiv.org/abs/2601.20185",
      "pdfUrl": "https://arxiv.org/pdf/2601.20185.pdf",
      "titleJa": "多言語音声のための X-Codec-2.0 の改良: 25 Hz の潜在レートと 24 kHz のサンプリング"
    },
    {
      "id": "2601.20142",
      "arxivId": "2601.20142",
      "title": "Mind the Shift: Using Delta SSL Embeddings to Enhance Child ASR",
      "authors": [
        "Zilai Wang",
        "Natarajan Balaji Shankar",
        "Kaiyuan Zhang",
        "Zihan Wang",
        "Abeer Alwan"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Self-supervised learning (SSL) models have achieved impressive results across many speech tasks, yet child automatic speech recognition (ASR) remains challenging due to limited data and pretraining domain mismatch. Fine-tuning SSL models on child speech induces shifts in the representation space. We hypothesize that delta SSL embeddings, defined as the differences between embeddings from a finetuned model and those from its pretrained counterpart, encode task-specific information that complements finetuned features from another SSL model. We evaluate multiple fusion strategies on the MyST childrens corpus using different models. Results show that delta embedding fusion with WavLM yields up to a 10 percent relative WER reduction for HuBERT and a 4.4 percent reduction for W2V2, compared to finetuned embedding fusion. Notably, fusing WavLM with delta W2V2 embeddings achieves a WER of 9.64, setting a new state of the art among SSL models on the MyST corpus. These findings demonstrate the effectiveness of delta embeddings and highlight feature fusion as a promising direction for advancing child ASR.",
      "url": "https://arxiv.org/abs/2601.20142",
      "pdfUrl": "https://arxiv.org/pdf/2601.20142.pdf",
      "titleJa": "変化に注意: Delta SSL 埋め込みを使用して子供の ASR を強化する"
    },
    {
      "id": "2601.19786",
      "arxivId": "2601.19786",
      "title": "Rethinking Discrete Speech Representation Tokens for Accent Generation",
      "authors": [
        "Jinzuomu Zhong",
        "Yi Wang",
        "Korin Richmond",
        "Peter Bell"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "Discrete Speech Representation Tokens (DSRTs) have become a foundational component in speech generation. While prior work has extensively studied phonetic and speaker information in DSRTs, how accent information is encoded in DSRTs remains largely unexplored. In this paper, we present the first systematic investigation of accent information in DSRTs. We propose a unified evaluation framework that measures both accessibility of accent information via a novel Accent ABX task and recoverability via cross-accent Voice Conversion (VC) resynthesis. Using this framework, we analyse DSRTs derived from a variety of speech encoders. Our results reveal that accent information is substantially reduced when ASR supervision is used to fine-tune the encoder, but cannot be effectively disentangled from phonetic and speaker information through naive codebook size reduction. Based on these findings, we propose new content-only and content-accent DSRTs that significantly outperform existing designs in controllable accent generation. Our work highlights the importance of accent-aware evaluation and provides practical guidance for designing DSRTs for accent-controlled speech generation.",
      "url": "https://arxiv.org/abs/2601.19786",
      "pdfUrl": "https://arxiv.org/pdf/2601.19786.pdf",
      "titleJa": "アクセント生成のための離散音声表現トークンの再考"
    },
    {
      "id": "2601.19781",
      "arxivId": "2601.19781",
      "title": "Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means",
      "authors": [
        "Kentaro Onda",
        "Hayato Futami",
        "Yosuke Kashiwagi",
        "Emiru Tsunoo",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In recent years, there has been growing interest in representing speech with discrete tokens, which serve as pseudo-text for speech language models (speechLMs) and as efficient intermediate representations for downstream tasks. These tokens are typically categorized as acoustic and phonetic tokens: the former holds detailed acoustic information for reconstruction while the latter mainly captures linguistic content. In human speech communication, however, unnecessary acoustic details such as speaker information are abstracted, while both linguistic and prosodic information are utilized for speech comprehension and production. Given this, neither type of token seems an ideal representation for tasks sensitive to prosody, such as speechLMs. In this study, we propose the Phonological Tokenizer, a method that fine-tunes phonetic tokens via differentiable k-means with a multi-task objective of ASR and speech resynthesis. Experimental validation on diverse tasks confirms that our tokens retain phonological (both linguistic and prosodic) information while appropriately discarding speaker identity.",
      "url": "https://arxiv.org/abs/2601.19781",
      "pdfUrl": "https://arxiv.org/pdf/2601.19781.pdf",
      "titleJa": "音韻論トークナイザー: 微分可能K平均法を用いた多目的微調整による韻律を考慮した音声トークン"
    },
    {
      "id": "2601.19767",
      "arxivId": "2601.19767",
      "title": "Advanced Modeling of Interlanguage Speech Intelligibility Benefit with L1-L2 Multi-Task Learning Using Differentiable K-Means for Accent-Robust Discrete Token-Based ASR",
      "authors": [
        "Kentaro Onda",
        "Satoru Fukayama",
        "Daisuke Saito",
        "Nobuaki Minematsu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Building ASR systems robust to foreign-accented speech is an important challenge in today's globalized world. A prior study explored the way to enhance the performance of phonetic token-based ASR on accented speech by reproducing the phenomenon known as interlanguage speech intelligibility benefit (ISIB), where foreign-accented speech is more intelligible to listeners sharing the speaker's native language than to native listeners. ISIB was technically implemented by using the speaker's L1 to learn k-means cluster centroids in an SSL feature space to obtain phonetic tokens. In this study, we propose a more advanced modeling of ISIB. By employing differentiable k-means and optimizing the entire module for both L1 and L2 ASR, the proposed method outperformed the baselines, both when using only native speech and when additionally incorporating a limited amount of accented speech. Notably, in the latter scenario, our method achieved approximately a 20% relative improvement in recognition accuracy.",
      "url": "https://arxiv.org/abs/2601.19767",
      "pdfUrl": "https://arxiv.org/pdf/2601.19767.pdf",
      "titleJa": "アクセントロバストな離散トークンベース音声認識のための微分可能K平均法を用いたL1-L2マルチタスク学習による中間言語音声明瞭度の利点の高度なモデリング"
    },
    {
      "id": "2601.19712",
      "arxivId": "2601.19712",
      "title": "Physics-Aware Novel-View Acoustic Synthesis with Vision-Language Priors and 3D Acoustic Environment Modeling",
      "authors": [
        "Congyi Fan",
        "Jian Guan",
        "Youtian Lin",
        "Dongli Xu",
        "Tong Ye",
        "Qiaoxi Zhu",
        "Pengming Feng",
        "Wenwu Wang"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.MM"
      ],
      "abstract": "Spatial audio is essential for immersive experiences, yet novel-view acoustic synthesis (NVAS) remains challenging due to complex physical phenomena such as reflection, diffraction, and material absorption. Existing methods based on single-view or panoramic inputs improve spatial fidelity but fail to capture global geometry and semantic cues such as object layout and material properties. To address this, we propose Phys-NVAS, the first physics-aware NVAS framework that integrates spatial geometry modeling with vision-language semantic priors. A global 3D acoustic environment is reconstructed from multi-view images and depth maps to estimate room size and shape, enhancing spatial awareness of sound propagation. Meanwhile, a vision-language model extracts physics-aware priors of objects, layouts, and materials, capturing absorption and reflection beyond geometry. An acoustic feature fusion adapter unifies these cues into a physics-aware representation for binaural generation. Experiments on RWAVS demonstrate that Phys-NVAS yields binaural audio with improved realism and physical consistency.",
      "url": "https://arxiv.org/abs/2601.19712",
      "pdfUrl": "https://arxiv.org/pdf/2601.19712.pdf",
      "titleJa": "視覚言語事前分布と3D音響環境モデリングを用いた物理を考慮した新規視点音響合成"
    },
    {
      "id": "2601.19709",
      "arxivId": "2601.19709",
      "title": "Hyperbolic Additive Margin Softmax with Hierarchical Information for Speaker Verification",
      "authors": [
        "Zhihua Fang",
        "Liang He"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Speaker embedding learning based on Euclidean space has achieved significant progress, but it is still insufficient in modeling hierarchical information within speaker features. Hyperbolic space, with its negative curvature geometric properties, can efficiently represent hierarchical information within a finite volume, making it more suitable for the feature distribution of speaker embeddings. In this paper, we propose Hyperbolic Softmax (H-Softmax) and Hyperbolic Additive Margin Softmax (HAM-Softmax) based on hyperbolic space. H-Softmax incorporates hierarchical information into speaker embeddings by projecting embeddings and speaker centers into hyperbolic space and computing hyperbolic distances. HAM-Softmax further enhances inter-class separability by introducing margin constraint on this basis. Experimental results show that H-Softmax and HAM-Softmax achieve average relative EER reductions of 27.84% and 14.23% compared with standard Softmax and AM-Softmax, respectively, demonstrating that the proposed methods effectively improve speaker verification performance and at the same time preserve the capability of hierarchical structure modeling. The code will be released at https://github.com/PunkMale/HAM-Softmax.",
      "url": "https://arxiv.org/abs/2601.19709",
      "pdfUrl": "https://arxiv.org/pdf/2601.19709.pdf",
      "titleJa": "話者認証のための階層情報を用いた双曲型加法マージンソフトマックス"
    },
    {
      "id": "2601.19673",
      "arxivId": "2601.19673",
      "title": "A Benchmark for Audio Reasoning Capabilities of Multimodal Large Language Models",
      "authors": [
        "Iwona Christop",
        "Mateusz Czyżnikiewicz",
        "Paweł Skórzewski",
        "Łukasz Bondaruk",
        "Jakub Kubiak",
        "Marcin Lewandowski",
        "Marek Kubis"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "The present benchmarks for testing the audio modality of multimodal large language models concentrate on testing various audio tasks such as speaker diarization or gender identification in isolation. Whether a multimodal model can answer the questions that require reasoning skills to combine audio tasks of different categories, cannot be verified with their use. To address this issue, we propose Audio Reasoning Tasks (ART), a new benchmark for assessing the ability of multimodal models to solve problems that require reasoning over audio signal.",
      "url": "https://arxiv.org/abs/2601.19673",
      "pdfUrl": "https://arxiv.org/pdf/2601.19673.pdf",
      "titleJa": "マルチモーダル大規模言語モデルの音声推論能力のベンチマーク"
    },
    {
      "id": "2601.19606",
      "arxivId": "2601.19606",
      "title": "GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining",
      "authors": [
        "Shentong Mo",
        "Zehua Chen",
        "Jun Zhu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Recent advances in video-audio (V-A) understanding and generation have increasingly relied on joint V-A embeddings, which serve as the foundation for tasks such as cross-modal retrieval and generation. While prior methods like CAVP effectively model semantic and temporal correspondences between modalities using contrastive objectives, their performance remains suboptimal. A key limitation is the insufficient modeling of the dense, multi-scale nature of both video and audio signals, correspondences often span fine- to coarse-grained spatial-temporal structures, which are underutilized in existing frameworks. To this end, we propose GMS-CAVP, a novel framework that combines Multi-Scale Video-Audio Alignment and Multi-Scale Spatial-Temporal Diffusion-based pretraining objectives to enhance V-A correspondence modeling. First, GMS-CAVP introduces a multi-scale contrastive learning strategy that captures semantic and temporal relations across varying granularities. Second, we go beyond traditional contrastive learning by incorporating a diffusion-based generative objective, enabling modality translation and synthesis between video and audio. This unified discriminative-generative formulation facilitates deeper cross-modal understanding and paves the way for high-fidelity generation. Extensive experiments on VGGSound, AudioSet, and Panda70M demonstrate that GMS-CAVP outperforms previous methods in generation and retrieval.",
      "url": "https://arxiv.org/abs/2601.19606",
      "pdfUrl": "https://arxiv.org/pdf/2601.19606.pdf",
      "titleJa": "GMS-CAVP: マルチスケールの対照的および生成的事前学習による音声とビデオの対応の改善"
    },
    {
      "id": "2601.19533",
      "arxivId": "2601.19533",
      "title": "SLM-SS: Speech Language Model for Generative Speech Separation",
      "authors": [
        "Tianhua Li",
        "Chenda Li",
        "Wei Wang",
        "Xin Zhou",
        "Xihui Chen",
        "Jianqing Gao",
        "Yanmin Qian"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Speech separation (SS) has advanced significantly with neural network-based methods, showing improved performance on signal-level metrics. However, these methods often struggle to maintain speech intelligibility in the separated signals, which can negatively affect the performance of downstream tasks such as speech recognition. In this work, we propose SLM-SS, a novel approach that applies speech language models to SS, aiming to enhance the intelligibility and coherence of the separated signals. We frame SS as discrete multi-codebook sequence generation, using Encoder-Decoder models to map quantized speech mixtures to target tokens. In addition to the autoregressive modeling strategy, we introduce a non-autoregressive model to improve decoding efficiency for residual tokens. Experimental results on the LibriMix dataset demonstrate that our approach shows significantly better preservation of speech intelligibility, leading to improved linguistic consistency in a variety of downstream tasks compared to existing approaches.",
      "url": "https://arxiv.org/abs/2601.19533",
      "pdfUrl": "https://arxiv.org/pdf/2601.19533.pdf",
      "titleJa": "SLM-SS: 生成的音声分離のための音声言語モデル"
    },
    {
      "id": "2601.19472",
      "arxivId": "2601.19472",
      "title": "Dual-Strategy-Enhanced ConBiMamba for Neural Speaker Diarization",
      "authors": [
        "Zhen Liao",
        "Gaole Dai",
        "Mengqiao Chen",
        "Wenqing Cheng",
        "Wei Xu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Conformer and Mamba have achieved strong performance in speech modeling but face limitations in speaker diarization. Mamba is efficient but struggles with local details and nonlinear patterns. Conformer's self-attention incurs high memory overhead for long speech sequences and may cause instability in long-range dependency modeling. These limitations are critical for diarization, which requires both precise modeling of local variations and robust speaker consistency over extended spans. To address these challenges, we first apply ConBiMamba for speaker diarization. We follow the Pyannote pipeline and propose the Dual-Strategy-Enhanced ConBiMamba neural speaker diarization system. ConBiMamba integrates the strengths of Conformer and Mamba, where Conformer's convolutional and feed-forward structures are utilized to improve local feature extraction. By replacing Conformer's self-attention with ExtBiMamba, ConBiMamba efficiently handles long audio sequences while alleviating the high memory cost of self-attention. Furthermore, to address the problem of the higher DER around speaker change points, we introduce the Boundary-Enhanced Transition Loss to enhance the detection of speaker change points. We also propose Layer-wise Feature Aggregation to enhance the utilization of multi-layer representations. The system is evaluated on six diarization datasets and achieves state-of-the-art performance on four of them. The source code of our study is available at https://github.com/lz-hust/DSE-CBM.",
      "url": "https://arxiv.org/abs/2601.19472",
      "pdfUrl": "https://arxiv.org/pdf/2601.19472.pdf",
      "titleJa": "ニューラル話者ダイアライゼーションのためのデュアル戦略強化ConBiMamba"
    },
    {
      "id": "2601.19399",
      "arxivId": "2601.19399",
      "title": "Residual Tokens Enhance Masked Autoencoders for Speech Modeling",
      "authors": [
        "Samir Sadok",
        "Stéphane Lathuilière",
        "Xavier Alameda-Pineda"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent speech modeling relies on explicit attributes such as pitch, content, and speaker identity, but these alone cannot capture the full richness of natural speech. We introduce RT-MAE, a novel masked autoencoder framework that augments the supervised attributes-based modeling with unsupervised residual trainable tokens, designed to encode the information not explained by explicit labeled factors (e.g., timbre variations, noise, emotion etc). Experiments show that RT-MAE improves reconstruction quality, preserving content and speaker similarity while enhancing expressivity. We further demonstrate its applicability to speech enhancement, removing noise at inference while maintaining controllability and naturalness.",
      "url": "https://arxiv.org/abs/2601.19399",
      "pdfUrl": "https://arxiv.org/pdf/2601.19399.pdf",
      "titleJa": "残差トークンは音声モデリングのためのマスクオートエンコーダを強化する"
    },
    {
      "id": "2601.19960",
      "arxivId": "2601.19960",
      "title": "Do we really need Self-Attention for Streaming Automatic Speech Recognition?",
      "authors": [
        "Youness Dkhissi",
        "Valentin Vielzeuf",
        "Elys Allesiardo",
        "Anthony Larcher"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Transformer-based architectures are the most used architectures in many deep learning fields like Natural Language Processing, Computer Vision or Speech processing. It may encourage the direct use of Transformers in the constrained tasks, without questioning whether it will yield the same benefits as in standard tasks. Given specific constraints, it is essential to evaluate the relevance of transformer models. This work questions the suitability of transformers for specific domains. We argue that the high computational requirements and latency issues associated with these models do not align well with streaming applications. Our study promotes the search for alternative strategies to improve efficiency without sacrificing performance. In light of this observation, our paper critically examines the usefulness of transformer architecture in such constrained environments. As a first attempt, we show that the computational cost for Streaming Automatic Speech Recognition (ASR) can be reduced using deformable convolution instead of Self-Attention. Furthermore, we show that Self-Attention mechanisms can be entirely removed and not replaced, without observing significant degradation in the Word Error Rate.",
      "url": "https://arxiv.org/abs/2601.19960",
      "pdfUrl": "https://arxiv.org/pdf/2601.19960.pdf",
      "titleJa": "ストリーミング自動音声認識に Self-Attention は本当に必要ですか?"
    },
    {
      "id": "2601.20542",
      "arxivId": "2601.20542",
      "title": "Decoding Speech Envelopes from Electroencephalogram with a Contrastive Pearson Correlation Coefficient Loss",
      "authors": [
        "Yayun Liang",
        "Yuanming Zhang",
        "Fei Chen",
        "Jing Lu",
        "Zhibin Lin"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Recent advances in reconstructing speech envelopes from Electroencephalogram (EEG) signals have enabled continuous auditory attention decoding (AAD) in multi-speaker environments. Most Deep Neural Network (DNN)-based envelope reconstruction models are trained to maximize the Pearson correlation coefficients (PCC) between the attended envelope and the reconstructed envelope (attended PCC). While the difference between the attended PCC and the unattended PCC plays an essential role in auditory attention decoding, existing methods often focus on maximizing the attended PCC. We therefore propose a contrastive PCC loss which represents the difference between the attended PCC and the unattended PCC. The proposed approach is evaluated on three public EEG AAD datasets using four DNN architectures. Across many settings, the proposed objective improves envelope separability and AAD accuracy, while also revealing dataset- and architecture-dependent failure cases.",
      "url": "https://arxiv.org/abs/2601.20542",
      "pdfUrl": "https://arxiv.org/pdf/2601.20542.pdf",
      "titleJa": "対照的なピアソン相関係数損失を用いた脳波からの音声エンベロープのデコード"
    },
    {
      "id": "2601.20319",
      "arxivId": "2601.20319",
      "title": "ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy",
      "authors": [
        "Ya-Tse Wu",
        "Chi-Chun Lee"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This work investigates how emotional speech and generative strategies affect ASR performance. We analyze speech synthesized from three emotional TTS models and find that substitution errors dominate, with emotional expressiveness varying across models. Based on these insights, we introduce two generative strategies: one using transcription correctness and another using emotional salience, to construct fine-tuning subsets. Results show consistent WER improvements on real emotional datasets without noticeable degradation on clean LibriSpeech utterances. The combined strategy achieves the strongest gains, particularly for expressive speech. These findings highlight the importance of targeted augmentation for building emotion-aware ASR systems.",
      "url": "https://arxiv.org/abs/2601.20319",
      "pdfUrl": "https://arxiv.org/pdf/2601.20319.pdf",
      "titleJa": "感情音声のためのASR：感情と音声生成戦略の影響の調査"
    },
    {
      "id": "2601.20300",
      "arxivId": "2601.20300",
      "title": "MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting",
      "authors": [
        "Jing Xu",
        "Minglin Wu",
        "Xueyuan Chen",
        "Xixin Wu",
        "Helen Meng"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Self-supervised learning (SSL) has greatly advanced speech representation learning, but multilingual SSL models remain constrained to languages encountered during pretraining. Retraining from scratch to incorporate new languages is computationally expensive, while sequential training without migitation strategies often leads to catastrophic forgetting. To address this, we propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. LoRA provides efficient low-rank adaptation, while soft MoE promotes flexible expert sharing across languages, reducing cross-lingual interference. To further mitigate forgetting, we introduce limited replay data from existing languages, avoiding reliance on large historical corpora. Experiments on ML-SUPERB demonstrate that MiLorE-SSL achieves strong performance in new languages and improves the ability in existing ones with only 2.14% trainable parameters.",
      "url": "https://arxiv.org/abs/2601.20300",
      "pdfUrl": "https://arxiv.org/pdf/2601.20300.pdf",
      "titleJa": "MiLorE-SSL: 忘れずに自己教師型モデルで多言語機能を拡張する"
    },
    {
      "id": "2601.20094",
      "arxivId": "2601.20094",
      "title": "T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS",
      "authors": [
        "Haibin Wu",
        "Bach Viet Do",
        "Naveen Suda",
        "Julian Chan",
        "Madhavan C R",
        "Gene-Ping Yang",
        "Yi-Chiao Wu",
        "Naoyuki Kanda",
        "Yossef Adi",
        "Xin Lei",
        "Yue Liu",
        "Florian Metze",
        "Yuzong Liu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Neural audio codecs provide promising acoustic features for speech synthesis, with representative streaming codecs like Mimi providing high-quality acoustic features for real-time Text-to-Speech (TTS) applications. However, Mimi's decoder, which employs a hybrid transformer and convolution architecture, introduces significant latency bottlenecks on edge devices due to the the compute intensive nature of deconvolution layers which are not friendly for mobile-CPUs, such as the most representative framework XNNPACK. This paper introduces T-Mimi, a novel modification of the Mimi codec decoder that replaces its convolutional components with a purely transformer-based decoder, inspired by the TS3-Codec architecture. This change dramatically reduces on-device TTS latency from 42.1ms to just 4.4ms. Furthermore, we conduct quantization aware training and derive a crucial finding: the final two transformer layers and the concluding linear layers of the decoder, which are close to the waveform, are highly sensitive to quantization and must be preserved at full precision to maintain audio quality.",
      "url": "https://arxiv.org/abs/2601.20094",
      "pdfUrl": "https://arxiv.org/pdf/2601.20094.pdf",
      "titleJa": "T-Mimi: リアルタイムの電話音声合成のためのトランスフォーマーベースのMimiデコーダー"
    },
    {
      "id": "2601.19702",
      "arxivId": "2601.19702",
      "title": "SAM Audio Judge: A Unified Multimodal Framework for Perceptual Evaluation of Audio Separation",
      "authors": [
        "Helin Wang",
        "Bowen Shi",
        "Andros Tjandra",
        "John Hoffman",
        "Yi-Chiao Wu",
        "Apoorv Vyas",
        "Najim Dehak",
        "Ann Lee",
        "Wei-Ning Hsu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "The performance evaluation remains a complex challenge in audio separation, and existing evaluation metrics are often misaligned with human perception, course-grained, relying on ground truth signals. On the other hand, subjective listening tests remain the gold standard for real-world evaluation, but they are expensive, time-consuming, and difficult to scale. This paper addresses the growing need for automated systems capable of evaluating audio separation without human intervention. The proposed evaluation metric, SAM Audio Judge (SAJ), is a multimodal fine-grained reference-free objective metric, which shows highly alignment with human perceptions. SAJ supports three audio domains (speech, music and general sound events) and three prompt inputs (text, visual and span), covering four different dimensions of evaluation (recall, percision, faithfulness, and overall). SAM Audio Judge also shows potential applications in data filtering, pseudo-labeling large datasets and reranking in audio separation models. We release our code and pre-trained models at: https://github.com/facebookresearch/sam-audio.",
      "url": "https://arxiv.org/abs/2601.19702",
      "pdfUrl": "https://arxiv.org/pdf/2601.19702.pdf",
      "titleJa": "SAM Audio Judge: 音声分離の知覚評価のための統合マルチモーダルフレームワーク"
    },
    {
      "id": "2601.19573",
      "arxivId": "2601.19573",
      "title": "Audio Deepfake Detection at the First Greeting: \"Hi!\"",
      "authors": [
        "Haohan Shi",
        "Xiyu Shi",
        "Safak Dogan",
        "Tianjin Huang",
        "Yunxiao Zhang"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This paper focuses on audio deepfake detection under real-world communication degradations, with an emphasis on ultra-short inputs (0.5-2.0s), targeting the capability to detect synthetic speech at a conversation opening, e.g., when a scammer says \"Hi.\" We propose Short-MGAA (S-MGAA), a novel lightweight extension of Multi-Granularity Adaptive Time-Frequency Attention, designed to enhance discriminative representation learning for short, degraded inputs subjected to communication processing and perturbations. The S-MGAA integrates two tailored modules: a Pixel-Channel Enhanced Module (PCEM) that amplifies fine-grained time-frequency saliency, and a Frequency Compensation Enhanced Module (FCEM) to supplement limited temporal evidence via multi-scale frequency modeling and adaptive frequency-temporal interaction. Extensive experiments demonstrate that S-MGAA consistently surpasses nine state-of-the-art baselines while achieving strong robustness to degradations and favorable efficiency-accuracy trade-offs, including low RTF, competitive GFLOPs, compact parameters, and reduced training cost, highlighting its strong potential for real-time deployment in communication systems and edge devices.",
      "url": "https://arxiv.org/abs/2601.19573",
      "pdfUrl": "https://arxiv.org/pdf/2601.19573.pdf",
      "titleJa": "最初の挨拶「こんにちは！」でのオーディオディープフェイク検出"
    },
    {
      "id": "2601.19491",
      "arxivId": "2601.19491",
      "title": "Permutation-Invariant Physics-Informed Neural Network for Region-to-Region Sound Field Reconstruction",
      "authors": [
        "Xingyu Chen",
        "Sipei Zhao",
        "Fei Ma",
        "Eva Cheng",
        "Ian S. Burnett"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Most existing sound field reconstruction methods target point-to-region reconstruction, interpolating the Acoustic Transfer Functions (ATFs) between a fixed-position sound source and a receiver region. The applicability of these methods is limited because real-world ATFs tend to varying continuously with respect to the positions of sound sources and receiver regions. This paper presents a permutation-invariant physics-informed neural network for region-to-region sound field reconstruction, which aims to interpolate the ATFs across continuously varying sound sources and measurement regions. The proposed method employs a deep set architecture to process the receiver and sound source positions as an unordered set, preserving acoustic reciprocity. Furthermore, it incorporates the Helmholtz equation as a physical constraint to guide network training, ensuring physically consistent predictions.",
      "url": "https://arxiv.org/abs/2601.19491",
      "pdfUrl": "https://arxiv.org/pdf/2601.19491.pdf",
      "titleJa": "領域間音場再構成のための順列不変な物理学に基づくニューラルネットワーク"
    },
    {
      "id": "2601.19297",
      "arxivId": "2601.19297",
      "title": "Phase-Retrieval-Based Physics-Informed Neural Networks For Acoustic Magnitude Field Reconstruction",
      "authors": [
        "Karl Schrader",
        "Shoichi Koyama",
        "Tomohiko Nakamura",
        "Mirco Pezzoli"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We propose a method for estimating the magnitude distribution of an acoustic field from spatially sparse magnitude measurements. Such a method is useful when phase measurements are unreliable or inaccessible. Physics-informed neural networks (PINNs) have shown promise for sound field estimation by incorporating constraints derived from governing partial differential equations (PDEs) into neural networks. However, they do not extend to settings where phase measurements are unavailable, as the loss function based on the governing PDE relies on phase information. To remedy this, we propose a phase-retrieval-based PINN for magnitude field estimation. By representing the magnitude and phase distributions with separate networks, the PDE loss can be computed based on the reconstructed complex amplitude. We demonstrate the effectiveness of our phase-retrieval-based PINN through experimental evaluation.",
      "url": "https://arxiv.org/abs/2601.19297",
      "pdfUrl": "https://arxiv.org/pdf/2601.19297.pdf",
      "titleJa": "音響振幅場再構成のための位相回復ベースの物理学に基づくニューラルネットワーク"
    },
    {
      "id": "2601.19956",
      "arxivId": "2601.19956",
      "title": "VoxPrivacy: A Benchmark for Evaluating Interactional Privacy of Speech Language Models",
      "authors": [
        "Yuxiang Wang",
        "Hongyu Liu",
        "Dekun Chen",
        "Xueyao Zhang",
        "Zhizheng Wu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "As Speech Language Models (SLMs) transition from personal devices to shared, multi-user environments such as smart homes, a new challenge emerges: the model is expected to distinguish between users to manage information flow appropriately. Without this capability, an SLM could reveal one user's confidential schedule to another, a privacy failure we term interactional privacy. Thus, the ability to generate speaker-aware responses becomes essential for SLM safe deployment. Current SLM benchmarks test dialogue ability but overlook speaker identity. Multi-speaker benchmarks check who said what without assessing whether SLMs adapt their responses. Privacy benchmarks focus on globally sensitive data (e.g., bank passwords) while neglecting contextual privacy-sensitive information (e.g., a user's private appointment). To address this gap, we introduce VoxPrivacy, the first benchmark designed to evaluate interactional privacy in SLMs. VoxPrivacy spans three tiers of increasing difficulty, from following direct secrecy commands to proactively protecting privacy. Our evaluation of nine SLMs on a 32-hour bilingual dataset reveals a widespread vulnerability: most open-source models perform close to random chance (around 50% accuracy) on conditional privacy decisions, while even strong closed-source systems fall short on proactive privacy inference. We further validate these findings on Real-VoxPrivacy, a human-recorded subset, confirming that failures observed on synthetic data persist in real speech. Finally, we demonstrate a viable path forward: by fine-tuning on a new 4,000-hour training set, we improve privacy-preserving abilities while maintaining robustness. To support future work, we release the VoxPrivacy benchmark, the large-scale training set, and the fine-tuned model to foster the development of safer and more context-aware SLMs.",
      "url": "https://arxiv.org/abs/2601.19956",
      "pdfUrl": "https://arxiv.org/pdf/2601.19956.pdf",
      "titleJa": "VoxPrivacy: 音声言語モデルの対話的プライバシーを評価するためのベンチマーク"
    },
    {
      "id": "2601.19194",
      "arxivId": "2601.19194",
      "title": "SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper",
      "authors": [
        "Alexander Polok",
        "Dominik Klement",
        "Samuele Cornell",
        "Matthew Wiesner",
        "Jan Černocký",
        "Sanjeev Khudanpur",
        "Lukáš Burget"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Speaker-attributed automatic speech recognition (ASR) in multi-speaker environments remains a major challenge. While some approaches achieve strong performance when fine-tuned on specific domains, few systems generalize well across out-of-domain datasets. Our prior work, Diarization-Conditioned Whisper (DiCoW), leverages speaker diarization outputs as conditioning information and, with minimal fine-tuning, demonstrated strong multilingual and multi-domain performance. In this paper, we address a key limitation of DiCoW: ambiguity in Silence-Target-Non-target-Overlap (STNO) masks, where two or more fully overlapping speakers may have nearly identical conditioning despite differing transcriptions. We introduce SE-DiCoW (Self-Enrolled Diarization-Conditioned Whisper), which uses diarization output to locate an enrollment segment anywhere in the conversation where the target speaker is most active. This enrollment segment is used as fixed conditioning via cross-attention at each encoder layer. We further refine DiCoW with improved data segmentation, model initialization, and augmentation. Together, these advances yield substantial gains: SE-DiCoW reduces macro-averaged tcpWER by 52.4% relative to the original DiCoW on the EMMA MT-ASR benchmark.",
      "url": "https://arxiv.org/abs/2601.19194",
      "pdfUrl": "https://arxiv.org/pdf/2601.19194.pdf",
      "titleJa": "SE-DiCoW: 自己登録型ダイアライゼーション条件付きウィスパー"
    },
    {
      "id": "2601.19153",
      "arxivId": "2601.19153",
      "title": "LuSeeL: Language-queried Binaural Universal Sound Event Extraction and Localization",
      "authors": [
        "Zexu Pan",
        "Shengkui Zhao",
        "Yukun Ma",
        "Haoxu Wang",
        "Yiheng Jiang",
        "Biao Tian",
        "Bin Ma"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Most universal sound extraction algorithms focus on isolating a target sound event from single-channel audio mixtures. However, the real world is three-dimensional, and binaural audio, which mimics human hearing, can capture richer spatial information, including sound source location. This spatial context is crucial for understanding and modeling complex auditory scenes, as it inherently informs sound detection and extraction. In this work, we propose a language-driven universal sound extraction network that isolates text-described sound events from binaural mixtures by effectively leveraging the spatial cues present in binaural signals. Additionally, we jointly predict the direction of arrival (DoA) of the target sound using spatial features from the extraction network. This dual-task approach exploits complementary location information to improve extraction performance while enabling accurate DoA estimation. Experimental results on the in-the-wild AudioCaps dataset show that our proposed LuSeeL model significantly outperforms single-channel and uni-task baselines.",
      "url": "https://arxiv.org/abs/2601.19153",
      "pdfUrl": "https://arxiv.org/pdf/2601.19153.pdf",
      "titleJa": "LuSeeL: 言語クエリによるバイノーラルユニバーサルサウンドイベントの抽出と定位"
    },
    {
      "id": "2601.19130",
      "arxivId": "2601.19130",
      "title": "Beyond Lips: Integrating Gesture and Lip Cues for Robust Audio-visual Speaker Extraction",
      "authors": [
        "Zexu Pan",
        "Xinyuan Qian",
        "Shengkui Zhao",
        "Kun Zhou",
        "Bin Ma"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Most audio-visual speaker extraction methods rely on synchronized lip recording to isolate the speech of a target speaker from a multi-talker mixture. However, in natural human communication, co-speech gestures are also temporally aligned with speech, often emphasizing specific words or syllables. These gestures provide complementary visual cues that can be especially valuable when facial or lip regions are occluded or distant. In this work, we move beyond lip-centric approaches and propose SeLG, a model that integrates both lip and upper-body gesture information for robust speaker extraction. SeLG features a cross-attention-based fusion mechanism that enables each visual modality to query and selectively attend to relevant speech features in the mixture. To improve the alignment of gesture representations with speech dynamics, SeLG also employs a contrastive InfoNCE loss that encourages gesture embeddings to align more closely with corresponding lip embeddings, which are more strongly correlated with speech. Experimental results on the YGD dataset, containing TED talks, demonstrate that the proposed contrastive learning strategy significantly improves gesture-based speaker extraction, and that our proposed SeLG model, by effectively fusing lip and gesture cues with an attention mechanism and InfoNCE loss, achieves superior performance compared to baselines, across both complete and partial (i.e., missing-modality) conditions.",
      "url": "https://arxiv.org/abs/2601.19130",
      "pdfUrl": "https://arxiv.org/pdf/2601.19130.pdf",
      "titleJa": "唇を超えて：ジェスチャーと唇の手がかりを統合した堅牢なオーディオビジュアル話者抽出"
    },
    {
      "id": "2601.19113",
      "arxivId": "2601.19113",
      "title": "A Hybrid Discriminative and Generative System for Universal Speech Enhancement",
      "authors": [
        "Yinghao Liu",
        "Chengwei Liu",
        "Xiaotao Liang",
        "Haoyin Yan",
        "Shaofei Xue",
        "Zheng Xue"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Universal speech enhancement aims at handling inputs with various speech distortions and recording conditions. In this work, we propose a novel hybrid architecture that synergizes the signal fidelity of discriminative modeling with the reconstruction capabilities of generative modeling. Our system utilizes the discriminative TF-GridNet model with the Sampling-Frequency-Independent strategy to handle variable sampling rates universally. In parallel, an autoregressive model combined with spectral mapping modeling generates detail-rich speech while effectively suppressing generative artifacts. Finally, a fusion network learns adaptive weights of the two outputs under the optimization of signal-level losses and the comprehensive Speech Quality Assessment (SQA) loss. Our proposed system is evaluated in the ICASSP 2026 URGENT Challenge (Track 1) and ranks the third place.",
      "url": "https://arxiv.org/abs/2601.19113",
      "pdfUrl": "https://arxiv.org/pdf/2601.19113.pdf",
      "titleJa": "ユニバーサル音声強調のためのハイブリッド識別・生成システム"
    },
    {
      "id": "2601.19029",
      "arxivId": "2601.19029",
      "title": "Audio Foundation Models Outperform Symbolic Representations for Piano Performance Evaluation",
      "authors": [
        "Jai Dhiman"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Automated piano performance evaluation traditionally relies on symbolic (MIDI) representations, which capture note-level information but miss the acoustic nuances that characterize expressive playing. I propose using pre-trained audio foundation models, specifically MuQ and MERT, to predict 19 perceptual dimensions of piano performance quality. Using synthesized audio from PercePiano MIDI files (rendered via Pianoteq), I compare audio and symbolic approaches under controlled conditions where both derive from identical source data. The best model, MuQ layers 9-12 with Pianoteq soundfont augmentation, achieves R^2 = 0.537 (95% CI: [0.465, 0.575]), representing a 55% improvement over the symbolic baseline (R^2 = 0.347). Statistical analysis confirms significance (p < 10^-25) with audio outperforming symbolic on all 19 dimensions. I validate the approach through cross-soundfont generalization (R^2 = 0.534 +/- 0.075), difficulty correlation with an external dataset (rho = 0.623), and multi-performer consistency analysis. Analysis of audio-symbolic fusion reveals high error correlation (r = 0.738), explaining why fusion provides minimal benefit: audio representations alone are sufficient. I release the complete training pipeline, pretrained models, and inference code.",
      "url": "https://arxiv.org/abs/2601.19029",
      "pdfUrl": "https://arxiv.org/pdf/2601.19029.pdf",
      "titleJa": "ピアノ演奏評価において、オーディオ基礎モデルは記号表現よりも優れている"
    },
    {
      "id": "2601.20861",
      "arxivId": "2601.20861",
      "title": "Evolutionary Strategies lead to Catastrophic Forgetting in LLMs",
      "authors": [
        "Immanuel Abdi",
        "Akshat Gupta",
        "Micah Mok",
        "Alexander Lu",
        "Nicholas Lee",
        "Gopala Anumanchipalli"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.",
      "url": "https://arxiv.org/abs/2601.20861",
      "pdfUrl": "https://arxiv.org/pdf/2601.20861.pdf",
      "titleJa": "進化戦略は法学修士課程における壊滅的な忘却につながる"
    },
    {
      "id": "2601.20856",
      "arxivId": "2601.20856",
      "title": "SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models",
      "authors": [
        "Sebastiano Monti",
        "Carlo Nicolini",
        "Gianni Pellegrini",
        "Jacopo Staiano",
        "Bruno Lepri"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.",
      "url": "https://arxiv.org/abs/2601.20856",
      "pdfUrl": "https://arxiv.org/pdf/2601.20856.pdf",
      "titleJa": "SokoBench: 大規模言語モデルにおける長期計画と推論の評価"
    },
    {
      "id": "2601.20854",
      "arxivId": "2601.20854",
      "title": "Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation",
      "authors": [
        "Aníbal Silva",
        "Moisés Santos",
        "André Restivo",
        "Carlos Soares"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.",
      "url": "https://arxiv.org/abs/2601.20854",
      "pdfUrl": "https://arxiv.org/pdf/2601.20854.pdf",
      "titleJa": "表形式データ生成のための変分オートエンコーダにおけるトランスフォーマー配置の検討"
    },
    {
      "id": "2601.20848",
      "arxivId": "2601.20848",
      "title": "Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation",
      "authors": [
        "Weixin Chen",
        "Li Chen",
        "Yuhan Zhao"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ],
      "abstract": "Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.",
      "url": "https://arxiv.org/abs/2601.20848",
      "pdfUrl": "https://arxiv.org/pdf/2601.20848.pdf",
      "titleJa": "学習後の公平性制御：推薦における動的公平性のための単一学習フレームワーク"
    },
    {
      "id": "2601.20847",
      "arxivId": "2601.20847",
      "title": "A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion",
      "authors": [
        "Willams de Lima Costa",
        "Thifany Ketuli Silva de Souza",
        "Jonas Ferreira Silva",
        "Carlos Gabriel Bezerra Pereira",
        "Bruno Reis Vila Nova",
        "Leonardo Silvino Brito",
        "Rafael Raider Leoni",
        "Juliano Silva",
        "Valter Ferreira",
        "Sibele Miguel Soares Neto",
        "Samantha Uehara",
        "Daniel Giacomo",
        "João Marcelo Teixeira",
        "Veronica Teichrieb",
        "Cristiano Coelho de Araújo"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.",
      "url": "https://arxiv.org/abs/2601.20847",
      "pdfUrl": "https://arxiv.org/pdf/2601.20847.pdf",
      "titleJa": "カメラとIMUの融合による堅牢な路面分類のための新しいデータセットとフレームワーク"
    },
    {
      "id": "2601.20844",
      "arxivId": "2601.20844",
      "title": "$\\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval",
      "authors": [
        "Zihao Wang",
        "Hang Yin",
        "Lihui Liu",
        "Hanghang Tong",
        "Yangqiu Song",
        "Ginny Wong",
        "Simon See"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "abstract": "This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of \"distances\" or \"similarities,\" including the $\\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.",
      "url": "https://arxiv.org/abs/2601.20844",
      "pdfUrl": "https://arxiv.org/pdf/2601.20844.pdf",
      "titleJa": "$\\mathbb{R}^{2k}$は埋め込みベースのトップ$k$検索に理論的に十分な大きさである"
    },
    {
      "id": "2601.20843",
      "arxivId": "2601.20843",
      "title": "Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)",
      "authors": [
        "Saurav Prateek"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.AI"
      ],
      "abstract": "This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runtime. This dynamic adaptation contrasts with parallel approaches, which often suffer from siloed knowledge. The Candidates Crossover algorithm further enhances search efficiency by deploying multiple LLM candidates with varied parameters to explore a larger search space, with their findings synthesized to curate a comprehensive final research response. The process concludes with One Shot Report Generation, ensuring the final document is informed by a unified narrative and high fact density. Powered by the Gemini 2.5 Pro model, our Deep Researcher was evaluated on the DeepResearch Bench, a globally recognized benchmark of 100 doctoral level research tasks. Our architecture achieved an overall score of 46.21, demonstrating superior performance by surpassing leading deep research agents such as Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher and Grok Deeper Search present on the DeepResearch Bench actively running leaderboard. This performance marginally exceeds our previous work, Static DRA, and reinforces the finding that sequential scaling consistently outperforms the parallel self consistency paradigm.",
      "url": "https://arxiv.org/abs/2601.20843",
      "pdfUrl": "https://arxiv.org/pdf/2601.20843.pdf",
      "titleJa": "シーケンシャルプランリフレクションと候補者クロスオーバーを備えたディープリサーチャー（ディープリサーチャーリフレクションエボルブ）"
    },
    {
      "id": "2601.20838",
      "arxivId": "2601.20838",
      "title": "Reward Models Inherit Value Biases from Pretraining",
      "authors": [
        "Brian Christian",
        "Jessica A. F. Thompson",
        "Elle Michelle Yang",
        "Vincent Adam",
        "Hannah Rose Kirk",
        "Christopher Summerfield",
        "Tsvetomira Dumbalska"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "abstract": "Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the \"Big Two\" psychological axes, we show a robust preference of Llama RMs for \"agency\" and a corresponding robust preference of Gemma RMs for \"communion.\" This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.",
      "url": "https://arxiv.org/abs/2601.20838",
      "pdfUrl": "https://arxiv.org/pdf/2601.20838.pdf",
      "titleJa": "報酬モデルは事前学習から価値バイアスを継承する"
    },
    {
      "id": "2601.20835",
      "arxivId": "2601.20835",
      "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
      "authors": [
        "Jie Liu",
        "Yu Sun",
        "Alpar Cseke",
        "Yao Feng",
        "Nicolas Heron",
        "Michael J. Black",
        "Yan Zhang"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as \"sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., \"increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.",
      "url": "https://arxiv.org/abs/2601.20835",
      "pdfUrl": "https://arxiv.org/pdf/2601.20835.pdf",
      "titleJa": "オープンボキャブラリ機能的3Dヒューマンシーンインタラクション生成"
    },
    {
      "id": "2601.20831",
      "arxivId": "2601.20831",
      "title": "MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents",
      "authors": [
        "Vishnu Sashank Dorbala",
        "Dinesh Manocha"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "abstract": "Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.",
      "url": "https://arxiv.org/abs/2601.20831",
      "pdfUrl": "https://arxiv.org/pdf/2601.20831.pdf",
      "titleJa": "MemCtrl: MLLM をエンボディドエージェントのアクティブメモリコントローラとして使用する"
    },
    {
      "id": "2601.20829",
      "arxivId": "2601.20829",
      "title": "Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning",
      "authors": [
        "Minwu Kim",
        "Safal Shrestha",
        "Keith Ross"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.",
      "url": "https://arxiv.org/abs/2601.20829",
      "pdfUrl": "https://arxiv.org/pdf/2601.20829.pdf",
      "titleJa": "失敗プレフィックス条件付けによる飽和問題における推論モデルのトレーニング"
    },
    {
      "id": "2601.20815",
      "arxivId": "2601.20815",
      "title": "GNN Explanations that do not Explain and How to find Them",
      "authors": [
        "Steve Azzolin",
        "Stefano Teso",
        "Bruno Lepri",
        "Andrea Passerini",
        "Sagar Malhotra"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.",
      "url": "https://arxiv.org/abs/2601.20815",
      "pdfUrl": "https://arxiv.org/pdf/2601.20815.pdf",
      "titleJa": "GNNの説明が説明されていないことと、それを見つける方法"
    },
    {
      "id": "2601.20802",
      "arxivId": "2601.20802",
      "title": "Reinforcement Learning via Self-Distillation",
      "authors": [
        "Jonas Hübotter",
        "Frederike Lübeck",
        "Lejs Behric",
        "Anton Baumann",
        "Marco Bagatella",
        "Daniel Marta",
        "Ido Hakimi",
        "Idan Shenfeld",
        "Thomas Kleine Buening",
        "Carlos Guestrin",
        "Andreas Krause"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.",
      "url": "https://arxiv.org/abs/2601.20802",
      "pdfUrl": "https://arxiv.org/pdf/2601.20802.pdf",
      "titleJa": "自己蒸留による強化学習"
    },
    {
      "id": "2601.20800",
      "arxivId": "2601.20800",
      "title": "Conditional PED-ANOVA: Hyperparameter Importance in Hierarchical & Dynamic Search Spaces",
      "authors": [
        "Kaito Baba",
        "Yoshihiko Ozaki",
        "Shuhei Watanabe"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "We propose conditional PED-ANOVA (condPED-ANOVA), a principled framework for estimating hyperparameter importance (HPI) in conditional search spaces, where the presence or domain of a hyperparameter can depend on other hyperparameters. Although the original PED-ANOVA provides a fast and efficient way to estimate HPI within the top-performing regions of the search space, it assumes a fixed, unconditional search space and therefore cannot properly handle conditional hyperparameters. To address this, we introduce a conditional HPI for top-performing regions and derive a closed-form estimator that accurately reflects conditional activation and domain changes. Experiments show that naive adaptations of existing HPI estimators yield misleading or uninterpretable importance estimates in conditional settings, whereas condPED-ANOVA consistently provides meaningful importances that reflect the underlying conditional structure.",
      "url": "https://arxiv.org/abs/2601.20800",
      "pdfUrl": "https://arxiv.org/pdf/2601.20800.pdf",
      "titleJa": "条件付きPED-ANOVA：階層的および動的探索空間におけるハイパーパラメータの重要性"
    },
    {
      "id": "2601.20791",
      "arxivId": "2601.20791",
      "title": "FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models",
      "authors": [
        "Haonan Zhong",
        "Wei Song",
        "Tingxu Han",
        "Maurice Pagnucco",
        "Jingling Xue",
        "Yang Song"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos. Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.",
      "url": "https://arxiv.org/abs/2601.20791",
      "pdfUrl": "https://arxiv.org/pdf/2601.20791.pdf",
      "titleJa": "FAIRT2V: テキストからビデオへの拡散モデルのためのトレーニング不要のバイアス除去"
    },
    {
      "id": "2601.20784",
      "arxivId": "2601.20784",
      "title": "REASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence",
      "authors": [
        "Zishen Wan",
        "Che-Kai Liu",
        "Jiayi Qian",
        "Hanchen Yang",
        "Arijit Raychowdhury",
        "Tushar Krishna"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.AI",
        "cs.AR"
      ],
      "abstract": "Neuro-symbolic AI systems integrate neural perception with symbolic reasoning to enable data-efficient, interpretable, and robust intelligence beyond purely neural models. Although this compositional paradigm has shown superior performance in domains such as reasoning, planning, and verification, its deployment remains challenging due to severe inefficiencies in symbolic and probabilistic inference. Through systematic analysis of representative neuro-symbolic workloads, we identify probabilistic logical reasoning as the inefficiency bottleneck, characterized by irregular control flow, low arithmetic intensity, uncoalesced memory accesses, and poor hardware utilization on CPUs and GPUs. This paper presents REASON, an integrated acceleration framework for probabilistic logical reasoning in neuro-symbolic AI. REASON introduces a unified directed acyclic graph representation that captures common structure across symbolic and probabilistic models, coupled with adaptive pruning and regularization. At the architecture level, REASON features a reconfigurable, tree-based processing fabric optimized for irregular traversal, symbolic deduction, and probabilistic aggregation. At the system level, REASON is tightly integrated with GPU streaming multiprocessors through a programmable interface and multi-level pipeline that efficiently orchestrates compositional execution. Evaluated across six neuro-symbolic workloads, REASON achieves 12-50x speedup and 310-681x energy efficiency over desktop and edge GPUs under TSMC 28 nm node. REASON enables real-time probabilistic logical reasoning, completing end-to-end tasks in 0.8 s with 6 mm2 area and 2.12 W power, demonstrating that targeted acceleration of probabilistic logical reasoning is critical for practical and scalable neuro-symbolic AI and positioning REASON as a foundational system architecture for next-generation cognitive intelligence.",
      "url": "https://arxiv.org/abs/2601.20784",
      "pdfUrl": "https://arxiv.org/pdf/2601.20784.pdf",
      "titleJa": "REASON: スケーラブルなニューロシンボリックインテリジェンスのための確率的論理推論の加速"
    },
    {
      "id": "2601.20779",
      "arxivId": "2601.20779",
      "title": "Independence of Approximate Clones",
      "authors": [
        "Théo Delemazure"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "abstract": "In an ordinal election, two candidates are said to be perfect clones if every voter ranks them adjacently. The independence of clones axiom then states that removing one of the two clones should not change the election outcome. This axiom has been extensively studied in social choice theory, and several voting rules are known to satisfy it (such as IRV, Ranked Pairs and Schulze). However, perfect clones are unlikely to occur in practice, especially for political elections with many voters. In this work, we study different notions of approximate clones in ordinal elections. Informally, two candidates are approximate clones in a preference profile if they are close to being perfect clones. We discuss two measures to quantify this proximity, and we show under which conditions the voting rules that are known to be independent of clones are also independent of approximate clones. In particular, we show that for elections with at least four candidates, none of these rules are independent of approximate clones in the general case. However, we find a more positive result for the case of three candidates. Finally, we conduct an empirical study of approximate clones and independence of approximate clones based on three real-world datasets: votes in local Scottish elections, votes in mini-jury deliberations, and votes of judges in figure skating competitions. We find that approximate clones are common in some contexts, and that the closest two candidates are to being perfect clones, the less likely their removal is to change the election outcome, especially for voting rules that are independent of perfect clones.",
      "url": "https://arxiv.org/abs/2601.20779",
      "pdfUrl": "https://arxiv.org/pdf/2601.20779.pdf",
      "titleJa": "近似クローンの独立性"
    },
    {
      "id": "2601.20745",
      "arxivId": "2601.20745",
      "title": "HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs",
      "authors": [
        "Guoan Wang",
        "Feiyu Wang",
        "Zongwei Lv",
        "Yikun Zong",
        "Tong Yang"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "As large language models (LLMs) continue to scale, deployment is increasingly bottlenecked by the memory wall, motivating a shift toward extremely low-bit quantization. However, most quantization-aware training (QAT) methods apply hard rounding and the straight-through estimator (STE) from the beginning of the training, which prematurely discretizes the optimization landscape and induces persistent gradient mismatch between latent weights and quantized weights, hindering effective optimization of quantized models. To address this, we propose Hestia, a Hessian-guided differentiable QAT framework for extremely low-bit LLMs, which replaces the rigid step function with a temperature-controlled softmax relaxation to maintain gradient flow early in training while progressively hardening quantization. Furthermore, Hestia leverages a tensor-wise Hessian trace metric as a lightweight curvature signal to drive fine-grained temperature annealing, enabling sensitivity-aware discretization across the model. Evaluations on Llama-3.2 show that Hestia consistently outperforms existing ternary QAT baselines, yielding average zero-shot improvements of 5.39% and 4.34% for the 1B and 3B models. These results indicate that Hessian-guided relaxation effectively recovers representational capacity, establishing a more robust training path for 1.58-bit LLMs. The code is available at https://github.com/hestia2026/Hestia.",
      "url": "https://arxiv.org/abs/2601.20745",
      "pdfUrl": "https://arxiv.org/pdf/2601.20745.pdf",
      "titleJa": "HESTIA: 極低ビットLLMのためのヘッセ行列誘導微分可能量子化を考慮したトレーニングフレームワーク"
    },
    {
      "id": "2601.20735",
      "arxivId": "2601.20735",
      "title": "Implementing Metric Temporal Answer Set Programming",
      "authors": [
        "Arvid Becker",
        "Pedro Cabalar",
        "Martin Diéguez",
        "Susana Hahn",
        "Javier Romero",
        "Torsten Schaub"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "abstract": "We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.",
      "url": "https://arxiv.org/abs/2601.20735",
      "pdfUrl": "https://arxiv.org/pdf/2601.20735.pdf",
      "titleJa": "メトリック時間回答セットプログラミングの実装"
    },
    {
      "id": "2601.20731",
      "arxivId": "2601.20731",
      "title": "QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks",
      "authors": [
        "Mae Sosto",
        "Delfina Sol Martinez Pandiani",
        "Laura Hollink"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "abstract": "This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized \"unmarked\" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.",
      "url": "https://arxiv.org/abs/2601.20731",
      "pdfUrl": "https://arxiv.org/pdf/2601.20731.pdf",
      "titleJa": "QueerGen: LLMが文完成課題においてジェンダーとセクシュアリティに関する社会規範をどのように反映しているか"
    },
    {
      "id": "2601.19109",
      "arxivId": "2601.19109",
      "title": "Interpretable and Perceptually-Aligned Music Similarity with Pretrained Embeddings",
      "authors": [
        "Arhan Vohra",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Perceptual similarity representations enable music retrieval systems to determine which songs sound most similar to listeners. State-of-the-art approaches based on task-specific training via self-supervised metric learning show promising alignment with human judgment, but are difficult to interpret or generalize due to limited dataset availability. We show that pretrained text-audio embeddings (CLAP and MuQ-MuLan) offer comparable perceptual alignment on similarity tasks without any additional fine-tuning. To surpass this baseline, we introduce a novel method to perceptually align pretrained embeddings with source separation and linear optimization on ABX preference data from listening tests. Our model provides interpretable and controllable instrument-wise weights, allowing music producers to retrieve stem-level loops and samples based on mixed reference songs.",
      "url": "https://arxiv.org/abs/2601.19109",
      "pdfUrl": "https://arxiv.org/pdf/2601.19109.pdf",
      "titleJa": "事前学習済みの埋め込みによる解釈可能かつ知覚的に整合された音楽類似性"
    },
    {
      "id": "2601.18766",
      "arxivId": "2601.18766",
      "title": "Learning to Discover: A Generalized Framework for Raga Identification without Forgetting",
      "authors": [
        "Parampreet Singh",
        "Somya Kumar",
        "Chaitanya Shailendra Nitawe",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.",
      "url": "https://arxiv.org/abs/2601.18766",
      "pdfUrl": "https://arxiv.org/pdf/2601.18766.pdf",
      "titleJa": "発見することを学ぶ：忘れずにラーガを識別するための一般化された枠組み"
    },
    {
      "id": "2601.18339",
      "arxivId": "2601.18339",
      "title": "A Dataset for Automatic Vocal Mode Classification",
      "authors": [
        "Reemt Hinrichs",
        "Sonja Stephan",
        "Alexander Lange",
        "Jörn Ostermann"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\\,\\% was achieved with a ResNet18. The dataset can be downloaded under https://zenodo.org/records/14276415.",
      "url": "https://arxiv.org/abs/2601.18339",
      "pdfUrl": "https://arxiv.org/pdf/2601.18339.pdf",
      "titleJa": "自動音声モード分類のためのデータセット"
    },
    {
      "id": "2601.19951",
      "arxivId": "2601.19951",
      "title": "Pianoroll-Event: A Novel Score Representation for Symbolic Music",
      "authors": [
        "Lekai Qian",
        "Haoyu Gu",
        "Dehan Li",
        "Boyu Cao",
        "Qi Liu"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Symbolic music representation is a fundamental challenge in computational musicology. While grid-based representations effectively preserve pitch-time spatial correspondence, their inherent data sparsity leads to low encoding efficiency. Discrete-event representations achieve compact encoding but fail to adequately capture structural invariance and spatial locality. To address these complementary limitations, we propose Pianoroll-Event, a novel encoding scheme that describes pianoroll representations through events, combining structural properties with encoding efficiency while maintaining temporal dependencies and local spatial patterns. Specifically, we design four complementary event types: Frame Events for temporal boundaries, Gap Events for sparse regions, Pattern Events for note patterns, and Musical Structure Events for musical metadata. Pianoroll-Event strikes an effective balance between sequence length and vocabulary size, improving encoding efficiency by 1.36\\times to 7.16\\times over representative discrete sequence methods. Experiments across multiple autoregressive architectures show models using our representation consistently outperform baselines in both quantitative and human evaluations.",
      "url": "https://arxiv.org/abs/2601.19951",
      "pdfUrl": "https://arxiv.org/pdf/2601.19951.pdf",
      "titleJa": "ピアノロールイベント：象徴音楽のための新しい楽譜表現"
    },
    {
      "id": "2601.17645",
      "arxivId": "2601.17645",
      "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
      "authors": [
        "Xilin Jiang",
        "Qiaolin Wang",
        "Junkai Wu",
        "Xiaomin He",
        "Zhongweiyang Xu",
        "Yinghao Ma",
        "Minshuo Piao",
        "Kaiyi Yang",
        "Xiuwen Zheng",
        "Riki Shimizu",
        "Yicong Chen",
        "Arsalan Firoozi",
        "Gavin Mischler",
        "Sukru Samet Dindar",
        "Richard Antonello",
        "Linyang He",
        "Tsun-An Hsieh",
        "Xulin Fan",
        "Yulun Wu",
        "Yuesheng Ma",
        "Chaitanya Amballa",
        "Weixiong Chen",
        "Jiarui Hai",
        "Ruisi Li",
        "Vishal Choudhari",
        "Cong Han",
        "Yinghao Aaron Li",
        "Adeen Flinker",
        "Mounya Elhilali",
        "Emmanouil Benetos",
        "Mark Hasegawa-Johnson",
        "Romit Roy Choudhury",
        "Nima Mesgarani"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public",
      "url": "https://arxiv.org/abs/2601.17645",
      "pdfUrl": "https://arxiv.org/pdf/2601.17645.pdf",
      "titleJa": "AVMeme試験：法学修士（LLM）の文脈的・文化的知識と思考力を評価するマルチモーダル・多言語・多文化ベンチマーク"
    },
    {
      "id": "2601.17517",
      "arxivId": "2601.17517",
      "title": "EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding",
      "authors": [
        "Luca Cerovaz",
        "Michele Mancusi",
        "Emanuele Rodolà"
      ],
      "publishedDate": "2026-01-24",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Audio codecs power discrete music generative modelling, music streaming and immersive media by shrinking PCM audio to bandwidth-friendly bit-rates. Recent works have gravitated towards processing in the spectral domain; however, spectrogram-domains typically struggle with phase modeling which is naturally complex-valued. Most frequency-domain neural codecs either disregard phase information or encode it as two separate real-valued channels, limiting spatial fidelity. This entails the need to introduce adversarial discriminators at the expense of convergence speed and training stability to compensate for the inadequate representation power of the audio signal. In this work we introduce an end-to-end complex-valued RVQ-VAE audio codec that preserves magnitude-phase coupling across the entire analysis-quantization-synthesis pipeline and removes adversarial discriminators and diffusion post-filters. Without GANs or diffusion we match or surpass much longer-trained baselines in-domain and reach SOTA out-of-domain performance. Compared to standard baselines that train for hundreds of thousands of steps, our model reducing training budget by an order of magnitude is markedly more compute-efficient while preserving high perceptual quality.",
      "url": "https://arxiv.org/abs/2601.17517",
      "pdfUrl": "https://arxiv.org/pdf/2601.17517.pdf",
      "titleJa": "EuleroDec: 効率的かつ堅牢なオーディオコーディングのための複素値RVQ-VAE"
    },
    {
      "id": "2601.16675",
      "arxivId": "2601.16675",
      "title": "I Guess That's Why They Call it the Blues: Causal Analysis for Audio Classifiers",
      "authors": [
        "David A. Kelly",
        "Hana Chockler"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "It is well-known that audio classifiers often rely on non-musically relevant features and spurious correlations to classify audio. Hence audio classifiers are easy to manipulate or confuse, resulting in wrong classifications. While inducing a misclassification is not hard, until now the set of features that the classifiers rely on was not well understood. In this paper we introduce a new method that uses causal reasoning to discover features of the frequency space that are sufficient and necessary for a given classification. We describe an implementation of this algorithm in the tool FreqReX and provide experimental results on a number of standard benchmark datasets. Our experiments show that causally sufficient and necessary subsets allow us to manipulate the outputs of the models in a variety of ways by changing the input very slightly. Namely, a change to one out of 240,000 frequencies results in a change in classification 58% of the time, and the change can be so small that it is practically inaudible. These results show that causal analysis is useful for understanding the reasoning process of audio classifiers and can be used to successfully manipulate their outputs.",
      "url": "https://arxiv.org/abs/2601.16675",
      "pdfUrl": "https://arxiv.org/pdf/2601.16675.pdf",
      "titleJa": "ブルースと呼ばれる理由：音声分類器の因果分析"
    },
    {
      "id": "2601.16273",
      "arxivId": "2601.16273",
      "title": "The CMU-AIST submission for the ICME 2025 Audio Encoder Challenge",
      "authors": [
        "Shikhar Bharadwaj",
        "Samuele Cornell",
        "Kwanghee Choi",
        "Hye-jin Shim",
        "Soham Deshmukh",
        "Satoru Fukayama",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This technical report describes our submission to the ICME 2025 audio encoder challenge. Our submitted system is built on BEATs, a masked speech token prediction based audio encoder. We extend the BEATs model using 74,000 hours of data derived from various speech, music, and sound corpora and scale its architecture upto 300 million parameters. We experiment with speech-heavy and balanced pre-training mixtures to study the impact of different domains on final performance. Our submitted system consists of an ensemble of the Dasheng 1.2 billion model with two custom scaled-up BEATs models trained on the aforementioned pre-training data mixtures. We also propose a simple ensembling technique that retains the best capabilities of constituent models and surpasses both the baseline and Dasheng 1.2B. For open science, we publicly release our trained checkpoints via huggingface at https://huggingface.co/shikhar7ssu/OpenBEATs-ICME-SOUND and https://huggingface.co/shikhar7ssu/OpenBEATs-ICME.",
      "url": "https://arxiv.org/abs/2601.16273",
      "pdfUrl": "https://arxiv.org/pdf/2601.16273.pdf",
      "titleJa": "ICME 2025オーディオエンコーダチャレンジへのCMU-AISTの応募"
    },
    {
      "id": "2601.16150",
      "arxivId": "2601.16150",
      "title": "Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization",
      "authors": [
        "Maximos Kaliakatsos-Papakostas",
        "Dimos Makris",
        "Konstantinos Soiledis",
        "Konstantinos-Theodoros Tsamis",
        "Vassilis Katsouros",
        "Emilios Cambouropoulos"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody-harmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.",
      "url": "https://arxiv.org/abs/2601.16150",
      "pdfUrl": "https://arxiv.org/pdf/2601.16150.pdf",
      "titleJa": "メロディーに注意を払う（交差させる）：単一エンコーダーによるメロディーハーモニーのためのカリキュラムマスキング"
    },
    {
      "id": "2601.15872",
      "arxivId": "2601.15872",
      "title": "PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation",
      "authors": [
        "Jaekwon Im",
        "Natalia Polouliakh",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.",
      "url": "https://arxiv.org/abs/2601.15872",
      "pdfUrl": "https://arxiv.org/pdf/2601.15872.pdf",
      "titleJa": "PF-D2M: ユニバーサルなダンス・トゥ・ミュージック生成のためのポーズフリー拡散モデル"
    },
    {
      "id": "2601.15083",
      "arxivId": "2601.15083",
      "title": "Bangla Music Genre Classification Using Bidirectional LSTMS",
      "authors": [
        "Muntakimur Rahaman",
        "Md Mahmudul Hoque",
        "Md Mehedi Hassain"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Bangla music is enrich in its own music cultures. Now a days music genre classification is very significant because of the exponential increase in available music, both in digital and physical formats. It is necessary to index them accordingly to facilitate improved retrieval. Automatically classifying Bangla music by genre is essential for efficiently locating specific pieces within a vast and diverse music library. Prevailing methods for genre classification predominantly employ conventional machine learning or deep learning approaches. This work introduces a novel music dataset comprising ten distinct genres of Bangla music. For the task of audio classification, we utilize a recurrent neural network (RNN) architecture. Specifically, a Long Short-Term Memory (LSTM) network is implemented to train the model and perform the classification. Feature extraction represents a foundational stage in audio data processing. This study utilizes Mel-Frequency Cepstral Coefficients (MFCCs) to transform raw audio waveforms into a compact and representative set of features. The proposed framework facilitates music genre classification by leveraging these extracted features. Experimental results demonstrate a classification accuracy of 78%, indicating the system's strong potential to enhance and streamline the organization of Bangla music genres.",
      "url": "https://arxiv.org/abs/2601.15083",
      "pdfUrl": "https://arxiv.org/pdf/2601.15083.pdf",
      "titleJa": "双方向LSTMSを用いたバングラ音楽のジャンル分類"
    },
    {
      "id": "2601.14931",
      "arxivId": "2601.14931",
      "title": "Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali",
      "authors": [
        "Nouhoum Coulibaly",
        "Ousmane Ly",
        "Michael Leventhal",
        "Ousmane Goro"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "This study explores the capacity of generative artificial intelligence (Gen AI) to contribute to the construction of peace narratives and the revitalization of musical heritage in Mali. The study has been made in a political and social context where inter-community tensions and social fractures motivate a search for new symbolic frameworks for reconciliation. The study empirically explores three questions: (1) how Gen AI can be used as a tool for musical creation rooted in national languages and traditions; (2) to what extent Gen AI systems enable a balanced hybridization between technological innovation and cultural authenticity; and (3) how AI-assisted musical co-creation can strengthen social cohesion and cultural sovereignty. The experimental results suggest that Gen AI, embedded in a culturally conscious participatory framework, can act as a catalyst for symbolic diplomacy, amplifying local voices instead of standardizing them. However, challenges persist regarding the availability of linguistic corpora, algorithmic censorship, and the ethics of generating compositions derived from copyrighted sources.",
      "url": "https://arxiv.org/abs/2601.14931",
      "pdfUrl": "https://arxiv.org/pdf/2601.14931.pdf",
      "titleJa": "生成型人工知能、音楽遺産、そして平和物語の構築：マリにおける事例研究"
    },
    {
      "id": "2601.14786",
      "arxivId": "2601.14786",
      "title": "Training-Efficient Text-to-Music Generation with State-Space Modeling",
      "authors": [
        "Wei-Jaw Lee",
        "Fang-Chih Hsieh",
        "Xuanjun Chen",
        "Fang-Duo Tsai",
        "Yi-Hsuan Yang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in text-to-music generation (TTM) have yielded high-quality results, but often at the cost of extensive compute and the use of large proprietary internal data. To improve the affordability and openness of TTM training, an open-source generative model backbone that is more training- and data-efficient is needed. In this paper, we constrain the number of trainable parameters in the generative model to match that of the MusicGen-small benchmark (with about 300M parameters), and replace its Transformer backbone with the emerging class of state-space models (SSMs). Specifically, we explore different SSM variants for sequence modeling, and compare a single-stage SSM-based design with a decomposable two-stage SSM/diffusion hybrid design. All proposed models are trained from scratch on a purely public dataset comprising 457 hours of CC-licensed music, ensuring full openness. Our experimental findings are three-fold. First, we show that SSMs exhibit superior training efficiency compared to the Transformer counterpart. Second, despite using only 9% of the FLOPs and 2% of the training data size compared to the MusicGen-small benchmark, our model achieves competitive performance in both objective metrics and subjective listening tests based on MusicCaps captions. Finally, our scaling-down experiment demonstrates that SSMs can maintain competitive performance relative to the Transformer baseline even at the same training budget (measured in iterations), when the model size is reduced to four times smaller. To facilitate the democratization of TTM research, the processed captions, model checkpoints, and source code are available on GitHub via the project page: https://lonian6.github.io/ssmttm/.",
      "url": "https://arxiv.org/abs/2601.14786",
      "pdfUrl": "https://arxiv.org/pdf/2601.14786.pdf",
      "titleJa": "状態空間モデリングによる効率的なテキストから音楽への生成"
    },
    {
      "id": "2601.14684",
      "arxivId": "2601.14684",
      "title": "Dissecting Performance Degradation in Audio Source Separation under Sampling Frequency Mismatch",
      "authors": [
        "Kanami Imamura",
        "Tomohiko Nakamura",
        "Kohei Yatabe",
        "Hiroshi Saruwatari"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio processing methods based on deep neural networks are typically trained at a single sampling frequency (SF). To handle untrained SFs, signal resampling is commonly employed, but it can degrade performance, particularly when the input SF is lower than the trained SF. This paper investigates the causes of this degradation through two hypotheses: (i) the lack of high-frequency components introduced by up-sampling, and (ii) the greater importance of their presence than their precise representation. To examine these hypotheses, we compare conventional resampling with three alternatives: post-resampling noise addition, which adds Gaussian noise to the resampled signal; noisy-kernel resampling, which perturbs the kernel with Gaussian noise to enrich high-frequency components; and trainable-kernel resampling, which adapts the interpolation kernel through training. Experiments on music source separation show that noisy-kernel and trainable-kernel resampling alleviate the degradation observed with conventional resampling. We further demonstrate that noisy-kernel resampling is effective across diverse models, highlighting it as a simple yet practical option.",
      "url": "https://arxiv.org/abs/2601.14684",
      "pdfUrl": "https://arxiv.org/pdf/2601.14684.pdf",
      "titleJa": "サンプリング周波数の不一致による音源分離の性能劣化の解析"
    },
    {
      "id": "2601.15348",
      "arxivId": "2601.15348",
      "title": "Abusive music and song transformation using GenAI and LLMs",
      "authors": [
        "Jiyang Choi",
        "Rohitash Chandra"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Repeated exposure to violence and abusive content in music and song content can influence listeners' emotions and behaviours, potentially normalising aggression or reinforcing harmful stereotypes. In this study, we explore the use of generative artificial intelligence (GenAI) and Large Language Models (LLMs) to automatically transform abusive words (vocal delivery) and lyrical content in popular music. Rather than simply muting or replacing a single word, our approach transforms the tone, intensity, and sentiment, thus not altering just the lyrics, but how it is expressed. We present a comparative analysis of four selected English songs and their transformed counterparts, evaluating changes through both acoustic and sentiment-based lenses. Our findings indicate that Gen-AI significantly reduces vocal aggressiveness, with acoustic analysis showing improvements in Harmonic to Noise Ratio, Cepstral Peak Prominence, and Shimmer. Sentiment analysis reduced aggression by 63.3-85.6\\% across artists, with major improvements in chorus sections (up to 88.6\\% reduction). The transformed versions maintained musical coherence while mitigating harmful content, offering a promising alternative to traditional content moderation that avoids triggering the \"forbidden fruit\" effect, where the censored content becomes more appealing simply because it is restricted. This approach demonstrates the potential for GenAI to create safer listening experiences while preserving artistic expression.",
      "url": "https://arxiv.org/abs/2601.15348",
      "pdfUrl": "https://arxiv.org/pdf/2601.15348.pdf",
      "titleJa": "GenAIとLLMを使用した虐待的な音楽と歌の変換"
    },
    {
      "id": "2601.14356",
      "arxivId": "2601.14356",
      "title": "Single-step Controllable Music Bandwidth Extension With Flow Matching",
      "authors": [
        "Carlos Hernandez-Olivan",
        "Hendrik Vincent Koops",
        "Hao Hao Tan",
        "Elio Quinton"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio restoration consists in inverting degradations of a digital audio signal to recover what would have been the pristine quality signal before the degradation occurred. This is valuable in contexts such as archives of music recordings, particularly those of precious historical value, for which a clean version may have been lost or simply does not exist. Recent work applied generative models to audio restoration, showing promising improvement over previous methods, and opening the door to the ability to perform restoration operations that were not possible before. However, making these models finely controllable remains a challenge. In this paper, we propose an extension of FLowHigh and introduce the Dynamic Spectral Contour (DSC) as a control signal for bandwidth extension via classifier-free guidance. Our experiments show competitive model performance, and indicate that DSC is a promising feature to support fine-grained conditioning.",
      "url": "https://arxiv.org/abs/2601.14356",
      "pdfUrl": "https://arxiv.org/pdf/2601.14356.pdf",
      "titleJa": "フローマッチングによるシングルステップ制御可能な音楽帯域幅拡張"
    },
    {
      "id": "2601.14157",
      "arxivId": "2601.14157",
      "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models",
      "authors": [
        "Bruno Sienkiewicz",
        "Łukasz Neumann",
        "Mateusz Modrzejewski"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.",
      "url": "https://arxiv.org/abs/2601.14157",
      "pdfUrl": "https://arxiv.org/pdf/2601.14157.pdf",
      "titleJa": "ConceptCaps - 音楽モデルの解釈可能性のための蒸留概念データセット"
    },
    {
      "id": "2601.13931",
      "arxivId": "2601.13931",
      "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music",
      "authors": [
        "Yannis Vasilakis",
        "Rachel Bittner",
        "Johan Pauwels"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG"
      ],
      "abstract": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.",
      "url": "https://arxiv.org/abs/2601.13931",
      "pdfUrl": "https://arxiv.org/pdf/2601.13931.pdf",
      "titleJa": "音楽のための音声テキスト結合モデルにおける効果的な否定モデリングに向けて"
    },
    {
      "id": "2601.18908",
      "arxivId": "2601.18908",
      "title": "Enhancing Speech Emotion Recognition using Dynamic Spectral Features and Kalman Smoothing",
      "authors": [
        "Marouane El Hizabri",
        "Abdelfattah Bezzaz",
        "Ismail Hayoukane",
        "Youssef Taki"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Speech Emotion Recognition systems often use static features like Mel-Frequency Cepstral Coefficients (MFCCs), Zero Crossing Rate (ZCR), and Root Mean Square Energy (RMSE). Because of this, they can misclassify emotions when there is acoustic noise in vocal signals. To address this, we added dynamic features using Dynamic Spectral features (Deltas and Delta-Deltas) along with the Kalman Smoothing algorithm. This approach reduces noise and improves emotion classification. Since emotion changes over time, the Kalman Smoothing filter also helped make the classifier outputs more stable. Tests on the RAVDESS dataset showed that this method achieved a state-of-the-art accuracy of 87\\% and reduced misclassification between emotions with similar acoustic features",
      "url": "https://arxiv.org/abs/2601.18908",
      "pdfUrl": "https://arxiv.org/pdf/2601.18908.pdf",
      "titleJa": "動的スペクトル特徴とカルマンスムージングを用いた音声感情認識の強化"
    },
    {
      "id": "2601.18415",
      "arxivId": "2601.18415",
      "title": "Pisets: A Robust Speech Recognition System for Lectures and Interviews",
      "authors": [
        "Ivan Bondarenko",
        "Daniil Grebenkin",
        "Oleg Sedukhin",
        "Mikhail Klementev",
        "Roman Derunets",
        "Lyudmila Budneva"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.",
      "url": "https://arxiv.org/abs/2601.18415",
      "pdfUrl": "https://arxiv.org/pdf/2601.18415.pdf",
      "titleJa": "Pisets: 講義やインタビューのための堅牢な音声認識システム"
    },
    {
      "id": "2601.18086",
      "arxivId": "2601.18086",
      "title": "From Human Speech to Ocean Signals: Transferring Speech Large Models for Underwater Acoustic Target Recognition",
      "authors": [
        "Mengcheng Huang",
        "Xue Zhou",
        "Chen Xu",
        "Dapeng Man"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Underwater acoustic target recognition (UATR) plays a vital role in marine applications but remains challenging due to limited labeled data and the complexity of ocean environments. This paper explores a central question: can speech large models (SLMs), trained on massive human speech corpora, be effectively transferred to underwater acoustics? To investigate this, we propose UATR-SLM, a simple framework that reuses the speech feature pipeline, adapts the SLM as an acoustic encoder, and adds a lightweight classifier.Experiments on the DeepShip and ShipsEar benchmarks show that UATR-SLM achieves over 99% in-domain accuracy, maintains strong robustness across variable signal lengths, and reaches up to 96.67% accuracy in cross-domain evaluation. These results highlight the strong transferability of SLMs to UATR, establishing a promising paradigm for leveraging speech foundation models in underwater acoustics.",
      "url": "https://arxiv.org/abs/2601.18086",
      "pdfUrl": "https://arxiv.org/pdf/2601.18086.pdf",
      "titleJa": "人間の音声から海洋信号へ：水中音響ターゲット認識のための音声大規模モデルの転送"
    },
    {
      "id": "2601.19949",
      "arxivId": "2601.19949",
      "title": "RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation",
      "authors": [
        "Mandip Goswami"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "Despite decades of research on reverberant speech, comparing methods remains difficult because most corpora lack per-file acoustic annotations or provide limited documentation for reproduction. We present RIR-Mega-Speech, a corpus of approximately 117.5 hours created by convolving LibriSpeech utterances with roughly 5,000 simulated room impulse responses from the RIR-Mega collection. Every file includes RT60, direct-to-reverberant ratio (DRR), and clarity index ($C_{50}$) computed from the source RIR using clearly defined, reproducible procedures. We also provide scripts to rebuild the dataset and reproduce all evaluation results. Using Whisper small on 1,500 paired utterances, we measure 5.20% WER (95% CI: 4.69--5.78) on clean speech and 7.70% (7.04--8.35) on reverberant versions, corresponding to a paired increase of 2.50 percentage points (2.06--2.98). This represents a 48% relative degradation. WER increases monotonically with RT60 and decreases with DRR, consistent with prior perceptual studies. While the core finding that reverberation harms recognition is well established, we aim to provide the community with a standardized resource where acoustic conditions are transparent and results can be verified independently. The repository includes one-command rebuild instructions for both Windows and Linux environments.",
      "url": "https://arxiv.org/abs/2601.19949",
      "pdfUrl": "https://arxiv.org/pdf/2601.19949.pdf",
      "titleJa": "RIR-Mega-Speech: 包括的な音響メタデータと再現可能な評価を備えた残響音声コーパス"
    },
    {
      "id": "2601.17902",
      "arxivId": "2601.17902",
      "title": "dLLM-ASR: A Faster Diffusion LLM-based Framework for Speech Recognition",
      "authors": [
        "Wenjie Tian",
        "Bingshen Mu",
        "Guobin Ma",
        "Xuelong Geng",
        "Zhixian Zhao",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Automatic speech recognition (ASR) systems based on large language models (LLMs) achieve superior performance by leveraging pretrained LLMs as decoders, but their token-by-token generation mechanism leads to inference latency that grows linearly with sequence length. Meanwhile, discrete diffusion large language models (dLLMs) offer a promising alternative, enabling high-quality parallel sequence generation with pretrained decoders. However, directly applying native text-oriented dLLMs to ASR leads to a fundamental mismatch between open-ended text generation and the acoustically conditioned transcription paradigm required by ASR. As a result, it introduces unnecessary difficulty and computational redundancy, such as denoising from pure noise, inflexible generation lengths, and fixed denoising steps. We propose dLLM-ASR, an efficient dLLM-based ASR framework that formulates dLLM's decoding as a prior-guided and adaptive denoising process. It leverages an ASR prior to initialize the denoising process and provide an anchor for sequence length. Building upon this prior, length-adaptive pruning dynamically removes redundant tokens, while confidence-based denoising allows converged tokens to exit the denoising loop early, enabling token-level adaptive computation. Experiments demonstrate that dLLM-ASR achieves recognition accuracy comparable to autoregressive LLM-based ASR systems and delivers a 4.44$\\times$ inference speedup, establishing a practical and efficient paradigm for ASR.",
      "url": "https://arxiv.org/abs/2601.17902",
      "pdfUrl": "https://arxiv.org/pdf/2601.17902.pdf",
      "titleJa": "dLLM-ASR: 音声認識のための高速拡散LLMベースのフレームワーク"
    },
    {
      "id": "2601.17690",
      "arxivId": "2601.17690",
      "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
      "authors": [
        "Ziling Gong",
        "Yunyan Ouyang",
        "Iram Kamdar",
        "Melody Ma",
        "Hongjie Chen",
        "Franck Dernoncourt",
        "Ryan A. Rossi",
        "Nesreen K. Ahmed"
      ],
      "publishedDate": "2026-01-25",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.",
      "url": "https://arxiv.org/abs/2601.17690",
      "pdfUrl": "https://arxiv.org/pdf/2601.17690.pdf",
      "titleJa": "セグメント長の重要性：オーディオフィンガープリンティングの性能におけるセグメント長の研究"
    },
    {
      "id": "2601.16774",
      "arxivId": "2601.16774",
      "title": "E2E-AEC: Implementing an end-to-end neural network learning approach for acoustic echo cancellation",
      "authors": [
        "Yiheng Jiang",
        "Biao Tian",
        "Haoxu Wang",
        "Shengkui Zhao",
        "Bin Ma",
        "Daren Chen",
        "Xiangang Li"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We propose a novel neural network-based end-to-end acoustic echo cancellation (E2E-AEC) method capable of streaming inference, which operates effectively without reliance on traditional linear AEC (LAEC) techniques and time delay estimation. Our approach includes several key strategies: First, we introduce and refine progressive learning to gradually enhance echo suppression. Second, our model employs knowledge transfer by initializing with a pre-trained LAECbased model, harnessing the insights gained from LAEC training. Third, we optimize the attention mechanism with a loss function applied on attention weights to achieve precise time alignment between the reference and microphone signals. Lastly, we incorporate voice activity detection to enhance speech quality and improve echo removal by masking the network output when near-end speech is absent. The effectiveness of our approach is validated through experiments conducted on public datasets.",
      "url": "https://arxiv.org/abs/2601.16774",
      "pdfUrl": "https://arxiv.org/pdf/2601.16774.pdf",
      "titleJa": "E2E-AEC: 音響エコーキャンセルのためのエンドツーエンドのニューラルネットワーク学習アプローチの実装"
    },
    {
      "id": "2601.17086",
      "arxivId": "2601.17086",
      "title": "SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS",
      "authors": [
        "Ayush Pratap Singh",
        "Harshit Singh",
        "Nityanand Mathur",
        "Akshat Mandloi",
        "Sudarshan Kamath"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Neural text-to-speech (TTS) systems systematically mispronounce low-resource proper nouns, particularly non-English names, brands, and geographic locations, due to their underrepresentation in predominantly English training corpora. Existing solutions typically rely on expensive multilingual data collection, supervised finetuning, or manual phonetic annotation, which limits the deployment of TTS systems in linguistically diverse settings. We introduce SonoEdit, a model editing technique that surgically corrects pronunciation errors in pre-trained TTS models without retraining. Instead of costly finetuning or explicit phoneme injection, we propose a parsimonious alternative based on Null-Space Pronunciation Editing, which performs a single-shot parameter update to modify the pronunciation of specific words while provably preserving all other model behavior. We first adapt Acoustic Causal Tracing to identify the Transformer layers responsible for text-to-pronunciation mapping. We then apply Null-Space Constrained Editing to compute a closed-form weight update that corrects the target pronunciation while remaining mathematically orthogonal to the subspace governing general speech generation. This constrained update steers the model's acoustic output toward a desired pronunciation exemplar while guaranteeing zero first-order change on a preserved speech corpus.",
      "url": "https://arxiv.org/abs/2601.17086",
      "pdfUrl": "https://arxiv.org/pdf/2601.17086.pdf",
      "titleJa": "SonoEdit: LLMベースのTTSにおける発音訂正のためのヌル空間制約知識編集"
    },
    {
      "id": "2601.16547",
      "arxivId": "2601.16547",
      "title": "CORD: Bridging the Audio-Text Reasoning Gap via Weighted On-policy Cross-modal Distillation",
      "authors": [
        "Jing Hu",
        "Danxiang Zhu",
        "Xianlong Luo",
        "Dan Zhang",
        "Shuwei He",
        "Yishu Lei",
        "Haitao Zheng",
        "Shikun Feng",
        "Jingzhou He",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Large Audio Language Models (LALMs) have garnered significant research interest. Despite being built upon text-based large language models (LLMs), LALMs frequently exhibit a degradation in knowledge and reasoning capabilities. We hypothesize that this limitation stems from the failure of current training paradigms to effectively bridge the acoustic-semantic gap within the feature representation space. To address this challenge, we propose CORD, a unified alignment framework that performs online cross-modal self-distillation. Specifically, it aligns audio-conditioned reasoning with its text-conditioned counterpart within a unified model. Leveraging the text modality as an internal teacher, CORD performs multi-granularity alignment throughout the audio rollout process. At the token level, it employs on-policy reverse KL divergence with importance-aware weighting to prioritize early and semantically critical tokens. At the sequence level, CORD introduces a judge-based global reward to optimize complete reasoning trajectories via Group Relative Policy Optimization (GRPO). Empirical results across multiple benchmarks demonstrate that CORD consistently enhances audio-conditioned reasoning and substantially bridges the audio-text performance gap with only 80k synthetic training samples, validating the efficacy and data efficiency of our on-policy, multi-level cross-modal alignment approach.",
      "url": "https://arxiv.org/abs/2601.16547",
      "pdfUrl": "https://arxiv.org/pdf/2601.16547.pdf",
      "titleJa": "CORD: 重み付けされたポリシーに基づくクロスモーダル蒸留による音声テキスト推論ギャップの橋渡し"
    },
    {
      "id": "2601.17085",
      "arxivId": "2601.17085",
      "title": "Recovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration",
      "authors": [
        "Esther Sun",
        "Abinay Reddy Naini",
        "Carlos Busso"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Discrete speech tokens offer significant advantages for storage and language model integration, but their application in speech emotion recognition (SER) is limited by paralinguistic information loss during quantization. This paper presents a comprehensive investigation of discrete tokens for SER. Using a fine-tuned WavLM-Large model, we systematically quantify performance degradation across different layer configurations and k-means quantization granularities. To recover the information loss, we propose two key strategies: (1) attention-based multi-layer fusion to recapture complementary information from different layers, and (2) integration of openSMILE features to explicitly reintroduce paralinguistic cues. We also compare mainstream neural codec tokenizers (SpeechTokenizer, DAC, EnCodec) and analyze their behaviors when fused with acoustic features. Our findings demonstrate that through multi-layer fusion and acoustic feature integration, discrete tokens can close the performance gap with continuous representations in SER tasks.",
      "url": "https://arxiv.org/abs/2601.17085",
      "pdfUrl": "https://arxiv.org/pdf/2601.17085.pdf",
      "titleJa": "多層融合とパラ言語的特徴の統合による離散トークンからの音声感情認識のパフォーマンス回復"
    },
    {
      "id": "2601.16316",
      "arxivId": "2601.16316",
      "title": "EdgeSpot: Efficient and High-Performance Few-Shot Model for Keyword Spotting",
      "authors": [
        "Oguzhan Buyuksolak",
        "Alican Gok",
        "Osman Erman Okman"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "We introduce an efficient few-shot keyword spotting model for edge devices, EdgeSpot, that pairs an optimized version of a BC-ResNet-based acoustic backbone with a trainable Per-Channel Energy Normalization frontend and lightweight temporal self-attention. Knowledge distillation is utilized during training by employing a self-supervised teacher model, optimized with Sub-center ArcFace loss. This study demonstrates that the EdgeSpot model consistently provides better accuracy at a fixed false-alarm rate (FAR) than strong BC-ResNet baselines. The largest variant, EdgeSpot-4, improves the 10-shot accuracy at 1% FAR from 73.7% to 82.0%, which requires only 29.4M MACs with 128k parameters.",
      "url": "https://arxiv.org/abs/2601.16316",
      "pdfUrl": "https://arxiv.org/pdf/2601.16316.pdf",
      "titleJa": "EdgeSpot: キーワードスポッティングのための効率的で高性能な少数ショットモデル"
    },
    {
      "id": "2601.16023",
      "arxivId": "2601.16023",
      "title": "Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs",
      "authors": [
        "Lalaram Arya",
        "Mrinmoy Bhattacharjee",
        "Adarsh C. R.",
        "S. R. Mahadeva Prasanna"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "cs.HC"
      ],
      "abstract": "Direct Speech-to-Speech Translation (S2ST) has gained increasing attention for its ability to translate speech from one language to another, while reducing error propagation and latency inherent in traditional cascaded pipelines. However, existing direct S2ST systems continue to face notable challenges, including instability in semantic-acoustic alignment when parallel speech data is scarce, difficulty in preserving speaker identity, and limited multilingual scalability. In this work, we introduce DS2ST-LM, a scalable, single-stage direct S2ST framework leveraging a multilingual Large Language Model (LLM). The architecture integrates a Whisper speech encoder, a learnable projection module, a Qwen2-0.5B LLM, and a timbre-controlled vocoder. We construct GigaS2S-1000, a 1000-hour bilingual corpus by extending the GigaST dataset with high-fidelity synthetic target speech, and show that this synthetic data alleviates data scarcity to some extent. We investigate two semantic token generation strategies: speech-derived S3 tokens and text-derived tokens generated by a pre-trained LLM, and analyze their impact on training stability and semantic consistency. We further evaluate three projection architectures (Linear, Conv1D-Linear, and Q-Former) and observe that while higher-capacity projectors converge faster, the simple Linear projector achieves higher performance. Extensive experiments demonstrate that DS2ST-LM outperforms traditional cascaded and ST (Qwen-Audio) + TTS baselines across both lexical (BLEU, METEOR) and semantic (BLEURT, COMET) metrics, while extending to multiple language pairs, including French, Spanish, German, Hindi, Bengali, and Urdu. Furthermore, we incorporate timbre-aware speech synthesis to preserve speaker information, enabling DS2ST-LM to surpass prior direct S2ST systems in both speaker similarity and perceptual naturalness.",
      "url": "https://arxiv.org/abs/2601.16023",
      "pdfUrl": "https://arxiv.org/pdf/2601.16023.pdf",
      "titleJa": "音色を考慮したLLMベースの直接音声翻訳は複数の言語ペアに拡張可能"
    },
    {
      "id": "2601.15668",
      "arxivId": "2601.15668",
      "title": "EmotionThinker: Prosody-Aware Reinforcement Learning for Explainable Speech Emotion Reasoning",
      "authors": [
        "Dingdong Wang",
        "Shujie Liu",
        "Tianhua Zhang",
        "Youjun Chen",
        "Jinyu Li",
        "Helen Meng"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Emotional information in speech plays a unique role in multimodal perception. However, current Speech Large Language Models (SpeechLLMs), similar to conventional speech emotion recognition (SER) systems, still treat emotion understanding as a simple classification problem. This provides limited interpretability of predictions, while leaving the LLMs' expressive and reasoning capabilities underutilized. In this work, we take the first step to reformulate SER as a deep reasoning problem through reinforcement learning (RL). We propose EmotionThinker, which is designed to generate accurate emotion predictions with interpretable explanations grounded in fine-grained acoustic cues. To achieve this, we first construct EmotionCoT-35K, an emotional reasoning dataset with Chain-of-Thought annotations and detailed captions. Second, we observe that current SpeechLLMs exhibit weak prosody perception, whereas prosodic cues constitute fundamental signals for interpreting emotions. To address this, we develop the prosody-enhanced foundation model EmotionThinker-Base, and demonstrate that prosody enhancement improves emotion understanding. Third, we introduce Group-Relative-Policy-Optimization with Progressive-Trust-aware-Reasoning-Reward (GRPO-PTR) for RL. Different from standard GRPO, which relies only on rule-based outcome rewards, GRPO-PTR progressively introduces reasoning reward, dynamically adjusts it with a trustworthiness weight reflecting the alignment between reasoning and outcome, and evaluates the overall reasoning quality with a reward model based on multi-dimensional criteria. EmotionThinker outperforms previous state-of-the-art evaluation models both in emotion accuracy and explanation quality, advancing SER toward interpretable multimodal reasoning. Project page: https://github.com/dingdongwang/EmotionThinker",
      "url": "https://arxiv.org/abs/2601.15668",
      "pdfUrl": "https://arxiv.org/pdf/2601.15668.pdf",
      "titleJa": "EmotionThinker: 説明可能な音声感情推論のための韻律を考慮した強化学習"
    }
  ],
  "lastUpdated": "2026-01-30T01:02:51.466497",
  "totalCount": 85
}