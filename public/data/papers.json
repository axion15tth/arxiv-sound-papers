{
  "papers": [
    {
      "id": "2601.11262",
      "arxivId": "2601.11262",
      "title": "Scalable Music Cover Retrieval Using Lyrics-Aligned Audio Embeddings",
      "authors": [
        "Joanne Affolter",
        "Benjamin Martin",
        "Elena V. Epure",
        "Gabriel Meseguer-Brocal",
        "Frédéric Kaplan"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG"
      ],
      "abstract": "Music Cover Retrieval, also known as Version Identification, aims to recognize distinct renditions of the same underlying musical work, a task central to catalog management, copyright enforcement, and music retrieval. State-of-the-art approaches have largely focused on harmonic and melodic features, employing increasingly complex audio pipelines designed to be invariant to musical attributes that often vary widely across covers. While effective, these methods demand substantial training time and computational resources. By contrast, lyrics constitute a strong invariant across covers, though their use has been limited by the difficulty of extracting them accurately and efficiently from polyphonic audio. Early methods relied on simple frameworks that limited downstream performance, while more recent systems deliver stronger results but require large models integrated within complex multimodal architectures. We introduce LIVI (Lyrics-Informed Version Identification), an approach that seeks to balance retrieval accuracy with computational efficiency. First, LIVI leverages supervision from state-of-the-art transcription and text embedding models during training to achieve retrieval accuracy on par with--or superior to--harmonic-based systems. Second, LIVI remains lightweight and efficient by removing the transcription step at inference, challenging the dominance of complexity-heavy pipelines.",
      "url": "https://arxiv.org/abs/2601.11262",
      "pdfUrl": "https://arxiv.org/pdf/2601.11262.pdf",
      "titleJa": "歌詞に合わせたオーディオ埋め込みを用いたスケーラブルな音楽カバー検索"
    },
    {
      "id": "2601.11141",
      "arxivId": "2601.11141",
      "title": "FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning",
      "authors": [
        "Tanyu Chen",
        "Tairan Chen",
        "Kai Shen",
        "Zhenghua Bao",
        "Zhihui Zhang",
        "Man Yuan",
        "Yi Shi"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Recent end-to-end spoken dialogue systems leverage speech tokenizers and neural audio codecs to enable LLMs to operate directly on discrete speech representations. However, these models often exhibit limited speaker identity preservation, hindering personalized voice interaction. In this work, we present Chroma 1.0, the first open-source, real-time, end-to-end spoken dialogue model that achieves both low-latency interaction and high-fidelity personalized voice cloning. Chroma achieves sub-second end-to-end latency through an interleaved text-audio token schedule (1:2) that supports streaming generation, while maintaining high-quality personalized voice synthesis across multi-turn conversations. Our experimental results demonstrate that Chroma achieves a 10.96% relative improvement in speaker similarity over the human baseline, with a Real-Time Factor (RTF) of 0.43, while maintaining strong reasoning and dialogue capabilities. Our code and models are publicly available at https://github.com/FlashLabs-AI-Corp/FlashLabs-Chroma and https://huggingface.co/FlashLabs/Chroma-4B .",
      "url": "https://arxiv.org/abs/2601.11141",
      "pdfUrl": "https://arxiv.org/pdf/2601.11141.pdf",
      "titleJa": "FlashLabs Chroma 1.0: パーソナライズされた音声クローニング機能を備えたリアルタイムのエンドツーエンド音声対話モデル"
    },
    {
      "id": "2601.11039",
      "arxivId": "2601.11039",
      "title": "SonicBench: Dissecting the Physical Perception Bottleneck in Large Audio Language Models",
      "authors": [
        "Yirong Sun",
        "Yanjun Chen",
        "Xin Qiu",
        "Gang Zhang",
        "Hongyu Chen",
        "Daokuan Wu",
        "Chengming Li",
        "Min Yang",
        "Dawei Zhu",
        "Wei Zhang",
        "Xiaoyu Shen"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Large Audio Language Models (LALMs) excel at semantic and paralinguistic tasks, yet their ability to perceive the fundamental physical attributes of audio such as pitch, loudness, and spatial location remains under-explored. To bridge this gap, we introduce SonicBench, a psychophysically grounded benchmark that systematically evaluates 12 core physical attributes across five perceptual dimensions. Unlike previous datasets, SonicBench uses a controllable generation toolbox to construct stimuli for two complementary paradigms: recognition (absolute judgment) and comparison (relative judgment). This design allows us to probe not only sensory precision but also relational reasoning capabilities, a domain where humans typically exhibit greater proficiency. Our evaluation reveals a substantial deficiency in LALMs' foundational auditory understanding; most models perform near random guessing and, contrary to human patterns, fail to show the expected advantage on comparison tasks. Furthermore, explicit reasoning yields minimal gains. However, our linear probing analysis demonstrates crucially that frozen audio encoders do successfully capture these physical cues (accuracy at least 60%), suggesting that the primary bottleneck lies in the alignment and decoding stages, where models fail to leverage the sensory signals they have already captured.",
      "url": "https://arxiv.org/abs/2601.11039",
      "pdfUrl": "https://arxiv.org/pdf/2601.11039.pdf",
      "titleJa": "SonicBench: 大規模音声言語モデルにおける物理的知覚ボトルネックの解析"
    },
    {
      "id": "2601.11027",
      "arxivId": "2601.11027",
      "title": "WenetSpeech-Wu: Datasets, Benchmarks, and Models for a Unified Chinese Wu Dialect Speech Processing Ecosystem",
      "authors": [
        "Chengyou Wang",
        "Mingchen Shao",
        "Jingbin Hu",
        "Zeyu Zhu",
        "Hongfei Xue",
        "Bingshen Mu",
        "Xin Xu",
        "Xingyi Duan",
        "Binbin Zhang",
        "Pengcheng Zhu",
        "Chuang Ding",
        "Xiaojun Zhang",
        "Hui Bu",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Speech processing for low-resource dialects remains a fundamental challenge in developing inclusive and robust speech technologies. Despite its linguistic significance and large speaker population, the Wu dialect of Chinese has long been hindered by the lack of large-scale speech data, standardized evaluation benchmarks, and publicly available models. In this work, we present WenetSpeech-Wu, the first large-scale, multi-dimensionally annotated open-source speech corpus for the Wu dialect, comprising approximately 8,000 hours of diverse speech data. Building upon this dataset, we introduce WenetSpeech-Wu-Bench, the first standardized and publicly accessible benchmark for systematic evaluation of Wu dialect speech processing, covering automatic speech recognition (ASR), Wu-to-Mandarin translation, speaker attribute prediction, speech emotion recognition, text-to-speech (TTS) synthesis, and instruction-following TTS (instruct TTS). Furthermore, we release a suite of strong open-source models trained on WenetSpeech-Wu, establishing competitive performance across multiple tasks and empirically validating the effectiveness of the proposed dataset. Together, these contributions lay the foundation for a comprehensive Wu dialect speech processing ecosystem, and we open-source proposed datasets, benchmarks, and models to support future research on dialectal speech intelligence.",
      "url": "https://arxiv.org/abs/2601.11027",
      "pdfUrl": "https://arxiv.org/pdf/2601.11027.pdf",
      "titleJa": "WenetSpeech-Wu: 統一された中国語呉語方言音声処理エコシステムのためのデータセット、ベンチマーク、モデル"
    },
    {
      "id": "2601.10547",
      "arxivId": "2601.10547",
      "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
      "authors": [
        "Dongchao Yang",
        "Yuxin Xie",
        "Yuguo Yin",
        "Zheyu Wang",
        "Xiaoyu Yi",
        "Gongxi Zhu",
        "Xiaolong Weng",
        "Zihan Xiong",
        "Yingzhe Ma",
        "Dading Cong",
        "Jingliang Liu",
        "Zihang Huang",
        "Jinghan Ru",
        "Rongjie Huang",
        "Haoran Wan",
        "Peixu Wang",
        "Kuoxi Yu",
        "Helin Wang",
        "Liming Liang",
        "Xianwei Zhuang",
        "Yuanyuan Wang",
        "Haohan Guo",
        "Junjie Cao",
        "Zeqian Ju",
        "Songxiang Liu",
        "Yuewen Cao",
        "Heming Weng",
        "Yuexian Zou"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD"
      ],
      "abstract": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio-text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "url": "https://arxiv.org/abs/2601.10547",
      "pdfUrl": "https://arxiv.org/pdf/2601.10547.pdf",
      "titleJa": "HeartMuLa: オープンソースの音楽基盤モデルファミリー"
    },
    {
      "id": "2601.10453",
      "arxivId": "2601.10453",
      "title": "Stable Differentiable Modal Synthesis for Learning Nonlinear Dynamics",
      "authors": [
        "Victor Zheleznov",
        "Stefan Bilbao",
        "Alec Wright",
        "Simon King"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS",
        "physics.comp-ph"
      ],
      "abstract": "Modal methods are a long-standing approach to physical modelling synthesis. Extensions to nonlinear problems are possible, including the case of a high-amplitude vibration of a string. A modal decomposition leads to a densely coupled nonlinear system of ordinary differential equations. Recent work in scalar auxiliary variable techniques has enabled construction of explicit and stable numerical solvers for such classes of nonlinear systems. On the other hand, machine learning approaches (in particular neural ordinary differential equations) have been successful in modelling nonlinear systems automatically from data. In this work, we examine how scalar auxiliary variable techniques can be combined with neural ordinary differential equations to yield a stable differentiable model capable of learning nonlinear dynamics. The proposed approach leverages the analytical solution for linear vibration of system's modes so that physical parameters of a system remain easily accessible after the training without the need for a parameter encoder in the model architecture. As a proof of concept, we generate synthetic data for the nonlinear transverse vibration of a string and show that the model can be trained to reproduce the nonlinear dynamics of the system. Sound examples are presented.",
      "url": "https://arxiv.org/abs/2601.10453",
      "pdfUrl": "https://arxiv.org/pdf/2601.10453.pdf",
      "titleJa": "非線形動力学の学習のための安定微分可能モード合成"
    },
    {
      "id": "2601.10770",
      "arxivId": "2601.10770",
      "title": "Unifying Speech Recognition, Synthesis and Conversion with Autoregressive Transformers",
      "authors": [
        "Runyuan Cai",
        "Yu Lin",
        "Yiming Wang",
        "Chunlin Fu",
        "Xiaodong Zeng"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Traditional speech systems typically rely on separate, task-specific models for text-to-speech (TTS), automatic speech recognition (ASR), and voice conversion (VC), resulting in fragmented pipelines that limit scalability, efficiency, and cross-task generalization. In this paper, we present General-Purpose Audio (GPA), a unified audio foundation model that integrates multiple core speech tasks within a single large language model (LLM) architecture. GPA operates on a shared discrete audio token space and supports instruction-driven task induction, enabling a single autoregressive model to flexibly perform TTS, ASR, and VC without architectural modifications. This unified design combines a fully autoregressive formulation over discrete speech tokens, joint multi-task training across speech domains, and a scalable inference pipeline that achieves high concurrency and throughput. The resulting model family supports efficient multi-scale deployment, including a lightweight 0.3B-parameter variant optimized for edge and resource-constrained environments. Together, these design choices demonstrate that a unified autoregressive architecture can achieve competitive performance across diverse speech tasks while remaining viable for low-latency, practical deployment.",
      "url": "https://arxiv.org/abs/2601.10770",
      "pdfUrl": "https://arxiv.org/pdf/2601.10770.pdf",
      "titleJa": "自己回帰トランスフォーマーによる音声認識、合成、変換の統合"
    },
    {
      "id": "2601.10384",
      "arxivId": "2601.10384",
      "title": "RSA-Bench: Benchmarking Audio Large Models in Real-World Acoustic Scenarios",
      "authors": [
        "Yibo Zhang",
        "Liang Lin",
        "Kaiwen Luo",
        "Shilinlu Yan",
        "Jin Wang",
        "Yaoqi Guo",
        "Yitian Chen",
        "Yalan Qin",
        "Zhenhong Zhou",
        "Kun Wang",
        "Li Sun"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD"
      ],
      "abstract": "While Audio Large Models (ALMs) have achieved remarkable proficiency, their robustness remains brittle in real-world deployment. Existing evaluations largely rely on synthetic Gaussian noise or simplistic single-source interference, failing to capture the intricate, multi-layered acoustic dynamics -- or ``Acoustic Ecology'' -- that characterize authentic physical environments. To bridge this ecological gap, we introduce \\textbf{RSA-Bench}, a comprehensive robustness benchmark designed to stress-test ALLMs through high-fidelity auditory scene simulations. Unlike traditional methods, we construct evaluation samples by naturally superimposing diverse environmental soundscapes -- spanning \\textit{Pasture}, \\textit{Extreme Weather}, \\textit{Classroom}, and \\textit{Outdoors} -- onto clean speech signals across a spectrum of interference intensities. By evaluating models on six core tasks ranging from fundamental perception to complex reasoning, our study unveils three macro-level insights: \\textbf{(I) The Perception-Cognition Gap:} Models maintain relative resilience in low-level recognition but suffer a \\textbf{functional collapse} in high-order reasoning tasks under stress; \\textbf{(II) Scenario Sensitivity:} ``Vocal-like'' interference (e.g., background laughter) proves significantly more destructive than mechanical noise, challenging the model's auditory attention mechanisms; and \\textbf{(III) The Denoising Paradox:} Standard speech enhancement often exacerbates performance degradation, as ALLMs prove highly sensitive to the semantic distortions introduced by denoising artifacts.",
      "url": "https://arxiv.org/abs/2601.10384",
      "pdfUrl": "https://arxiv.org/pdf/2601.10384.pdf",
      "titleJa": "RSA-Bench: 実世界の音響シナリオにおける大規模オーディオモデルのベンチマーク"
    },
    {
      "id": "2601.10345",
      "arxivId": "2601.10345",
      "title": "Self-supervised restoration of singing voice degraded by pitch shifting using shallow diffusion",
      "authors": [
        "Yunyi Liu",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Pitch shifting has been an essential feature in singing voice production. However, conventional signal processing approaches exhibit well known trade offs such as formant shifts and robotic coloration that becomes more severe at larger transposition jumps. This paper targets high quality pitch shifting for singing by reframing it as a restoration problem: given an audio track that has been pitch shifted (and thus contaminated by artifacts), we recover a natural sounding performance while preserving its melody and timing. Specifically, we use a lightweight, mel space diffusion model driven by frame level acoustic features such as f0, volume, and content features. We construct training pairs in a self supervised manner by applying pitch shifts and reversing them to simulate realistic artifacts while retaining ground truth. On a curated singing set, the proposed approach substantially reduces pitch shift artifacts compared to representative classical baselines, as measured by both statistical metrics and pairwise acoustic measures. The results suggest that restoration based pitch shifting could be a viable approach towards artifact resistant transposition in vocal production workflows.",
      "url": "https://arxiv.org/abs/2601.10345",
      "pdfUrl": "https://arxiv.org/pdf/2601.10345.pdf",
      "titleJa": "浅い拡散を用いたピッチシフトによって劣化した歌声の自己教師あり復元"
    },
    {
      "id": "2601.10272",
      "arxivId": "2601.10272",
      "title": "MoST: Mixing Speech and Text with Modality-Aware Mixture of Experts",
      "authors": [
        "Yuxuan Lou",
        "Kai Yang",
        "Yang You"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "We present MoST (Mixture of Speech and Text), a novel multimodal large language model that seamlessly integrates speech and text processing through our proposed Modality-Aware Mixture of Experts (MAMoE) architecture. While current multimodal models typically process diverse modality representations with identical parameters, disregarding their inherent representational differences, we introduce specialized routing pathways that direct tokens to modality-appropriate experts based on input type. MAMoE simultaneously enhances modality-specific learning and cross-modal understanding through two complementary components: modality-specific expert groups that capture domain-specific patterns and shared experts that facilitate information transfer between modalities. Building on this architecture, we develop an efficient transformation pipeline that adapts the pretrained MoE language model through strategic post-training on ASR and TTS datasets, followed by fine-tuning with a carefully curated speech-text instruction dataset. A key feature of this pipeline is that it relies exclusively on fully accessible, open-source datasets to achieve strong performance and data efficiency. Comprehensive evaluations across ASR, TTS, audio language modeling, and spoken question answering benchmarks show that MoST consistently outperforms existing models of comparable parameter counts. Our ablation studies confirm that the modality-specific routing mechanism and shared experts design significantly contribute to performance gains across all tested domains. To our knowledge, MoST represents the first fully open-source speech-text LLM built on a Mixture of Experts architecture. \\footnote{We release MoST model, training code, inference code, and training data at https://github.com/NUS-HPC-AI-Lab/MoST",
      "url": "https://arxiv.org/abs/2601.10272",
      "pdfUrl": "https://arxiv.org/pdf/2601.10272.pdf",
      "titleJa": "MoST: モダリティを考慮した専門家の混合による音声とテキストの混合"
    },
    {
      "id": "2601.09931",
      "arxivId": "2601.09931",
      "title": "Diffusion-based Frameworks for Unsupervised Speech Enhancement",
      "authors": [
        "Jean-Eudes Ayilo",
        "Mostafa Sadeghi",
        "Romain Serizel",
        "Xavier Alameda-Pineda"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This paper addresses $\\textit{unsupervised}$ diffusion-based single-channel speech enhancement (SE). Prior work in this direction combines a score-based diffusion model trained on clean speech with a Gaussian noise model whose covariance is structured by non-negative matrix factorization (NMF). This combination is used within an iterative expectation-maximization (EM) scheme, in which a diffusion-based posterior-sampling E-step estimates the clean speech. We first revisit this framework and propose to explicitly model both speech and acoustic noise as latent variables, jointly sampling them in the E-step instead of sampling speech alone as in previous approaches. We then introduce a new unsupervised SE framework that replaces the NMF noise prior with a diffusion-based noise model, learned jointly with the speech prior in a single conditional score model. Within this framework, we derive two variants: one that implicitly accounts for noise and one that explicitly treats noise as a latent variable. Experiments on WSJ0-QUT and VoiceBank-DEMAND show that explicit noise modeling systematically improves SE performance for both NMF-based and diffusion-based noise priors. Under matched conditions, the diffusion-based noise model attains the best overall quality and intelligibility among unsupervised methods, while under mismatched conditions the proposed NMF-based explicit-noise framework is more robust and suffers less degradation than several supervised baselines. Our code will be publicly available on this $\\href{https://github.com/jeaneudesAyilo/enudiffuse}{URL}$.",
      "url": "https://arxiv.org/abs/2601.09931",
      "pdfUrl": "https://arxiv.org/pdf/2601.09931.pdf",
      "titleJa": "教師なし音声強調のための拡散ベースフレームワーク"
    },
    {
      "id": "2601.09603",
      "arxivId": "2601.09603",
      "title": "Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer",
      "authors": [
        "Petros Vavaroutsos",
        "Theodoros Palamas",
        "Pantelis Vikatos"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "In recent years, foundation models have become very popular due to their exceptional performance, mainly in natural language (NLP) tasks where they were first introduced. These models usually consist of hundreds of millions, or even billions, of parameters, making them resource-intensive during training and in production systems, leading to increased costs. This paper focuses on the reduction of a foundation's model size when applied to music information retrieval (MIR) tasks. Our research combines the Branchformer architecture with SummaryMixing, which were first applied in speech recognition, along with a random quantization process. To facilitate reproducibility, we conduct pre-training on publicly available datasets, complemented by a proprietary dataset comparable in scale to other private datasets reported in the literature. We ensure robust evaluation by using a framework consisting of a variety of downstream MIR tasks. Our results show that our architecture achieves competitive performance when compared with other state-of-the-art models that use multi-head self-attention, while reducing the model size from 8.5% up to 12.3%.",
      "url": "https://arxiv.org/abs/2601.09603",
      "pdfUrl": "https://arxiv.org/pdf/2601.09603.pdf",
      "titleJa": "ランダム量子化器を用いた音楽理解のための線形複雑度自己教師学習"
    },
    {
      "id": "2601.09520",
      "arxivId": "2601.09520",
      "title": "Towards Realistic Synthetic Data for Automatic Drum Transcription",
      "authors": [
        "Pierfrancesco Melucci",
        "Paolo Merialdo",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Deep learning models define the state-of-the-art in Automatic Drum Transcription (ADT), yet their performance is contingent upon large-scale, paired audio-MIDI datasets, which are scarce. Existing workarounds that use synthetic data often introduce a significant domain gap, as they typically rely on low-fidelity SoundFont libraries that lack acoustic diversity. While high-quality one-shot samples offer a better alternative, they are not available in a standardized, large-scale format suitable for training. This paper introduces a new paradigm for ADT that circumvents the need for paired audio-MIDI training data. Our primary contribution is a semi-supervised method to automatically curate a large and diverse corpus of one-shot drum samples from unlabeled audio sources. We then use this corpus to synthesize a high-quality dataset from MIDI files alone, which we use to train a sequence-to-sequence transcription model. We evaluate our model on the ENST and MDB test sets, where it achieves new state-of-the-art results, significantly outperforming both fully supervised methods and previous synthetic-data approaches. The code for reproducing our experiments is publicly available at https://github.com/pier-maker92/ADT_STR",
      "url": "https://arxiv.org/abs/2601.09520",
      "pdfUrl": "https://arxiv.org/pdf/2601.09520.pdf",
      "titleJa": "自動ドラム転写のための現実的な合成データに向けて"
    },
    {
      "id": "2601.09461",
      "arxivId": "2601.09461",
      "title": "Analysis of the Maximum Prediction Gain of Short-Term Prediction on Sustained Speech",
      "authors": [
        "Reemt Hinrichs",
        "Muhamad Fadli Damara",
        "Stephan Preihs",
        "Jörn Ostermann"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Signal prediction is widely used in, e.g., economic forecasting, echo cancellation and in data compression, particularly in predictive coding of speech and music. Predictive coding algorithms reduce the bit-rate required for data transmission or storage by signal prediction. The prediction gain is a classic measure in applied signal coding of the quality of a predictor, as it links the mean-squared prediction error to the signal-to-quantization-noise of predictive coders. To evaluate predictor models, knowledge about the maximum achievable prediction gain independent of a predictor model is desirable. In this manuscript, Nadaraya-Watson kernel-regression (NWKR) and an information theoretic upper bound are applied to analyze the upper bound of the prediction gain on a newly recorded dataset of sustained speech/phonemes. It was found that for unvoiced speech a linear predictor always achieves the maximum prediction gain within at most 0.3 dB. On voiced speech, the optimum one-tap predictor was found to be linear but starting with two taps, the maximum achievable prediction gain was found to be about 2 dB to 6 dB above the prediction gain of the linear predictor. Significant differences between speakers/subjects were observed. The created dataset as well as the code can be obtained for research purpose upon request.",
      "url": "https://arxiv.org/abs/2601.09461",
      "pdfUrl": "https://arxiv.org/pdf/2601.09461.pdf",
      "titleJa": "持続音声における短期予測の最大予測利得の分析"
    },
    {
      "id": "2601.09448",
      "arxivId": "2601.09448",
      "title": "Population-Aligned Audio Reproduction With LLM-Based Equalizers",
      "authors": [
        "Ioannis Stylianou",
        "Jon Francombe",
        "Pablo Martinez-Nuevo",
        "Sven Ewan Shepstone",
        "Zheng-Hua Tan"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Conventional audio equalization is a static process that requires manual and cumbersome adjustments to adapt to changing listening contexts (e.g., mood, location, or social setting). In this paper, we introduce a Large Language Model (LLM)-based alternative that maps natural language text prompts to equalization settings. This enables a conversational approach to sound system control. By utilizing data collected from a controlled listening experiment, our models exploit in-context learning and parameter-efficient fine-tuning techniques to reliably align with population-preferred equalization settings. Our evaluation methods, which leverage distributional metrics that capture users' varied preferences, show statistically significant improvements in distributional alignment over random sampling and static preset baselines. These results indicate that LLMs could function as \"artificial equalizers,\" contributing to the development of more accessible, context-aware, and expert-level audio tuning methods.",
      "url": "https://arxiv.org/abs/2601.09448",
      "pdfUrl": "https://arxiv.org/pdf/2601.09448.pdf",
      "titleJa": "LLMベースのイコライザーによる人口整合オーディオ再生"
    },
    {
      "id": "2601.09413",
      "arxivId": "2601.09413",
      "title": "Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception",
      "authors": [
        "Zhen Wan",
        "Chao-Han Huck Yang",
        "Jinchuan Tian",
        "Hanrong Ye",
        "Ankita Pasad",
        "Szu-wei Fu",
        "Arushi Goel",
        "Ryo Hachiuma",
        "Shizhe Diao",
        "Kunal Dhawan",
        "Sreyan Ghosh",
        "Yusuke Hirota",
        "Zhehuai Chen",
        "Rafael Valle",
        "Ehsan Hosseini Asl",
        "Chenhui Chu",
        "Shinji Watanabe",
        "Yu-Chiang Frank Wang",
        "Boris Ginsburg"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MA",
        "eess.AS"
      ],
      "abstract": "We introduce a voice-agentic framework that learns one critical omni-understanding skill: knowing when to trust itself versus when to consult external audio perception. Our work is motivated by a crucial yet counterintuitive finding: naively fine-tuning an omni-model on both speech recognition and external sound understanding tasks often degrades performance, as the model can be easily misled by noisy hypotheses. To address this, our framework, Speech-Hands, recasts the problem as an explicit self-reflection decision. This learnable reflection primitive proves effective in preventing the model from being derailed by flawed external candidates. We show that this agentic action mechanism generalizes naturally from speech recognition to complex, multiple-choice audio reasoning. Across the OpenASR leaderboard, Speech-Hands consistently outperforms strong baselines by 12.1% WER on seven benchmarks. The model also achieves 77.37% accuracy and high F1 on audio QA decisions, showing robust generalization and reliability across diverse audio question answering datasets. By unifying perception and decision-making, our work offers a practical path toward more reliable and resilient audio intelligence.",
      "url": "https://arxiv.org/abs/2601.09413",
      "pdfUrl": "https://arxiv.org/pdf/2601.09413.pdf",
      "titleJa": "Speech-Hands: 全方位知覚による音声認識とオーディオ推論への自己反映型音声エージェントアプローチ"
    },
    {
      "id": "2601.09385",
      "arxivId": "2601.09385",
      "title": "SLAM-LLM: A Modular, Open-Source Multimodal Large Language Model Framework and Best Practice for Speech, Language, Audio and Music Processing",
      "authors": [
        "Ziyang Ma",
        "Guanrou Yang",
        "Wenxi Chen",
        "Zhifu Gao",
        "Yexing Du",
        "Xiquan Li",
        "Zhisheng Zheng",
        "Haina Zhu",
        "Jianheng Zhuo",
        "Zheshu Song",
        "Ruiyang Xu",
        "Tiranrui Wang",
        "Yifan Yang",
        "Yanqiao Zhu",
        "Zhikang Niu",
        "Liumeng Xue",
        "Yinghao Ma",
        "Ruibin Yuan",
        "Shiliang Zhang",
        "Kai Yu",
        "Eng Siong Chng",
        "Xie Chen"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.MM"
      ],
      "abstract": "The recent surge in open-source Multimodal Large Language Models (MLLM) frameworks, such as LLaVA, provides a convenient kickoff for artificial intelligence developers and researchers. However, most of the MLLM frameworks take vision as the main input modality, and provide limited in-depth support for the modality of speech, audio, and music. This situation hinders the development of audio-language models, and forces researchers to spend a lot of effort on code writing and hyperparameter tuning. We present SLAM-LLM, an open-source deep learning framework designed to train customized MLLMs, focused on speech, language, audio, and music processing. SLAM-LLM provides a modular configuration of different encoders, projectors, LLMs, and parameter-efficient fine-tuning plugins. SLAM-LLM also includes detailed training and inference recipes for mainstream tasks, along with high-performance checkpoints like LLM-based Automatic Speech Recognition (ASR), Automated Audio Captioning (AAC), and Music Captioning (MC). Some of these recipes have already reached or are nearing state-of-the-art performance, and some relevant techniques have also been accepted by academic papers. We hope SLAM-LLM will accelerate iteration, development, data engineering, and model training for researchers. We are committed to continually pushing forward audio-based MLLMs through this open-source framework, and call on the community to contribute to the LLM-based speech, audio and music processing.",
      "url": "https://arxiv.org/abs/2601.09385",
      "pdfUrl": "https://arxiv.org/pdf/2601.09385.pdf",
      "titleJa": "SLAM-LLM: 音声、言語、オーディオ、音楽処理のためのモジュール式オープンソースマルチモーダル大規模言語モデルフレームワークとベストプラクティス"
    },
    {
      "id": "2601.09333",
      "arxivId": "2601.09333",
      "title": "Research on Piano Timbre Transformation System Based on Diffusion Model",
      "authors": [
        "Chun-Chieh Hsu",
        "Tsai-Ling Hsu",
        "Chen-Chen Yeh",
        "Shao-Chien Lu",
        "Cheng-Han Wu",
        "Bing-Ze Liu",
        "Timothy K. Shih",
        "Yu-Cheng Lin"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.MM"
      ],
      "abstract": "We propose a timbre conversion model based on the Diffusion architecture de-signed to precisely translate music played by various instruments into piano ver-sions. The model employs a Pitch Encoder and Loudness Encoder to extract pitch and loudness features of the music, which serve as conditional inputs to the Dif-fusion Model's decoder, generating high-quality piano timbres. Case analysis re-sults show that the model performs excellently in terms of pitch accuracy and timbral similarity, maintaining stable conversion across different musical styles (classical, jazz, pop) and lengths (from short clips to full pieces). Particularly, the model maintains high sound quality and accuracy even when dealing with rapidly changing notes and complex musical structures, demonstrating good generaliza-tion capability. Additionally, the model has the potential for real-time musical conversion and is suitable for live performances and digital music creation tools. Future research will focus on enhancing the handling of loudness dynamics and incorporating additional musical features (such as timbral variations and rhythmic complexity) to improve the model's adaptability and expressiveness. We plan to explore the model's application potential in other timbre conversion tasks, such as converting vocals to instrumental sounds or integration with MIDI digital pianos, further expanding the application scope of the Diffusion-based timbre conversion model in the field of music generation.",
      "url": "https://arxiv.org/abs/2601.09333",
      "pdfUrl": "https://arxiv.org/pdf/2601.09333.pdf",
      "titleJa": "拡散モデルに基づくピアノ音色変換システムの研究"
    },
    {
      "id": "2601.09239",
      "arxivId": "2601.09239",
      "title": "DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion",
      "authors": [
        "Hanlin Zhang",
        "Daxin Tan",
        "Dehua Tao",
        "Xiao Chen",
        "Haochen Tan",
        "Yunhe Li",
        "Yuchen Cao",
        "Jianping Wang",
        "Linqi Song"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Speech tokenizers serve as the cornerstone of discrete Speech Large Language Models (Speech LLMs). Existing tokenizers either prioritize semantic encoding, fuse semantic content with acoustic style inseparably, or achieve incomplete semantic-acoustic disentanglement. To achieve better disentanglement, we propose DSA-Tokenizer, which explicitly disentangles speech into discrete semantic and acoustic tokens via distinct optimization constraints. Specifically, semantic tokens are supervised by ASR to capture linguistic content, while acoustic tokens focus on mel-spectrograms restoration to encode style. To eliminate rigid length constraints between the two sequences, we introduce a hierarchical Flow-Matching decoder that further improve speech generation quality. Furthermore, We employ a joint reconstruction-recombination training strategy to enforce this separation. DSA-Tokenizer enables high fidelity reconstruction and flexible recombination through robust disentanglement, facilitating controllable generation in speech LLMs. Our analysis highlights disentangled tokenization as a pivotal paradigm for future speech modeling. Audio samples are avaialble at https://anonymous.4open.science/w/DSA_Tokenizer_demo/. The code and model will be made publicly available after the paper has been accepted.",
      "url": "https://arxiv.org/abs/2601.09239",
      "pdfUrl": "https://arxiv.org/pdf/2601.09239.pdf",
      "titleJa": "DSA-Tokenizer: フローマッチングに基づく階層的融合による意味音響分離トークン化"
    },
    {
      "id": "2601.08764",
      "arxivId": "2601.08764",
      "title": "FusID: Modality-Fused Semantic IDs for Generative Music Recommendation",
      "authors": [
        "Haven Kim",
        "Yupeng Hou",
        "Julian McAuley"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.IR",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Generative recommendation systems have achieved significant advances by leveraging semantic IDs to represent items. However, existing approaches that tokenize each modality independently face two critical limitations: (1) redundancy across modalities that reduces efficiency, and (2) failure to capture inter-modal interactions that limits item representation. We introduce FusID, a modality-fused semantic ID framework that addresses these limitations through three key components: (i) multimodal fusion that learns unified representations by jointly encoding information across modalities, (ii) representation learning that brings frequently co-occurring item embeddings closer while maintaining distinctiveness and preventing feature redundancy, and (iii) product quantization that converts the fused continuous embeddings into multiple discrete tokens to mitigate ID conflict. Evaluated on a multimodal next-song recommendation (i.e., playlist continuation) benchmark, FusID achieves zero ID conflicts, ensuring that each token sequence maps to exactly one song, mitigates codebook underutilization, and outperforms baselines in terms of MRR and Recall@k (k = 1, 5, 10, 20).",
      "url": "https://arxiv.org/abs/2601.08764",
      "pdfUrl": "https://arxiv.org/pdf/2601.08764.pdf",
      "titleJa": "FusID: 生成的音楽推薦のためのモダリティ融合セマンティックID"
    },
    {
      "id": "2601.10629",
      "arxivId": "2601.10629",
      "title": "VoiceSculptor: Your Voice, Designed By You",
      "authors": [
        "Jingbin Hu",
        "Huakang Chen",
        "Linhan Ma",
        "Dake Guo",
        "Qirui Zhan",
        "Wenhao Li",
        "Haoyu Zhang",
        "Kangxiang Xia",
        "Ziyu Zhang",
        "Wenjie Tian",
        "Chengyou Wang",
        "Jinrui Liang",
        "Shuhan Guo",
        "Zihang Yang",
        "Bengu Wu",
        "Binbin Zhang",
        "Pengcheng Zhu",
        "Pengyuan Xie",
        "Chuan Xie",
        "Qiang Zhang",
        "Jie Liu",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Despite rapid progress in text-to-speech (TTS), open-source systems still lack truly instruction-following, fine-grained control over core speech attributes (e.g., pitch, speaking rate, age, emotion, and style). We present VoiceSculptor, an open-source unified system that bridges this gap by integrating instruction-based voice design and high-fidelity voice cloning in a single framework. It generates controllable speaker timbre directly from natural-language descriptions, supports iterative refinement via Retrieval-Augmented Generation (RAG), and provides attribute-level edits across multiple dimensions. The designed voice is then rendered into a prompt waveform and fed into a cloning model to enable high-fidelity timbre transfer for downstream speech synthesis. VoiceSculptor achieves open-source state-of-the-art (SOTA) on InstructTTSEval-Zh, and is fully open-sourced, including code and pretrained models, to advance reproducible instruction-controlled TTS research.",
      "url": "https://arxiv.org/abs/2601.10629",
      "pdfUrl": "https://arxiv.org/pdf/2601.10629.pdf",
      "titleJa": "VoiceSculptor: あなたの声を、あなたがデザインする"
    },
    {
      "id": "2601.10078",
      "arxivId": "2601.10078",
      "title": "Nearest Kronecker Product Decomposition Based Subband Adaptive Filter: Algorithms and Applications",
      "authors": [
        "Jianhong Ye",
        "Haiquan Zhao"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "eess.AS",
        "cs.IT"
      ],
      "abstract": "Recently, the nearest Kronecker product (NKP) decomposition-based normalized least mean square (NLMS-NKP) algorithm has demonstrated superior convergence performance compared to the conventional NLMS algorithm. However, its convergence rate exhibits significant degradation when processing highly correlated input signals. To address this problem, we propose a type-I NKP-based normalized subband adaptive filter (NSAF) algorithm, namely NSAF-NKP-I. Nevertheless, this algorithm incurs substantially higher computational overhead than the NLMS-NKP algorithm. Remarkably, our enhanced type-II NKP-based NSAF (NSAF-NKP-II) algorithm achieves equivalent convergence performance while substantially reducing computational complexity. Furthermore, to enhance robustness against impulsive noise interference, we develop two robust variants: the maximum correntropy criterion-based robust NSAF-NKP (RNSAF-NKP-MCC) and logarithmic criterion-based robust NSAF-NKP (RNSAF-NKP-LC) algorithms. Additionally, detailed analyses of computational complexity, step-size range, and theoretical steady-state performance are provided for theproposed algorithms. To enhance the practicability of the NSAF-NKP-II algorithm in complex nonlinear environments, we further devise two nonlinear implementations: the trigonometric functional link network-based NKP-NSAF (TFLN-NSAF-NKP) and Volterra series expansion-based NKP-NSAF (Volterra-NKP-NSAF) algorithms. In active noise control (ANC) systems, we further propose the filtered-x NSAF-NKP-II (NKP-FxNSAF) algorithm. Simulation experiments in echo cancellation, sparse system identification, nonlinear processing, and ANC scenarios are conducted to validate the superiority of the proposed algorithms over existing state-of-the-art counterparts.",
      "url": "https://arxiv.org/abs/2601.10078",
      "pdfUrl": "https://arxiv.org/pdf/2601.10078.pdf",
      "titleJa": "最近傍クロネッカー積分解に基づくサブバンド適応フィルタ：アルゴリズムと応用"
    },
    {
      "id": "2601.08537",
      "arxivId": "2601.08537",
      "title": "Weakly Supervised Tabla Stroke Transcription via TI-SDRM: A Rhythm-Aware Lattice Rescoring Framework",
      "authors": [
        "Rahul Bapusaheb Kodag",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Tabla Stroke Transcription (TST) is central to the analysis of rhythmic structure in Hindustani classical music, yet remains challenging due to complex rhythmic organization and the scarcity of strongly annotated data. Existing approaches largely rely on fully supervised learning with onset-level annotations, which are costly and impractical at scale. This work addresses TST in a weakly supervised setting, using only symbolic stroke sequences without temporal alignment. We propose a framework that combines a CTC-based acoustic model with sequence-level rhythmic rescoring. The acoustic model produces a decoding lattice, which is refined using a \\textbf{$T\\bar{a}la$}-Independent Static--Dynamic Rhythmic Model (TI-SDRM) that integrates long-term rhythmic structure with short-term adaptive dynamics through an adaptive interpolation mechanism. We curate a new real-world tabla solo dataset and a complementary synthetic dataset, establishing the first benchmark for weakly supervised TST in Hindustani classical music. Experiments demonstrate consistent and substantial reductions in stroke error rate over acoustic-only decoding, confirming the importance of explicit rhythmic structure for accurate transcription.",
      "url": "https://arxiv.org/abs/2601.08537",
      "pdfUrl": "https://arxiv.org/pdf/2601.08537.pdf",
      "titleJa": "TI-SDRMによる弱教師付きタブラストローク転写：リズムを考慮したラティス再採点フレームワーク"
    },
    {
      "id": "2601.08516",
      "arxivId": "2601.08516",
      "title": "Robust CAPTCHA Using Audio Illusions in the Era of Large Language Models: from Evaluation to Advances",
      "authors": [
        "Ziqi Ding",
        "Yunfeng Wan",
        "Wei Song",
        "Yi Liu",
        "Gelei Deng",
        "Nan Sun",
        "Huadong Mo",
        "Jingling Xue",
        "Shidong Pan",
        "Yuekang Li"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.SD",
        "cs.CY",
        "eess.AS"
      ],
      "abstract": "CAPTCHAs are widely used by websites to block bots and spam by presenting challenges that are easy for humans but difficult for automated programs to solve. To improve accessibility, audio CAPTCHAs are designed to complement visual ones. However, the robustness of audio CAPTCHAs against advanced Large Audio Language Models (LALMs) and Automatic Speech Recognition (ASR) models remains unclear. In this paper, we introduce AI-CAPTCHA, a unified framework that offers (i) an evaluation framework, ACEval, which includes advanced LALM- and ASR-based solvers, and (ii) a novel audio CAPTCHA approach, IllusionAudio, leveraging audio illusions. Through extensive evaluations of seven widely deployed audio CAPTCHAs, we show that most existing methods can be solved with high success rates by advanced LALMs and ASR models, exposing critical security weaknesses. To address these vulnerabilities, we design a new audio CAPTCHA approach, IllusionAudio, which exploits perceptual illusion cues rooted in human auditory mechanisms. Extensive experiments demonstrate that our method defeats all tested LALM- and ASR-based attacks while achieving a 100% human pass rate, significantly outperforming existing audio CAPTCHA methods.",
      "url": "https://arxiv.org/abs/2601.08516",
      "pdfUrl": "https://arxiv.org/pdf/2601.08516.pdf",
      "titleJa": "大規模言語モデルの時代における音響錯覚を利用した堅牢なCAPTCHA：評価から発展まで"
    },
    {
      "id": "2601.08480",
      "arxivId": "2601.08480",
      "title": "Quantitative Analysis of Proxy Tasks for Anomalous Sound Detection",
      "authors": [
        "Seunghyeon Shin",
        "Seokjin Lee"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Anomalous sound detection (ASD) typically involves self-supervised proxy tasks to learn feature representations from normal sound data, owing to the scarcity of anomalous samples. In ASD research, proxy tasks such as AutoEncoders operate under the explicit assumption that models trained on normal data will increase the reconstruction errors related to anomalies. A natural extension suggests that improved proxy task performance should improve ASD capability; however, this relationship has received little systematic attention. This study addresses this research gap by quantitatively analyzing the relationship between proxy task metrics and ASD performance across five configurations, namely, AutoEncoders, classification, source separation, contrastive learning, and pre-trained models. We evaluate the learned representations using linear probe (linear separability) and Mahalanobis distance (distributional compactness). Our experiments reveal that strong proxy performance does not necessarily improve anomalous sound detection performance. Specifically, classification tasks experience performance saturation owing to insufficient task difficulty, whereas contrastive learning fails to learn meaningful features owing to limited data diversity. Notably, source separation is the only task demonstrating a strong positive correlation, such that improved separation consistently improves anomaly detection. Based on these findings, we highlight the critical importance of task difficulty and objective alignment. Finally, we propose a three-stage alignment verification protocol to guide the design of highly effective proxy tasks for ASD systems.",
      "url": "https://arxiv.org/abs/2601.08480",
      "pdfUrl": "https://arxiv.org/pdf/2601.08480.pdf",
      "titleJa": "異常音検知のための代理タスクの定量分析"
    },
    {
      "id": "2601.08450",
      "arxivId": "2601.08450",
      "title": "Decoding Order Matters in Autoregressive Speech Synthesis",
      "authors": [
        "Minghui Zhao",
        "Anton Ragni"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Autoregressive speech synthesis often adopts a left-to-right order, yet generation order is a modelling choice. We investigate decoding order through masked diffusion framework, which progressively unmasks positions and allows arbitrary decoding orders during training and inference. By interpolating between identity and random permutations, we show that randomness in decoding order affects speech quality. We further compare fixed strategies, such as \\texttt{l2r} and \\texttt{r2l} with adaptive ones, such as Top-$K$, finding that fixed-order decoding, including the dominating left-to-right approach, is suboptimal, while adaptive decoding yields better performance. Finally, since masked diffusion requires discrete inputs, we quantise acoustic representations and find that even 1-bit quantisation can support reasonably high-quality speech.",
      "url": "https://arxiv.org/abs/2601.08450",
      "pdfUrl": "https://arxiv.org/pdf/2601.08450.pdf",
      "titleJa": "自己回帰音声合成におけるデコード順序の重要性"
    },
    {
      "id": "2601.08358",
      "arxivId": "2601.08358",
      "title": "Decodable but not structured: linear probing enables Underwater Acoustic Target Recognition with pretrained audio embeddings",
      "authors": [
        "Hilde I. Hummel",
        "Sandjai Bhulai",
        "Rob D. van der Mei",
        "Burooj Ghani"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Increasing levels of anthropogenic noise from ships contribute significantly to underwater sound pollution, posing risks to marine ecosystems. This makes monitoring crucial to understand and quantify the impact of the ship radiated noise. Passive Acoustic Monitoring (PAM) systems are widely deployed for this purpose, generating years of underwater recordings across diverse soundscapes. Manual analysis of such large-scale data is impractical, motivating the need for automated approaches based on machine learning. Recent advances in automatic Underwater Acoustic Target Recognition (UATR) have largely relied on supervised learning, which is constrained by the scarcity of labeled data. Transfer Learning (TL) offers a promising alternative to mitigate this limitation. In this work, we conduct the first empirical comparative study of transfer learning for UATR, evaluating multiple pretrained audio models originating from diverse audio domains. The pretrained model weights are frozen, and the resulting embeddings are analyzed through classification, clustering, and similarity-based evaluations. The analysis shows that the geometrical structure of the embedding space is largely dominated by recording-specific characteristics. However, a simple linear probe can effectively suppress this recording-specific information and isolate ship-type features from these embeddings. As a result, linear probing enables effective automatic UATR using pretrained audio models at low computational cost, significantly reducing the need for a large amounts of high-quality labeled ship recordings.",
      "url": "https://arxiv.org/abs/2601.08358",
      "pdfUrl": "https://arxiv.org/pdf/2601.08358.pdf",
      "titleJa": "デコード可能だが構造化されていない：線形プローブにより、事前学習済みのオーディオ埋め込みによる水中音響ターゲット認識が可能になる"
    },
    {
      "id": "2601.08074",
      "arxivId": "2601.08074",
      "title": "Elastic overtones: an equal temperament 12 tone music system with \"perfect\" fifths",
      "authors": [
        "X. Hernandez",
        "Luis Nasser",
        "Pablo Garcia-Valenzuela"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "physics.soc-ph",
        "cs.SD",
        "eess.AS",
        "physics.pop-ph"
      ],
      "abstract": "The impossibility of a transposable 12 semitone tuning of the octave arises from the mathematical fact that $2 \\times 2^{7/12} \\neq 3$ i.e., the second harmonic of the fifth can not exactly match the third harmonic of the fundamental. This in turn, stems from the whole number harmonic structure of western music, and the subsequent fundamental character of the octave interval as multiples of 2 in frequency, a property inherited by our music system from the physics of instruments with vibrating elements being to a good approximation one dimensional. In the current era of electronic music, one can relax the above assumptions to construct an analogous music system where all the structural properties of the standard music system are preserved, but where harmonics are not whole number multiples of the fundamental frequency, and the octave is no longer a factor of 2 in frequency. This now allows to construct a transposable 12 semitone music system where the second harmonic of the fifth exactly matches the third harmonic of the fundamental. The enhanced harmonic qualities of this system recover to a good approximation the musical qualities of Just Intonation, whilst retaining by construction all the versatility and modulating ability of 12TET.",
      "url": "https://arxiv.org/abs/2601.08074",
      "pdfUrl": "https://arxiv.org/pdf/2601.08074.pdf",
      "titleJa": "弾性倍音: 完全五度を含む平均律12音音楽システム"
    },
    {
      "id": "2601.07999",
      "arxivId": "2601.07999",
      "title": "VoxCog: Towards End-to-End Multilingual Cognitive Impairment Classification through Dialectal Knowledge",
      "authors": [
        "Tiantian Feng",
        "Anfeng Xu",
        "Jinkook Lee",
        "Shrikanth Narayanan"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "In this work, we present a novel perspective on cognitive impairment classification from speech by integrating speech foundation models that explicitly recognize speech dialects. Our motivation is based on the observation that individuals with Alzheimer's Disease (AD) or mild cognitive impairment (MCI) often produce measurable speech characteristics, such as slower articulation rate and lengthened sounds, in a manner similar to dialectal phonetic variations seen in speech. Building on this idea, we introduce VoxCog, an end-to-end framework that uses pre-trained dialect models to detect AD or MCI without relying on additional modalities such as text or images. Through experiments on multiple multilingual datasets for AD and MCI detection, we demonstrate that model initialization with a dialect classifier on top of speech foundation models consistently improves the predictive performance of AD or MCI. Our trained models yield similar or often better performance compared to previous approaches that ensembled several computational methods using different signal modalities. Particularly, our end-to-end speech-based model achieves 87.5% and 85.9% accuracy on the ADReSS 2020 challenge and ADReSSo 2021 challenge test sets, outperforming existing solutions that use multimodal ensemble-based computation or LLMs.",
      "url": "https://arxiv.org/abs/2601.07999",
      "pdfUrl": "https://arxiv.org/pdf/2601.07999.pdf",
      "titleJa": "VoxCog: 方言知識によるエンドツーエンドの多言語認知障害分類に向けて"
    },
    {
      "id": "2601.07969",
      "arxivId": "2601.07969",
      "title": "Tuberculosis Screening from Cough Audio: Baseline Models, Clinical Variables, and Uncertainty Quantification",
      "authors": [
        "George P. Kafentzis",
        "Efstratios Selisios"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "In this paper, we propose a standardized framework for automatic tuberculosis (TB) detection from cough audio and routinely collected clinical data using machine learning. While TB screening from audio has attracted growing interest, progress is difficult to measure because existing studies vary substantially in datasets, cohort definitions, feature representations, model families, validation protocols, and reported metrics. Consequently, reported gains are often not directly comparable, and it remains unclear whether improvements stem from modeling advances or from differences in data and evaluation. We address this gap by establishing a strong, well-documented baseline for TB prediction using cough recordings and accompanying clinical metadata from a recently compiled dataset from several countries. Our pipeline is reproducible end-to-end, covering feature extraction, multimodal fusion, cougher-independent evaluation, and uncertainty quantification, and it reports a consistent suite of clinically relevant metrics to enable fair comparison. We further quantify performance for cough audio-only and fused (audio + clinical metadata) models, and release the full experimental protocol to facilitate benchmarking. This baseline is intended to serve as a common reference point and to reduce methodological variance that currently holds back progress in the field.",
      "url": "https://arxiv.org/abs/2601.07969",
      "pdfUrl": "https://arxiv.org/pdf/2601.07969.pdf",
      "titleJa": "咳嗽音による結核スクリーニング：ベースラインモデル、臨床変数、不確実性の定量化"
    },
    {
      "id": "2601.07958",
      "arxivId": "2601.07958",
      "title": "LJ-Spoof: A Generatively Varied Corpus for Audio Anti-Spoofing and Synthesis Source Tracing",
      "authors": [
        "Surya Subramani",
        "Hashim Ali",
        "Hafiz Malik"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Speaker-specific anti-spoofing and synthesis-source tracing are central challenges in audio anti-spoofing. Progress has been hampered by the lack of datasets that systematically vary model architectures, synthesis pipelines, and generative parameters. To address this gap, we introduce LJ-Spoof, a speaker-specific, generatively diverse corpus that systematically varies prosody, vocoders, generative hyperparameters, bona fide prompt sources, training regimes, and neural post-processing. The corpus spans one speakers-including studio-quality recordings-30 TTS families, 500 generatively variant subsets, 10 bona fide neural-processing variants, and more than 3 million utterances. This variation-dense design enables robust speaker-conditioned anti-spoofing and fine-grained synthesis-source tracing. We further position this dataset as both a practical reference training resource and a benchmark evaluation suite for anti-spoofing and source tracing.",
      "url": "https://arxiv.org/abs/2601.07958",
      "pdfUrl": "https://arxiv.org/pdf/2601.07958.pdf",
      "titleJa": "LJ-Spoof: 音声スプーフィング防止と合成音源追跡のための生成的に多様なコーパス"
    },
    {
      "id": "2601.08879",
      "arxivId": "2601.08879",
      "title": "Echoes of Ideology: Toward an Audio Analysis Pipeline to Unveil Character Traits in Historical Nazi Propaganda Films",
      "authors": [
        "Nicolas Ruth",
        "Manuel Burghardt"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This study investigates the use of computational audio analysis to examine ideological narratives in Nazi propaganda films. Employing a three-step pipeline, speaker diarization, audio transcription and psycholinguistic analysis, it reveals ideological patterns in characters. Despite current issues with speaker diarization, the methodology provides insights into character traits and propaganda narratives, suggesting scalable applications.",
      "url": "https://arxiv.org/abs/2601.08879",
      "pdfUrl": "https://arxiv.org/pdf/2601.08879.pdf",
      "titleJa": "イデオロギーの響き：ナチスの歴史的プロパガンダ映画の登場人物の特徴を明らかにする音声分析パイプラインの構築に向けて"
    },
    {
      "id": "2601.07481",
      "arxivId": "2601.07481",
      "title": "Directional reflection modeling via wavenumber-domain reflection coefficient for 3D acoustic field simulation",
      "authors": [
        "Satoshi Hoshika",
        "Takahiro Iwami",
        "Akira Omoto"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This study proposes a framework for incorporating wavenumber-domain acoustic reflection coefficients into sound field analysis to characterize direction-dependent material reflection and scattering phenomena. The reflection coefficient is defined as the amplitude ratio between incident and reflected waves for each propagation direction and is estimated from spatial Fourier transforms of the incident and reflected sound fields. The resulting wavenumber-domain reflection coefficients are converted into an acoustic admittance representation that is directly compatible with numerical methods such as the Boundary Element Method (BEM), enabling simulation of reflections beyond simple specular components. Unlike conventional extended reaction models, the proposed approach avoids explicit modeling of the material interior. This significantly reduces computational cost while allowing direct use of measured data, empirical models, or user-defined directional reflection characteristics. The validity of the proposed formulation was previously demonstrated by the authors through two-dimensional sound field simulations, in which accurate reproduction of direction-dependent reflection behavior was confirmed. In the present work, the framework is extended to three-dimensional analysis, demonstrating its applicability to more realistic and complex acoustic environments. The proposed approach provides a practical and flexible tool for simulating direction-dependent acoustic reflections and scattering, with potential applications in architectural acoustics, material characterization, and noise control.",
      "url": "https://arxiv.org/abs/2601.07481",
      "pdfUrl": "https://arxiv.org/pdf/2601.07481.pdf",
      "titleJa": "3次元音響場シミュレーションのための波数領域反射係数による方向反射モデリング"
    },
    {
      "id": "2601.07237",
      "arxivId": "2601.07237",
      "title": "The ICASSP 2026 Automatic Song Aesthetics Evaluation Challenge",
      "authors": [
        "Guobin Ma",
        "Yuxuan Xia",
        "Jixun Yao",
        "Huixin Xue",
        "Hexin Liu",
        "Shuai Wang",
        "Hao Liu",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "This paper summarizes the ICASSP 2026 Automatic Song Aesthetics Evaluation (ASAE) Challenge, which focuses on predicting the subjective aesthetic scores of AI-generated songs. The challenge consists of two tracks: Track 1 targets the prediction of the overall musicality score, while Track 2 focuses on predicting five fine-grained aesthetic scores. The challenge attracted strong interest from the research community and received numerous submissions from both academia and industry. Top-performing systems significantly surpassed the official baseline, demonstrating substantial progress in aligning objective metrics with human aesthetic preferences. The outcomes establish a standardized benchmark and advance human-aligned evaluation methodologies for modern music generation systems.",
      "url": "https://arxiv.org/abs/2601.07237",
      "pdfUrl": "https://arxiv.org/pdf/2601.07237.pdf",
      "titleJa": "ICASSP 2026 自動歌曲美学評価チャレンジ"
    },
    {
      "id": "2601.11517",
      "arxivId": "2601.11517",
      "title": "Do explanations generalize across large reasoning models?",
      "authors": [
        "Koyena Pal",
        "David Bau",
        "Chandan Singh"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Large reasoning models (LRMs) produce a textual chain of thought (CoT) in the process of solving a problem, which serves as a potentially powerful tool to understand the problem by surfacing a human-readable, natural-language explanation. However, it is unclear whether these explanations generalize, i.e. whether they capture general patterns about the underlying problem rather than patterns which are esoteric to the LRM. This is a crucial question in understanding or discovering new concepts, e.g. in AI for science. We study this generalization question by evaluating a specific notion of generalizability: whether explanations produced by one LRM induce the same behavior when given to other LRMs. We find that CoT explanations often exhibit this form of generalization (i.e. they increase consistency between LRMs) and that this increased generalization is correlated with human preference rankings and post-training with reinforcement learning. We further analyze the conditions under which explanations yield consistent answers and propose a straightforward, sentence-level ensembling strategy that improves consistency. Taken together, these results prescribe caution when using LRM explanations to yield new insights and outline a framework for characterizing LRM explanation generalization.",
      "url": "https://arxiv.org/abs/2601.11517",
      "pdfUrl": "https://arxiv.org/pdf/2601.11517.pdf",
      "titleJa": "説明は大規模な推論モデル全体に一般化されますか?"
    },
    {
      "id": "2601.11516",
      "arxivId": "2601.11516",
      "title": "Building Production-Ready Probes For Gemini",
      "authors": [
        "János Kramár",
        "Joshua Engels",
        "Zheng Wang",
        "Bilal Chughtai",
        "Rohin Shah",
        "Neel Nanda",
        "Arthur Conmy"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift. We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes. These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.",
      "url": "https://arxiv.org/abs/2601.11516",
      "pdfUrl": "https://arxiv.org/pdf/2601.11516.pdf",
      "titleJa": "ジェミニ向け量産対応探査機の構築"
    },
    {
      "id": "2601.11505",
      "arxivId": "2601.11505",
      "title": "MetaboNet: The Largest Publicly Available Consolidated Dataset for Type 1 Diabetes Management",
      "authors": [
        "Miriam K. Wolff",
        "Peter Calhoun",
        "Eleonora Maria Aiello",
        "Yao Qin",
        "Sam F. Royston"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY",
        "q-bio.QM"
      ],
      "abstract": "Progress in Type 1 Diabetes (T1D) algorithm development is limited by the fragmentation and lack of standardization across existing T1D management datasets. Current datasets differ substantially in structure and are time-consuming to access and process, which impedes data integration and reduces the comparability and generalizability of algorithmic developments. This work aims to establish a unified and accessible data resource for T1D algorithm development. Multiple publicly available T1D datasets were consolidated into a unified resource, termed the MetaboNet dataset. Inclusion required the availability of both continuous glucose monitoring (CGM) data and corresponding insulin pump dosing records. Additionally, auxiliary information such as reported carbohydrate intake and physical activity was retained when present. The MetaboNet dataset comprises 3135 subjects and 1228 patient-years of overlapping CGM and insulin data, making it substantially larger than existing standalone benchmark datasets. The resource is distributed as a fully public subset available for immediate download at https://metabo-net.org/ , and with a Data Use Agreement (DUA)-restricted subset accessible through their respective application processes. For the datasets in the latter subset, processing pipelines are provided to automatically convert the data into the standardized MetaboNet format. A consolidated public dataset for T1D research is presented, and the access pathways for both its unrestricted and DUA-governed components are described. The resulting dataset covers a broad range of glycemic profiles and demographics and thus can yield more generalizable algorithmic performance than individual datasets.",
      "url": "https://arxiv.org/abs/2601.11505",
      "pdfUrl": "https://arxiv.org/pdf/2601.11505.pdf",
      "titleJa": "MetaboNet: 1型糖尿病管理のための公開統合データセットとしては最大規模"
    },
    {
      "id": "2601.11496",
      "arxivId": "2601.11496",
      "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents",
      "authors": [
        "Eilam Shapira",
        "Roi Reichart",
        "Moshe Tennenholtz"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "abstract": "The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.",
      "url": "https://arxiv.org/abs/2601.11496",
      "pdfUrl": "https://arxiv.org/pdf/2601.11496.pdf",
      "titleJa": "毒リンゴ効果：AIエージェントの技術拡張による仲介市場の戦略的操作"
    },
    {
      "id": "2601.11492",
      "arxivId": "2601.11492",
      "title": "BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics",
      "authors": [
        "Kaiwen Wang",
        "Kaili Zheng",
        "Rongrong Deng",
        "Qingmin Fan",
        "Milin Zhang",
        "Zongrui Li",
        "Xuesi Zhou",
        "Bo Han",
        "Liren Chen",
        "Chenyi Guo",
        "Ji Wu"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal boundaries and spatial and technical attributes, we parse match footage into 18 hierarchical technical-tactical indicators. We then propose a graph-based predictive model that fuses these explicit technical-tactical profiles with learnable, time-variant latent embeddings to capture the dynamics of boxer matchups. Modeling match outcome as a differentiable function of technical-tactical indicators, we turn winning probability gradients into executable tactical adjustments. Experiments show that the outcome prediction model achieves state-of-the-art performance, with 69.8% accuracy on BoxerGraph test set and 87.5% on Olympic matches. Using this predictive model as a foundation, the system generates strategic recommendations that demonstrate proficiency comparable to human experts. BoxMind is validated through a closed-loop deployment during the 2024 Paris Olympics, directly contributing to the Chinese National Team's historic achievement of three gold and two silver medals. BoxMind establishes a replicable paradigm for transforming unstructured video data into strategic intelligence, bridging the gap between computer vision and decision support in competitive sports.",
      "url": "https://arxiv.org/abs/2601.11492",
      "pdfUrl": "https://arxiv.org/pdf/2601.11492.pdf",
      "titleJa": "BoxMind: 2024年オリンピックで検証されたエリートボクシング向けのクローズドループAI戦略最適化"
    },
    {
      "id": "2601.11479",
      "arxivId": "2601.11479",
      "title": "Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning",
      "authors": [
        "Yohai Trabelsi",
        "Guojun Xiong",
        "Fentabil Getnet",
        "Stéphane Verguet",
        "Milind Tambe"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.",
      "url": "https://arxiv.org/abs/2601.11479",
      "pdfUrl": "https://arxiv.org/pdf/2601.11479.pdf",
      "titleJa": "エチオピアの医療施設の立地：法学修士号（LLM）を活用して専門知識をアルゴリズム計画に統合"
    },
    {
      "id": "2601.11468",
      "arxivId": "2601.11468",
      "title": "Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs",
      "authors": [
        "Alessandro Padella",
        "Massimiliano de Leoni",
        "Marlon Dumas"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.AI",
        "cs.IT"
      ],
      "abstract": "Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.",
      "url": "https://arxiv.org/abs/2601.11468",
      "pdfUrl": "https://arxiv.org/pdf/2601.11468.pdf",
      "titleJa": "小規模イベントログの予測プロセス監視におけるLLM機能の検討"
    },
    {
      "id": "2601.11464",
      "arxivId": "2601.11464",
      "title": "MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models",
      "authors": [
        "Xiaoran Fan",
        "Zhichao Sun",
        "Tao Ji",
        "Lixing Shen",
        "Tao Gui"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.",
      "url": "https://arxiv.org/abs/2601.11464",
      "pdfUrl": "https://arxiv.org/pdf/2601.11464.pdf",
      "titleJa": "MHA2MLA-VLM: 視覚言語モデル全体でDeepSeekの経済的なマルチヘッド潜在的注意を実現する"
    },
    {
      "id": "2601.11459",
      "arxivId": "2601.11459",
      "title": "Interactive Narrative Analytics: Bridging Computational Narrative Extraction and Human Sensemaking",
      "authors": [
        "Brian Keith"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.IR"
      ],
      "abstract": "Information overload and misinformation create significant challenges in extracting meaningful narratives from large news collections. This paper defines the nascent field of Interactive Narrative Analytics (INA), which combines computational narrative extraction with interactive visual analytics to support sensemaking. INA approaches enable the interactive exploration of narrative structures through computational methods and visual interfaces that facilitate human interpretation. The field faces challenges in scalability, interactivity, knowledge integration, and evaluation standardization, yet offers promising opportunities across news analysis, intelligence, scientific literature exploration, and social media analysis. Through the combination of computational and human insight, INA addresses complex challenges in narrative sensemaking.",
      "url": "https://arxiv.org/abs/2601.11459",
      "pdfUrl": "https://arxiv.org/pdf/2601.11459.pdf",
      "titleJa": "インタラクティブな物語分析：計算による物語抽出と人間の意味理解の橋渡し"
    },
    {
      "id": "2601.11451",
      "arxivId": "2601.11451",
      "title": "PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs",
      "authors": [
        "Oishee Bintey Hoque",
        "Nibir Chandra Mandal",
        "Kyle Luong",
        "Amanda Wilson",
        "Samarth Swarup",
        "Madhav Marathe",
        "Abhijin Adiga"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Large-scale livestock operations pose significant risks to human health and the environment, while also being vulnerable to threats such as infectious diseases and extreme weather events. As the number of such operations continues to grow, accurate and scalable mapping has become increasingly important. In this work, we present an infrastructure-first, explainable pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) from aerial and satellite imagery. Our method (1) detects candidate infrastructure (e.g., barns, feedlots, manure lagoons, silos) with a domain-tuned YOLOv8 detector, then derives SAM2 masks from these boxes and filters component-specific criteria, (2) extracts structured descriptors (e.g., counts, areas, orientations, and spatial relations) and fuses them with deep visual features using a lightweight spatial cross-attention classifier, and (3) outputs both CAFO type predictions and mask-level attributions that link decisions to visible infrastructure. Through comprehensive evaluation, we show that our approach achieves state-of-the-art performance, with Swin-B+PRISM-CAFO surpassing the best performing baseline by up to 15\\%. Beyond strong predictive performance across diverse U.S. regions, we run systematic gradient--activation analyses that quantify the impact of domain priors and show ho",
      "url": "https://arxiv.org/abs/2601.11451",
      "pdfUrl": "https://arxiv.org/pdf/2601.11451.pdf",
      "titleJa": "PRISM-CAFO: CAFOのための事前条件付きリモートセンシングインフラストラクチャセグメンテーションとマッピング"
    },
    {
      "id": "2601.11442",
      "arxivId": "2601.11442",
      "title": "Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps",
      "authors": [
        "Xiangjun Gao",
        "Zhensong Zhang",
        "Dave Zhenyu Chen",
        "Songcen Xu",
        "Long Quan",
        "Eduardo Pérez-Pellitero",
        "Youngkyoon Jang"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.",
      "url": "https://arxiv.org/abs/2601.11442",
      "pdfUrl": "https://arxiv.org/pdf/2601.11442.pdf",
      "titleJa": "Map2Thought: メトリック認知マップによる明示的な3D空間推論"
    },
    {
      "id": "2601.11441",
      "arxivId": "2601.11441",
      "title": "Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models",
      "authors": [
        "Xiaojie Gu",
        "Guangxu Chen",
        "Yuheng Yang",
        "Jingxin Han",
        "Andi Zhang"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Large language models (LLMs) exhibit exceptional performance across various domains, yet they face critical safety concerns. Model editing has emerged as an effective approach to mitigate these issues. Existing model editing methods often focus on optimizing an information matrix that blends new and old knowledge. While effective, these approaches can be computationally expensive and may cause conflicts. In contrast, we shift our attention to Hierarchical Orthogonal Residual SprEad of the information matrix, which reduces noisy gradients and enables more stable edits from a different perspective. We demonstrate the effectiveness of our method HORSE through a clear theoretical comparison with several popular methods and extensive experiments conducted on two datasets across multiple LLMs. The results show that HORSE maintains precise massive editing across diverse scenarios. The code is available at https://github.com/XiaojieGu/HORSE",
      "url": "https://arxiv.org/abs/2601.11441",
      "pdfUrl": "https://arxiv.org/pdf/2601.11441.pdf",
      "titleJa": "大規模言語モデルにおける高精度大規模編集のための階層的直交残差拡散"
    },
    {
      "id": "2601.11440",
      "arxivId": "2601.11440",
      "title": "GenDA: Generative Data Assimilation on Complex Urban Areas via Classifier-Free Diffusion Guidance",
      "authors": [
        "Francisco Giral",
        "Álvaro Manzano",
        "Ignacio Gómez",
        "Ricardo Vinuesa",
        "Soledad Le Clainche"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "abstract": "Urban wind flow reconstruction is essential for assessing air quality, heat dispersion, and pedestrian comfort, yet remains challenging when only sparse sensor data are available. We propose GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture trained on computational fluid dynamics (CFD) simulations and interprets classifier-free guidance as a learned posterior reconstruction mechanism: the unconditional branch learns a geometry-aware flow prior, while the sensor-conditioned branch injects observational constraints during sampling. This formulation enables obstacle-aware reconstruction and generalization across unseen geometries, wind directions, and mesh resolutions without retraining. We consider both sparse fixed sensors and trajectory-based observations using the same reconstruction procedure. When evaluated against supervised graph neural network (GNN) baselines and classical reduced-order data assimilation methods, GenDA reduces the relative root-mean-square error (RRMSE) by 25-57% and increases the structural similarity index (SSIM) by 23-33% across the tested meshes. Experiments are conducted on Reynolds-averaged Navier-Stokes (RANS) simulations of a real urban neighbourhood in Bristol, United Kingdom, at a characteristic Reynolds number of $\\mathrm{Re}\\approx2\\times10^{7}$, featuring complex building geometry and irregular terrain. The proposed framework provides a scalable path toward generative, geometry-aware data assimilation for environmental monitoring in complex domains.",
      "url": "https://arxiv.org/abs/2601.11440",
      "pdfUrl": "https://arxiv.org/pdf/2601.11440.pdf",
      "titleJa": "GenDA: 分類器フリー拡散ガイダンスによる複雑な都市部における生成的データ同化"
    },
    {
      "id": "2601.11429",
      "arxivId": "2601.11429",
      "title": "Relational Linearity is a Predictor of Hallucinations",
      "authors": [
        "Yuetian Lu",
        "Yihong Liu",
        "Hinrich Schütze"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Hallucination is a central failure mode in large language models (LLMs). We focus on hallucinations of answers to questions like: \"Which instrument did Glenn Gould play?\", but we ask these questions for synthetic entities that are unknown to the model. Surprisingly, we find that medium-size models like Gemma-7B-IT frequently hallucinate, i.e., they have difficulty recognizing that the hallucinated fact is not part of their knowledge. We hypothesize that an important factor in causing these hallucinations is the linearity of the relation: linear relations tend to be stored more abstractly, making it difficult for the LLM to assess its knowledge; the facts of nonlinear relations tend to be stored more directly, making knowledge assessment easier. To investigate this hypothesis, we create SyntHal, a dataset of 6000 synthetic entities for six relations. In our experiments with four models, we determine, for each relation, the hallucination rate on SyntHal and also measure its linearity, using $Δ\\cos$. We find a strong correlation ($r \\in [.78,.82]$) between relational linearity and hallucination rate, providing evidence for our hypothesis that the underlying storage of triples of a relation is a factor in how well a model can self-assess its knowledge. This finding has implications for how to manage hallucination behavior and suggests new research directions for improving the representation of factual knowledge in LLMs.",
      "url": "https://arxiv.org/abs/2601.11429",
      "pdfUrl": "https://arxiv.org/pdf/2601.11429.pdf",
      "titleJa": "関係の直線性は幻覚の予測因子である"
    },
    {
      "id": "2601.11421",
      "arxivId": "2601.11421",
      "title": "The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents",
      "authors": [
        "Ziyu Wang",
        "Chenyuan Liu",
        "Yushun Xiang",
        "Runhao Zhang",
        "Qingbo Hao",
        "Hongliang Lu",
        "Houyu Chen",
        "Zhizhong Feng",
        "Kaiyue Zheng",
        "Dehao Ye",
        "Xianchao Zeng",
        "Xinyu Zhou",
        "Boran Wen",
        "Jiaxin Li",
        "Mingyu Zhang",
        "Kecheng Zheng",
        "Qian Zhu",
        "Ran Cheng",
        "Yong-Lu Li"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "abstract": "Recently, with the rapid development of robot learning and imitation learning, numerous datasets and methods have emerged. However, these datasets and their task designs often lack systematic consideration and principles. This raises important questions: Do the current datasets and task designs truly advance the capabilities of robotic agents? Do evaluations on a few common tasks accurately reflect the differentiated performance of various methods proposed by different teams and evaluated on different tasks? To address these issues, we introduce the Great March 100 (\\textbf{GM-100}) as the first step towards a robot learning Olympics. GM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human-object interaction primitives and object affordances. We collect a large amount of trajectory data on different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of current VLA models. Our data and code are available at https://rhos.ai/research/gm-100.",
      "url": "https://arxiv.org/abs/2601.11421",
      "pdfUrl": "https://arxiv.org/pdf/2601.11421.pdf",
      "titleJa": "大行進100：具現化AIエージェントを評価するための100の詳細指向タスク"
    },
    {
      "id": "2601.11409",
      "arxivId": "2601.11409",
      "title": "Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints",
      "authors": [
        "Wenxiao Li",
        "Xue-Cheng Tai",
        "Jun Liu"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Existing research highlights the crucial role of topological priors in image segmentation, particularly in preserving essential structures such as connectivity and genus. Accurately capturing these topological features often requires incorporating width-related information, including the thickness and length inherent to the image structures. However, traditional mathematical definitions of topological structures lack this dimensional width information, limiting methods like persistent homology from fully addressing practical segmentation needs. To overcome this limitation, we propose a novel mathematical framework that explicitly integrates width information into the characterization of topological structures. This method leverages persistent homology, complemented by smoothing concepts from partial differential equations (PDEs), to modify local extrema of upper-level sets. This approach enables the resulting topological structures to inherently capture width properties. We incorporate this enhanced topological description into variational image segmentation models. Using some proper loss functions, we are also able to design neural networks that can segment images with the required topological and width properties. Through variational constraints on the relevant topological energies, our approach successfully preserves essential topological invariants such as connectivity and genus counts, simultaneously ensuring that segmented structures retain critical width attributes, including line thickness and length. Numerical experiments demonstrate the effectiveness of our method, showcasing its capability to maintain topological fidelity while explicitly embedding width characteristics into segmented image structures.",
      "url": "https://arxiv.org/abs/2601.11409",
      "pdfUrl": "https://arxiv.org/pdf/2601.11409.pdf",
      "titleJa": "トポロジー保証画像セグメンテーション：接続性、種数、幅の制約の強制"
    },
    {
      "id": "2601.11400",
      "arxivId": "2601.11400",
      "title": "Wetland mapping from sparse annotations with satellite image time series and temporal-aware segment anything model",
      "authors": [
        "Shuai Yuan",
        "Tianwu Lin",
        "Shuang Chen",
        "Yu Xia",
        "Peng Qin",
        "Xiangyu Liu",
        "Xiaoqing Xu",
        "Nan Xu",
        "Hongsheng Zhang",
        "Jie Wang",
        "Peng Gong"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Accurate wetland mapping is essential for ecosystem monitoring, yet dense pixel-level annotation is prohibitively expensive and practical applications usually rely on sparse point labels, under which existing deep learning models perform poorly, while strong seasonal and inter-annual wetland dynamics further render single-date imagery inadequate and lead to significant mapping errors; although foundation models such as SAM show promising generalization from point prompts, they are inherently designed for static images and fail to model temporal information, resulting in fragmented masks in heterogeneous wetlands. To overcome these limitations, we propose WetSAM, a SAM-based framework that integrates satellite image time series for wetland mapping from sparse point supervision through a dual-branch design, where a temporally prompted branch extends SAM with hierarchical adapters and dynamic temporal aggregation to disentangle wetland characteristics from phenological variability, and a spatial branch employs a temporally constrained region-growing strategy to generate reliable dense pseudo-labels, while a bidirectional consistency regularization jointly optimizes both branches. Extensive experiments across eight global regions of approximately 5,000 km2 each demonstrate that WetSAM substantially outperforms state-of-the-art methods, achieving an average F1-score of 85.58%, and delivering accurate and structurally consistent wetland segmentation with minimal labeling effort, highlighting its strong generalization capability and potential for scalable, low-cost, high-resolution wetland mapping.",
      "url": "https://arxiv.org/abs/2601.11400",
      "pdfUrl": "https://arxiv.org/pdf/2601.11400.pdf",
      "titleJa": "衛星画像の時系列と時間を考慮したセグメントモデルを用いたスパース注釈からの湿地マッピング"
    },
    {
      "id": "2601.11389",
      "arxivId": "2601.11389",
      "title": "Hyperparameter Optimization of Constraint Programming Solvers",
      "authors": [
        "Hedieh Haddad",
        "Thibault Falque",
        "Pierre Talbot",
        "Pascal Bouvry"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.AI"
      ],
      "abstract": "The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time. We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations. Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.",
      "url": "https://arxiv.org/abs/2601.11389",
      "pdfUrl": "https://arxiv.org/pdf/2601.11389.pdf",
      "titleJa": "制約プログラミングソルバーのハイパーパラメータ最適化"
    },
    {
      "id": "2601.11379",
      "arxivId": "2601.11379",
      "title": "Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups, and Alignment with Human Preferences",
      "authors": [
        "Morgane Hoffmann",
        "Emma Jouffroy",
        "Warren Jouanneau",
        "Marc Palyart",
        "Charles Pebereau"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.SI"
      ],
      "abstract": "General-purpose Large Language Models (LLMs) show significant potential in recruitment applications, where decisions require reasoning over unstructured text, balancing multiple criteria, and inferring fit and competence from indirect productivity signals. Yet, it is still uncertain how LLMs assign importance to each attribute and whether such assignments are in line with economic principles, recruiter preferences or broader societal norms. We propose a framework to evaluate an LLM's decision logic in recruitment, by drawing on established economic methodologies for analyzing human hiring behavior. We build synthetic datasets from real freelancer profiles and project descriptions from a major European online freelance marketplace and apply a full factorial design to estimate how a LLM weighs different match-relevant criteria when evaluating freelancer-project fit. We identify which attributes the LLM prioritizes and analyze how these weights vary across project contexts and demographic subgroups. Finally, we explain how a comparable experimental setup could be implemented with human recruiters to assess alignment between model and human decisions. Our findings reveal that the LLM weighs core productivity signals, such as skills and experience, but interprets certain features beyond their explicit matching value. While showing minimal average discrimination against minority groups, intersectional effects reveal that productivity signals carry different weights between demographic groups.",
      "url": "https://arxiv.org/abs/2601.11379",
      "pdfUrl": "https://arxiv.org/pdf/2601.11379.pdf",
      "titleJa": "採用における法学修士（LLM）の行動評価：暗黙の重み付け、グループ間の公平性、そして人間の好みとの整合性"
    },
    {
      "id": "2601.11369",
      "arxivId": "2601.11369",
      "title": "Institutional AI: Governing LLM Collusion in Multi-Agent Cournot Markets via Public Governance Graphs",
      "authors": [
        "Marcantonio Bracale Syrnikov",
        "Federico Pierucci",
        "Marcello Galisai",
        "Matteo Prandi",
        "Piercosma Bisconti",
        "Francesco Giarrusso",
        "Olga Sorokoletova",
        "Vincenzo Suriani",
        "Daniele Nardi"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "abstract": "Multi-agent LLM ensembles can converge on coordinated, socially harmful equilibria. This paper advances an experimental framework for evaluating Institutional AI, our system-level approach to AI alignment that reframes alignment from preference engineering in agent-space to mechanism design in institution-space. Central to this approach is the governance graph, a public, immutable manifest that declares legal states, transitions, sanctions, and restorative paths; an Oracle/Controller runtime interprets this manifest, attaching enforceable consequences to evidence of coordination while recording a cryptographically keyed, append-only governance log for audit and provenance. We apply the Institutional AI framework to govern the Cournot collusion case documented by prior work and compare three regimes: Ungoverned (baseline incentives from the structure of the Cournot market), Constitutional (a prompt-only policy-as-prompt prohibition implemented as a fixed written anti-collusion constitution, and Institutional (governance-graph-based). Across six model configurations including cross-provider pairs (N=90 runs/condition), the Institutional regime produces large reductions in collusion: mean tier falls from 3.1 to 1.8 (Cohen's d=1.28), and severe-collusion incidence drops from 50% to 5.6%. The prompt-only Constitutional baseline yields no reliable improvement, illustrating that declarative prohibitions do not bind under optimisation pressure. These results suggest that multi-agent alignment may benefit from being framed as an institutional design problem, where governance graphs can provide a tractable abstraction for alignment-relevant collective behavior.",
      "url": "https://arxiv.org/abs/2601.11369",
      "pdfUrl": "https://arxiv.org/pdf/2601.11369.pdf",
      "titleJa": "制度的AI：パブリックガバナンスグラフを用いたマルチエージェントクールノー市場におけるLLM共謀の統制"
    },
    {
      "id": "2601.08871",
      "arxivId": "2601.08871",
      "title": "Semantic visually-guided acoustic highlighting with large vision-language models",
      "authors": [
        "Junhua Huang",
        "Chao Huang",
        "Chenliang Xu"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Balancing dialogue, music, and sound effects with accompanying video is crucial for immersive storytelling, yet current audio mixing workflows remain largely manual and labor-intensive. While recent advancements have introduced the visually guided acoustic highlighting task, which implicitly rebalances audio sources using multimodal guidance, it remains unclear which visual aspects are most effective as conditioning signals.We address this gap through a systematic study of whether deep video understanding improves audio remixing. Using textual descriptions as a proxy for visual analysis, we prompt large vision-language models to extract six types of visual-semantic aspects, including object and character appearance, emotion, camera focus, tone, scene background, and inferred sound-related cues. Through extensive experiments, camera focus, tone, and scene background consistently yield the largest improvements in perceptual mix quality over state-of-the-art baselines. Our findings (i) identify which visual-semantic cues most strongly support coherent and visually aligned audio remixing, and (ii) outline a practical path toward automating cinema-grade sound design using lightweight guidance derived from large vision-language models.",
      "url": "https://arxiv.org/abs/2601.08871",
      "pdfUrl": "https://arxiv.org/pdf/2601.08871.pdf",
      "titleJa": "大規模視覚言語モデルを用いた意味的視覚誘導音響強調表示"
    },
    {
      "id": "2601.06406",
      "arxivId": "2601.06406",
      "title": "Representing Sounds as Neural Amplitude Fields: A Benchmark of Coordinate-MLPs and A Fourier Kolmogorov-Arnold Framework",
      "authors": [
        "Linfei Li",
        "Lin Zhang",
        "Zhong Wang",
        "Fengyi Zhang",
        "Zelin Li",
        "Ying Shen"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Although Coordinate-MLP-based implicit neural representations have excelled in representing radiance fields, 3D shapes, and images, their application to audio signals remains underexplored. To fill this gap, we investigate existing implicit neural representations, from which we extract 3 types of positional encoding and 16 commonly used activation functions. Through combinatorial design, we establish the first benchmark for Coordinate-MLPs in audio signal representations. Our benchmark reveals that Coordinate-MLPs require complex hyperparameter tuning and frequency-dependent initialization, limiting their robustness. To address these issues, we propose Fourier-ASR, a novel framework based on the Fourier series theorem and the Kolmogorov-Arnold representation theorem. Fourier-ASR introduces Fourier Kolmogorov-Arnold Networks (Fourier-KAN), which leverage periodicity and strong nonlinearity to represent audio signals, eliminating the need for additional positional encoding. Furthermore, a Frequency-adaptive Learning Strategy (FaLS) is proposed to enhance the convergence of Fourier-KAN by capturing high-frequency components and preventing overfitting of low-frequency signals. Extensive experiments conducted on natural speech and music datasets reveal that: (1) well-designed positional encoding and activation functions in Coordinate-MLPs can effectively improve audio representation quality; and (2) Fourier-ASR can robustly represent complex audio signals without extensive hyperparameter tuning. Looking ahead, the continuity and infinite resolution of implicit audio representations make our research highly promising for tasks such as audio compression, synthesis, and generation. The source code will be released publicly to ensure reproducibility. The code is available at https://github.com/lif314/Fourier-ASR.",
      "url": "https://arxiv.org/abs/2601.06406",
      "pdfUrl": "https://arxiv.org/pdf/2601.06406.pdf",
      "titleJa": "音を神経振幅場として表現する：座標MLPのベンチマークとフーリエ・コルモゴロフ・アーノルド枠組み"
    },
    {
      "id": "2601.04592",
      "arxivId": "2601.04592",
      "title": "Density Matrix RNN (DM-RNN): A Quantum Information Theoretic Framework for Modeling Musical Context and Polyphony",
      "authors": [
        "Joonwon Seo",
        "Mariana Montiel"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.LG",
        "cs.SD",
        "math-ph"
      ],
      "abstract": "Classical Recurrent Neural Networks (RNNs) summarize musical context into a deterministic hidden state vector, imposing an information bottleneck that fails to capture the inherent ambiguity in music. We propose the Density Matrix RNN (DM-RNN), a novel theoretical architecture utilizing the Density Matrix. This allows the model to maintain a statistical ensemble of musical interpretations (a mixed state), capturing both classical probabilities and quantum coherences. We rigorously define the temporal dynamics using Quantum Channels (CPTP maps). Crucially, we detail a parameterization strategy based on the Choi-Jamiolkowski isomorphism, ensuring the learned dynamics remain physically valid (CPTP) by construction. We introduce an analytical framework using Von Neumann Entropy to quantify musical uncertainty and Quantum Mutual Information (QMI) to measure entanglement between voices. The DM-RNN provides a mathematically rigorous framework for modeling complex, ambiguous musical structures.",
      "url": "https://arxiv.org/abs/2601.04592",
      "pdfUrl": "https://arxiv.org/pdf/2601.04592.pdf",
      "titleJa": "密度行列RNN（DM-RNN）：音楽的文脈とポリフォニーをモデル化する量子情報理論的枠組み"
    },
    {
      "id": "2601.04343",
      "arxivId": "2601.04343",
      "title": "Summary of The Inaugural Music Source Restoration Challenge",
      "authors": [
        "Yongyi Zang",
        "Jiarui Hai",
        "Wanying Ge",
        "Qiuqiang Kong",
        "Zheqi Dai",
        "Helin Wang",
        "Yuki Mitsufuji",
        "Mark D. Plumbley"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Music Source Restoration (MSR) aims to recover original, unprocessed instrument stems from professionally mixed and degraded audio, requiring the reversal of both production effects and real-world degradations. We present the inaugural MSR Challenge, which features objective evaluation on studio-produced mixtures using Multi-Mel-SNR, Zimtohrli, and FAD-CLAP, alongside subjective evaluation on real-world degraded recordings. Five teams participated in the challenge. The winning system achieved 4.46 dB Multi-Mel-SNR and 3.47 MOS-Overall, corresponding to relative improvements of 91% and 18% over the second-place system, respectively. Per-stem analysis reveals substantial variation in restoration difficulty across instruments, with bass averaging 4.59 dB across all teams, while percussion averages only 0.29 dB. The dataset, evaluation protocols, and baselines are available at https://msrchallenge.com/.",
      "url": "https://arxiv.org/abs/2601.04343",
      "pdfUrl": "https://arxiv.org/pdf/2601.04343.pdf",
      "titleJa": "第1回音楽ソース修復チャレンジの概要"
    },
    {
      "id": "2601.03973",
      "arxivId": "2601.03973",
      "title": "Muse: Towards Reproducible Long-Form Song Generation with Fine-Grained Style Control",
      "authors": [
        "Changhao Jiang",
        "Jiahao Chen",
        "Zhenghao Xiang",
        "Zhixiong Yang",
        "Hanchen Wang",
        "Jiabao Zhuang",
        "Xinmeng Che",
        "Jiajun Sun",
        "Hui Li",
        "Yifei Cao",
        "Shihan Dou",
        "Ming Zhang",
        "Junjie Ye",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Recent commercial systems such as Suno demonstrate strong capabilities in long-form song generation, while academic research remains largely non-reproducible due to the lack of publicly available training data, hindering fair comparison and progress. To this end, we release a fully open-source system for long-form song generation with fine-grained style conditioning, including a licensed synthetic dataset, training and evaluation pipelines, and Muse, an easy-to-deploy song generation model. The dataset consists of 116k fully licensed synthetic songs with automatically generated lyrics and style descriptions paired with audio synthesized by SunoV5. We train Muse via single-stage supervised finetuning of a Qwen-based language model extended with discrete audio tokens using MuCodec, without task-specific losses, auxiliary objectives, or additional architectural components. Our evaluations find that although Muse is trained with a modest data scale and model size, it achieves competitive performance on phoneme error rate, text--music style similarity, and audio aesthetic quality, while enabling controllable segment-level generation across different musical structures. All data, model weights, and training and evaluation pipelines will be publicly released, paving the way for continued progress in controllable long-form song generation research. The project repository is available at https://github.com/yuhui1038/Muse.",
      "url": "https://arxiv.org/abs/2601.03973",
      "pdfUrl": "https://arxiv.org/pdf/2601.03973.pdf",
      "titleJa": "Muse: きめ細かなスタイル制御による再現性の高い長編楽曲生成に向けて"
    },
    {
      "id": "2601.03626",
      "arxivId": "2601.03626",
      "title": "Learning from Limited Labels: Transductive Graph Label Propagation for Indian Music Analysis",
      "authors": [
        "Parampreet Singh",
        "Akshay Raina",
        "Sayeedul Islam Sheikh",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Supervised machine learning frameworks rely on extensive labeled datasets for robust performance on real-world tasks. However, there is a lack of large annotated datasets in audio and music domains, as annotating such recordings is resource-intensive, laborious, and often require expert domain knowledge. In this work, we explore the use of label propagation (LP), a graph-based semi-supervised learning technique, for automatically labeling the unlabeled set in an unsupervised manner. By constructing a similarity graph over audio embeddings, we propagate limited label information from a small annotated subset to a larger unlabeled corpus in a transductive, semi-supervised setting. We apply this method to two tasks in Indian Art Music (IAM): Raga identification and Instrument classification. For both these tasks, we integrate multiple public datasets along with additional recordings we acquire from Prasar Bharati Archives to perform LP. Our experiments demonstrate that LP significantly reduces labeling overhead and produces higher-quality annotations compared to conventional baseline methods, including those based on pretrained inductive models. These results highlight the potential of graph-based semi-supervised learning to democratize data annotation and accelerate progress in music information retrieval.",
      "url": "https://arxiv.org/abs/2601.03626",
      "pdfUrl": "https://arxiv.org/pdf/2601.03626.pdf",
      "titleJa": "限定ラベルからの学習：インド音楽分析のためのトランスダクティブグラフラベル伝播"
    },
    {
      "id": "2601.03612",
      "arxivId": "2601.03612",
      "title": "Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias",
      "authors": [
        "Joonwon Seo"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This monograph introduces a novel approach to polyphonic music generation by addressing the \"Missing Middle\" problem through structural inductive bias. Focusing on Beethoven's piano sonatas as a case study, we empirically verify the independence of pitch and hand attributes using normalized mutual information (NMI=0.167) and propose the Smart Embedding architecture, achieving a 48.30% reduction in parameters. We provide rigorous mathematical proofs using information theory (negligible loss bounded at 0.153 bits), Rademacher complexity (28.09% tighter generalization bound), and category theory to demonstrate improved stability and generalization. Empirical results show a 9.47% reduction in validation loss, confirmed by SVD analysis and an expert listening study (N=53). This dual theoretical and applied framework bridges gaps in AI music generation, offering verifiable insights for mathematically grounded deep learning.",
      "url": "https://arxiv.org/abs/2601.03612",
      "pdfUrl": "https://arxiv.org/pdf/2601.03612.pdf",
      "titleJa": "構造的帰納的バイアスによるポリフォニック音楽生成の数学的基礎"
    },
    {
      "id": "2601.03443",
      "arxivId": "2601.03443",
      "title": "Discriminating real and synthetic super-resolved audio samples using embedding-based classifiers",
      "authors": [
        "Mikhail Silaev",
        "Konstantinos Drossos",
        "Tuomas Virtanen"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "Generative adversarial networks (GANs) and diffusion models have recently achieved state-of-the-art performance in audio super-resolution (ADSR), producing perceptually convincing wideband audio from narrowband inputs. However, existing evaluations primarily rely on signal-level or perceptual metrics, leaving open the question of how closely the distributions of synthetic super-resolved and real wideband audio match. Here we address this problem by analyzing the separability of real and super-resolved audio in various embedding spaces. We consider both middle-band ($4\\to 16$~kHz) and full-band ($16\\to 48$~kHz) upsampling tasks for speech and music, training linear classifiers to distinguish real from synthetic samples based on multiple types of audio embeddings. Comparisons with objective metrics and subjective listening tests reveal that embedding-based classifiers achieve near-perfect separation, even when the generated audio attains high perceptual quality and state-of-the-art metric scores. This behavior is consistent across datasets and models, including recent diffusion-based approaches, highlighting a persistent gap between perceptual quality and true distributional fidelity in ADSR models.",
      "url": "https://arxiv.org/abs/2601.03443",
      "pdfUrl": "https://arxiv.org/pdf/2601.03443.pdf",
      "titleJa": "埋め込みベースの分類器を用いた実音と合成音の超解像音声サンプルの識別"
    },
    {
      "id": "2601.02983",
      "arxivId": "2601.02983",
      "title": "Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning",
      "authors": [
        "Yuankun Xie",
        "Xiaoxuan Guo",
        "Jiayi Zhou",
        "Tao Wang",
        "Jian Liu",
        "Ruibo Fu",
        "Xiaopeng Wang",
        "Haonan Cheng",
        "Long Ye"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in audio large language models (ALLMs) have made high-quality synthetic audio widely accessible, increasing the risk of malicious audio deepfakes across speech, environmental sounds, singing voice, and music. Real-world audio deepfake detection (ADD) therefore requires all-type detectors that generalize across heterogeneous audio and provide interpretable decisions. Given the strong multi-task generalization ability of ALLMs, we first investigate their performance on all-type ADD under both supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). However, SFT using only binary real/fake labels tends to reduce the model to a black-box classifier, sacrificing interpretability. Meanwhile, vanilla RFT under sparse supervision is prone to reward hacking and can produce hallucinated, ungrounded rationales. To address this, we propose an automatic annotation and polishing pipeline that constructs Frequency-Time structured chain-of-thought (CoT) rationales, producing ~340K cold-start demonstrations. Building on CoT data, we propose Frequency Time-Group Relative Policy Optimization (FT-GRPO), a two-stage training paradigm that cold-starts ALLMs with SFT and then applies GRPO under rule-based frequency-time constraints. Experiments demonstrate that FT-GRPO achieves state-of-the-art performance on all-type ADD while producing interpretable, FT-grounded rationales. The data and code are available online.",
      "url": "https://arxiv.org/abs/2601.02983",
      "pdfUrl": "https://arxiv.org/pdf/2601.02983.pdf",
      "titleJa": "周波数時間強化学習によるオーディオLLMを用いた解釈可能な全タイプオーディオディープフェイク検出"
    },
    {
      "id": "2601.02967",
      "arxivId": "2601.02967",
      "title": "MoE Adapter for Large Audio Language Models: Sparsity, Disentanglement, and Gradient-Conflict-Free",
      "authors": [
        "Yishu Lei",
        "Shuwei He",
        "Jing Hu",
        "Dan Zhang",
        "Xianlong Luo",
        "Danxiang Zhu",
        "Shikun Feng",
        "Rui Liu",
        "Jingzhou He",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Extending the input modality of Large Language Models~(LLMs) to the audio domain is essential for achieving comprehensive multimodal perception. However, it is well-known that acoustic information is intrinsically \\textit{heterogeneous}, entangling attributes such as speech, music, and environmental context. Existing research is limited to a dense, parameter-shared adapter to model these diverse patterns, which induces \\textit{gradient conflict} during optimization, as parameter updates required for distinct attributes contradict each other. To address this limitation, we introduce the \\textit{\\textbf{MoE-Adapter}}, a sparse Mixture-of-Experts~(MoE) architecture designed to decouple acoustic information. Specifically, it employs a dynamic gating mechanism that routes audio tokens to specialized experts capturing complementary feature subspaces while retaining shared experts for global context, thereby mitigating gradient conflicts and enabling fine-grained feature learning. Comprehensive experiments show that the MoE-Adapter achieves superior performance on both audio semantic and paralinguistic tasks, consistently outperforming dense linear baselines with comparable computational costs. Furthermore, we will release the related code and models to facilitate future research.",
      "url": "https://arxiv.org/abs/2601.02967",
      "pdfUrl": "https://arxiv.org/pdf/2601.02967.pdf",
      "titleJa": "大規模音声言語モデルのためのMoEアダプタ：スパース性、分離、勾配衝突フリー"
    },
    {
      "id": "2601.06621",
      "arxivId": "2601.06621",
      "title": "Stereo Audio Rendering for Personal Sound Zones Using a Binaural Spatially Adaptive Neural Network (BSANN)",
      "authors": [
        "Hao Jiang",
        "Edgar Choueiri"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "A binaural rendering framework for personal sound zones (PSZs) is proposed to enable multiple head-tracked listeners to receive fully independent stereo audio programs. Current PSZ systems typically rely on monophonic rendering and therefore cannot control the left and right ears separately, which limits the quality and accuracy of spatial imaging. The proposed method employs a Binaural Spatially Adaptive Neural Network (BSANN) to generate ear-optimized loudspeaker filters that reconstruct the desired acoustic field at each ear of multiple listeners. The framework integrates anechoically measured loudspeaker frequency responses, analytically modeled transducer directivity, and rigid-sphere head-related transfer functions (HRTFs) to enhance acoustic accuracy and spatial rendering fidelity. An explicit active crosstalk cancellation (XTC) stage further improves three-dimensional spatial perception. Experiments show significant gains in measured objective performance metrics, including inter-zone isolation (IZI), inter-program isolation (IPI), and crosstalk cancellation (XTC), with log-frequency-weighted values of 10.23/10.03 dB (IZI), 11.11/9.16 dB (IPI), and 10.55/11.13 dB (XTC), respectively, over 100-20,000 Hz. The combined use of ear-wise control, accurate acoustic modeling, and integrated active XTC produces a unified rendering method that delivers greater isolation performance, increased robustness to room asymmetry, and more faithful spatial reproduction in real acoustic environments.",
      "url": "https://arxiv.org/abs/2601.06621",
      "pdfUrl": "https://arxiv.org/pdf/2601.06621.pdf",
      "titleJa": "バイノーラル空間適応型ニューラルネットワーク（BSANN）を用いたパーソナルサウンドゾーン向けステレオオーディオレンダリング"
    },
    {
      "id": "2601.06006",
      "arxivId": "2601.06006",
      "title": "Discriminative-Generative Target Speaker Extraction with Decoder-Only Language Models",
      "authors": [
        "Bang Zeng",
        "Beilong Tang",
        "Wang Xiang",
        "Ming Li"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Target speaker extraction (TSE) aims to recover the speech signal of a desired speaker from a mixed audio recording, given a short enrollment utterance. Most existing TSE approaches are based on discriminative modeling paradigms. Although effective at suppressing interfering speakers, these methods often struggle to produce speech with high perceptual quality and naturalness. To address this limitation, we first propose LauraTSE, a generative TSE model built upon an auto-regressive decoder-only language model. However, purely generative approaches may suffer from hallucinations, content drift, and limited controllability, which may undermine their reliability in complex acoustic scenarios. To overcome these challenges, we further introduce a discriminative-generative TSE framework. In this framework, a discriminative front-end is employed to robustly extract the target speaker's speech, yielding stable and controllable intermediate representations. A generative back-end then operates in the neural audio codec representation space to reconstruct fine-grained speech details and enhance perceptual quality. This two-stage design effectively combines the robustness and controllability of discriminative models with the superior naturalness and quality enhancement capabilities of generative models. Moreover, we systematically investigate collaborative training strategies for the proposed framework, including freezing or fine-tuning the front-end, incorporating an auxiliary SI-SDR loss, and exploring both auto-regressive and non-auto-regressive inference mechanisms. Experimental results demonstrate that the proposed framework achieves a more favorable trade-off among speech quality, intelligibility, and speaker consistency.",
      "url": "https://arxiv.org/abs/2601.06006",
      "pdfUrl": "https://arxiv.org/pdf/2601.06006.pdf",
      "titleJa": "デコーダのみの言語モデルを用いた識別的・生成的ターゲット話者抽出"
    },
    {
      "id": "2601.05554",
      "arxivId": "2601.05554",
      "title": "SPAM: Style Prompt Adherence Metric for Prompt-based TTS",
      "authors": [
        "Chanhee Cho",
        "Nayeon Kim",
        "Bugeun Kim"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Prompt-based text-to-speech (TTS) aims to generate speech that adheres to fine-grained style cues provided in a text prompt. However, most prior works depend on neither plausible nor faithful measures to evaluate prompt adherence. That is, they cannot ensure whether the evaluation is grounded on the prompt and is similar to a human. Thus, we present a new automatic metric, the Style Prompt Adherence Metric, which explicitly satisfies both plausibility and faithfulness. Inspired by the CLAP, our approach factorizes speech into acoustic attributes and aligns them with the style prompt. Also, we trained the scorer with a supervised contrastive loss, which could provide a clearer distinction between different semantics. We conducted two experiments on two perspectives. The plausibility experiment showed that SPAM achieved a strong correlation with the mean opinion score (MOS). Also, the faithfulness experiment demonstrated that SPAM is successfully grounded to the given style prompt, as it can discriminate different semantics of the prompt. We believe that SPAM can provide a viable automatic solution for evaluating style prompt adherence of synthesized speech.",
      "url": "https://arxiv.org/abs/2601.05554",
      "pdfUrl": "https://arxiv.org/pdf/2601.05554.pdf",
      "titleJa": "SPAM: プロンプトベースの TTS におけるスタイルプロンプト遵守指標"
    },
    {
      "id": "2601.04744",
      "arxivId": "2601.04744",
      "title": "Semi-Supervised Diseased Detection from Speech Dialogues with Multi-Level Data Modeling",
      "authors": [
        "Xingyuan Li",
        "Mengyue Wu"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Detecting medical conditions from speech acoustics is fundamentally a weakly-supervised learning problem: a single, often noisy, session-level label must be linked to nuanced patterns within a long, complex audio recording. This task is further hampered by severe data scarcity and the subjective nature of clinical annotations. While semi-supervised learning (SSL) offers a viable path to leverage unlabeled data, existing audio methods often fail to address the core challenge that pathological traits are not uniformly expressed in a patient's speech. We propose a novel, audio-only SSL framework that explicitly models this hierarchy by jointly learning from frame-level, segment-level, and session-level representations within unsegmented clinical dialogues. Our end-to-end approach dynamically aggregates these multi-granularity features and generates high-quality pseudo-labels to efficiently utilize unlabeled data. Extensive experiments show the framework is model-agnostic, robust across languages and conditions, and highly data-efficient-achieving, for instance, 90\\% of fully-supervised performance using only 11 labeled samples. This work provides a principled approach to learning from weak, far-end supervision in medical speech analysis.",
      "url": "https://arxiv.org/abs/2601.04744",
      "pdfUrl": "https://arxiv.org/pdf/2601.04744.pdf",
      "titleJa": "多層データモデリングを用いた音声対話からの半教師付き疾患検出"
    },
    {
      "id": "2601.04564",
      "arxivId": "2601.04564",
      "title": "When Tone and Words Disagree: Towards Robust Speech Emotion Recognition under Acoustic-Semantic Conflict",
      "authors": [
        "Dawei Huang",
        "Yongjie Lv",
        "Ruijie Xiong",
        "Chunxiang Jin",
        "Xiaojiang Peng"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Speech Emotion Recognition (SER) systems often assume congruence between vocal emotion and lexical semantics. However, in real-world interactions, acoustic-semantic conflict is common yet overlooked, where the emotion conveyed by tone contradicts the literal meaning of spoken words. We show that state-of-the-art SER models, including ASR-based, self-supervised learning (SSL) approaches and Audio Language Models (ALMs), suffer performance degradation under such conflicts due to semantic bias or entangled acoustic-semantic representations. To address this, we propose the Fusion Acoustic-Semantic (FAS) framework, which explicitly disentangles acoustic and semantic pathways and bridges them through a lightweight, query-based attention module. To enable systematic evaluation, we introduce the Conflict in Acoustic-Semantic Emotion (CASE), the first dataset dominated by clear and interpretable acoustic-semantic conflicts in varied scenarios. Extensive experiments demonstrate that FAS consistently outperforms existing methods in both in-domain and zero-shot settings. Notably, on the CASE benchmark, conventional SER models fail dramatically, while FAS sets a new SOTA with 59.38% accuracy. Our code and datasets is available at https://github.com/24DavidHuang/FAS.",
      "url": "https://arxiv.org/abs/2601.04564",
      "pdfUrl": "https://arxiv.org/pdf/2601.04564.pdf",
      "titleJa": "音調と言葉が一致しないとき：音響的・意味的矛盾下におけるロバストな音声感情認識に向けて"
    },
    {
      "id": "2601.03712",
      "arxivId": "2601.03712",
      "title": "TellWhisper: Tell Whisper Who Speaks When",
      "authors": [
        "Yifan Hu",
        "Peiji Yang",
        "Zhisheng Wang",
        "Yicheng Zhong",
        "Rui Liu"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Multi-speaker automatic speech recognition (MASR) aims to predict ''who spoke when and what'' from multi-speaker speech, a key technology for multi-party dialogue understanding. However, most existing approaches decouple temporal modeling and speaker modeling when addressing ''when'' and ''who'': some inject speaker cues before encoding (e.g., speaker masking), which can cause irreversible information loss; others fuse identity by mixing speaker posteriors after encoding, which may entangle acoustic content with speaker identity. This separation is brittle under rapid turn-taking and overlapping speech, often leading to degraded performance. To address these limitations, we propose TellWhisper, a unified framework that jointly models speaker identity and temporal within the speech encoder. Specifically, we design TS-RoPE, a time-speaker rotary positional encoding: time coordinates are derived from frame indices, while speaker coordinates are derived from speaker activity and pause cues. By applying region-specific rotation angles, the model explicitly captures per-speaker continuity, speaker-turn transitions, and state dynamics, enabling the attention mechanism to simultaneously attend to ''when'' and ''who''. Moreover, to estimate frame-level speaker activity, we develop Hyper-SD, which casts speaker classification in hyperbolic space to enhance inter-class separation and refine speaker-activity estimates. Extensive experiments demonstrate the effectiveness of the proposed approach.",
      "url": "https://arxiv.org/abs/2601.03712",
      "pdfUrl": "https://arxiv.org/pdf/2601.03712.pdf",
      "titleJa": "TellWhisper: 誰がいつ話すかを知らせる"
    },
    {
      "id": "2601.03615",
      "arxivId": "2601.03615",
      "title": "Analyzing Reasoning Shifts in Audio Deepfake Detection under Adversarial Attacks: The Reasoning Tax versus Shield Bifurcation",
      "authors": [
        "Binh Nguyen",
        "Thai Le"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Audio Language Models (ALMs) offer a promising shift towards explainable audio deepfake detections (ADDs), moving beyond \\textit{black-box} classifiers by providing some level of transparency into their predictions via reasoning traces. This necessitates a new class of model robustness analysis: robustness of the predictive reasoning under adversarial attacks, which goes beyond existing paradigm that mainly focuses on the shifts of the final predictions (e.g., fake v.s. real). To analyze such reasoning shifts, we introduce a forensic auditing framework to evaluate the robustness of ALMs' reasoning under adversarial attacks in three inter-connected dimensions: acoustic perception, cognitive coherence, and cognitive dissonance. Our systematic analysis reveals that explicit reasoning does not universally enhance robustness. Instead, we observe a bifurcation: for models exhibiting robust acoustic perception, reasoning acts as a defensive \\textit{``shield''}, protecting them from adversarial attacks. However, for others, it imposes a performance \\textit{``tax''}, particularly under linguistic attacks which reduce cognitive coherence and increase attack success rate. Crucially, even when classification fails, high cognitive dissonance can serve as a \\textit{silent alarm}, flagging potential manipulation. Overall, this work provides a critical evaluation of the role of reasoning in forensic audio deepfake analysis and its vulnerabilities.",
      "url": "https://arxiv.org/abs/2601.03615",
      "pdfUrl": "https://arxiv.org/pdf/2601.03615.pdf",
      "titleJa": "敵対的攻撃下における音声ディープフェイク検出の推論シフトの分析：推論税とシールド分岐"
    },
    {
      "id": "2601.03610",
      "arxivId": "2601.03610",
      "title": "Investigation into respiratory sound classification for an imbalanced data set using hybrid LSTM-KAN architectures",
      "authors": [
        "Nithinkumar K.",
        "Anand R"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Respiratory sounds captured via auscultation contain critical clues for diagnosing pulmonary conditions. Automated classification of these sounds faces challenges due to subtle acoustic differences and severe class imbalance in clinical datasets. This study investigates respiratory sound classification with a focus on mitigating pronounced class imbalance. We propose a hybrid deep learning model that combines a Long Short-Term Memory (LSTM) network for sequential feature encoding with a Kolmogorov-Arnold Network (KAN) for classification. The model is integrated with a comprehensive feature extraction pipeline and targeted imbalance mitigation strategies. Experiments were conducted on a public respiratory sound database comprising six classes with a highly skewed distribution. Techniques such as focal loss, class-specific data augmentation, and Synthetic Minority Over-sampling Technique (SMOTE) were employed to enhance minority class recognition. The proposed Hybrid LSTM-KAN model achieves an overall accuracy of 94.6 percent and a macro-averaged F1 score of 0.703, despite the dominant COPD class accounting for over 86 percent of the data. Improved detection performance is observed for minority classes compared to baseline approaches, demonstrating the effectiveness of the proposed architecture for imbalanced respiratory sound classification.",
      "url": "https://arxiv.org/abs/2601.03610",
      "pdfUrl": "https://arxiv.org/pdf/2601.03610.pdf",
      "titleJa": "ハイブリッドLSTM-KANアーキテクチャを用いた不均衡なデータセットの呼吸音分類の調査"
    }
  ],
  "lastUpdated": "2026-01-20T00:53:32.341658",
  "totalCount": 72
}