{
  "papers": [
    {
      "id": "2602.17599",
      "arxivId": "2602.17599",
      "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
      "authors": [
        "Ivan Rinaldi",
        "Matteo Mendula",
        "Nicola Fanelli",
        "Florence Levé",
        "Matteo Testi",
        "Giovanna Castellano",
        "Gennaro Vessio"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.",
      "url": "https://arxiv.org/abs/2602.17599",
      "pdfUrl": "https://arxiv.org/pdf/2602.17599.pdf",
      "titleJa": "Art2Mus: 視覚条件付けと大規模クロスモーダルアライメントによるアートワークから音楽への生成"
    },
    {
      "id": "2602.17394",
      "arxivId": "2602.17394",
      "title": "Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks",
      "authors": [
        "Nuno Saavedra",
        "Pedro Ribeiro",
        "André Coelho",
        "Rui Campos"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Unmanned Aerial Vehicle (UAV)-assisted networks are increasingly foreseen as a promising approach for emergency response, providing rapid, flexible, and resilient communications in environments where terrestrial infrastructure is degraded or unavailable. In such scenarios, voice radio communications remain essential for first responders due to their robustness; however, their unstructured nature prevents direct integration with automated UAV-assisted network management. This paper proposes SIREN, an AI-driven framework that enables voice-driven perception for UAV-assisted networks. By integrating Automatic Speech Recognition (ASR) with Large Language Model (LLM)-based semantic extraction and Natural Language Processing (NLP) validation, SIREN converts emergency voice traffic into structured, machine-readable information, including responding units, location references, emergency severity, and Quality-of-Service (QoS) requirements. SIREN is evaluated using synthetic emergency scenarios with controlled variations in language, speaker count, background noise, and message complexity. The results demonstrate robust transcription and reliable semantic extraction across diverse operating conditions, while highlighting speaker diarization and geographic ambiguity as the main limiting factors. These findings establish the feasibility of voice-driven situational awareness for UAV-assisted networks and show a practical foundation for human-in-the-loop decision support and adaptive network management in emergency response operations.",
      "url": "https://arxiv.org/abs/2602.17394",
      "pdfUrl": "https://arxiv.org/pdf/2602.17394.pdf",
      "titleJa": "UAV支援緊急ネットワークのための音声駆動型意味認識"
    },
    {
      "id": "2602.17097",
      "arxivId": "2602.17097",
      "title": "AudioChat: Unified Audio Storytelling, Editing, and Understanding with Transfusion Forcing",
      "authors": [
        "William Chen",
        "Prem Seetharaman",
        "Rithesh Kumar",
        "Oriol Nieto",
        "Shinji Watanabe",
        "Justin Salamon",
        "Zeyu Jin"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Despite recent breakthroughs, audio foundation models struggle in processing complex multi-source acoustic scenes. We refer to this challenging domain as audio stories, which can have multiple speakers and background/foreground sound effects. Compared to traditional audio processing tasks, audio stories introduce new layers of semantic, temporal, and physical complexity. To address this challenge, we propose AudioChat, a framework for developing audio foundation models that can generate, edit, and understand audio stories. AudioChat introduces a new paradigm in which LLM-based toolcalling agents simulate interactions between users and the system, and these simulated dialogues are used as training data. We also introduce a novel Audio Transfusion Forcing objective to train the AudioChat model, allowing it to simultaneously decompose high-level instructions via structured chain-of-thought reasoning and perform interactive multi-turn audio understanding/generation. To evaluate generation and editing performance, we develop three new metrics that directly measure task performance instead of relying upon distribution-based scoring. We highly encourage readers to visit our demo to better understand the capabilities of AudioChat: https://wanchichen.github.io/audiochat/.",
      "url": "https://arxiv.org/abs/2602.17097",
      "pdfUrl": "https://arxiv.org/pdf/2602.17097.pdf",
      "titleJa": "AudioChat: Transfusion Forcingによる統合オーディオストーリーテリング、編集、理解"
    },
    {
      "id": "2602.16790",
      "arxivId": "2602.16790",
      "title": "Generative Audio Extension and Morphing",
      "authors": [
        "Prem Seetharaman",
        "Oriol Nieto",
        "Justin Salamon"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.MM"
      ],
      "abstract": "In audio-related creative tasks, sound designers often seek to extend and morph different sounds from their libraries. Generative audio models, capable of creating audio using examples as references, offer promising solutions. By masking the noisy latents of a DiT and applying a novel variant of classifier-free guidance on such masked latents, we demonstrate that: (i) given an audio reference, we can extend it both forward and backward for a specified duration, and (ii) given two audio references, we can morph them seamlessly for the desired duration. Furthermore, we show that by fine-tuning the model on different types of stationary audio data we mitigate potential hallucinations. The effectiveness of our method is supported by objective metrics, with the generated audio achieving Fréchet Audio Distances (FADs) comparable to those of real samples from the training data. Additionally, we validate our results through a subjective listener test, where subjects gave positive ratings to the proposed model generations. This technique paves the way for more controllable and expressive generative sound frameworks, enabling sound designers to focus less on tedious, repetitive tasks and more on their actual creative process.",
      "url": "https://arxiv.org/abs/2602.16790",
      "pdfUrl": "https://arxiv.org/pdf/2602.16790.pdf",
      "titleJa": "生成オーディオの拡張とモーフィング"
    },
    {
      "id": "2602.16687",
      "arxivId": "2602.16687",
      "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
      "authors": [
        "Potsawee Manakul",
        "Woody Haosheng Gan",
        "Martijn Bartelds",
        "Guangzhi Sun",
        "William Held",
        "Diyi Yang"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.",
      "url": "https://arxiv.org/abs/2602.16687",
      "pdfUrl": "https://arxiv.org/pdf/2602.16687.pdf",
      "titleJa": "インターリーブされたセマンティック、音響、テキストトークンによるオープン離散オーディオ基盤モデルのスケーリング"
    },
    {
      "id": "2602.16442",
      "arxivId": "2602.16442",
      "title": "Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA",
      "authors": [
        "Kamil Jeziorek",
        "Piotr Wzorek",
        "Krzysztof Blachut",
        "Hiroshi Nakano",
        "Manon Dampfhoffer",
        "Thomas Mesquida",
        "Hiroaki Nishi",
        "Thomas Dalgaty",
        "Tomasz Kryjak"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA implementation of event-graph neural networks for audio processing. We utilise an artificial cochlea that converts time-series signals into sparse event data, reducing memory and computation costs. Our architecture was implemented on a SoC FPGA and evaluated on two open-source datasets. For classification task, our baseline floating-point model achieves 92.7% accuracy on SHD dataset - only 2.4% below the state of the art - while requiring over 10x and 67x fewer parameters. On SSC, our models achieve 66.9-71.0% accuracy. Compared to FPGA-based spiking neural networks, our quantised model reaches 92.3% accuracy, outperforming them by up to 19.3% while reducing resource usage and latency. For SSC, we report the first hardware-accelerated evaluation. We further demonstrate the first end-to-end FPGA implementation of event-audio keyword spotting, combining graph convolutional layers with recurrent sequence modelling. The system achieves up to 95% word-end detection accuracy, with only 10.53 microsecond latency and 1.18 W power consumption, establishing a strong benchmark for energy-efficient event-driven KWS.",
      "url": "https://arxiv.org/abs/2602.16442",
      "pdfUrl": "https://arxiv.org/pdf/2602.16442.pdf",
      "titleJa": "ハードウェアアクセラレーショングラフニューラルネットワーク：SoC FPGA上でのニューロモルフィックイベントベースのオーディオ分類とキーワードスポッティングの代替アプローチ"
    },
    {
      "id": "2602.16421",
      "arxivId": "2602.16421",
      "title": "SELEBI: Percussion-aware Time Stretching via Selective Magnitude Spectrogram Compression by Nonstationary Gabor Transform",
      "authors": [
        "Natsuki Akaishi",
        "Nicki Holighaus",
        "Kohei Yatabe"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Phase vocoder-based time-stretching is a widely used technique for the time-scale modification of audio signals. However, conventional implementations suffer from ``percussion smearing,'' a well-known artifact that significantly degrades the quality of percussive components. We attribute this artifact to a fundamental time-scale mismatch between the temporally smeared magnitude spectrogram and the localized, newly generated phase. To address this, we propose SELEBI, a signal-adaptive phase vocoder algorithm that significantly reduces percussion smearing while preserving stability and the perfect reconstruction property. Unlike conventional methods that rely on heuristic processing or component separation, our approach leverages the nonstationary Gabor transform. By dynamically adapting analysis window lengths to assign short windows to intervals containing significant energy associated with percussive components, we directly compute a temporally localized magnitude spectrogram from the time-domain signal. This approach ensures greater consistency between the temporal structures of the magnitude and phase. Furthermore, the perfect reconstruction property of the nonstationary Gabor transform guarantees stable, high-fidelity signal synthesis, in contrast to previous heuristic approaches. Experimental results demonstrate that the proposed method effectively mitigates percussion smearing and yields natural sound quality.",
      "url": "https://arxiv.org/abs/2602.16421",
      "pdfUrl": "https://arxiv.org/pdf/2602.16421.pdf",
      "titleJa": "SELEBI: 非定常ガボール変換による選択的振幅スペクトログラム圧縮による打楽器を考慮した時間伸縮"
    },
    {
      "id": "2602.16416",
      "arxivId": "2602.16416",
      "title": "Online Single-Channel Audio-Based Sound Speed Estimation for Robust Multi-Channel Audio Control",
      "authors": [
        "Andreas Jonas Fuglsig",
        "Mads Græsbøll Christensen",
        "Jesper Rindom Jensen"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Robust spatial audio control relies on accurate acoustic propagation models, yet environmental variations, especially changes in the speed of sound, cause systematic mismatches that degrade performance. Existing methods either assume known sound speed, require multiple microphones, or rely on separate calibration, making them impractical for systems with minimal sensing. We propose an online sound speed estimator that operates during general multichannel audio playback and requires only a single observation microphone. The method exploits the structured effect of sound speed on the reproduced signal and estimates it by minimizing the mismatch between the measured audio and a parametric acoustic model. Simulations show accurate tracking of sound speed for diverse input signals and improved spatial control performance when the estimates are used to compensate propagation errors in a sound zone control framework.",
      "url": "https://arxiv.org/abs/2602.16416",
      "pdfUrl": "https://arxiv.org/pdf/2602.16416.pdf",
      "titleJa": "堅牢なマルチチャンネルオーディオ制御のためのオンラインシングルチャンネルオーディオベースの音速推定"
    },
    {
      "id": "2602.16399",
      "arxivId": "2602.16399",
      "title": "Multi-Channel Replay Speech Detection using Acoustic Maps",
      "authors": [
        "Michael Neri",
        "Tuomas Virtanen"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Replay attacks remain a critical vulnerability for automatic speaker verification systems, particularly in real-time voice assistant applications. In this work, we propose acoustic maps as a novel spatial feature representation for replay speech detection from multi-channel recordings. Derived from classical beamforming over discrete azimuth and elevation grids, acoustic maps encode directional energy distributions that reflect physical differences between human speech radiation and loudspeaker-based replay. A lightweight convolutional neural network is designed to operate on this representation, achieving competitive performance on the ReMASC dataset with approximately 6k trainable parameters. Experimental results show that acoustic maps provide a compact and physically interpretable feature space for replay attack detection across different devices and acoustic environments.",
      "url": "https://arxiv.org/abs/2602.16399",
      "pdfUrl": "https://arxiv.org/pdf/2602.16399.pdf",
      "titleJa": "音響マップを用いたマルチチャンネル再生音声検出"
    },
    {
      "id": "2602.16343",
      "arxivId": "2602.16343",
      "title": "How to Label Resynthesized Audio: The Dual Role of Neural Audio Codecs in Audio Deepfake Detection",
      "authors": [
        "Yixuan Xiao",
        "Florian Lux",
        "Alejandro Pérez-González-de-Martos",
        "Ngoc Thang Vu"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Since Text-to-Speech systems typically don't produce waveforms directly, recent spoof detection studies use resynthesized waveforms from vocoders and neural audio codecs to simulate an attacker. Unlike vocoders, which are specifically designed for speech synthesis, neural audio codecs were originally developed for compressing audio for storage and transmission. However, their ability to discretize speech also sparked interest in language-modeling-based speech synthesis. Owing to this dual functionality, codec resynthesized data may be labeled as either bonafide or spoof. So far, very little research has addressed this issue. In this study, we present a challenging extension of the ASVspoof 5 dataset constructed for this purpose. We examine how different labeling choices affect detection performance and provide insights into labeling strategies.",
      "url": "https://arxiv.org/abs/2602.16343",
      "pdfUrl": "https://arxiv.org/pdf/2602.16343.pdf",
      "titleJa": "再合成されたオーディオのラベル付け方法：オーディオディープフェイク検出におけるニューラルオーディオコーデックの二重の役割"
    },
    {
      "id": "2602.16334",
      "arxivId": "2602.16334",
      "title": "Spatial Audio Question Answering and Reasoning on Dynamic Source Movements",
      "authors": [
        "Arvind Krishna Sridhar",
        "Yinyi Guo",
        "Erik Visser"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Spatial audio understanding aims to enable machines to interpret complex auditory scenes, particularly when sound sources move over time. In this work, we study Spatial Audio Question Answering (Spatial AQA) with a focus on movement reasoning, where a model must infer object motion, position, and directional changes directly from stereo audio. First, we introduce a movement-centric spatial audio augmentation framework that synthesizes diverse motion patterns from isolated mono audio events, enabling controlled and scalable training data generation. Second, we propose an end-to-end multimodal finetuning approach with a thinking mode, which allows audio-language models to produce explicit intermediate reasoning steps before predicting an answer. Third, we investigate the impact of query-conditioned source separation as a preprocessing stage and compare three inference regimes: no masking, an audio grounding model (AGM), and ground-truth masks. Our results show that reasoning amplifies the benefits of source separation, with thinking mode showing significant improvement of +5.1% when a single event is present in the question. These findings highlight the interplay between movement modeling, reasoning, and separation quality, offering new insights for advancing spatial audio understanding.",
      "url": "https://arxiv.org/abs/2602.16334",
      "pdfUrl": "https://arxiv.org/pdf/2602.16334.pdf",
      "titleJa": "動的音源移動に関する空間オーディオ質問応答と推論"
    },
    {
      "id": "2602.16305",
      "arxivId": "2602.16305",
      "title": "BAT: Better Audio Transformer Guided by Convex Gated Probing",
      "authors": [
        "Houtan Ghaffari",
        "Lukas Rauch",
        "Christoph Scholz",
        "Paul Devos"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Probing is widely adopted in computer vision to faithfully evaluate self-supervised learning (SSL) embeddings, as fine-tuning may misrepresent their inherent quality. In contrast, audio SSL models still rely on fine-tuning because simple probing fails to unlock their full potential and alters their rankings when competing for SOTA on AudioSet. Hence, a robust and efficient probing mechanism is required to guide the trajectory of audio SSL towards reliable and reproducible methods. We introduce Convex Gated Probing (CGP), a prototype-based method that drastically closes the gap between fine-tuning and probing in audio. CGP efficiently utilizes all frozen layers via a gating mechanism and exposes the location of latent task-relevant information. Guided by CGP, we rework the entire SSL pipeline of current SOTA audio models that use legacy implementations of prior SSL methods. By refining data preprocessing, model architecture, and pre-training recipe, we introduce Better Audio Transformer (BAT), and establish new SOTA on audio benchmarks.",
      "url": "https://arxiv.org/abs/2602.16305",
      "pdfUrl": "https://arxiv.org/pdf/2602.16305.pdf",
      "titleJa": "BAT: 凸型ゲートプローブによる優れたオーディオトランスフォーマー"
    },
    {
      "id": "2602.16256",
      "arxivId": "2602.16256",
      "title": "Color-based Emotion Representation for Speech Emotion Recognition",
      "authors": [
        "Ryotaro Nagase",
        "Ryoichi Takashima",
        "Yoichi Yamashita"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Speech emotion recognition (SER) has traditionally relied on categorical or dimensional labels. However, this technique is limited in representing both the diversity and interpretability of emotions. To overcome this limitation, we focus on color attributes, such as hue, saturation, and value, to represent emotions as continuous and interpretable scores. We annotated an emotional speech corpus with color attributes via crowdsourcing and analyzed them. Moreover, we built regression models for color attributes in SER using machine learning and deep learning, and explored the multitask learning of color attribute regression and emotion classification. As a result, we demonstrated the relationship between color attributes and emotions in speech, and successfully developed color attribute regression models for SER. We also showed that multitask learning improved the performance of each task.",
      "url": "https://arxiv.org/abs/2602.16256",
      "pdfUrl": "https://arxiv.org/pdf/2602.16256.pdf",
      "titleJa": "音声感情認識のための色彩に基づく感情表現"
    },
    {
      "id": "2602.16253",
      "arxivId": "2602.16253",
      "title": "How Much Does Machine Identity Matter in Anomalous Sound Detection at Test Time?",
      "authors": [
        "Kevin Wilkinghoff",
        "Keisuke Imoto",
        "Zheng-Hua Tan"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Anomalous sound detection (ASD) benchmarks typically assume that the identity of the monitored machine is known at test time and that recordings are evaluated in a machine-wise manner. However, in realistic monitoring scenarios with multiple known machines operating concurrently, test recordings may not be reliably attributable to a specific machine, and requiring machine identity imposes deployment constraints such as dedicated sensors per machine. To reveal performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, we consider a minimal modification of the ASD evaluation protocol in which test recordings from multiple machines are merged and evaluated jointly without access to machine identity at inference time. Training data and evaluation metrics remain unchanged, and machine identity labels are used only for post hoc evaluation. Experiments with representative ASD methods show that relaxing this assumption reveals performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, and that these degradations are strongly related to implicit machine identification accuracy.",
      "url": "https://arxiv.org/abs/2602.16253",
      "pdfUrl": "https://arxiv.org/pdf/2602.16253.pdf",
      "titleJa": "テスト時の異常音検出において、マシン ID はどの程度重要ですか?"
    },
    {
      "id": "2602.16118",
      "arxivId": "2602.16118",
      "title": "Real time fault detection in 3D printers using Convolutional Neural Networks and acoustic signals",
      "authors": [
        "Muhammad Fasih Waheed",
        "Shonda Bernadin"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The reliability and quality of 3D printing processes are critically dependent on the timely detection of mechanical faults. Traditional monitoring methods often rely on visual inspection and hardware sensors, which can be both costly and limited in scope. This paper explores a scalable and contactless method for the use of real-time audio signal analysis for detecting mechanical faults in 3D printers. By capturing and classifying acoustic emissions during the printing process, we aim to identify common faults such as nozzle clogging, filament breakage, pully skipping and various other mechanical faults. Utilizing Convolutional neural networks, we implement algorithms capable of real-time audio classification to detect these faults promptly. Our methodology involves conducting a series of controlled experiments to gather audio data, followed by the application of advanced machine learning models for fault detection. Additionally, we review existing literature on audio-based fault detection in manufacturing and 3D printing to contextualize our research within the broader field. Preliminary results demonstrate that audio signals, when analyzed with machine learning techniques, provide a reliable and cost-effective means of enhancing real-time fault detection.",
      "url": "https://arxiv.org/abs/2602.16118",
      "pdfUrl": "https://arxiv.org/pdf/2602.16118.pdf",
      "titleJa": "畳み込みニューラルネットワークと音響信号を用いた3Dプリンターのリアルタイム故障検出"
    },
    {
      "id": "2602.16008",
      "arxivId": "2602.16008",
      "title": "MAEB: Massive Audio Embedding Benchmark",
      "authors": [
        "Adnan El Assadi",
        "Isaac Chung",
        "Chenghao Xiao",
        "Roman Solomatin",
        "Animesh Jha",
        "Rahul Chand",
        "Silky Singh",
        "Kaitlyn Wang",
        "Ali Sartaz Khan",
        "Marc Moussa Nasser",
        "Sufen Fong",
        "Pengfei He",
        "Alan Xiao",
        "Ayush Sunil Munot",
        "Aditya Shrivastava",
        "Artem Gazizov",
        "Niklas Muennighoff",
        "Kenneth Enevoldsen"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.",
      "url": "https://arxiv.org/abs/2602.16008",
      "pdfUrl": "https://arxiv.org/pdf/2602.16008.pdf",
      "titleJa": "MAEB: 大規模オーディオ埋め込みベンチマーク"
    },
    {
      "id": "2602.15766",
      "arxivId": "2602.15766",
      "title": "TAC: Timestamped Audio Captioning",
      "authors": [
        "Sonal Kumar",
        "Prem Seetharaman",
        "Ke Chen",
        "Oriol Nieto",
        "Jiaqi Su",
        "Zhepei Wang",
        "Rithesh Kumar",
        "Dinesh Manocha",
        "Nicholas J. Bryan",
        "Zeyu Jin",
        "Justin Salamon"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Large Audio Language Models struggle to disentangle overlapping events in complex acoustic scenes, yielding temporally inconsistent captions and frequent hallucinations. We introduce Timestamped Audio Captioner (TAC), a model that produces temporally grounded audio descriptions at varying degrees of detail and resolution. TAC is trained with a synthetic data pipeline that constructs challenging and dynamic mixtures from real-world audio sources, enabling robust learning under realistic polyphonic conditions. Across event detection and dense captioning, TAC outperforms all competing methods, with a low hallucination rate and accurate temporal grounding. We also introduce TAC-V, an audio-visual pipeline to generate semantically rich audio-visual descriptions. We then show that TAC and TAC-V serves as a \"semantic bridge\" for a text-only reasoner: a simple TAC$\\rightarrow$LLM and TAC-V$\\rightarrow$LLM cascade achieves state-of-the-art scores on benchmarks for both audio (MMAU-Pro, MMSU, MMAR) and audio-visual (DailyOmni, VideoHolmes) understanding and reasoning respectively.",
      "url": "https://arxiv.org/abs/2602.15766",
      "pdfUrl": "https://arxiv.org/pdf/2602.15766.pdf",
      "titleJa": "TAC: タイムスタンプ付き音声字幕"
    },
    {
      "id": "2602.15749",
      "arxivId": "2602.15749",
      "title": "A Generative-First Neural Audio Autoencoder",
      "authors": [
        "Jonah Casebeer",
        "Ge Zhu",
        "Zhepei Wang",
        "Nicholas J. Bryan"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Neural autoencoders underpin generative models. Practical, large-scale use of neural autoencoders for generative modeling necessitates fast encoding, low latent rates, and a single model across representations. Existing approaches are reconstruction-first: they incur high latent rates, slow encoding, and separate architectures for discrete vs. continuous latents and for different audio channel formats, hindering workflows from preprocessing to inference conditioning. We introduce a generative-first architecture for audio autoencoding that increases temporal downsampling from 2048x to 3360x and supports continuous and discrete representations and common audio channel formats in one model. By balancing compression, quality, and speed, it delivers 10x faster encoding, 1.6x lower rates, and eliminates channel-format-specific variants while maintaining competitive reconstruction quality. This enables applications previously constrained by processing costs: a 60-second mono signal compresses to 788 tokens, making generative modeling more tractable.",
      "url": "https://arxiv.org/abs/2602.15749",
      "pdfUrl": "https://arxiv.org/pdf/2602.15749.pdf",
      "titleJa": "ジェネレーティブファーストのニューラルオーディオオートエンコーダー"
    },
    {
      "id": "2602.15651",
      "arxivId": "2602.15651",
      "title": "UniTAF: A Modular Framework for Joint Text-to-Speech and Audio-to-Face Modeling",
      "authors": [
        "Qiangong Zhou",
        "Nagasaka Tomohiro"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "cs.CV",
        "eess.AS"
      ],
      "abstract": "This work considers merging two independent models, TTS and A2F, into a unified model to enable internal feature transfer, thereby improving the consistency between audio and facial expressions generated from text. We also discuss the extension of the emotion control mechanism from TTS to the joint model. This work does not aim to showcase generation quality; instead, from a system design perspective, it validates the feasibility of reusing intermediate representations from TTS for joint modeling of speech and facial expressions, and provides engineering practice references for subsequent speech expression co-design. The project code has been open source at: https://github.com/GoldenFishes/UniTAF",
      "url": "https://arxiv.org/abs/2602.15651",
      "pdfUrl": "https://arxiv.org/pdf/2602.15651.pdf",
      "titleJa": "UniTAF: テキスト音声合成と音声顔認識の統合モデリングのためのモジュール式フレームワーク"
    },
    {
      "id": "2602.15519",
      "arxivId": "2602.15519",
      "title": "Enroll-on-Wakeup: A First Comparative Study of Target Speech Extraction for Seamless Interaction in Real Noisy Human-Machine Dialogue Scenarios",
      "authors": [
        "Yiming Yang",
        "Guangyong Wang",
        "Haixin Guan",
        "Yanhua Long"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Target speech extraction (TSE) typically relies on pre-recorded high-quality enrollment speech, which disrupts user experience and limits feasibility in spontaneous interaction. In this paper, we propose Enroll-on-Wakeup (EoW), a novel framework where the wake-word segment, captured naturally during human-machine interaction, is automatically utilized as the enrollment reference. This eliminates the need for pre-collected speech to enable a seamless experience. We perform the first systematic study of EoW-TSE, evaluating advanced discriminative and generative models under real diverse acoustic conditions. Given the short and noisy nature of wake-word segments, we investigate enrollment augmentation using LLM-based TTS. Results show that while current TSE models face performance degradation in EoW-TSE, TTS-based assistance significantly enhances the listening experience, though gaps remain in speech recognition accuracy.",
      "url": "https://arxiv.org/abs/2602.15519",
      "pdfUrl": "https://arxiv.org/pdf/2602.15519.pdf",
      "titleJa": "エンロールオンウェイクアップ：実際のノイズ環境における人間と機械の対話シナリオにおけるシームレスなインタラクションのためのターゲット音声抽出に関する初の比較研究"
    },
    {
      "id": "2602.17598",
      "arxivId": "2602.17598",
      "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
      "authors": [
        "Jayadev Billa"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.",
      "url": "https://arxiv.org/abs/2602.17598",
      "pdfUrl": "https://arxiv.org/pdf/2602.17598.pdf",
      "titleJa": "カスケード等価性仮説: 音声 LLM が ASR$\\rightarrow$LLM パイプラインのように動作するのはどのような場合ですか?"
    },
    {
      "id": "2602.17157",
      "arxivId": "2602.17157",
      "title": "CC-G2PnP: Streaming Grapheme-to-Phoneme and prosody with Conformer-CTC for unsegmented languages",
      "authors": [
        "Yuma Shirahata",
        "Ryuichi Yamamoto"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We propose CC-G2PnP, a streaming grapheme-to-phoneme and prosody (G2PnP) model to connect large language model and text-to-speech in a streaming manner. CC-G2PnP is based on Conformer-CTC architecture. Specifically, the input grapheme tokens are processed chunk by chunk, which enables streaming inference of phonemic and prosodic (PnP) labels. By guaranteeing minimal look-ahead size to each input token, the proposed model can consider future context in each token, which leads to stable PnP label prediction. Unlike previous streaming methods that depend on explicit word boundaries, the CTC decoder in CC-G2PnP effectively learns the alignment between graphemes and phonemes during training, making it applicable to unsegmented languages. Experiments on a Japanese dataset, which has no explicit word boundaries, show that CC-G2PnP significantly outperforms the baseline streaming G2PnP model in the accuracy of PnP label prediction.",
      "url": "https://arxiv.org/abs/2602.17157",
      "pdfUrl": "https://arxiv.org/pdf/2602.17157.pdf",
      "titleJa": "CC-G2PnP: 非分節言語における Conformer-CTC を用いた書記素から音素への変換と韻律のストリーミング"
    },
    {
      "id": "2602.15537",
      "arxivId": "2602.15537",
      "title": "ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling",
      "authors": [
        "Nicol Visser",
        "Simon Malan",
        "Danel Slabbert",
        "Herman Kamper"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Pure speech language models aim to learn language directly from raw audio without textual resources. A key challenge is that discrete tokens from self-supervised speech encoders result in excessively long sequences, motivating recent work on syllable-like units. However, methods like Sylber and SyllableLM rely on intricate multi-stage training pipelines. We propose ZeroSyl, a simple training-free method to extract syllable boundaries and embeddings directly from a frozen WavLM model. Using L2 norms of features in WavLM's intermediate layers, ZeroSyl achieves competitive syllable segmentation performance. The resulting segments are mean-pooled, discretized using K-means, and used to train a language model. ZeroSyl outperforms prior syllabic tokenizers across lexical, syntactic, and narrative benchmarks. Scaling experiments show that while finer-grained units are beneficial for lexical tasks, our discovered syllabic units exhibit better scaling behavior for syntactic modeling.",
      "url": "https://arxiv.org/abs/2602.15537",
      "pdfUrl": "https://arxiv.org/pdf/2602.15537.pdf",
      "titleJa": "ZeroSyl: 音声言語モデルのためのシンプルなゼロリソース音節トークン化"
    },
    {
      "id": "2602.15484",
      "arxivId": "2602.15484",
      "title": "Bottleneck Transformer-Based Approach for Improved Automatic STOI Score Prediction",
      "authors": [
        " Amartyaveer",
        "Murali Kadambi",
        "Chandra Mohan Sharma",
        "Anupam Mondal",
        "Prasanta Kumar Ghosh"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "eess.AS",
        "cs.LG",
        "eess.SP"
      ],
      "abstract": "In this study, we have presented a novel approach to predict the Short-Time Objective Intelligibility (STOI) metric using a bottleneck transformer architecture. Traditional methods for calculating STOI typically requires clean reference speech, which limits their applicability in the real world. To address this, numerous deep learning-based nonintrusive speech assessment models have garnered significant interest. Many studies have achieved commendable performance, but there is room for further improvement. We propose the use of bottleneck transformer, incorporating convolution blocks for learning frame-level features and a multi-head self-attention (MHSA) layer to aggregate the information. These components enable the transformer to focus on the key aspects of the input data. Our model has shown higher correlation and lower mean squared error for both seen and unseen scenarios compared to the state-of-the-art model using self-supervised learning (SSL) and spectral features as inputs.",
      "url": "https://arxiv.org/abs/2602.15484",
      "pdfUrl": "https://arxiv.org/pdf/2602.15484.pdf",
      "titleJa": "ボトルネックトランスフォーマーベースのアプローチによるSTOIスコア自動予測の改善"
    },
    {
      "id": "2602.15307",
      "arxivId": "2602.15307",
      "title": "What Do Neurons Listen To? A Neuron-level Dissection of a General-purpose Audio Model",
      "authors": [
        "Takao Kawamura",
        "Daisuke Niizumi",
        "Nobutaka Ono"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "In this paper, we analyze the internal representations of a general-purpose audio self-supervised learning (SSL) model from a neuron-level perspective. Despite their strong empirical performance as feature extractors, the internal mechanisms underlying the robust generalization of SSL audio models remain unclear. Drawing on the framework of mechanistic interpretability, we identify and examine class-specific neurons by analyzing conditional activation patterns across diverse tasks. Our analysis reveals that SSL models foster the emergence of class-specific neurons that provide extensive coverage across novel task classes. These neurons exhibit shared responses across different semantic categories and acoustic similarities, such as speech attributes and musical pitch. We also confirm that these neurons have a functional impact on classification performance. To our knowledge, this is the first systematic neuron-level analysis of a general-purpose audio SSL model, providing new insights into its internal representation.",
      "url": "https://arxiv.org/abs/2602.15307",
      "pdfUrl": "https://arxiv.org/pdf/2602.15307.pdf",
      "titleJa": "ニューロンは何を聞いているのか？汎用オーディオモデルのニューロンレベルの解析"
    },
    {
      "id": "2602.15909",
      "arxivId": "2602.15909",
      "title": "Resp-Agent: An Agent-Based System for Multimodal Respiratory Sound Generation and Disease Diagnosis",
      "authors": [
        "Pengfei Zhang",
        "Tianxin Xie",
        "Minghao Yang",
        "Li Liu"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.DB",
        "cs.HC",
        "cs.MA",
        "cs.SD"
      ],
      "abstract": "Deep learning-based respiratory auscultation is currently hindered by two fundamental challenges: (i) inherent information loss, as converting signals into spectrograms discards transient acoustic events and clinical context; (ii) limited data availability, exacerbated by severe class imbalance. To bridge these gaps, we present Resp-Agent, an autonomous multimodal system orchestrated by a novel Active Adversarial Curriculum Agent (Thinker-A$^2$CA). Unlike static pipelines, Thinker-A$^2$CA serves as a central controller that actively identifies diagnostic weaknesses and schedules targeted synthesis in a closed loop. To address the representation gap, we introduce a Modality-Weaving Diagnoser that weaves EHR data with audio tokens via Strategic Global Attention and sparse audio anchors, capturing both long-range clinical context and millisecond-level transients. To address the data gap, we design a Flow Matching Generator that adapts a text-only Large Language Model (LLM) via modality injection, decoupling pathological content from acoustic style to synthesize hard-to-diagnose samples. As a foundation for these efforts, we introduce Resp-229k, a benchmark corpus of 229k recordings paired with LLM-distilled clinical narratives. Extensive experiments demonstrate that Resp-Agent consistently outperforms prior approaches across diverse evaluation settings, improving diagnostic robustness under data scarcity and long-tailed class imbalance. Our code and data are available at https://github.com/zpforlove/Resp-Agent.",
      "url": "https://arxiv.org/abs/2602.15909",
      "pdfUrl": "https://arxiv.org/pdf/2602.15909.pdf",
      "titleJa": "Resp-Agent: マルチモーダル呼吸音生成と疾患診断のためのエージェントベースシステム"
    },
    {
      "id": "2602.14785",
      "arxivId": "2602.14785",
      "title": "SA-SSL-MOS: Self-supervised Learning MOS Prediction with Spectral Augmentation for Generalized Multi-Rate Speech Assessment",
      "authors": [
        "Fengyuan Cao",
        "Xinyu Liang",
        "Fredrik Cumlin",
        "Victor Ungureanu",
        "Chandan K. A. Reddy",
        "Christian Schuldt",
        "Saikat Chatterjee"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Designing a speech quality assessment (SQA) system for estimating mean-opinion-score (MOS) of multi-rate speech with varying sampling frequency (16-48 kHz) is a challenging task. The challenge arises due to the limited availability of a MOS-labeled training dataset comprising multi-rate speech samples. While self-supervised learning (SSL) models have been widely adopted in SQA to boost performance, a key limitation is that they are pretrained on 16 kHz speech and therefore discard high-frequency information present in higher sampling rates. To address this issue, we propose a spectrogram-augmented SSL method that incorporates high-frequency features (up to 48 kHz sampling rate) through a parallel-branch architecture. We further introduce a two-step training scheme: the model is first pre-trained on a large 48 kHz dataset and then fine-tuned on a smaller multi-rate dataset. Experimental results show that leveraging high-frequency information overlooked by SSL features is crucial for accurate multi-rate SQA, and that the proposed two-step training substantially improves generalization when multi-rate data is limited.",
      "url": "https://arxiv.org/abs/2602.14785",
      "pdfUrl": "https://arxiv.org/pdf/2602.14785.pdf",
      "titleJa": "SA-SSL-MOS: スペクトル拡張を用いた自己教師学習によるMOS予測と一般化マルチレート音声評価"
    },
    {
      "id": "2602.14686",
      "arxivId": "2602.14686",
      "title": "Disentangling Pitch and Creak for Speaker Identity Preservation in Speech Synthesis",
      "authors": [
        "Frederik Rautenberg",
        "Jana Wiechmann",
        "Petra Wagner",
        "Reinhold Haeb-Umbach"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We introduce a system capable of faithfully modifying the perceptual voice quality of creak while preserving the speaker's perceived identity. While it is well known that high creak probability is typically correlated with low pitch, it is important to note that this is a property observed on a population of speakers but does not necessarily hold across all situations. Disentanglement of pitch from creak is achieved by augmentation of the training dataset of a speech synthesis system with a speaker manipulation block based on conditional continuous normalizing flow. The experiments show greatly improved speaker verification performance over a range of creak manipulation strengths.",
      "url": "https://arxiv.org/abs/2602.14686",
      "pdfUrl": "https://arxiv.org/pdf/2602.14686.pdf",
      "titleJa": "音声合成における話者識別の保持のためのピッチとキーキー音の分離"
    },
    {
      "id": "2602.14671",
      "arxivId": "2602.14671",
      "title": "Data Augmentation for Pathological Speech Enhancement",
      "authors": [
        "Mingchi Hou",
        "Enno Hermann",
        "Ina Kodrasi"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS"
      ],
      "abstract": "The performance of state-of-the-art speech enhancement (SE) models considerably degrades for pathological speech due to atypical acoustic characteristics and limited data availability. This paper systematically investigates data augmentation (DA) strategies to improve SE performance for pathological speakers, evaluating both predictive and generative SE models. We examine three DA categories, i.e., transformative, generative, and noise augmentation, assessing their impact with objective SE metrics. Experimental results show that noise augmentation consistently delivers the largest and most robust gains, transformative augmentations provide moderate improvements, while generative augmentation yields limited benefits and can harm performance as the amount of synthetic data increases. Furthermore, we show that the effectiveness of DA varies depending on the SE model, with DA being more beneficial for predictive SE models. While our results demonstrate that DA improves SE performance for pathological speakers, a performance gap between neurotypical and pathological speech persists, highlighting the need for future research on targeted DA strategies for pathological speech.",
      "url": "https://arxiv.org/abs/2602.14671",
      "pdfUrl": "https://arxiv.org/pdf/2602.14671.pdf",
      "titleJa": "病的な音声強調のためのデータ拡張"
    },
    {
      "id": "2602.17664",
      "arxivId": "2602.17664",
      "title": "Sink-Aware Pruning for Diffusion Language Models",
      "authors": [
        "Aidar Myrzakhan",
        "Tianyi Li",
        "Bowei Guo",
        "Shengkun Tang",
        "Zhiqiang Shen"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\\bf \\texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.",
      "url": "https://arxiv.org/abs/2602.17664",
      "pdfUrl": "https://arxiv.org/pdf/2602.17664.pdf",
      "titleJa": "拡散言語モデルのためのシンクを考慮した枝刈り"
    },
    {
      "id": "2602.17663",
      "arxivId": "2602.17663",
      "title": "CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts",
      "authors": [
        "Juri Opitz",
        "Corina Raclé",
        "Emanuela Boros",
        "Andrianos Michail",
        "Matteo Romanello",
        "Maud Ehrmann",
        "Simon Clematide"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "abstract": "HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ (\"Has the person ever been at this place?\") and $isAt$ (\"Is the person located at this place around publication time?\") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.",
      "url": "https://arxiv.org/abs/2602.17663",
      "pdfUrl": "https://arxiv.org/pdf/2602.17663.pdf",
      "titleJa": "CLEF HIPE-2026: 多言語歴史文書からの正確かつ効率的な人・場所関係抽出の評価"
    },
    {
      "id": "2602.17658",
      "arxivId": "2602.17658",
      "title": "MARS: Margin-Aware Reward-Modeling with Self-Refinement",
      "authors": [
        "Payel Bhattacharjee",
        "Osvaldo Simeone",
        "Ravi Tandon"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "abstract": "Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.",
      "url": "https://arxiv.org/abs/2602.17658",
      "pdfUrl": "https://arxiv.org/pdf/2602.17658.pdf",
      "titleJa": "MARS: 自己改良によるマージンを考慮した報酬モデリング"
    },
    {
      "id": "2602.17645",
      "arxivId": "2602.17645",
      "title": "Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting",
      "authors": [
        "Xiaohan Zhao",
        "Zhaoyi Li",
        "Yaxin Luo",
        "Jiacheng Cui",
        "Zhiqiang Shen"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "abstract": "Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.",
      "url": "https://arxiv.org/abs/2602.17645",
      "pdfUrl": "https://arxiv.org/pdf/2602.17645.pdf",
      "titleJa": "きめ細かな詳細ターゲティングによるブラックボックスLVLM攻撃の限界を押し広げる"
    },
    {
      "id": "2602.17641",
      "arxivId": "2602.17641",
      "title": "FAMOSE: A ReAct Approach to Automated Feature Discovery",
      "authors": [
        "Keith Burghardt",
        "Jienan Liu",
        "Sadman Sakib",
        "Yuning Hao",
        "Bo Li"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.",
      "url": "https://arxiv.org/abs/2602.17641",
      "pdfUrl": "https://arxiv.org/pdf/2602.17641.pdf",
      "titleJa": "FAMOSE: 自動特徴検出のためのReActアプローチ"
    },
    {
      "id": "2602.17634",
      "arxivId": "2602.17634",
      "title": "Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting",
      "authors": [
        "Xinghong Fu",
        "Yanhong Li",
        "Georgios Papaioannou",
        "Yoon Kim"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.",
      "url": "https://arxiv.org/abs/2602.17634",
      "pdfUrl": "https://arxiv.org/pdf/2602.17634.pdf",
      "titleJa": "Reverso: ゼロショット予測のための効率的な時系列基礎モデル"
    },
    {
      "id": "2602.17633",
      "arxivId": "2602.17633",
      "title": "When to Trust the Cheap Check: Weak and Strong Verification for Reasoning",
      "authors": [
        "Shayan Kiyani",
        "Sima Noorani",
        "George Pappas",
        "Hamed Hassani"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "abstract": "Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier.",
      "url": "https://arxiv.org/abs/2602.17633",
      "pdfUrl": "https://arxiv.org/pdf/2602.17633.pdf",
      "titleJa": "安価なチェックをいつ信頼するか：推論のための弱い検証と強い検証"
    },
    {
      "id": "2602.17632",
      "arxivId": "2602.17632",
      "title": "SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer",
      "authors": [
        "Nathan S. de Lara",
        "Florian Shkurti"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces regret by 34-58% over the best baseline.",
      "url": "https://arxiv.org/abs/2602.17632",
      "pdfUrl": "https://arxiv.org/pdf/2602.17632.pdf",
      "titleJa": "SMAC: 堅牢なオフラインからオンラインへの転送のためのスコアマッチングされたアクター・クリティック"
    },
    {
      "id": "2602.17616",
      "arxivId": "2602.17616",
      "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
      "authors": [
        "Luke Huang",
        "Zhuoyang Zhang",
        "Qinghao Hu",
        "Shang Yang",
        "Song Han"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\\textbf{V}$ariance $\\textbf{C}$ontrolled $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.",
      "url": "https://arxiv.org/abs/2602.17616",
      "pdfUrl": "https://arxiv.org/pdf/2602.17616.pdf",
      "titleJa": "安定した非同期性：LLMのための分散制御オフポリシー強化学習"
    },
    {
      "id": "2602.17608",
      "arxivId": "2602.17608",
      "title": "Towards Anytime-Valid Statistical Watermarking",
      "authors": [
        "Baihe Huang",
        "Eric Xu",
        "Kannan Ramchandran",
        "Jiantao Jiao",
        "Michael I. Jordan"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.",
      "url": "https://arxiv.org/abs/2602.17608",
      "pdfUrl": "https://arxiv.org/pdf/2602.17608.pdf",
      "titleJa": "いつでも有効な統計的透かしに向けて"
    },
    {
      "id": "2602.17607",
      "arxivId": "2602.17607",
      "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
      "authors": [
        "Jianda Du",
        "Youran Sun",
        "Haizhao Yang"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "abstract": "PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \\texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \\texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.",
      "url": "https://arxiv.org/abs/2602.17607",
      "pdfUrl": "https://arxiv.org/pdf/2602.17607.pdf",
      "titleJa": "AutoNumerics: 科学計算のための自律型、偏微分方程式非依存型マルチエージェントパイプライン"
    },
    {
      "id": "2602.17605",
      "arxivId": "2602.17605",
      "title": "Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery",
      "authors": [
        "Jowaria Khan",
        "Anindya Sarkar",
        "Yevgeniy Vorobeychik",
        "Elizabeth Bondi-Kelly"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "abstract": "In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.",
      "url": "https://arxiv.org/abs/2602.17605",
      "pdfUrl": "https://arxiv.org/pdf/2602.17605.pdf",
      "titleJa": "リアルタイムで能動的に適応する：地理空間発見のための潜在概念を用いた関連性誘導型オンラインメタ学習"
    },
    {
      "id": "2602.17602",
      "arxivId": "2602.17602",
      "title": "MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models",
      "authors": [
        "Hojung Jung",
        "Rodrigo Hormazabal",
        "Jaehyeong Jo",
        "Youngrok Park",
        "Kyunggeun Roh",
        "Se-Young Yun",
        "Sehui Han",
        "Dae-Woong Jeong"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.",
      "url": "https://arxiv.org/abs/2602.17602",
      "pdfUrl": "https://arxiv.org/pdf/2602.17602.pdf",
      "titleJa": "MolHIT: 階層的離散拡散モデルを用いた分子グラフ生成の高度化"
    },
    {
      "id": "2602.17594",
      "arxivId": "2602.17594",
      "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
      "authors": [
        "Lance Ying",
        "Ryan Truong",
        "Prafull Sharma",
        "Kaiya Ivy Zhao",
        "Nathan Cloos",
        "Kelsey R. Allen",
        "Thomas L. Griffiths",
        "Katherine M. Collins",
        "José Hernández-Orallo",
        "Phillip Isola",
        "Samuel J. Gershman",
        "Joshua B. Tenenbaum"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \\textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.",
      "url": "https://arxiv.org/abs/2602.17594",
      "pdfUrl": "https://arxiv.org/pdf/2602.17594.pdf",
      "titleJa": "AIゲームストア：人間のゲームによる機械の汎用知能のスケーラブルでオープンエンドな評価"
    },
    {
      "id": "2602.17586",
      "arxivId": "2602.17586",
      "title": "Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space",
      "authors": [
        "Antonio Guillen-Perez"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.",
      "url": "https://arxiv.org/abs/2602.17586",
      "pdfUrl": "https://arxiv.org/pdf/2602.17586.pdf",
      "titleJa": "多様体を考慮したスペクトル空間における自動運転における連続異常検出のための条件付きフローマッチング"
    },
    {
      "id": "2602.17568",
      "arxivId": "2602.17568",
      "title": "Be Wary of Your Time Series Preprocessing",
      "authors": [
        "Sofiane Ennadir",
        "Tianze Wang",
        "Oleg Smirnov",
        "Sahar Asadi",
        "Lele Cao"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model's representational capacity, depending on the task and data characteristics. We complement our theory with empirical validation on classification and forecasting benchmarks using multiple Transformer-based models. Our results show that no single normalization method consistently outperforms others, and in some cases, omitting normalization entirely leads to superior performance. These findings highlight the critical role of preprocessing in time series learning and motivate the need for more principled normalization strategies tailored to specific tasks and datasets.",
      "url": "https://arxiv.org/abs/2602.17568",
      "pdfUrl": "https://arxiv.org/pdf/2602.17568.pdf",
      "titleJa": "時系列の前処理に注意する"
    },
    {
      "id": "2602.17566",
      "arxivId": "2602.17566",
      "title": "A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN",
      "authors": [
        "Asif Hasan Chowdhury",
        "Md. Fahim Islam",
        "M Ragib Anjum Riad",
        "Faiyaz Bin Hashem",
        "Md Tanzim Reza",
        "Md. Golam Rabiul Alam"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.AI"
      ],
      "abstract": "The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.",
      "url": "https://arxiv.org/abs/2602.17566",
      "pdfUrl": "https://arxiv.org/pdf/2602.17566.pdf",
      "titleJa": "SWINトランスフォーマーとCNNの融合を活用した肺疾患診断のためのハイブリッド連合学習ベースのアンサンブルアプローチ"
    },
    {
      "id": "2602.17560",
      "arxivId": "2602.17560",
      "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
      "authors": [
        "Hongjue Zhao",
        "Haosen Sun",
        "Jiangtao Kong",
        "Xiaochang Li",
        "Qineng Wang",
        "Liwei Jiang",
        "Qi Zhu",
        "Tarek Abdelzaher",
        "Yejin Choi",
        "Manling Li",
        "Huajie Shao"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \\textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \\textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \\textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \\textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\\%$ improvement over TruthfulQA, $2.5\\%$ over UltraFeedback, and $2.4\\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.",
      "url": "https://arxiv.org/abs/2602.17560",
      "pdfUrl": "https://arxiv.org/pdf/2602.17560.pdf",
      "titleJa": "ODESteer: LLMアライメントのための統合ODEベースのステアリングフレームワーク"
    },
    {
      "id": "2602.17557",
      "arxivId": "2602.17557",
      "title": "Probability-Invariant Random Walk Learning on Gyral Folding-Based Cortical Similarity Networks for Alzheimer's and Lewy Body Dementia Diagnosis",
      "authors": [
        "Minheng Chen",
        "Jing Zhang",
        "Tong Chen",
        "Chao Cao",
        "Tianming Liu",
        "Li Su",
        "Dajiang Zhu"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV"
      ],
      "abstract": "Alzheimer's disease (AD) and Lewy body dementia (LBD) present overlapping clinical features yet require distinct diagnostic strategies. While neuroimaging-based brain network analysis is promising, atlas-based representations may obscure individualized anatomy. Gyral folding-based networks using three-hinge gyri provide a biologically grounded alternative, but inter-individual variability in cortical folding results in inconsistent landmark correspondence and highly irregular network sizes, violating the fixed-topology and node-alignment assumptions of most existing graph learning methods, particularly in clinical datasets where pathological changes further amplify anatomical heterogeneity. We therefore propose a probability-invariant random-walk-based framework that classifies individualized gyral folding networks without explicit node alignment. Cortical similarity networks are built from local morphometric features and represented by distributions of anonymized random walks, with an anatomy-aware encoding that preserves permutation invariance. Experiments on a large clinical cohort of AD and LBD subjects show consistent improvements over existing gyral folding and atlas-based models, demonstrating robustness and potential for dementia diagnosis.",
      "url": "https://arxiv.org/abs/2602.17557",
      "pdfUrl": "https://arxiv.org/pdf/2602.17557.pdf",
      "titleJa": "アルツハイマー病およびレビー小体型認知症の診断のための回旋折り畳みに基づく皮質類似性ネットワークにおける確率不変ランダムウォーク学習"
    },
    {
      "id": "2602.15491",
      "arxivId": "2602.15491",
      "title": "The Equalizer: Introducing Shape-Gain Decomposition in Neural Audio Codecs",
      "authors": [
        "Samir Sadok",
        "Laurent Girin",
        "Xavier Alameda-Pineda"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Neural audio codecs (NACs) typically encode the short-term energy (gain) and normalized structure (shape) of speech/audio signals jointly within the same latent space. As a result, they are poorly robust to a global variation of the input signal level in the sense that such variation has strong influence on the embedding vectors at the output of the encoder and their quantization. This methodology is inherently inefficient, leading to codebook redundancy and suboptimal bitrate-distortion performance. To address these limitations, we propose to introduce shape-gain decomposition, widely used in classical speech/audio coding, into the NAC framework. The principle of the proposed Equalizer methodology is to decompose the input signal -- before the NAC encoder -- into gain and normalized shape vector on a short-term basis. The shape vector is processed by the NAC, while the gain is quantized with scalar quantization and transmitted separately. The output (decoded) signal is reconstructed from the normalized output of the NAC and the quantized gain. Our experiments conducted on speech signals show that this general methodology, easily applicable to any NAC, enables a substantial gain in bitrate-distortion performance, as well as a massive reduction in complexity.",
      "url": "https://arxiv.org/abs/2602.15491",
      "pdfUrl": "https://arxiv.org/pdf/2602.15491.pdf",
      "titleJa": "イコライザー：ニューラルオーディオコーデックにおけるシェイプゲイン分解の導入"
    },
    {
      "id": "2602.13928",
      "arxivId": "2602.13928",
      "title": "voice2mode: Phonation Mode Classification in Singing using Self-Supervised Speech Models",
      "authors": [
        "Aju Ani Justus",
        "Ruchit Agrawal",
        "Sudarsana Reddy Kadiri",
        "Shrikanth Narayanan"
      ],
      "publishedDate": "2026-02-14",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "We present voice2mode, a method for classification of four singing phonation modes (breathy, neutral (modal), flow, and pressed) using embeddings extracted from large self-supervised speech models. Prior work on singing phonation has relied on handcrafted signal features or task-specific neural nets; this work evaluates the transferability of speech foundation models to singing phonation classification. voice2mode extracts layer-wise representations from HuBERT and two wav2vec2 variants, applies global temporal pooling, and classifies the pooled embeddings with lightweight classifiers (SVM, XGBoost). Experiments on a publicly available soprano dataset (763 sustained vowel recordings, four labels) show that foundation-model features substantially outperform conventional spectral baselines (spectrogram, mel-spectrogram, MFCC). HuBERT embeddings obtained from early layers yield the best result (~95.7% accuracy with SVM), an absolute improvement of ~12-15% over the best traditional baseline. We also show layer-wise behaviour: lower layers, which retain acoustic/phonetic detail, are more effective than top layers specialized for Automatic Speech Recognition (ASR).",
      "url": "https://arxiv.org/abs/2602.13928",
      "pdfUrl": "https://arxiv.org/pdf/2602.13928.pdf",
      "titleJa": "voice2mode: 自己教師あり音声モデルを用いた歌唱における発声モードの分類"
    },
    {
      "id": "2602.11910",
      "arxivId": "2602.11910",
      "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
      "authors": [
        "Łukasz Staniszewski",
        "Katarzyna Zaleska",
        "Mateusz Modrzejewski",
        "Kamil Deja"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood.",
      "url": "https://arxiv.org/abs/2602.11910",
      "pdfUrl": "https://arxiv.org/pdf/2602.11910.pdf",
      "titleJa": "TADA! アクティベーションステアリングによるオーディオ拡散モデルのチューニング"
    },
    {
      "id": "2602.11896",
      "arxivId": "2602.11896",
      "title": "Musical Metamerism with Time--Frequency Scattering",
      "authors": [
        "Vincent Lostanlen",
        "Han Han"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The concept of metamerism originates from colorimetry, where it describes a sensation of visual similarity between two colored lights despite significant differences in spectral content. Likewise, we propose to call ``musical metamerism'' the sensation of auditory similarity which is elicited by two music fragments which differ in terms of underlying waveforms. In this technical report, we describe a method to generate musical metamers from any audio recording. Our method is based on joint time--frequency scattering in Kymatio, an open-source software in Python which enables GPU computing and automatic differentiation. The advantage of our method is that it does not require any manual preprocessing, such as transcription, beat tracking, or source separation. We provide a mathematical description of JTFS as well as some excerpts from the Kymatio source code. Lastly, we review the prior work on JTFS and draw connections with closely related algorithms, such as spectrotemporal receptive fields (STRF), modulation power spectra (MPS), and Gabor filterbank (GBFB).",
      "url": "https://arxiv.org/abs/2602.11896",
      "pdfUrl": "https://arxiv.org/pdf/2602.11896.pdf",
      "titleJa": "時間周波数散乱を伴う音楽的メタメリズム"
    },
    {
      "id": "2602.10934",
      "arxivId": "2602.10934",
      "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
      "authors": [
        "Yitian Gong",
        "Kuangwei Chen",
        "Zhaoye Fei",
        "Xiaogui Yang",
        "Ke Chen",
        "Yang Wang",
        "Kexin Huang",
        "Mingshu Chen",
        "Ruixiao Li",
        "Qingyuan Cheng",
        "Shimin Li",
        "Xipeng Qiu"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.",
      "url": "https://arxiv.org/abs/2602.10934",
      "pdfUrl": "https://arxiv.org/pdf/2602.10934.pdf",
      "titleJa": "MOSS-Audio-Tokenizer: 将来のオーディオ基盤モデルに向けたオーディオトークナイザーのスケーリング"
    },
    {
      "id": "2602.12301",
      "arxivId": "2602.12301",
      "title": "Beyond Musical Descriptors: Extracting Preference-Bearing Intent in Music Queries",
      "authors": [
        "Marion Baranes",
        "Romain Hennequin",
        "Elena V. Epure"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.IR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Although annotated music descriptor datasets for user queries are increasingly common, few consider the user's intent behind these descriptors, which is essential for effectively meeting their needs. We introduce MusicRecoIntent, a manually annotated corpus of 2,291 Reddit music requests, labeling musical descriptors across seven categories with positive, negative, or referential preference-bearing roles. We then investigate how reliably large language models (LLMs) can extract these music descriptors, finding that they do capture explicit descriptors but struggle with context-dependent ones. This work can further serve as a benchmark for fine-grained modeling of user intent and for gaining insights into improving LLM-based music understanding systems.",
      "url": "https://arxiv.org/abs/2602.12301",
      "pdfUrl": "https://arxiv.org/pdf/2602.12301.pdf",
      "titleJa": "音楽記述子を超えて：音楽検索クエリにおける嗜好意図の抽出"
    },
    {
      "id": "2602.10656",
      "arxivId": "2602.10656",
      "title": "AudioRAG: A Challenging Benchmark for Audio Reasoning and Information Retrieval",
      "authors": [
        "Jingru Lin",
        "Chen Zhang",
        "Tianrui Wang",
        "Haizhou Li"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Due to recent advancements in Large Audio-Language Models (LALMs) that demonstrate remarkable performance across a range of sound-, speech- and music-related tasks, there is a growing interest in proposing benchmarks to assess these models. Existing benchmarks generally focus only on reasoning with internal knowledge, neglecting real-world scenarios that require external information grounding. To bridge this gap, we introduce AudioRAG, a novel benchmark designed to evaluate audio-based reasoning augmented by information retrieval in realistic web environments. This benchmark comprises both LLM-generated and manually curated question-answer pairs. Our evaluations reveal that even the state-of-the-art LALMs struggle to answer these questions. We therefore propose an agentic pipeline that integrates audio reasoning with retrieval-augmented generation, providing a stronger baseline for future research.",
      "url": "https://arxiv.org/abs/2602.10656",
      "pdfUrl": "https://arxiv.org/pdf/2602.10656.pdf",
      "titleJa": "AudioRAG: オーディオ推論と情報検索のための挑戦的なベンチマーク"
    },
    {
      "id": "2602.10058",
      "arxivId": "2602.10058",
      "title": "Evaluating Disentangled Representations for Controllable Music Generation",
      "authors": [
        "Laura Ibáñez-Martínez",
        "Chukwuemeka Nkama",
        "Andrea Poltronieri",
        "Xavier Serra",
        "Martín Rocamora"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Recent approaches in music generation rely on disentangled representations, often labeled as structure and timbre or local and global, to enable controllable synthesis. Yet the underlying properties of these embeddings remain underexplored. In this work, we evaluate such disentangled representations in a set of music audio models for controllable generation using a probing-based framework that goes beyond standard downstream tasks. The selected models reflect diverse unsupervised disentanglement strategies, including inductive biases, data augmentations, adversarial objectives, and staged training procedures. We further isolate specific strategies to analyze their effect. Our analysis spans four key axes: informativeness, equivariance, invariance, and disentanglement, which are assessed across datasets, tasks, and controlled transformations. Our findings reveal inconsistencies between intended and actual semantics of the embeddings, suggesting that current strategies fall short of producing truly disentangled representations, and prompting a re-examination of how controllability is approached in music generation.",
      "url": "https://arxiv.org/abs/2602.10058",
      "pdfUrl": "https://arxiv.org/pdf/2602.10058.pdf",
      "titleJa": "制御可能な音楽生成のための分離表現の評価"
    },
    {
      "id": "2602.09891",
      "arxivId": "2602.09891",
      "title": "Stemphonic: All-at-once Flexible Multi-stem Music Generation",
      "authors": [
        "Shih-Lun Wu",
        "Ge Zhu",
        "Juan-Pablo Caceres",
        "Cheng-Zhi Anna Huang",
        "Nicholas J. Bryan"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM"
      ],
      "abstract": "Music stem generation, the task of producing musically-synchronized and isolated instrument audio clips, offers the potential of greater user control and better alignment with musician workflows compared to conventional text-to-music models. Existing stem generation approaches, however, either rely on fixed architectures that output a predefined set of stems in parallel, or generate only one stem at a time, resulting in slow inference despite flexibility in stem combination. We propose Stemphonic, a diffusion-/flow-based framework that overcomes this trade-off and generates a variable set of synchronized stems in one inference pass. During training, we treat each stem as a batch element, group synchronized stems in a batch, and apply a shared noise latent to each group. At inference-time, we use a shared initial noise latent and stem-specific text inputs to generate synchronized multi-stem outputs in one pass. We further expand our approach to enable one-pass conditional multi-stem generation and stem-wise activity controls to empower users to iteratively generate and orchestrate the temporal layering of a mix. We benchmark our results on multiple open-source stem evaluation sets and show that Stemphonic produces higher-quality outputs while accelerating the full mix generation process by 25 to 50%. Demos at: https://stemphonic-demo.vercel.app.",
      "url": "https://arxiv.org/abs/2602.09891",
      "pdfUrl": "https://arxiv.org/pdf/2602.09891.pdf",
      "titleJa": "Stemphonic: 一度に柔軟なマルチステム音楽生成"
    },
    {
      "id": "2602.08794",
      "arxivId": "2602.08794",
      "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
      "authors": [
        "SII-OpenMOSS Team",
        " :",
        "Donghua Yu",
        "Mingshu Chen",
        "Qi Chen",
        "Qi Luo",
        "Qianyi Wu",
        "Qinyuan Cheng",
        "Ruixiao Li",
        "Tianyi Liang",
        "Wenbo Zhang",
        "Wenming Tu",
        "Xiangyu Peng",
        "Yang Gao",
        "Yanru Huo",
        "Ying Zhu",
        "Yinze Luo",
        "Yiyang Zhang",
        "Yuerong Song",
        "Zhe Xu",
        "Zhiyu Zhang",
        "Chenchen Yang",
        "Cheng Chang",
        "Chushu Zhou",
        "Hanfu Chen",
        "Hongnan Ma",
        "Jiaxi Li",
        "Jingqi Tong",
        "Junxi Liu",
        "Ke Chen",
        "Shimin Li",
        "Shiqi Jiang",
        "Songlin Wang",
        "Wei Jiang",
        "Zhaoye Fei",
        "Zhiyuan Ning",
        "Chunguo Li",
        "Chenhui Li",
        "Ziwei He",
        "Zengfeng Huang",
        "Xie Chen",
        "Xipeng Qiu"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.CV",
        "cs.SD"
      ],
      "abstract": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
      "url": "https://arxiv.org/abs/2602.08794",
      "pdfUrl": "https://arxiv.org/pdf/2602.08794.pdf",
      "titleJa": "MOVA: スケーラブルで同期したビデオ・オーディオ生成に向けて"
    },
    {
      "id": "2602.08671",
      "arxivId": "2602.08671",
      "title": "Input-Adaptive Spectral Feature Compression by Sequence Modeling for Source Separation",
      "authors": [
        "Kohei Saijo",
        "Yoshiaki Bando"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Time-frequency domain dual-path models have demonstrated strong performance and are widely used in source separation. Because their computational cost grows with the number of frequency bins, these models often use the band-split (BS) module in high-sampling-rate tasks such as music source separation (MSS) and cinematic audio source separation (CASS). The BS encoder compresses frequency information by encoding features for each predefined subband. It achieves effective compression by introducing an inductive bias that places greater emphasis on low-frequency parts. Despite its success, the BS module has two inherent limitations: (i) it is not input-adaptive, preventing the use of input-dependent information, and (ii) the parameter count is large, since each subband requires a dedicated module. To address these issues, we propose Spectral Feature Compression (SFC). SFC compresses the input using a single sequence modeling module, making it both input-adaptive and parameter-efficient. We investigate two variants of SFC, one based on cross-attention and the other on Mamba, and introduce inductive biases inspired by the BS module to make them suitable for frequency information compression. Experiments on MSS and CASS tasks demonstrate that the SFC module consistently outperforms the BS module across different separator sizes and compression ratios. We also provide an analysis showing that SFC adaptively captures frequency patterns from the input.",
      "url": "https://arxiv.org/abs/2602.08671",
      "pdfUrl": "https://arxiv.org/pdf/2602.08671.pdf",
      "titleJa": "音源分離のためのシーケンスモデリングによる入力適応型スペクトル特徴圧縮"
    },
    {
      "id": "2602.09070",
      "arxivId": "2602.09070",
      "title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
      "authors": [
        "Yufan Wen",
        "Zhaocheng Liu",
        "YeGuo Hua",
        "Ziyi Guo",
        "Lihua Zhang",
        "Chun Yuan",
        "Jian Wu"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a \\textit{Global Semantic Anchor} ensures stylistic stability, while a surgical \\textit{Token-Level Affective Adapter} modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation.",
      "url": "https://arxiv.org/abs/2602.09070",
      "pdfUrl": "https://arxiv.org/pdf/2602.09070.pdf",
      "titleJa": "NarraScore: 階層的感情制御による視覚的物語と音楽的ダイナミクスの橋渡し"
    },
    {
      "id": "2602.08233",
      "arxivId": "2602.08233",
      "title": "Tutti: Expressive Multi-Singer Synthesis via Structure-Level Timbre Control and Vocal Texture Modeling",
      "authors": [
        "Jiatao Chen",
        "Xing Tang",
        "Xiaoyue Duan",
        "Yutang Feng",
        "Jinchao Zhang",
        "Jie Zhou"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "While existing Singing Voice Synthesis systems achieve high-fidelity solo performances, they are constrained by global timbre control, failing to address dynamic multi-singer arrangement and vocal texture within a single song. To address this, we propose Tutti, a unified framework designed for structured multi-singer generation. Specifically, we introduce a Structure-Aware Singer Prompt to enable flexible singer scheduling evolving with musical structure, and propose Complementary Texture Learning via Condition-Guided VAE to capture implicit acoustic textures (e.g., spatial reverberation and spectral fusion) that are complementary to explicit controls. Experiments demonstrate that Tutti excels in precise multi-singer scheduling and significantly enhances the acoustic realism of choral generation, offering a novel paradigm for complex multi-singer arrangement. Audio samples are available at https://annoauth123-ctrl.github.io/Tutii_Demo/.",
      "url": "https://arxiv.org/abs/2602.08233",
      "pdfUrl": "https://arxiv.org/pdf/2602.08233.pdf",
      "titleJa": "Tutti: 構造レベルの音色制御とボーカルテクスチャモデリングによる表現力豊かなマルチシンガー合成"
    },
    {
      "id": "2602.08148",
      "arxivId": "2602.08148",
      "title": "SNC: A Stem-Native Codec for Efficient Lossless Audio Storage with Adaptive Playback Capabilities",
      "authors": [
        "Shaad Sufi"
      ],
      "publishedDate": "2026-02-08",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Current audio formats present a fundamental trade-off between file size and functionality: lossless formats like FLAC preserve quality but lack adaptability, while lossy formats reduce size at the cost of fidelity and offer no stem-level access.We introduce the Stem-Native Codec (SNC), a novel audio container format that stores music as independently encoded stems plus a low-energy mastering residual. By exploiting the lower information entropy of separated stems compared to mixed audio, SNC achieves a 38.2% file size reduction versus FLAC (7.76 MB vs. 12.55 MB for a 2:18 test track) while maintaining perceptual transparency (STOI = 0.996). Unlike existing formats, SNC enables context-aware adaptive playback, spatial audio rendering, and user-controlled remixing without requiring additional storage. Our experimental validation demonstrates that the stems-plus residual architecture successfully decouples the conflicting requirements of compression efficiency and feature richness, offering a practical path toward next-generation audio distribution systems.",
      "url": "https://arxiv.org/abs/2602.08148",
      "pdfUrl": "https://arxiv.org/pdf/2602.08148.pdf",
      "titleJa": "SNC: 適応型再生機能を備えた効率的なロスレスオーディオストレージのためのステムネイティブコーデック"
    },
    {
      "id": "2602.07803",
      "arxivId": "2602.07803",
      "title": "SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis",
      "authors": [
        "Jiale Qian",
        "Hao Meng",
        "Tian Zheng",
        "Pengcheng Zhu",
        "Haopeng Lin",
        "Yuhang Dai",
        "Hanke Xie",
        "Wenxiao Cao",
        "Ruixuan Shang",
        "Jun Wu",
        "Hongmei Liu",
        "Hanlin Wen",
        "Jian Zhao",
        "Zhonglin Jiang",
        "Yong Chen",
        "Shunshun Yin",
        "Ming Tao",
        "Jianguo Wei",
        "Lei Xie",
        "Xinsheng Wang"
      ],
      "publishedDate": "2026-02-08",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.",
      "url": "https://arxiv.org/abs/2602.07803",
      "pdfUrl": "https://arxiv.org/pdf/2602.07803.pdf",
      "titleJa": "SoulX-Singer: 高品質なゼロショット歌声合成に向けて"
    },
    {
      "id": "2602.06917",
      "arxivId": "2602.06917",
      "title": "Automatic Detection and Analysis of Singing Mistakes for Music Pedagogy",
      "authors": [
        "Sumit Kumar",
        "Suraj Jaiswal",
        "Parampreet Singh",
        "Vipul Arora"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "The advancement of machine learning in audio analysis has opened new possibilities for technology-enhanced music education. This paper introduces a framework for automatic singing mistake detection in the context of music pedagogy, supported by a newly curated dataset. The dataset comprises synchronized teacher learner vocal recordings, with annotations marking different types of mistakes made by learners. Using this dataset, we develop different deep learning models for mistake detection and benchmark them. To compare the efficacy of mistake detection systems, a new evaluation methodology is proposed. Experiments indicate that the proposed learning-based methods are superior to rule-based methods. A systematic study of errors and a cross-teacher study reveal insights into music pedagogy that can be utilised for various music applications. This work sets out new directions of research in music pedagogy. The codes and dataset are publicly available.",
      "url": "https://arxiv.org/abs/2602.06917",
      "pdfUrl": "https://arxiv.org/pdf/2602.06917.pdf",
      "titleJa": "音楽教育のための歌唱ミスの自動検出と分析"
    },
    {
      "id": "2602.06823",
      "arxivId": "2602.06823",
      "title": "AI-Generated Music Detection in Broadcast Monitoring",
      "authors": [
        "David Lopez-Ayala",
        "Asier Cabello",
        "Pablo Zinemanas",
        "Emilio Molina",
        "Martin Rocamora"
      ],
      "publishedDate": "2026-02-06",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "AI music generators have advanced to the point where their outputs are often indistinguishable from human compositions. While detection methods have emerged, they are typically designed and validated in music streaming contexts with clean, full-length tracks. Broadcast audio, however, poses a different challenge: music appears as short excerpts, often masked by dominant speech, conditions under which existing detectors fail. In this work, we introduce AI-OpenBMAT, the first dataset tailored to broadcast-style AI-music detection. It contains 3,294 one-minute audio excerpts (54.9 hours) that follow the duration patterns and loudness relations of real television audio, combining human-made production music with stylistically matched continuations generated with Suno v3.5. We benchmark a CNN baseline and state-of-the-art SpectTTTra models to assess SNR and duration robustness, and evaluate on a full broadcast scenario. Across all settings, models that excel in streaming scenarios suffer substantial degradation, with F1-scores dropping below 60% when music is in the background or has a short duration. These results highlight speech masking and short music length as critical open challenges for AI music detection, and position AI-OpenBMAT as a benchmark for developing detectors capable of meeting industrial broadcast requirements.",
      "url": "https://arxiv.org/abs/2602.06823",
      "pdfUrl": "https://arxiv.org/pdf/2602.06823.pdf",
      "titleJa": "放送監視におけるAI生成音楽検出"
    },
    {
      "id": "2602.07063",
      "arxivId": "2602.07063",
      "title": "Video-based Music Generation",
      "authors": [
        "Serkan Sulun"
      ],
      "publishedDate": "2026-02-05",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called \"boundary offset encodings,\" aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.",
      "url": "https://arxiv.org/abs/2602.07063",
      "pdfUrl": "https://arxiv.org/pdf/2602.07063.pdf",
      "titleJa": "ビデオベースの音楽生成"
    },
    {
      "id": "2602.14664",
      "arxivId": "2602.14664",
      "title": "Probing Human Articulatory Constraints in End-to-End TTS with Reverse and Mismatched Speech-Text Directions",
      "authors": [
        "Parth Khadse",
        "Sunil Kumar Kopparapu"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "cs.SD"
      ],
      "abstract": "An end-to-end (e2e) text-to-speech (TTS) system is a deep architecture that learns to associate a text string with acoustic speech patterns from a curated dataset. It is expected that all aspects associated with speech production, such as phone duration, speaker characteristics, and intonation among other things are captured in the trained TTS model to enable the synthesized speech to be natural and intelligible. Human speech is complex, involving smooth transitions between articulatory configurations (ACs). Due to anatomical constraints, some ACs are challenging to mimic or transition between. In this paper, we experimentally study if the constraints imposed by human anatomy have an implication on training an e2e-TTS systems. We experiment with two e2e-TTS architectures, namely, Tacotron-2 an autoregressive model and VITS-TTS a non-autoregressive model. In this study, we build TTS systems using (a) forward text, forward speech (conventional, e2e-TTS), (b) reverse text, reverse speech (r-e2e-TTS), and (c) reverse text, forward speech (rtfs-e2e-TTS). Experiments demonstrate that e2e-TTS systems are purely data-driven. Interestingly, the generated speech by r-e2e-TTS systems exhibits better fidelity, better perceptual intelligibility, and better naturalness",
      "url": "https://arxiv.org/abs/2602.14664",
      "pdfUrl": "https://arxiv.org/pdf/2602.14664.pdf",
      "titleJa": "逆方向および不一致な音声テキスト方向を持つエンドツーエンドTTSにおける人間の調音制約の調査"
    },
    {
      "id": "2602.15082",
      "arxivId": "2602.15082",
      "title": "S-PRESSO: Ultra Low Bitrate Sound Effect Compression With Diffusion Autoencoders And Offline Quantization",
      "authors": [
        "Zineb Lahrichi",
        "Gaëtan Hadjeres",
        "Gaël Richard",
        "Geoffroy Peeters"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM"
      ],
      "abstract": "Neural audio compression models have recently achieved extreme compression rates, enabling efficient latent generative modeling. Conversely, latent generative models have been applied to compression, pushing the limits of continuous and discrete approaches. However, existing methods remain constrained to low-resolution audio and degrade substantially at very low bitrates, where audible artifacts are prominent. In this paper, we present S-PRESSO, a 48kHz sound effect compression model that produces both continuous and discrete embeddings at ultra-low bitrates, down to 0.096 kbps, via offline quantization. Our model relies on a pretrained latent diffusion model to decode compressed audio embeddings learned by a latent encoder. Leveraging the generative priors of the diffusion decoder, we achieve extremely low frame rates, down to 1Hz (750x compression rate), producing convincing and realistic reconstructions at the cost of exact fidelity. Despite operating at high compression rates, we demonstrate that S-PRESSO outperforms both continuous and discrete baselines in audio quality, acoustic similarity and reconstruction metrics.",
      "url": "https://arxiv.org/abs/2602.15082",
      "pdfUrl": "https://arxiv.org/pdf/2602.15082.pdf",
      "titleJa": "S-PRESSO: 拡散オートエンコーダとオフライン量子化による超低ビットレート効果音圧縮"
    },
    {
      "id": "2602.14612",
      "arxivId": "2602.14612",
      "title": "LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio",
      "authors": [
        "Naveen Vakada",
        "Kartik Hegde",
        "Arvind Krishna Sridhar",
        "Yinyi Guo",
        "Erik Visser"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Long-duration audio is increasingly common in industrial and consumer settings, yet reviewing multi-hour recordings is impractical, motivating systems that answer natural-language queries with precise temporal grounding and minimal hallucination. Existing audio-language models show promise, but long-audio question answering remains difficult due to context-length limits. We introduce LongAudio-RAG (LA-RAG), a hybrid framework that grounds Large Language Model (LLM) outputs in retrieved, timestamped acoustic event detections rather than raw audio. Multi-hour streams are converted into structured event records stored in an SQL database, and at inference time the system resolves natural-language time references, classifies intent, retrieves only the relevant events, and generates answers using this constrained evidence. To evaluate performance, we construct a synthetic long-audio benchmark by concatenating recordings with preserved timestamps and generating template-based question-answer pairs for detection, counting, and summarization tasks. Finally, we demonstrate the practicality of our approach by deploying it in a hybrid edge-cloud environment, where the audio grounding model runs on-device on IoT-class hardware while the LLM is hosted on a GPU-backed server. This architecture enables low-latency event extraction at the edge and high-quality language reasoning in the cloud. Experiments show that structured, event-level retrieval significantly improves accuracy compared to vanilla Retrieval-Augmented Generation (RAG) or text-to-SQL approaches.",
      "url": "https://arxiv.org/abs/2602.14612",
      "pdfUrl": "https://arxiv.org/pdf/2602.14612.pdf",
      "titleJa": "LongAudio-RAG: 数時間にわたる音声によるイベントベースの質問応答"
    },
    {
      "id": "2602.14560",
      "arxivId": "2602.14560",
      "title": "Preliminary sonification of ENSO using traditional Javanese gamelan scales",
      "authors": [
        "Sandy H. S. Herho",
        "Rusmawan Suwarman",
        "Nurjanna J. Trilaksono",
        "Iwan P. Anwar",
        "Faiz R. Fajary"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "physics.soc-ph",
        "cs.SD",
        "physics.ao-ph"
      ],
      "abstract": "Sonification -- the mapping of data to non-speech audio -- offers an underexplored channel for representing complex dynamical systems. We treat El Niño-Southern Oscillation (ENSO), a canonical example of low-dimensional climate chaos, as a test case for culturally-situated sonification evaluated through complex systems diagnostics. Using parameter-mapping sonification of the Niño 3.4 sea surface temperature anomaly index (1870--2024), we encode ENSO variability into two traditional Javanese gamelan pentatonic systems (pelog and slendro) across four composition strategies, then analyze the resulting audio as trajectories in a two-dimensional acoustic phase space. Recurrence-based diagnostics, convex hull geometry, and coupling analysis reveal that the sonification pipeline preserves key dynamical signatures: alternating modes produce the highest trajectory recurrence rates, echoing ENSO's quasi-periodicity; layered polyphonic modes explore the broadest phase space regions; and the two scale families induce qualitatively distinct coupling regimes between spectral brightness and energy -- predominantly anti-phase in pelog but near-independent in slendro. Phase space trajectory analysis provides a rigorous geometric framework for comparing sonification designs within a complex systems context. Perceptual validation remains necessary; we contribute the dynamical systems methodology for evaluating such mappings.",
      "url": "https://arxiv.org/abs/2602.14560",
      "pdfUrl": "https://arxiv.org/pdf/2602.14560.pdf",
      "titleJa": "伝統的なジャワガムラン音階を用いたENSOの予備的な音響化"
    },
    {
      "id": "2602.14172",
      "arxivId": "2602.14172",
      "title": "Investigation for Relative Voice Impression Estimation",
      "authors": [
        "Kenichi Fujita",
        "Yusuke Ijima"
      ],
      "publishedDate": "2026-02-15",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "Paralinguistic and non-linguistic aspects of speech strongly influence listener impressions. While most research focuses on absolute impression scoring, this study investigates relative voice impression estimation (RIE), a framework for predicting the perceptual difference between two utterances from the same speaker. The estimation target is a low-dimensional vector derived from subjective evaluations, quantifying the perceptual shift of the second utterance relative to the first along an antonymic axis (e.g., ``Dark--Bright''). To isolate expressive and prosodic variation, we used recordings of a professional speaker reading a text in various styles. We compare three modeling approaches: classical acoustic features commonly used for speech emotion recognition, self-supervised speech representations, and multimodal large language models (MLLMs). Our results demonstrate that models using self-supervised representations outperform methods with classical acoustic features, particularly in capturing complex and dynamic impressions (e.g., ``Cold--Warm'') where classical features fail. In contrast, current MLLMs prove unreliable for this fine-grained pairwise task. This study provides the first systematic investigation of RIE and demonstrates the strength of self-supervised speech models in capturing subtle perceptual variations.",
      "url": "https://arxiv.org/abs/2602.14172",
      "pdfUrl": "https://arxiv.org/pdf/2602.14172.pdf",
      "titleJa": "相対的な音声印象推定に関する調査"
    },
    {
      "id": "2602.13891",
      "arxivId": "2602.13891",
      "title": "GSRM: Generative Speech Reward Model for Speech RLHF",
      "authors": [
        "Maohao Shen",
        "Tejas Jayashankar",
        "Osama Hanna",
        "Naoyuki Kanda",
        "Yancheng Wang",
        "Kateřina Žmolíková",
        "Ruiming Xie",
        "Niko Moritz",
        "Anfeng Xu",
        "Yashesh Gaur",
        "Gregory Wornell",
        "Qing He",
        "Jilong Wu"
      ],
      "publishedDate": "2026-02-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions. Experiments show that GSRM substantially outperforms existing speech naturalness predictors, achieving model-human correlation of naturalness score prediction that approaches human inter-rater consistency. We further show how GSRM can improve the naturalness of speech LLM generations by serving as an effective verifier for online RLHF.",
      "url": "https://arxiv.org/abs/2602.13891",
      "pdfUrl": "https://arxiv.org/pdf/2602.13891.pdf",
      "titleJa": "GSRM: 音声RLHFのための生成音声報酬モデル"
    },
    {
      "id": "2602.13835",
      "arxivId": "2602.13835",
      "title": "Audiocards: Structured Metadata Improves Audio Language Models For Sound Design",
      "authors": [
        "Sripathi Sridhar",
        "Prem Seetharaman",
        "Oriol Nieto",
        "Mark Cartwright",
        "Justin Salamon"
      ],
      "publishedDate": "2026-02-14",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Sound designers search for sounds in large sound effects libraries using aspects such as sound class or visual context. However, the metadata needed for such search is often missing or incomplete, and requires significant manual effort to add. Existing solutions to automate this task by generating metadata, i.e. captioning, and search using learned embeddings, i.e. text-audio retrieval, are not trained on metadata with the structure and information pertinent to sound design. To this end we propose audiocards, structured metadata grounded in acoustic attributes and sonic descriptors, by exploiting the world knowledge of LLMs. We show that training on audiocards improves downstream text-audio retrieval, descriptive captioning, and metadata generation on professional sound effects libraries. Moreover, audiocards also improve performance on general audio captioning and retrieval over the baseline single-sentence captioning approach. We release a curated dataset of sound effects audiocards to invite further research in audio language modeling for sound design.",
      "url": "https://arxiv.org/abs/2602.13835",
      "pdfUrl": "https://arxiv.org/pdf/2602.13835.pdf",
      "titleJa": "オーディオカード：構造化メタデータがサウンドデザインのためのオーディオ言語モデルを改善"
    },
    {
      "id": "2602.13685",
      "arxivId": "2602.13685",
      "title": "AuTAgent: A Reinforcement Learning Framework for Tool-Augmented Audio Reasoning",
      "authors": [
        "Siqian Tong",
        "Xuan Li",
        "Yiwei Wang",
        "Baolong Bi",
        "Yujun Cai",
        "Shenghua Liu",
        "Yuchen He",
        "Chengpeng Hao"
      ],
      "publishedDate": "2026-02-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Large Audio Language Models (LALMs) excel at perception but struggle with complex reasoning requiring precise acoustic measurements. While external tools can extract fine-grained features like exact tempo or pitch, effective integration remains challenging: naively using all tools causes information overload, while prompt-based selection fails to assess context-dependent utility. To address this, we propose AuTAgent (Audio Tool Agent), a reinforcement learning framework that learns when and which tools to invoke. By employing a sparse-feedback training strategy with a novel Differential Reward mechanism, the agent learns to filter out irrelevant tools and invokes external assistance only when it yields a net performance gain over the base model. Experimental results confirm that AuTAgent complements the representation bottleneck of LALMs by providing verifiable acoustic evidence. It improves accuracy by 4.20% / 6.20% and 9.80% / 8.00% for open-source and closed-source backbones on the MMAU Test-mini and the MMAR benchmarks, respectively. In addition, further experiments demonstrate exceptional transferability. We highlight the complementary role of external tools in augmenting audio model reasoning.",
      "url": "https://arxiv.org/abs/2602.13685",
      "pdfUrl": "https://arxiv.org/pdf/2602.13685.pdf",
      "titleJa": "AuTAgent: ツール拡張型音声推論のための強化学習フレームワーク"
    }
  ],
  "lastUpdated": "2026-02-22T01:06:39.625230",
  "totalCount": 74
}