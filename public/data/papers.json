{
  "papers": [
    {
      "id": "2601.16793",
      "arxivId": "2601.16793",
      "title": "A Novel Transfer Learning Approach for Mental Stability Classification from Voice Signal",
      "authors": [
        "Rafiul Islam",
        "Md. Taimur Ahad"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.NE",
        "eess.AS"
      ],
      "abstract": "This study presents a novel transfer learning approach and data augmentation technique for mental stability classification using human voice signals and addresses the challenges associated with limited data availability. Convolutional neural networks (CNNs) have been employed to analyse spectrogram images generated from voice recordings. Three CNN architectures, VGG16, InceptionV3, and DenseNet121, were evaluated across three experimental phases: training on non-augmented data, augmented data, and transfer learning. This proposed transfer learning approach involves pre-training models on the augmented dataset and fine-tuning them on the non-augmented dataset while ensuring strict data separation to prevent data leakage. The results demonstrate significant improvements in classification performance compared to the baseline approach. Among three CNN architectures, DenseNet121 achieved the highest accuracy of 94% and an AUC score of 99% using the proposed transfer learning approach. This finding highlights the effectiveness of combining data augmentation and transfer learning to enhance CNN-based classification of mental stability using voice spectrograms, offering a promising non-invasive tool for mental health diagnostics.",
      "url": "https://arxiv.org/abs/2601.16793",
      "pdfUrl": "https://arxiv.org/pdf/2601.16793.pdf",
      "titleJa": "音声信号からの精神安定性分類のための新しい転移学習アプローチ"
    },
    {
      "id": "2601.16774",
      "arxivId": "2601.16774",
      "title": "E2E-AEC: Implementing an end-to-end neural network learning approach for acoustic echo cancellation",
      "authors": [
        "Yiheng Jiang",
        "Biao Tian",
        "Haoxu Wang",
        "Shengkui Zhao",
        "Bin Ma",
        "Daren Chen",
        "Xiangang Li"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We propose a novel neural network-based end-to-end acoustic echo cancellation (E2E-AEC) method capable of streaming inference, which operates effectively without reliance on traditional linear AEC (LAEC) techniques and time delay estimation. Our approach includes several key strategies: First, we introduce and refine progressive learning to gradually enhance echo suppression. Second, our model employs knowledge transfer by initializing with a pre-trained LAECbased model, harnessing the insights gained from LAEC training. Third, we optimize the attention mechanism with a loss function applied on attention weights to achieve precise time alignment between the reference and microphone signals. Lastly, we incorporate voice activity detection to enhance speech quality and improve echo removal by masking the network output when near-end speech is absent. The effectiveness of our approach is validated through experiments conducted on public datasets.",
      "url": "https://arxiv.org/abs/2601.16774",
      "pdfUrl": "https://arxiv.org/pdf/2601.16774.pdf",
      "titleJa": "E2E-AEC: 音響エコーキャンセルのためのエンドツーエンドのニューラルネットワーク学習アプローチの実装"
    },
    {
      "id": "2601.16675",
      "arxivId": "2601.16675",
      "title": "I Guess That's Why They Call it the Blues: Causal Analysis for Audio Classifiers",
      "authors": [
        "David A. Kelly",
        "Hana Chockler"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "It is well-known that audio classifiers often rely on non-musically relevant features and spurious correlations to classify audio. Hence audio classifiers are easy to manipulate or confuse, resulting in wrong classifications. While inducing a misclassification is not hard, until now the set of features that the classifiers rely on was not well understood. In this paper we introduce a new method that uses causal reasoning to discover features of the frequency space that are sufficient and necessary for a given classification. We describe an implementation of this algorithm in the tool FreqReX and provide experimental results on a number of standard benchmark datasets. Our experiments show that causally sufficient and necessary subsets allow us to manipulate the outputs of the models in a variety of ways by changing the input very slightly. Namely, a change to one out of 240,000 frequencies results in a change in classification 58% of the time, and the change can be so small that it is practically inaudible. These results show that causal analysis is useful for understanding the reasoning process of audio classifiers and can be used to successfully manipulate their outputs.",
      "url": "https://arxiv.org/abs/2601.16675",
      "pdfUrl": "https://arxiv.org/pdf/2601.16675.pdf",
      "titleJa": "ブルースと呼ばれる理由：音声分類器の因果分析"
    },
    {
      "id": "2601.16603",
      "arxivId": "2601.16603",
      "title": "Omni-directional attention mechanism based on Mamba for speech separation",
      "authors": [
        "Ke Xue",
        "Chang Sun",
        "Rongfei Fan",
        "Jing Wang",
        "Han Hu"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Mamba, a selective state-space model (SSM), has emerged as an efficient alternative to Transformers for speech modeling, enabling long-sequence processing with linear complexity. While effective in speech separation, existing approaches, whether in the time or time-frequency domain, typically decompose the input along a single dimension into short one-dimensional sequences before processing them with Mamba, which restricts it to local 1D modeling and limits its ability to capture global dependencies across the 2D spectrogram. In this work, we propose an efficient omni-directional attention (OA) mechanism built upon unidirectional Mamba, which models global dependencies from ten different directions on the spectrogram. We expand the proposed mechanism into two baseline separation models and evaluate on three public datasets. Experimental results show that our approach consistently achieves significant performance gains over the baselines while preserving linear complexity, outperforming existing state-of-the-art (SOTA) systems.",
      "url": "https://arxiv.org/abs/2601.16603",
      "pdfUrl": "https://arxiv.org/pdf/2601.16603.pdf",
      "titleJa": "音声分離のためのMambaに基づく全方向注意メカニズム"
    },
    {
      "id": "2601.16547",
      "arxivId": "2601.16547",
      "title": "CORD: Bridging the Audio-Text Reasoning Gap via Weighted On-policy Cross-modal Distillation",
      "authors": [
        "Jing Hu",
        "Danxiang Zhu",
        "Xianlong Luo",
        "Dan Zhang",
        "Shuwei He",
        "Yishu Lei",
        "Haitao Zheng",
        "Shikun Feng",
        "Jingzhou He",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Large Audio Language Models (LALMs) have garnered significant research interest. Despite being built upon text-based large language models (LLMs), LALMs frequently exhibit a degradation in knowledge and reasoning capabilities. We hypothesize that this limitation stems from the failure of current training paradigms to effectively bridge the acoustic-semantic gap within the feature representation space. To address this challenge, we propose CORD, a unified alignment framework that performs online cross-modal self-distillation. Specifically, it aligns audio-conditioned reasoning with its text-conditioned counterpart within a unified model. Leveraging the text modality as an internal teacher, CORD performs multi-granularity alignment throughout the audio rollout process. At the token level, it employs on-policy reverse KL divergence with importance-aware weighting to prioritize early and semantically critical tokens. At the sequence level, CORD introduces a judge-based global reward to optimize complete reasoning trajectories via Group Relative Policy Optimization (GRPO). Empirical results across multiple benchmarks demonstrate that CORD consistently enhances audio-conditioned reasoning and substantially bridges the audio-text performance gap with only 80k synthetic training samples, validating the efficacy and data efficiency of our on-policy, multi-level cross-modal alignment approach.",
      "url": "https://arxiv.org/abs/2601.16547",
      "pdfUrl": "https://arxiv.org/pdf/2601.16547.pdf",
      "titleJa": "CORD: 重み付けされたポリシーに基づくクロスモーダル蒸留による音声テキスト推論ギャップの橋渡し"
    },
    {
      "id": "2601.16540",
      "arxivId": "2601.16540",
      "title": "Do Models Hear Like Us? Probing the Representational Alignment of Audio LLMs and Naturalistic EEG",
      "authors": [
        "Haoyun Yang",
        "Xin Xiao",
        "Jiang Zhong",
        "Yu Tian",
        "Dong Xiaohua",
        "Yu Mao",
        "Hao Wu",
        "Kaiwen Wei"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Audio Large Language Models (Audio LLMs) have demonstrated strong capabilities in integrating speech perception with language understanding. However, whether their internal representations align with human neural dynamics during naturalistic listening remains largely unexplored. In this work, we systematically examine layer-wise representational alignment between 12 open-source Audio LLMs and Electroencephalogram (EEG) signals across 2 datasets. Specifically, we employ 8 similarity metrics, such as Spearman-based Representational Similarity Analysis (RSA), to characterize within-sentence representational geometry. Our analysis reveals 3 key findings: (1) we observe a rank-dependence split, in which model rankings vary substantially across different similarity metrics; (2) we identify spatio-temporal alignment patterns characterized by depth-dependent alignment peaks and a pronounced increase in RSA within the 250-500 ms time window, consistent with N400-related neural dynamics; (3) we find an affective dissociation whereby negative prosody, identified using a proposed Tri-modal Neighborhood Consistency (TNC) criterion, reduces geometric similarity while enhancing covariance-based dependence. These findings provide new neurobiological insights into the representational mechanisms of Audio LLMs.",
      "url": "https://arxiv.org/abs/2601.16540",
      "pdfUrl": "https://arxiv.org/pdf/2601.16540.pdf",
      "titleJa": "モデルは人間と同じように聞こえるか？音声LLMと自然脳波の表象的整合を探る"
    },
    {
      "id": "2601.16442",
      "arxivId": "2601.16442",
      "title": "Auditory Attention Decoding without Spatial Information: A Diotic EEG Study",
      "authors": [
        "Masahiro Yoshino",
        "Haruki Yokota",
        "Junya Hara",
        "Yuichi Tanaka",
        "Hiroshi Higashi"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "eess.SP",
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Auditory attention decoding (AAD) identifies the attended speech stream in multi-speaker environments by decoding brain signals such as electroencephalography (EEG). This technology is essential for realizing smart hearing aids that address the cocktail party problem and for facilitating objective audiometry systems. Existing AAD research mainly utilizes dichotic environments where different speech signals are presented to the left and right ears, enabling models to classify directional attention rather than speech content. However, this spatial reliance limits applicability to real-world scenarios, such as the \"cocktail party\" situation, where speakers overlap or move dynamically. To address this challenge, we propose an AAD framework for diotic environments where identical speech mixtures are presented to both ears, eliminating spatial cues. Our approach maps EEG and speech signals into a shared latent space using independent encoders. We extract speech features using wav2vec 2.0 and encode them with a 2-layer 1D convolutional neural network (CNN), while employing the BrainNetwork architecture for EEG encoding. The model identifies the attended speech by calculating the cosine similarity between EEG and speech representations. We evaluate our method on a diotic EEG dataset and achieve 72.70% accuracy, which is 22.58% higher than the state-of-the-art direction-based AAD method.",
      "url": "https://arxiv.org/abs/2601.16442",
      "pdfUrl": "https://arxiv.org/pdf/2601.16442.pdf",
      "titleJa": "空間情報なしの聴覚注意のデコード：双極性障害の脳波研究"
    },
    {
      "id": "2601.16358",
      "arxivId": "2601.16358",
      "title": "TidyVoice: A Curated Multilingual Dataset for Speaker Verification Derived from Common Voice",
      "authors": [
        "Aref Farhadipour",
        "Jan Marquenie",
        "Srikanth Madikeri",
        "Eleanor Chodroff"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "The development of robust, multilingual speaker recognition systems is hindered by a lack of large-scale, publicly available and multilingual datasets, particularly for the read-speech style crucial for applications like anti-spoofing. To address this gap, we introduce the TidyVoice dataset derived from the Mozilla Common Voice corpus after mitigating its inherent speaker heterogeneity within the provided client IDs. TidyVoice currently contains training and test data from over 212,000 monolingual speakers (Tidy-M) and around 4,500 multilingual speakers (Tidy-X) from which we derive two distinct conditions. The Tidy-M condition contains target and non-target trials from monolingual speakers across 81 languages. The Tidy-X condition contains target and non-target trials from multilingual speakers in both same- and cross-language trials. We employ two architectures of ResNet models, achieving a 0.35% EER by fine-tuning on our comprehensive Tidy-M partition. Moreover, we show that this fine-tuning enhances the model's generalization, improving performance on unseen conversational interview data from the CANDOR corpus. The complete dataset, evaluation trials, and our models are publicly released to provide a new resource for the community.",
      "url": "https://arxiv.org/abs/2601.16358",
      "pdfUrl": "https://arxiv.org/pdf/2601.16358.pdf",
      "titleJa": "TidyVoice: Common Voice から派生した話者認証のための多言語データセット"
    },
    {
      "id": "2601.16316",
      "arxivId": "2601.16316",
      "title": "EdgeSpot: Efficient and High-Performance Few-Shot Model for Keyword Spotting",
      "authors": [
        "Oguzhan Buyuksolak",
        "Alican Gok",
        "Osman Erman Okman"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "We introduce an efficient few-shot keyword spotting model for edge devices, EdgeSpot, that pairs an optimized version of a BC-ResNet-based acoustic backbone with a trainable Per-Channel Energy Normalization frontend and lightweight temporal self-attention. Knowledge distillation is utilized during training by employing a self-supervised teacher model, optimized with Sub-center ArcFace loss. This study demonstrates that the EdgeSpot model consistently provides better accuracy at a fixed false-alarm rate (FAR) than strong BC-ResNet baselines. The largest variant, EdgeSpot-4, improves the 10-shot accuracy at 1% FAR from 73.7% to 82.0%, which requires only 29.4M MACs with 128k parameters.",
      "url": "https://arxiv.org/abs/2601.16316",
      "pdfUrl": "https://arxiv.org/pdf/2601.16316.pdf",
      "titleJa": "EdgeSpot: キーワードスポッティングのための効率的で高性能な少数ショットモデル"
    },
    {
      "id": "2601.16273",
      "arxivId": "2601.16273",
      "title": "The CMU-AIST submission for the ICME 2025 Audio Encoder Challenge",
      "authors": [
        "Shikhar Bharadwaj",
        "Samuele Cornell",
        "Kwanghee Choi",
        "Hye-jin Shim",
        "Soham Deshmukh",
        "Satoru Fukayama",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This technical report describes our submission to the ICME 2025 audio encoder challenge. Our submitted system is built on BEATs, a masked speech token prediction based audio encoder. We extend the BEATs model using 74,000 hours of data derived from various speech, music, and sound corpora and scale its architecture upto 300 million parameters. We experiment with speech-heavy and balanced pre-training mixtures to study the impact of different domains on final performance. Our submitted system consists of an ensemble of the Dasheng 1.2 billion model with two custom scaled-up BEATs models trained on the aforementioned pre-training data mixtures. We also propose a simple ensembling technique that retains the best capabilities of constituent models and surpasses both the baseline and Dasheng 1.2B. For open science, we publicly release our trained checkpoints via huggingface at https://huggingface.co/shikhar7ssu/OpenBEATs-ICME-SOUND and https://huggingface.co/shikhar7ssu/OpenBEATs-ICME.",
      "url": "https://arxiv.org/abs/2601.16273",
      "pdfUrl": "https://arxiv.org/pdf/2601.16273.pdf",
      "titleJa": "ICME 2025オーディオエンコーダチャレンジへのCMU-AISTの応募"
    },
    {
      "id": "2601.16158",
      "arxivId": "2601.16158",
      "title": "Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems",
      "authors": [
        "Prakash Dhungana",
        "Sayed Ahmad Salehi"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions. To address this, we propose a comprehensive framework for continual learning designed to adapt to new domains while maintaining computational efficiency. The proposed pipeline integrates a dual-input Convolutional Neural Network, utilizing both Mel Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features, supported by a multi-stage denoising process, involving discrete wavelet transform and spectral subtraction techniques, plus model and prototype update blocks. Unlike prior methods that restrict updates to specific layers, our approach updates the complete quantized model, made possible due to compact model architecture. A subset of input samples are selected during runtime using class prototypes and confidence-driven filtering, which are then pseudo-labeled and combined with rehearsal buffer for incremental model retraining. Experimental results on noisy test dataset demonstrate the framework's effectiveness, achieving 99.63\\% accuracy on clean data and maintaining robust performance (exceeding 94\\% accuracy) across diverse noisy environments, even at -10 dB Signal-to-Noise Ratio. The proposed framework work confirms that integrating efficient denoising with prototype-based continual learning enables KWS models to operate autonomously and robustly in resource-constrained, dynamic environments.",
      "url": "https://arxiv.org/abs/2601.16158",
      "pdfUrl": "https://arxiv.org/pdf/2601.16158.pdf",
      "titleJa": "リソース制約システムにおける堅牢かつ効率的なキーワードスポッティングのためのドメイン増分継続学習"
    },
    {
      "id": "2601.16150",
      "arxivId": "2601.16150",
      "title": "Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization",
      "authors": [
        "Maximos Kaliakatsos-Papakostas",
        "Dimos Makris",
        "Konstantinos Soiledis",
        "Konstantinos-Theodoros Tsamis",
        "Vassilis Katsouros",
        "Emilios Cambouropoulos"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody-harmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.",
      "url": "https://arxiv.org/abs/2601.16150",
      "pdfUrl": "https://arxiv.org/pdf/2601.16150.pdf",
      "titleJa": "メロディーに注意を払う（交差させる）：単一エンコーダーによるメロディーハーモニーのためのカリキュラムマスキング"
    },
    {
      "id": "2601.16117",
      "arxivId": "2601.16117",
      "title": "Distillation-based Layer Dropping (DLD) Effective End-to-end Framework for Dynamic Speech Networks",
      "authors": [
        "Abdul Hannan",
        "Daniele Falavigna",
        "Shah Nawaz",
        "Mubashir Noman",
        "Markus Schedl",
        "Alessio Brutti"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.CV"
      ],
      "abstract": "Edge devices operate in constrained and varying resource settings, requiring dynamic architectures that can adapt to limitations of the available resources. To meet such demands, layer dropping ($\\mathcal{LD}$) approach is typically used to transform static models into dynamic ones by skipping parts of the network along with reducing overall computational complexity. However, existing $\\mathcal{LD}$ methods greatly impact the dynamic model's performance for low and high dropping cases, deteriorating the performance-computation trade-off. To this end, we propose a distillation-based layer dropping (DLD) framework that effectively combines the capabilities of knowledge distillation and $\\mathcal{LD}$ in an end-to-end fashion, thereby achieving state-of-the-art performance for dynamic speech networks. Comprehensive experimentation utilizing well-known speech recognition methods, including conformer and WavLM, on three public benchmarks demonstrates the effectiveness of our framework, reducing the word error rate by $9.32\\%$ and $2.25\\%$ for high and no dropping cases with $33.3\\%$ reduction in training time.",
      "url": "https://arxiv.org/abs/2601.16117",
      "pdfUrl": "https://arxiv.org/pdf/2601.16117.pdf",
      "titleJa": "蒸留ベースのレイヤードロッピング（DLD）動的音声ネットワークのための効果的なエンドツーエンドフレームワーク"
    },
    {
      "id": "2601.15889",
      "arxivId": "2601.15889",
      "title": "A Stabilized Hybrid Active Noise Control Algorithm of GFANC and FxNLMS with Online Clustering",
      "authors": [
        "Zhengding Luo",
        "Haozhe Ma",
        "Boxiang Wang",
        "Ziyi Yang",
        "Dongyuan Shi",
        "Woon-Seng Gan"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "The Filtered-x Normalized Least Mean Square (FxNLMS) algorithm suffers from slow convergence and a risk of divergence, although it can achieve low steady-state errors after sufficient adaptation. In contrast, the Generative Fixed-Filter Active Noise Control (GFANC) method offers fast response speed, but its lack of adaptability may lead to large steady-state errors. This paper proposes a hybrid GFANC-FxNLMS algorithm to leverage the complementary advantages of both approaches. In the hybrid GFANC-FxNLMS algorithm, GFANC provides a frame-level control filter as an initialization for FxNLMS, while FxNLMS performs continuous adaptation at the sampling rate. Small variations in the GFANC-generated filter may repeatedly reinitialize FxNLMS, interrupting its adaptation process and destabilizing the system. An online clustering module is introduced to avoid unnecessary re-initializations and improve system stability. Simulation results show that the proposed algorithm achieves fast response, very low steady-state error, and high stability, requiring only one pre-trained broadband filter.",
      "url": "https://arxiv.org/abs/2601.15889",
      "pdfUrl": "https://arxiv.org/pdf/2601.15889.pdf",
      "titleJa": "オンラインクラスタリングによるGFANCとFxNLMSの安定化ハイブリッドアクティブノイズ制御アルゴリズム"
    },
    {
      "id": "2601.15872",
      "arxivId": "2601.15872",
      "title": "PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation",
      "authors": [
        "Jaekwon Im",
        "Natalia Polouliakh",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.",
      "url": "https://arxiv.org/abs/2601.15872",
      "pdfUrl": "https://arxiv.org/pdf/2601.15872.pdf",
      "titleJa": "PF-D2M: ユニバーサルなダンス・トゥ・ミュージック生成のためのポーズフリー拡散モデル"
    },
    {
      "id": "2601.15719",
      "arxivId": "2601.15719",
      "title": "U3-xi: Pushing the Boundaries of Speaker Recognition via Incorporating Uncertainty",
      "authors": [
        "Junjie Li",
        "Kong Aik Lee"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD"
      ],
      "abstract": "An utterance-level speaker embedding is typically obtained by aggregating a sequence of frame-level representations. However, in real-world scenarios, individual frames encode not only speaker-relevant information but also various nuisance factors. As a result, different frames contribute unequally to the final utterance-level speaker representation for Automatic Speaker Verification systems. To address this issue, we propose to estimate the inherent uncertainty of each frame and assign adaptive weights accordingly, where frames with higher uncertainty receive lower attention. Based on this idea, we present U3-xi, a comprehensive framework designed to produce more reliable and interpretable uncertainty estimates for speaker embeddings. Specifically, we introduce several strategies for uncertainty supervision. First, we propose speaker-level uncertainty supervision via a Stochastic Variance Loss, where the distance between an utterance embedding and its corresponding speaker centroid serves as a pseudo ground truth for uncertainty learning. Second, we incorporate global-level uncertainty supervision by injecting the predicted uncertainty into the sof tmax scale during training. This adaptive scaling mechanism adjusts the sharpness of the decision boundary according to sample difficulty, providing global guidance. Third, we redesign the uncertainty estimation module by integrating a Transformer encoder with multi-view self-attention, enabling the model to capture rich local and long-range temporal dependencies. Comprehensive experiments demonstrate that U3-xi is model-agnostic and can be seamlessly applied to various speaker encoders. In particular, when applied to ECAPA-TDNN, it achieves 21.1% and 15.57% relative improvements on the VoxCeleb1 test sets in terms of EER and minDCF, respectively.",
      "url": "https://arxiv.org/abs/2601.15719",
      "pdfUrl": "https://arxiv.org/pdf/2601.15719.pdf",
      "titleJa": "U3-xi: 不確実性を組み込むことで話者認識の限界を押し広げる"
    },
    {
      "id": "2601.15676",
      "arxivId": "2601.15676",
      "title": "Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for Edge Audio Systems",
      "authors": [
        "Hengfan Zhang",
        "Yueqian Lin",
        "Hai Helen Li",
        "Yiran Chen"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Deploying Audio-Language Models (Audio-LLMs) on edge infrastructure exposes a persistent tension between perception depth and computational efficiency. Lightweight local models tend to produce passive perception - generic summaries that miss the subtle evidence required for multi-step audio reasoning - while indiscriminate cloud offloading incurs unacceptable latency, bandwidth cost, and privacy risk. We propose CoFi-Agent (Tool-Augmented Coarse-to-Fine Agent), a hybrid architecture targeting edge servers and gateways. It performs fast local perception and triggers conditional forensic refinement only when uncertainty is detected. CoFi-Agent runs an initial single-pass on a local 7B Audio-LLM, then a cloud controller gates difficult cases and issues lightweight plans for on-device tools such as temporal re-listening and local ASR. On the MMAR benchmark, CoFi-Agent improves accuracy from 27.20% to 53.60%, while achieving a better accuracy-efficiency trade-off than an always-on investigation pipeline. Overall, CoFi-Agent bridges the perception gap via tool-enabled, conditional edge-cloud collaboration under practical system constraints.",
      "url": "https://arxiv.org/abs/2601.15676",
      "pdfUrl": "https://arxiv.org/pdf/2601.15676.pdf",
      "titleJa": "認識ギャップを埋める: エッジオーディオシステム向けの軽量な粗密アーキテクチャ"
    },
    {
      "id": "2601.15668",
      "arxivId": "2601.15668",
      "title": "EmotionThinker: Prosody-Aware Reinforcement Learning for Explainable Speech Emotion Reasoning",
      "authors": [
        "Dingdong Wang",
        "Shujie Liu",
        "Tianhua Zhang",
        "Youjun Chen",
        "Jinyu Li",
        "Helen Meng"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Emotional information in speech plays a unique role in multimodal perception. However, current Speech Large Language Models (SpeechLLMs), similar to conventional speech emotion recognition (SER) systems, still treat emotion understanding as a simple classification problem. This provides limited interpretability of predictions, while leaving the LLMs' expressive and reasoning capabilities underutilized. In this work, we take the first step to reformulate SER as a deep reasoning problem through reinforcement learning (RL). We propose EmotionThinker, which is designed to generate accurate emotion predictions with interpretable explanations grounded in fine-grained acoustic cues. To achieve this, we first construct EmotionCoT-35K, an emotional reasoning dataset with Chain-of-Thought annotations and detailed captions. Second, we observe that current SpeechLLMs exhibit weak prosody perception, whereas prosodic cues constitute fundamental signals for interpreting emotions. To address this, we develop the prosody-enhanced foundation model EmotionThinker-Base, and demonstrate that prosody enhancement improves emotion understanding. Third, we introduce Group-Relative-Policy-Optimization with Progressive-Trust-aware-Reasoning-Reward (GRPO-PTR) for RL. Different from standard GRPO, which relies only on rule-based outcome rewards, GRPO-PTR progressively introduces reasoning reward, dynamically adjusts it with a trustworthiness weight reflecting the alignment between reasoning and outcome, and evaluates the overall reasoning quality with a reward model based on multi-dimensional criteria. EmotionThinker outperforms previous state-of-the-art evaluation models both in emotion accuracy and explanation quality, advancing SER toward interpretable multimodal reasoning. Project page: https://github.com/dingdongwang/EmotionThinker",
      "url": "https://arxiv.org/abs/2601.15668",
      "pdfUrl": "https://arxiv.org/pdf/2601.15668.pdf",
      "titleJa": "EmotionThinker: 説明可能な音声感情推論のための韻律を考慮した強化学習"
    },
    {
      "id": "2601.15621",
      "arxivId": "2601.15621",
      "title": "Qwen3-TTS Technical Report",
      "authors": [
        "Hangrui Hu",
        "Xinfa Zhu",
        "Ting He",
        "Dake Guo",
        "Bin Zhang",
        "Xiong Wang",
        "Zhifang Guo",
        "Ziyue Jiang",
        "Hongkun Hao",
        "Zishan Guo",
        "Xinyu Zhang",
        "Pei Zhang",
        "Baosong Yang",
        "Jin Xu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission ($97\\,\\mathrm{ms}$) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.",
      "url": "https://arxiv.org/abs/2601.15621",
      "pdfUrl": "https://arxiv.org/pdf/2601.15621.pdf",
      "titleJa": "Qwen3-TTS 技術レポート"
    },
    {
      "id": "2601.15596",
      "arxivId": "2601.15596",
      "title": "DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice",
      "authors": [
        "Leying Zhang",
        "Tingxiao Zhou",
        "Haiyang Sun",
        "Mengxiao Bi",
        "Yanmin Qian"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "While modern Text-to-Speech (TTS) systems achieve high fidelity for read-style speech, they struggle to generate Autonomous Sensory Meridian Response (ASMR), a specialized, low-intensity speech style essential for relaxation. The inherent challenges include ASMR's subtle, often unvoiced characteristics and the demand for zero-shot speaker adaptation. In this paper, we introduce DeepASMR, the first framework designed for zero-shot ASMR generation. We demonstrate that a single short snippet of a speaker's ordinary, read-style speech is sufficient to synthesize high-fidelity ASMR in their voice, eliminating the need for whispered training data from the target speaker. Methodologically, we first identify that discrete speech tokens provide a soft factorization of ASMR style from speaker timbre. Leveraging this insight, we propose a two-stage pipeline incorporating a Large Language Model (LLM) for content-style encoding and a flow-matching acoustic decoder for timbre reconstruction. Furthermore, we contribute DeepASMR-DB, a comprehensive 670-hour English-Chinese multi-speaker ASMR speech corpus, and introduce a novel evaluation protocol integrating objective metrics, human listening tests, LLM-based scoring and unvoiced speech analysis. Extensive experiments confirm that DeepASMR achieves state-of-the-art naturalness and style fidelity in ASMR generation for anyone of any voice, while maintaining competitive performance on normal speech synthesis.",
      "url": "https://arxiv.org/abs/2601.15596",
      "pdfUrl": "https://arxiv.org/pdf/2601.15596.pdf",
      "titleJa": "DeepASMR: LLMベースのゼロショットASMR音声生成（あらゆる声質の人向け）"
    },
    {
      "id": "2601.16483",
      "arxivId": "2601.16483",
      "title": "FlowSE-GRPO: Training Flow Matching Speech Enhancement via Online Reinforcement Learning",
      "authors": [
        "Haoxu Wang",
        "Biao Tian",
        "Yiheng Jiang",
        "Zexu Pan",
        "Shengkui Zhao",
        "Bin Ma",
        "Daren Chen",
        "Xiangang Li"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Generative speech enhancement offers a promising alternative to traditional discriminative methods by modeling the distribution of clean speech conditioned on noisy inputs. Post-training alignment via reinforcement learning (RL) effectively aligns generative models with human preferences and downstream metrics in domains such as natural language processing, but its use in speech enhancement remains limited, especially for online RL. Prior work explores offline methods like Direct Preference Optimization (DPO); online methods such as Group Relative Policy Optimization (GRPO) remain largely uninvestigated. In this paper, we present the first successful integration of online GRPO into a flow-matching speech enhancement framework, enabling efficient post-training alignment to perceptual and task-oriented metrics with few update steps. Unlike prior GRPO work on Large Language Models, we adapt the algorithm to the continuous, time-series nature of speech and to the dynamics of flow-matching generative models. We show that optimizing a single reward yields rapid metric gains but often induces reward hacking that degrades audio fidelity despite higher scores. To mitigate this, we propose a multi-metric reward optimization strategy that balances competing objectives, substantially reducing overfitting and improving overall performance. Our experiments validate online GRPO for speech enhancement and provide practical guidance for RL-based post-training of generative audio models.",
      "url": "https://arxiv.org/abs/2601.16483",
      "pdfUrl": "https://arxiv.org/pdf/2601.16483.pdf",
      "titleJa": "FlowSE-GRPO: オンライン強化学習によるフローマッチング音声強調のトレーニング"
    },
    {
      "id": "2601.16077",
      "arxivId": "2601.16077",
      "title": "Loose coupling of spectral and spatial models for multi-channel diarization and enhancement of meetings in dynamic environments",
      "authors": [
        "Adrian Meise",
        "Tobias Cord-Landwehr",
        "Christoph Boeddeker",
        "Marc Delcroix",
        "Tomohiro Nakatani",
        "Reinhold Haeb-Umbach"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Sound capture by microphone arrays opens the possibility to exploit spatial, in addition to spectral, information for diarization and signal enhancement, two important tasks in meeting transcription. However, there is no one-to-one mapping of positions in space to speakers if speakers move. Here, we address this by proposing a novel joint spatial and spectral mixture model, whose two submodels are loosely coupled by modeling the relationship between speaker and position index probabilistically. Thus, spatial and spectral information can be jointly exploited, while at the same time allowing for speakers speaking from different positions. Experiments on the LibriCSS data set with simulated speaker position changes show great improvements over tightly coupled subsystems.",
      "url": "https://arxiv.org/abs/2601.16077",
      "pdfUrl": "https://arxiv.org/pdf/2601.16077.pdf",
      "titleJa": "動的環境におけるマルチチャネルダイアリゼーションと会議の強化のためのスペクトルモデルと空間モデルの疎結合"
    },
    {
      "id": "2601.16023",
      "arxivId": "2601.16023",
      "title": "Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs",
      "authors": [
        "Lalaram Arya",
        "Mrinmoy Bhattacharjee",
        "Adarsh C. R.",
        "S. R. Mahadeva Prasanna"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "cs.HC"
      ],
      "abstract": "Direct Speech-to-Speech Translation (S2ST) has gained increasing attention for its ability to translate speech from one language to another, while reducing error propagation and latency inherent in traditional cascaded pipelines. However, existing direct S2ST systems continue to face notable challenges, including instability in semantic-acoustic alignment when parallel speech data is scarce, difficulty in preserving speaker identity, and limited multilingual scalability. In this work, we introduce DS2ST-LM, a scalable, single-stage direct S2ST framework leveraging a multilingual Large Language Model (LLM). The architecture integrates a Whisper speech encoder, a learnable projection module, a Qwen2-0.5B LLM, and a timbre-controlled vocoder. We construct GigaS2S-1000, a 1000-hour bilingual corpus by extending the GigaST dataset with high-fidelity synthetic target speech, and show that this synthetic data alleviates data scarcity to some extent. We investigate two semantic token generation strategies: speech-derived S3 tokens and text-derived tokens generated by a pre-trained LLM, and analyze their impact on training stability and semantic consistency. We further evaluate three projection architectures (Linear, Conv1D-Linear, and Q-Former) and observe that while higher-capacity projectors converge faster, the simple Linear projector achieves higher performance. Extensive experiments demonstrate that DS2ST-LM outperforms traditional cascaded and ST (Qwen-Audio) + TTS baselines across both lexical (BLEU, METEOR) and semantic (BLEURT, COMET) metrics, while extending to multiple language pairs, including French, Spanish, German, Hindi, Bengali, and Urdu. Furthermore, we incorporate timbre-aware speech synthesis to preserve speaker information, enabling DS2ST-LM to surpass prior direct S2ST systems in both speaker similarity and perceptual naturalness.",
      "url": "https://arxiv.org/abs/2601.16023",
      "pdfUrl": "https://arxiv.org/pdf/2601.16023.pdf",
      "titleJa": "音色を考慮したLLMベースの直接音声翻訳は複数の言語ペアに拡張可能"
    },
    {
      "id": "2601.15653",
      "arxivId": "2601.15653",
      "title": "Distributed Multichannel Active Noise Control with Asynchronous Communication",
      "authors": [
        "Junwei Ji",
        "Dongyuan Shi",
        "Boxiang Wang",
        "Ziyi Yang",
        "Haowen Li",
        "Woon-Seng Gan"
      ],
      "publishedDate": "2026-01-22",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "Distributed multichannel active noise control (DMCANC) offers effective noise reduction across large spatial areas by distributing the computational load of centralized control to multiple low-cost nodes. Conventional DMCANC methods, however, typically assume synchronous communication and require frequent data exchange, resulting in high communication overhead. To enhance efficiency and adaptability, this work proposes an asynchronous communication strategy where each node executes a weight-constrained filtered-x LMS (WCFxLMS) algorithm and independently requests communication only when its local noise reduction performance degrades. Upon request, other nodes transmit the weight difference between their local control filter and the center point in WCFxLMS, which are then integrated to update both the control filter and the center point. This design enables nodes to operate asynchronously while preserving cooperative behavior. Simulation results demonstrate that the proposed asynchronous communication DMCANC (ACDMCANC) system maintains effective noise reduction with significantly reduced communication load, offering improved scalability for heterogeneous networks.",
      "url": "https://arxiv.org/abs/2601.15653",
      "pdfUrl": "https://arxiv.org/pdf/2601.15653.pdf",
      "titleJa": "非同期通信による分散型マルチチャネルアクティブノイズコントロール"
    },
    {
      "id": "2601.16240",
      "arxivId": "2601.16240",
      "title": "Test-Time Adaptation for Speech Emotion Recognition",
      "authors": [
        "Jiaheng Dong",
        "Hong Jia",
        "Ting Dang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "The practical utility of Speech Emotion Recognition (SER) systems is undermined by their fragility to domain shifts, such as speaker variability, the distinction between acted and naturalistic emotions, and cross-corpus variations. While domain adaptation and fine-tuning are widely studied, they require either source data or labelled target data, which are often unavailable or raise privacy concerns in SER. Test-time adaptation (TTA) bridges this gap by adapting models at inference using only unlabeled target data. Yet, having been predominantly designed for image classification and speech recognition, the efficacy of TTA for mitigating the unique domain shifts in SER has not been investigated. In this paper, we present the first systematic evaluation and comparison covering 11 TTA methods across three representative SER tasks. The results indicate that backpropagation-free TTA methods are the most promising. Conversely, entropy minimization and pseudo-labeling generally fail, as their core assumption of a single, confident ground-truth label is incompatible with the inherent ambiguity of emotional expression. Further, no single method universally excels, and its effectiveness is highly dependent on the distributional shifts and tasks.",
      "url": "https://arxiv.org/abs/2601.16240",
      "pdfUrl": "https://arxiv.org/pdf/2601.16240.pdf",
      "titleJa": "音声感情認識のためのテスト時適応"
    },
    {
      "id": "2601.16979",
      "arxivId": "2601.16979",
      "title": "A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs",
      "authors": [
        "Dayal Singh Kalra",
        "Jean-Christophe Gagnon-Audet",
        "Andrey Gromov",
        "Ishita Mediratta",
        "Kelvin Niu",
        "Alexander H Miller",
        "Michael Shvartsman"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "stat.ML"
      ],
      "abstract": "Understanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks. The most commonly studied measure, Hessian sharpness ($λ_{\\max}^H$) -- the largest eigenvalue of the loss Hessian -- determines local training stability and interacts with the learning rate throughout training. Despite its significance in analyzing training dynamics, direct measurement of Hessian sharpness remains prohibitive for Large Language Models (LLMs) due to high computational cost. We analyze $\\textit{critical sharpness}$ ($λ_c$), a computationally efficient measure requiring fewer than $10$ forward passes given the update direction $Δ\\mathbfθ$. Critically, this measure captures well-documented Hessian sharpness phenomena, including progressive sharpening and Edge of Stability. Using this measure, we provide the first demonstration of these sharpness phenomena at scale, up to $7$B parameters, spanning both pre-training and mid-training of OLMo-2 models. We further introduce $\\textit{relative critical sharpness}$ ($λ_c^{1\\to 2}$), which quantifies the curvature of one loss landscape while optimizing another, to analyze the transition from pre-training to fine-tuning and guide data mixing strategies. Critical sharpness provides practitioners with a practical tool for diagnosing curvature dynamics and informing data composition choices at scale. More broadly, our work shows that scalable curvature measures can provide actionable insights for large-scale training.",
      "url": "https://arxiv.org/abs/2601.16979",
      "pdfUrl": "https://arxiv.org/pdf/2601.16979.pdf",
      "titleJa": "LLMのトレーニングダイナミクスを分析するためのスケーラブルな損失ランドスケープ曲率の測定"
    },
    {
      "id": "2601.16970",
      "arxivId": "2601.16970",
      "title": "BONO-Bench: A Comprehensive Test Suite for Bi-objective Numerical Optimization with Traceable Pareto Sets",
      "authors": [
        "Lennart Schäpermeier",
        "Pascal Kerschke"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "math.OC",
        "cs.AI"
      ],
      "abstract": "The evaluation of heuristic optimizers on test problems, better known as \\emph{benchmarking}, is a cornerstone of research in multi-objective optimization. However, most test problems used in benchmarking numerical multi-objective black-box optimizers come from one of two flawed approaches: On the one hand, problems are constructed manually, which result in problems with well-understood optimal solutions, but unrealistic properties and biases. On the other hand, more realistic and complex single-objective problems are composited into multi-objective problems, but with a lack of control and understanding of problem properties. This paper proposes an extensive problem generation approach for bi-objective numerical optimization problems consisting of the combination of theoretically well-understood convex-quadratic functions into unimodal and multimodal landscapes with and without global structure. It supports configuration of test problem properties, such as the number of decision variables, local optima, Pareto front shape, plateaus in the objective space, or degree of conditioning, while maintaining theoretical tractability: The optimal front can be approximated to an arbitrary degree of precision regarding Pareto-compliant performance indicators such as the hypervolume or the exact R2 indicator. To demonstrate the generator's capabilities, a test suite of 20 problem categories, called \\emph{BONO-Bench}, is created and subsequently used as a basis of an illustrative benchmark study. Finally, the general approach underlying our proposed generator, together with the associated test suite, is publicly released in the Python package \\texttt{bonobench} to facilitate reproducible benchmarking.",
      "url": "https://arxiv.org/abs/2601.16970",
      "pdfUrl": "https://arxiv.org/pdf/2601.16970.pdf",
      "titleJa": "BONO-Bench: 追跡可能なパレート集合を用いた二目的数値最適化のための包括的なテストスイート"
    },
    {
      "id": "2601.16967",
      "arxivId": "2601.16967",
      "title": "Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians",
      "authors": [
        "Bernes Lorier Atabonfack",
        "Ahmed Tahiru Issah",
        "Mohammed Hardi Abdul Baaki",
        "Clemence Ingabire",
        "Tolulope Olusuyi",
        "Maruf Adewole",
        "Udunna C. Anazodo",
        "Timothy X Brown"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "abstract": "In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.",
      "url": "https://arxiv.org/abs/2601.16967",
      "pdfUrl": "https://arxiv.org/pdf/2601.16967.pdf",
      "titleJa": "限られた資源環境における医療機器の持続可能性を強化：バイオメディカル技術者向けのAIを活用した診断・サポートプラットフォーム"
    },
    {
      "id": "2601.16965",
      "arxivId": "2601.16965",
      "title": "Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts",
      "authors": [
        "Riyang Bao",
        "Cheng Yang",
        "Dazhou Yu",
        "Zhexiang Tang",
        "Gengchen Mai",
        "Liang Zhao"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.",
      "url": "https://arxiv.org/abs/2601.16965",
      "pdfUrl": "https://arxiv.org/pdf/2601.16965.pdf",
      "titleJa": "空間エージェント：科学的コアコンセプトを備えたエージェント的地理空間推論"
    },
    {
      "id": "2601.16964",
      "arxivId": "2601.16964",
      "title": "AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems",
      "authors": [
        "Mohamed Amine Ferrag",
        "Abderrahmane Lakas",
        "Merouane Debbah"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.AI"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has sparked growing interest in their integration into autonomous systems for reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, structured, and safety-critical benchmarks. This paper introduces AgentDrive, an open benchmark dataset containing 300,000 LLM-generated driving scenarios designed for training, fine-tuning, and evaluating autonomous agents under diverse conditions. AgentDrive formalizes a factorized scenario space across seven orthogonal axes: scenario type, driver behavior, environment, road layout, objective, difficulty, and traffic density. An LLM-driven prompt-to-JSON pipeline generates semantically rich, simulation-ready specifications that are validated against physical and schema constraints. Each scenario undergoes simulation rollouts, surrogate safety metric computation, and rule-based outcome labeling. To complement simulation-based evaluation, we introduce AgentDrive-MCQ, a 100,000-question multiple-choice benchmark spanning five reasoning dimensions: physics, policy, hybrid, scenario, and comparative reasoning. We conduct a large-scale evaluation of fifty leading LLMs on AgentDrive-MCQ. Results show that while proprietary frontier models perform best in contextual and policy reasoning, advanced open models are rapidly closing the gap in structured and physics-grounded reasoning. We release the AgentDrive dataset, AgentDrive-MCQ benchmark, evaluation code, and related materials at https://github.com/maferrag/AgentDrive",
      "url": "https://arxiv.org/abs/2601.16964",
      "pdfUrl": "https://arxiv.org/pdf/2601.16964.pdf",
      "titleJa": "AgentDrive: 自律システムにおける LLM 生成シナリオを用いたエージェント AI 推論のためのオープンベンチマークデータセット"
    },
    {
      "id": "2601.16934",
      "arxivId": "2601.16934",
      "title": "Information Representation Fairness in Long-Document Embeddings: The Peculiar Interaction of Positional and Language Bias",
      "authors": [
        "Elias Schuhmacher",
        "Andrianos Michail",
        "Juri Opitz",
        "Rico Sennrich",
        "Simon Clematide"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "To be discoverable in an embedding-based search process, each part of a document should be reflected in its embedding representation. To quantify any potential reflection biases, we introduce a permutation-based evaluation framework. With this, we observe that state-of-the-art embedding models exhibit systematic positional and language biases when documents are longer and consist of multiple segments. Specifically, early segments and segments in higher-resource languages like English are over-represented, while later segments and segments in lower-resource languages are marginalized. In our further analysis, we find that the positional bias stems from front-loaded attention distributions in pooling-token embeddings, where early tokens receive more attention. To mitigate this issue, we introduce an inference-time attention calibration method that redistributes attention more evenly across document positions, increasing discoverabiltiy of later segments. Our evaluation framework and attention calibration is available at https://github.com/impresso/fair-sentence-transformers",
      "url": "https://arxiv.org/abs/2601.16934",
      "pdfUrl": "https://arxiv.org/pdf/2601.16934.pdf",
      "titleJa": "長文文書埋め込みにおける情報表現の公平性：位置バイアスと言語バイアスの特異な相互作用"
    },
    {
      "id": "2601.16926",
      "arxivId": "2601.16926",
      "title": "Nishpaksh: TEC Standard-Compliant Framework for Fairness Auditing and Certification of AI Models",
      "authors": [
        "Shashank Prakash",
        "Ranjitha Prasad",
        "Avinash Agarwal"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "abstract": "The growing reliance on Artificial Intelligence (AI) models in high-stakes decision-making systems, particularly within emerging telecom and 6G applications, underscores the urgent need for transparent and standardized fairness assessment frameworks. While global toolkits such as IBM AI Fairness 360 and Microsoft Fairlearn have advanced bias detection, they often lack alignment with region-specific regulatory requirements and national priorities. To address this gap, we propose Nishpaksh, an indigenous fairness evaluation tool that operationalizes the Telecommunication Engineering Centre (TEC) Standard for the Evaluation and Rating of Artificial Intelligence Systems. Nishpaksh integrates survey-based risk quantification, contextual threshold determination, and quantitative fairness evaluation into a unified, web-based dashboard. The tool employs vectorized computation, reactive state management, and certification-ready reporting to enable reproducible, audit-grade assessments, thereby addressing a critical post-standardization implementation need. Experimental validation on the COMPAS dataset demonstrates Nishpaksh's effectiveness in identifying attribute-specific bias and generating standardized fairness scores compliant with the TEC framework. The system bridges the gap between research-oriented fairness methodologies and regulatory AI governance in India, marking a significant step toward responsible and auditable AI deployment within critical infrastructure like telecommunications.",
      "url": "https://arxiv.org/abs/2601.16926",
      "pdfUrl": "https://arxiv.org/pdf/2601.16926.pdf",
      "titleJa": "Nishpaksh: AI モデルの公平性監査と認証のための TEC 標準準拠フレームワーク"
    },
    {
      "id": "2601.16914",
      "arxivId": "2601.16914",
      "title": "LoL: Longer than Longer, Scaling Video Generation to Hour",
      "authors": [
        "Justin Cui",
        "Jie Wu",
        "Ming Li",
        "Tao Yang",
        "Xiaojie Li",
        "Rui Wang",
        "Andrew Bai",
        "Yuanhao Ban",
        "Cho-Jui Hsieh"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation.",
      "url": "https://arxiv.org/abs/2601.16914",
      "pdfUrl": "https://arxiv.org/pdf/2601.16914.pdf",
      "titleJa": "LoL: より長く、ビデオ生成を1時間に拡大"
    },
    {
      "id": "2601.16909",
      "arxivId": "2601.16909",
      "title": "Preventing the Collapse of Peer Review Requires Verification-First AI",
      "authors": [
        "Lei You",
        "Lele Cao",
        "Iryna Gurevych"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.AI"
      ],
      "abstract": "This paper argues that AI-assisted peer review should be verification-first rather than review-mimicking. We propose truth-coupling, i.e. how tightly venue scores track latent scientific truth, as the right objective for review tools. We formalize two forces that drive a phase transition toward proxy-sovereign evaluation: verification pressure, when claims outpace verification capacity, and signal shrinkage, when real improvements become hard to separate from noise. In a minimal model that mixes occasional high-fidelity checks with frequent proxy judgment, we derive an explicit coupling law and an incentive-collapse condition under which rational effort shifts from truth-seeking to proxy optimization, even when current decisions still appear reliable. These results motivate actions for tool builders and program chairs: deploy AI as an adversarial auditor that generates auditable verification artifacts and expands effective verification bandwidth, rather than as a score predictor that amplifies claim inflation.",
      "url": "https://arxiv.org/abs/2601.16909",
      "pdfUrl": "https://arxiv.org/pdf/2601.16909.pdf",
      "titleJa": "ピアレビューの崩壊を防ぐには検証重視のAIが必要"
    },
    {
      "id": "2601.16905",
      "arxivId": "2601.16905",
      "title": "GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints",
      "authors": [
        "Andy Zhu",
        "Rongzhe Wei",
        "Yupu Gu",
        "Pan Li"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE's architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model's router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs.",
      "url": "https://arxiv.org/abs/2601.16905",
      "pdfUrl": "https://arxiv.org/pdf/2601.16905.pdf",
      "titleJa": "GRIP: 幾何学的ルーター制約による専門家混合モデルのためのアルゴリズムに依存しない機械反学習"
    },
    {
      "id": "2601.16895",
      "arxivId": "2601.16895",
      "title": "Evaluating Large Vision-language Models for Surgical Tool Detection",
      "authors": [
        "Nakul Poudel",
        "Richard Simon",
        "Cristian A. Linte"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the interrelated components of surgical scenes. Recent advances in large vision-language models that integrate multimodal data processing offer strong potential for modeling surgical tasks and providing human-like scene reasoning and understanding. Despite their promise, systematic investigations of VLMs in surgical applications remain limited. In this study, we evaluate the effectiveness of large VLMs for the fundamental surgical vision task of detecting surgical tools. Specifically, we investigate three state-of-the-art VLMs, Qwen2.5, LLaVA1.5, and InternVL3.5, on the GraSP robotic surgery dataset under both zero-shot and parameter-efficient LoRA fine-tuning settings. Our results demonstrate that Qwen2.5 consistently achieves superior detection performance in both configurations among the evaluated VLMs. Furthermore, compared with the open-set detection baseline Grounding DINO, Qwen2.5 exhibits stronger zero-shot generalization and comparable fine-tuned performance. Notably, Qwen2.5 shows superior instrument recognition, while Grounding DINO demonstrates stronger localization.",
      "url": "https://arxiv.org/abs/2601.16895",
      "pdfUrl": "https://arxiv.org/pdf/2601.16895.pdf",
      "titleJa": "手術器具検出のための大規模視覚言語モデルの評価"
    },
    {
      "id": "2601.16890",
      "arxivId": "2601.16890",
      "title": "LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems",
      "authors": [
        "João A. Leite",
        "Olesya Razuvayevskaya",
        "Kalina Bontcheva",
        "Carolina Scarton"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Automated fact-checking (AFC) systems are susceptible to adversarial attacks, enabling false claims to evade detection. Existing adversarial frameworks typically rely on injecting noise or altering semantics, yet no existing framework exploits the adversarial potential of persuasion techniques, which are widely used in disinformation campaigns to manipulate audiences. In this paper, we introduce a novel class of persuasive adversarial attacks on AFCs by employing a generative LLM to rephrase claims using persuasion techniques. Considering 15 techniques grouped into 6 categories, we study the effects of persuasion on both claim verification and evidence retrieval using a decoupled evaluation strategy. Experiments on the FEVER and FEVEROUS benchmarks show that persuasion attacks can substantially degrade both verification performance and evidence retrieval. Our analysis identifies persuasion techniques as a potent class of adversarial attacks, highlighting the need for more robust AFC systems.",
      "url": "https://arxiv.org/abs/2601.16890",
      "pdfUrl": "https://arxiv.org/pdf/2601.16890.pdf",
      "titleJa": "ファクトチェックシステムに対するLLMベースの敵対的説得攻撃"
    },
    {
      "id": "2601.16886",
      "arxivId": "2601.16886",
      "title": "MAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing with Subgraph Retrieval and Asymmetric Fusion",
      "authors": [
        "Chi Yu",
        "Hongyu Yuan",
        "Zhiyi Duan"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Knowledge Tracing (KT) aims to model a student's learning trajectory and predict performance on the next question. A key challenge is how to better represent the relationships among students, questions, and knowledge concepts (KCs). Recently, graph-based KT paradigms have shown promise for this problem. However, existing methods have not sufficiently explored inter-concept relations, often inferred solely from interaction sequences. In addition, the scale and heterogeneity of KT graphs make full-graph encoding both computationally both costly and noise-prone, causing attention to bleed into student-irrelevant regions and degrading the fidelity of inter-KC relations. To address these issues, we propose a novel framework: Multi-Agent Graph-Enhanced Knowledge Tracing (MAGE-KT). It constructs a multi-view heterogeneous graph by combining a multi-agent KC relation extractor and a student-question interaction graph, capturing complementary semantic and behavioral signals. Conditioned on the target student's history, it retrieves compact, high-value subgraphs and integrates them using an Asymmetric Cross-attention Fusion Module to enhance prediction while avoiding attention diffusion and irrelevant computation. Experiments on three widely used KT datasets show substantial improvements in KC-relation accuracy and clear gains in next-question prediction over existing methods.",
      "url": "https://arxiv.org/abs/2601.16886",
      "pdfUrl": "https://arxiv.org/pdf/2601.16886.pdf",
      "titleJa": "MAGE-KT: サブグラフ検索と非対称融合によるマルチエージェントグラフ強化知識追跡"
    },
    {
      "id": "2601.16882",
      "arxivId": "2601.16882",
      "title": "Explaining Group Recommendations via Counterfactuals",
      "authors": [
        "Maria Stratigi",
        "Nikos Bikakis"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "abstract": "Group recommender systems help users make collective choices but often lack transparency, leaving group members uncertain about why items are suggested. Existing explanation methods focus on individuals, offering limited support for groups where multiple preferences interact. In this paper, we propose a framework for group counterfactual explanations, which reveal how removing specific past interactions would change a group recommendation. We formalize this concept, introduce utility and fairness measures tailored to groups, and design heuristic algorithms, such as Pareto-based filtering and grow-and-prune strategies, for efficient explanation discovery. Experiments on MovieLens and Amazon datasets show clear trade-offs: low-cost methods produce larger, less fair explanations, while other approaches yield concise and balanced results at higher cost. Furthermore, the Pareto-filtering heuristic demonstrates significant efficiency improvements in sparse settings.",
      "url": "https://arxiv.org/abs/2601.16882",
      "pdfUrl": "https://arxiv.org/pdf/2601.16882.pdf",
      "titleJa": "反事実的仮定によるグループ推奨の説明"
    },
    {
      "id": "2601.16874",
      "arxivId": "2601.16874",
      "title": "No Validation, No Problem: Predicting Model Performance from a Single Gradient",
      "authors": [
        "Fangzheng Wu",
        "Brian Summa"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "We propose a validation-free checkpointing signal from a single forward-backward pass: the Frobenius norm of the classifier-head gradient on one detached-feature batch, ||g||_F = ||dL/dW||_F. Across ImageNet-1k CNNs and Transformers, this proxy is strongly negative with Top-1 and positive with loss. Selecting the checkpoint with the minimum head gradient in a short tail window closes most of the gap to the oracle (4.24% +/- 2.00% with a universal setup, about 1.12% with light per-family tuning). For practical deployment, a head-scale normalization is more stable within classic CNN families (e.g., ResNets), while a feature-scale normalization works well for Transformers and modern CNNs. The same one-batch probe also predicts COCO detection/segmentation mAP. In diffusion (UNet/DDPM on CIFAR-10), it tracks progress and enables near-oracle tail-window selection; it is positively correlated with same-distribution probe MSE and negatively with FID (lower is better), so it can be used as a lightweight, label-free monitor. Validation labels are never used beyond reporting. The probe adds much less than 0.1% of an epoch and works as a drop-in for validation-free checkpoint selection and early stopping.",
      "url": "https://arxiv.org/abs/2601.16874",
      "pdfUrl": "https://arxiv.org/pdf/2601.16874.pdf",
      "titleJa": "検証なしでも問題なし：単一の勾配からモデルのパフォーマンスを予測する"
    },
    {
      "id": "2601.16866",
      "arxivId": "2601.16866",
      "title": "Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators",
      "authors": [
        "Lucía Güitta-López",
        "Vincenzo Suriani",
        "Jaime Boal",
        "Álvaro J. López-López",
        "Daniele Nardi"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "abstract": "Deep Reinforcement Learning (DRL) is a powerful framework for solving complex sequential decision-making problems, particularly in robotic control. However, its practical deployment is often hindered by the substantial amount of experience required for learning, which results in high computational and time costs. In this work, we propose a novel integration of DRL with semantic knowledge in the form of Knowledge Graph Embeddings (KGEs), aiming to enhance learning efficiency by providing contextual information to the agent. Our architecture combines KGEs with visual observations, enabling the agent to exploit environmental knowledge during training. Experimental validation with robotic manipulators in environments featuring both fixed and randomized target attributes demonstrates that our method achieves up to {60}{\\%} reduction in learning time and improves task accuracy by approximately 15 percentage points, without increasing training time or computational complexity. These results highlight the potential of semantic knowledge to reduce sample complexity and improve the effectiveness of DRL in robotic applications.",
      "url": "https://arxiv.org/abs/2601.16866",
      "pdfUrl": "https://arxiv.org/pdf/2601.16866.pdf",
      "titleJa": "ロボットマニピュレータのための意味知識による深層強化学習の促進"
    },
    {
      "id": "2601.16863",
      "arxivId": "2601.16863",
      "title": "Mixture-of-Models: Unifying Heterogeneous Agents via N-Way Self-Evaluating Deliberation",
      "authors": [
        "Tims Pecerskis",
        "Aivars Smirnovs"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "eess.SY"
      ],
      "abstract": "This paper introduces the N-Way Self-Evaluating Deliberation (NSED) protocol, a Runtime Mixture-of-Models (MoM) architecture that constructs emergent composite models from a plurality of distinct expert agents. Unlike traditional Mixture-of-Experts (MoE) which rely on static gating networks, NSED employs a Dynamic Expertise Broker - a runtime optimization engine that treats model selection as a variation of the Knapsack Problem, binding heterogeneous checkpoints to functional roles based on live telemetry and cost constraints. At the execution layer, we formalize deliberation as a Macro-Scale Recurrent Neural Network (RNN), where the consensus state loops back through a semantic forget gate to enable iterative refinement without proportional VRAM scaling. Key components include an orchestration fabric for trustless N-to-N peer review, a Quadratic Voting activation function for non-linear consensus, and a feedback-driven state update. Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench) demonstrates that this topology allows ensembles of small (less than 20B) consumer-grade models to match or exceed the performance of state-of-the-art 100B+ parameter models, establishing a new hardware arbitrage efficiency frontier. Furthermore, testing on the DarkBench safety suite reveals intrinsic alignment properties, with peer-mediated correction reducing sycophancy scores below that of any individual agent.",
      "url": "https://arxiv.org/abs/2601.16863",
      "pdfUrl": "https://arxiv.org/pdf/2601.16863.pdf",
      "titleJa": "混合モデル：N方向自己評価審議による異種エージェントの統合"
    },
    {
      "id": "2601.16855",
      "arxivId": "2601.16855",
      "title": "Orbitopal Fixing in SAT",
      "authors": [
        "Markus Anders",
        "Cayden Codel",
        "Marijn J. H. Heule"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "abstract": "Despite their sophisticated heuristics, boolean satisfiability (SAT) solvers are still vulnerable to symmetry, causing them to visit search regions that are symmetric to ones already explored. While symmetry handling is routine in other solving paradigms, integrating it into state-of-the-art proof-producing SAT solvers is difficult: added reasoning must be fast, non-interfering with solver heuristics, and compatible with formal proof logging. To address these issues, we present a practical static symmetry breaking approach based on orbitopal fixing, a technique adapted from mixed-integer programming. Our approach adds only unit clauses, which minimizes downstream slowdowns, and it emits succinct proof certificates in the substitution redundancy proof system. Implemented in the satsuma tool, our methods deliver consistent speedups on symmetry-rich benchmarks with negligible regressions elsewhere.",
      "url": "https://arxiv.org/abs/2601.16855",
      "pdfUrl": "https://arxiv.org/pdf/2601.16855.pdf",
      "titleJa": "SATにおける軌道固定"
    },
    {
      "id": "2601.16853",
      "arxivId": "2601.16853",
      "title": "Reasoning Promotes Robustness in Theory of Mind Tasks",
      "authors": [
        "Ian B. de Haan",
        "Peter van der Putten",
        "Max van Duijn"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.",
      "url": "https://arxiv.org/abs/2601.16853",
      "pdfUrl": "https://arxiv.org/pdf/2601.16853.pdf",
      "titleJa": "推論は心の理論課題における堅牢性を促進する"
    },
    {
      "id": "2601.16830",
      "arxivId": "2601.16830",
      "title": "Uncertainty propagation through trained multi-layer perceptrons: Exact analytical results",
      "authors": [
        "Andrew Thompson",
        "Miles McCrory"
      ],
      "publishedDate": "2026-01-23",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "math.ST"
      ],
      "abstract": "We give analytical results for propagation of uncertainty through trained multi-layer perceptrons (MLPs) with a single hidden layer and ReLU activation functions. More precisely, we give expressions for the mean and variance of the output when the input is multivariate Gaussian. In contrast to previous results, we obtain exact expressions without resort to a series expansion.",
      "url": "https://arxiv.org/abs/2601.16830",
      "pdfUrl": "https://arxiv.org/pdf/2601.16830.pdf",
      "titleJa": "訓練された多層パーセプトロンによる不確実性の伝播：正確な解析結果"
    },
    {
      "id": "2601.15083",
      "arxivId": "2601.15083",
      "title": "Bangla Music Genre Classification Using Bidirectional LSTMS",
      "authors": [
        "Muntakimur Rahaman",
        "Md Mahmudul Hoque",
        "Md Mehedi Hassain"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Bangla music is enrich in its own music cultures. Now a days music genre classification is very significant because of the exponential increase in available music, both in digital and physical formats. It is necessary to index them accordingly to facilitate improved retrieval. Automatically classifying Bangla music by genre is essential for efficiently locating specific pieces within a vast and diverse music library. Prevailing methods for genre classification predominantly employ conventional machine learning or deep learning approaches. This work introduces a novel music dataset comprising ten distinct genres of Bangla music. For the task of audio classification, we utilize a recurrent neural network (RNN) architecture. Specifically, a Long Short-Term Memory (LSTM) network is implemented to train the model and perform the classification. Feature extraction represents a foundational stage in audio data processing. This study utilizes Mel-Frequency Cepstral Coefficients (MFCCs) to transform raw audio waveforms into a compact and representative set of features. The proposed framework facilitates music genre classification by leveraging these extracted features. Experimental results demonstrate a classification accuracy of 78%, indicating the system's strong potential to enhance and streamline the organization of Bangla music genres.",
      "url": "https://arxiv.org/abs/2601.15083",
      "pdfUrl": "https://arxiv.org/pdf/2601.15083.pdf",
      "titleJa": "双方向LSTMSを用いたバングラ音楽のジャンル分類"
    },
    {
      "id": "2601.14931",
      "arxivId": "2601.14931",
      "title": "Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali",
      "authors": [
        "Nouhoum Coulibaly",
        "Ousmane Ly",
        "Michael Leventhal",
        "Ousmane Goro"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "This study explores the capacity of generative artificial intelligence (Gen AI) to contribute to the construction of peace narratives and the revitalization of musical heritage in Mali. The study has been made in a political and social context where inter-community tensions and social fractures motivate a search for new symbolic frameworks for reconciliation. The study empirically explores three questions: (1) how Gen AI can be used as a tool for musical creation rooted in national languages and traditions; (2) to what extent Gen AI systems enable a balanced hybridization between technological innovation and cultural authenticity; and (3) how AI-assisted musical co-creation can strengthen social cohesion and cultural sovereignty. The experimental results suggest that Gen AI, embedded in a culturally conscious participatory framework, can act as a catalyst for symbolic diplomacy, amplifying local voices instead of standardizing them. However, challenges persist regarding the availability of linguistic corpora, algorithmic censorship, and the ethics of generating compositions derived from copyrighted sources.",
      "url": "https://arxiv.org/abs/2601.14931",
      "pdfUrl": "https://arxiv.org/pdf/2601.14931.pdf",
      "titleJa": "生成型人工知能、音楽遺産、そして平和物語の構築：マリにおける事例研究"
    },
    {
      "id": "2601.14786",
      "arxivId": "2601.14786",
      "title": "Training-Efficient Text-to-Music Generation with State-Space Modeling",
      "authors": [
        "Wei-Jaw Lee",
        "Fang-Chih Hsieh",
        "Xuanjun Chen",
        "Fang-Duo Tsai",
        "Yi-Hsuan Yang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in text-to-music generation (TTM) have yielded high-quality results, but often at the cost of extensive compute and the use of large proprietary internal data. To improve the affordability and openness of TTM training, an open-source generative model backbone that is more training- and data-efficient is needed. In this paper, we constrain the number of trainable parameters in the generative model to match that of the MusicGen-small benchmark (with about 300M parameters), and replace its Transformer backbone with the emerging class of state-space models (SSMs). Specifically, we explore different SSM variants for sequence modeling, and compare a single-stage SSM-based design with a decomposable two-stage SSM/diffusion hybrid design. All proposed models are trained from scratch on a purely public dataset comprising 457 hours of CC-licensed music, ensuring full openness. Our experimental findings are three-fold. First, we show that SSMs exhibit superior training efficiency compared to the Transformer counterpart. Second, despite using only 9% of the FLOPs and 2% of the training data size compared to the MusicGen-small benchmark, our model achieves competitive performance in both objective metrics and subjective listening tests based on MusicCaps captions. Finally, our scaling-down experiment demonstrates that SSMs can maintain competitive performance relative to the Transformer baseline even at the same training budget (measured in iterations), when the model size is reduced to four times smaller. To facilitate the democratization of TTM research, the processed captions, model checkpoints, and source code are available on GitHub via the project page: https://lonian6.github.io/ssmttm/.",
      "url": "https://arxiv.org/abs/2601.14786",
      "pdfUrl": "https://arxiv.org/pdf/2601.14786.pdf",
      "titleJa": "状態空間モデリングによる効率的なテキストから音楽への生成"
    },
    {
      "id": "2601.14684",
      "arxivId": "2601.14684",
      "title": "Dissecting Performance Degradation in Audio Source Separation under Sampling Frequency Mismatch",
      "authors": [
        "Kanami Imamura",
        "Tomohiko Nakamura",
        "Kohei Yatabe",
        "Hiroshi Saruwatari"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio processing methods based on deep neural networks are typically trained at a single sampling frequency (SF). To handle untrained SFs, signal resampling is commonly employed, but it can degrade performance, particularly when the input SF is lower than the trained SF. This paper investigates the causes of this degradation through two hypotheses: (i) the lack of high-frequency components introduced by up-sampling, and (ii) the greater importance of their presence than their precise representation. To examine these hypotheses, we compare conventional resampling with three alternatives: post-resampling noise addition, which adds Gaussian noise to the resampled signal; noisy-kernel resampling, which perturbs the kernel with Gaussian noise to enrich high-frequency components; and trainable-kernel resampling, which adapts the interpolation kernel through training. Experiments on music source separation show that noisy-kernel and trainable-kernel resampling alleviate the degradation observed with conventional resampling. We further demonstrate that noisy-kernel resampling is effective across diverse models, highlighting it as a simple yet practical option.",
      "url": "https://arxiv.org/abs/2601.14684",
      "pdfUrl": "https://arxiv.org/pdf/2601.14684.pdf",
      "titleJa": "サンプリング周波数の不一致による音源分離の性能劣化の分析"
    },
    {
      "id": "2601.15348",
      "arxivId": "2601.15348",
      "title": "Abusive music and song transformation using GenAI and LLMs",
      "authors": [
        "Jiyang Choi",
        "Rohitash Chandra"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Repeated exposure to violence and abusive content in music and song content can influence listeners' emotions and behaviours, potentially normalising aggression or reinforcing harmful stereotypes. In this study, we explore the use of generative artificial intelligence (GenAI) and Large Language Models (LLMs) to automatically transform abusive words (vocal delivery) and lyrical content in popular music. Rather than simply muting or replacing a single word, our approach transforms the tone, intensity, and sentiment, thus not altering just the lyrics, but how it is expressed. We present a comparative analysis of four selected English songs and their transformed counterparts, evaluating changes through both acoustic and sentiment-based lenses. Our findings indicate that Gen-AI significantly reduces vocal aggressiveness, with acoustic analysis showing improvements in Harmonic to Noise Ratio, Cepstral Peak Prominence, and Shimmer. Sentiment analysis reduced aggression by 63.3-85.6\\% across artists, with major improvements in chorus sections (up to 88.6\\% reduction). The transformed versions maintained musical coherence while mitigating harmful content, offering a promising alternative to traditional content moderation that avoids triggering the \"forbidden fruit\" effect, where the censored content becomes more appealing simply because it is restricted. This approach demonstrates the potential for GenAI to create safer listening experiences while preserving artistic expression.",
      "url": "https://arxiv.org/abs/2601.15348",
      "pdfUrl": "https://arxiv.org/pdf/2601.15348.pdf",
      "titleJa": "GenAIとLLMを使用した虐待的な音楽と歌の変換"
    },
    {
      "id": "2601.14356",
      "arxivId": "2601.14356",
      "title": "Single-step Controllable Music Bandwidth Extension With Flow Matching",
      "authors": [
        "Carlos Hernandez-Olivan",
        "Hendrik Vincent Koops",
        "Hao Hao Tan",
        "Elio Quinton"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio restoration consists in inverting degradations of a digital audio signal to recover what would have been the pristine quality signal before the degradation occurred. This is valuable in contexts such as archives of music recordings, particularly those of precious historical value, for which a clean version may have been lost or simply does not exist. Recent work applied generative models to audio restoration, showing promising improvement over previous methods, and opening the door to the ability to perform restoration operations that were not possible before. However, making these models finely controllable remains a challenge. In this paper, we propose an extension of FLowHigh and introduce the Dynamic Spectral Contour (DSC) as a control signal for bandwidth extension via classifier-free guidance. Our experiments show competitive model performance, and indicate that DSC is a promising feature to support fine-grained conditioning.",
      "url": "https://arxiv.org/abs/2601.14356",
      "pdfUrl": "https://arxiv.org/pdf/2601.14356.pdf",
      "titleJa": "フローマッチングによるシングルステップ制御可能な音楽帯域幅拡張"
    },
    {
      "id": "2601.14157",
      "arxivId": "2601.14157",
      "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models",
      "authors": [
        "Bruno Sienkiewicz",
        "Łukasz Neumann",
        "Mateusz Modrzejewski"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.",
      "url": "https://arxiv.org/abs/2601.14157",
      "pdfUrl": "https://arxiv.org/pdf/2601.14157.pdf",
      "titleJa": "ConceptCaps - 音楽モデルの解釈可能性のための蒸留概念データセット"
    },
    {
      "id": "2601.13931",
      "arxivId": "2601.13931",
      "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music",
      "authors": [
        "Yannis Vasilakis",
        "Rachel Bittner",
        "Johan Pauwels"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG"
      ],
      "abstract": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.",
      "url": "https://arxiv.org/abs/2601.13931",
      "pdfUrl": "https://arxiv.org/pdf/2601.13931.pdf",
      "titleJa": "音楽のための音声テキスト結合モデルにおける効果的な否定モデリングに向けて"
    },
    {
      "id": "2601.13647",
      "arxivId": "2601.13647",
      "title": "Fusion Segment Transformer: Bi-Directional Attention Guided Fusion Network for AI-Generated Music Detection",
      "authors": [
        "Yumin Kim",
        "Seonghyeon Go"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "With the rise of generative AI technology, anyone can now easily create and deploy AI-generated music, which has heightened the need for technical solutions to address copyright and ownership issues. While existing works mainly focused on short-audio, the challenge of full-audio detection, which requires modeling long-term structure and context, remains insufficiently explored. To address this, we propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. As in our previous work, we extract content embeddings from short music segments using diverse feature extractors. Furthermore, we enhance the architecture for full-audio AI-generated music detection by introducing a Gated Fusion Layer that effectively integrates content and structural information, enabling the capture of long-term context. Experiments on the SONICS and AIME datasets show that our approach outperforms the previous model and recent baselines, achieving state-of-the-art results in AI-generated music detection.",
      "url": "https://arxiv.org/abs/2601.13647",
      "pdfUrl": "https://arxiv.org/pdf/2601.13647.pdf",
      "titleJa": "Fusion Segment Transformer: AI生成音楽検出のための双方向アテンションガイド融合ネットワーク"
    },
    {
      "id": "2601.12961",
      "arxivId": "2601.12961",
      "title": "Supervised Learning for Game Music Segmentation",
      "authors": [
        "Shangxuan Luo",
        "Joshua Reiss"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD"
      ],
      "abstract": "At present, neural network-based models, including transformers, struggle to generate memorable and readily comprehensible music from unified and repetitive musical material due to a lack of understanding of musical structure. Consequently, these models are rarely employed by the games industry. It is hypothesised by many scholars that the modelling of musical structure may inform models at a higher level, thereby enhancing the quality of music generation. The aim of this study is to explore the performance of supervised learning methods in the task of structural segmentation, which is the initial step in music structure modelling. An audio game music dataset with 309 structural annotations was created to train the proposed method, which combines convolutional neural networks and recurrent neural networks, achieving performance comparable to the state-of-the-art unsupervised learning methods with fewer training resources.",
      "url": "https://arxiv.org/abs/2601.12961",
      "pdfUrl": "https://arxiv.org/pdf/2601.12961.pdf",
      "titleJa": "ゲーム音楽セグメンテーションのための教師あり学習"
    },
    {
      "id": "2601.12802",
      "arxivId": "2601.12802",
      "title": "UNMIXX: Untangling Highly Correlated Singing Voices Mixtures",
      "authors": [
        "Jihoo Jung",
        "Ji-Hoon Kim",
        "Doyeop Kwak",
        "Junwon Lee",
        "Juhan Nam",
        "Joon Son Chung"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We introduce UNMIXX, a novel framework for multiple singing voices separation (MSVS). While related to speech separation, MSVS faces unique challenges: data scarcity and the highly correlated nature of singing voices mixture. To address these issues, we propose UNMIXX with three key components: (1) musically informed mixing strategy to construct highly correlated, music-like mixtures, (2) cross-source attention that drives representations of two singers apart via reverse attention, and (3) magnitude penalty loss penalizing erroneously assigned interfering energy. UNMIXX not only addresses data scarcity by simulating realistic training data, but also excels at separating highly correlated mixtures through cross-source interactions at both the architectural and loss levels. Our extensive experiments demonstrate that UNMIXX greatly enhances performance, with SDRi gains exceeding 2.2 dB over prior work.",
      "url": "https://arxiv.org/abs/2601.12802",
      "pdfUrl": "https://arxiv.org/pdf/2601.12802.pdf",
      "titleJa": "UNMIXX: 相関性の高い歌声ミックスを解きほぐす"
    },
    {
      "id": "2601.12314",
      "arxivId": "2601.12314",
      "title": "A Similarity Network for Correlating Musical Structure to Military Strategy",
      "authors": [
        "Yiwen Zhang",
        "Hui Zhang",
        "Fanqin Meng"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Music perception, a multi-sensory process based on the synesthesia effect, is an essential component of music aesthetic education. Understanding music structure helps both perception and aesthetic education. Music structure incorporates a range of information, the coordination of which forms the melody, just as different military actions cooperate to produce a military strategy. However, there are a few ways for assessing music perception from the perspectives of system operation and information management. In this paper, we explore the similarities between music structure and military strategy while creating the Music Clips Correlation Network (MCCN) based on Mel-frequency Cepstral Coefficients (MFCCs). The inspiration comes from the comparison between a concert conductor's musical score and a military war commander's sand table exercise. Specifically, we create MCCNs for various kinds of war movie soundtracks, then relate military tactics (Sun Tzu's Art of War, etc.) and political institutions to military operations networks. Our primary findings suggest a few similarities, implying that music perception and aesthetic education can be approached from a military strategy and management perspective through this interdisciplinary research. Similarly, we can discover similarities between the art of military scheming and the art of musical structure based on network analysis in order to facilitate the understanding of the relationship between technology and art.",
      "url": "https://arxiv.org/abs/2601.12314",
      "pdfUrl": "https://arxiv.org/pdf/2601.12314.pdf",
      "titleJa": "音楽構造と軍事戦略を相関させる類似性ネットワーク"
    },
    {
      "id": "2601.12245",
      "arxivId": "2601.12245",
      "title": "Sound2Hap: Learning Audio-to-Vibrotactile Haptic Generation from Human Ratings",
      "authors": [
        "Yinan Li",
        "Hasti Seifi"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Environmental sounds like footsteps, keyboard typing, or dog barking carry rich information and emotional context, making them valuable for designing haptics in user applications. Existing audio-to-vibration methods, however, rely on signal-processing rules tuned for music or games and often fail to generalize across diverse sounds. To address this, we first investigated user perception of four existing audio-to-haptic algorithms, then created a data-driven model for environmental sounds. In Study 1, 34 participants rated vibrations generated by the four algorithms for 1,000 sounds, revealing no consistent algorithm preferences. Using this dataset, we trained Sound2Hap, a CNN-based autoencoder, to generate perceptually meaningful vibrations from diverse sounds with low latency. In Study 2, 15 participants rated its output higher than signal-processing baselines on both audio-vibration match and Haptic Experience Index (HXI), finding it more harmonious with diverse sounds. This work demonstrates a perceptually validated approach to audio-haptic translation, broadening the reach of sound-driven haptics.",
      "url": "https://arxiv.org/abs/2601.12245",
      "pdfUrl": "https://arxiv.org/pdf/2601.12245.pdf",
      "titleJa": "Sound2Hap: 人間の評価から音声から振動触覚への触覚生成を学習する"
    },
    {
      "id": "2601.12222",
      "arxivId": "2601.12222",
      "title": "Song Aesthetics Evaluation with Multi-Stem Attention and Hierarchical Uncertainty Modeling",
      "authors": [
        "Yishan Lv",
        "Jing Luo",
        "Boyuan Ju",
        "Yang Zhang",
        "Xinda Wu",
        "Bo Yuan",
        "Xinyu Yang"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Music generative artificial intelligence (AI) is rapidly expanding music content, necessitating automated song aesthetics evaluation. However, existing studies largely focus on speech, audio or singing quality, leaving song aesthetics underexplored. Moreover, conventional approaches often predict a precise Mean Opinion Score (MOS) value directly, which struggles to capture the nuances of human perception in song aesthetics evaluation. This paper proposes a song-oriented aesthetics evaluation framework, featuring two novel modules: 1) Multi-Stem Attention Fusion (MSAF) builds bidirectional cross-attention between mixture-vocal and mixture-accompaniment pairs, fusing them to capture complex musical features; 2) Hierarchical Granularity-Aware Interval Aggregation (HiGIA) learns multi-granularity score probability distributions, aggregates them into a score interval, and applies a regression within the interval to produce the final score. We evaluated on two datasets of full-length songs: SongEval dataset (AI-generated) and an internal aesthetics dataset (human-created), and compared with two state-of-the-art (SOTA) models. Results show that the proposed method achieves stronger performance for multi-dimensional song aesthetics evaluation.",
      "url": "https://arxiv.org/abs/2601.12222",
      "pdfUrl": "https://arxiv.org/pdf/2601.12222.pdf",
      "titleJa": "マルチステムアテンションと階層的不確実性モデリングによる歌の美的評価"
    },
    {
      "id": "2601.12205",
      "arxivId": "2601.12205",
      "title": "Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks",
      "authors": [
        "Shih-Heng Wang",
        "Jiatong Shi",
        "Jinchuan Tian",
        "Haibin Wu",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "This paper investigates three crucial yet underexplored aspects of the generalization capabilities of neural audio codecs (NACs): (i) whether NACs can generalize to unseen languages during pre-training, (ii) whether speech-only pre-trained NACs can effectively generalize to non-speech applications such as environmental sounds, music, and animal vocalizations, and (iii) whether incorporating non-speech data during pre-training can improve performance on both speech and non-speech tasks. Existing studies typically rely on off-the-shelf NACs for comparison, which limits insight due to variations in implementation. In this work, we train NACs from scratch using strictly controlled configurations and carefully curated pre-training data to enable fair comparisons. We conduct a comprehensive evaluation of NAC performance on both signal reconstruction quality and downstream applications using 11 metrics. Our results show that NACs can generalize to unseen languages during pre-training, speech-only pre-trained NACs exhibit degraded performance on non-speech tasks, and incorporating non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks.",
      "url": "https://arxiv.org/abs/2601.12205",
      "pdfUrl": "https://arxiv.org/pdf/2601.12205.pdf",
      "titleJa": "ニューラルコーデックは一般化するか？未知の言語と非音声タスクを対象とした対照研究"
    },
    {
      "id": "2601.12180",
      "arxivId": "2601.12180",
      "title": "VidTune: Creating Video Soundtracks with Generative Music and Contextual Thumbnails",
      "authors": [
        "Mina Huh",
        "Ailie C. Fraser",
        "Dingzeyu Li",
        "Mira Dontcheva",
        "Bryan Wang"
      ],
      "publishedDate": "2026-01-17",
      "categories": [
        "cs.HC",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Music shapes the tone of videos, yet creators often struggle to find soundtracks that match their video's mood and narrative. Recent text-to-music models let creators generate music from text prompts, but our formative study (N=8) shows creators struggle to construct diverse prompts, quickly review and compare tracks, and understand their impact on the video. We present VidTune, a system that supports soundtrack creation by generating diverse music options from a creator's prompt and producing contextual thumbnails for rapid review. VidTune extracts representative video subjects to ground thumbnails in context, maps each track's valence and energy onto visual cues like color and brightness, and depicts prominent genres and instruments. Creators can refine tracks through natural language edits, which VidTune expands into new generations. In a controlled user study (N=12) and an exploratory case study (N=6), participants found VidTune helpful for efficiently reviewing and comparing music options and described the process as playful and enriching.",
      "url": "https://arxiv.org/abs/2601.12180",
      "pdfUrl": "https://arxiv.org/pdf/2601.12180.pdf",
      "titleJa": "VidTune: ジェネレーティブミュージックとコンテキストサムネイルを使ったビデオサウンドトラックの作成"
    },
    {
      "id": "2601.15433",
      "arxivId": "2601.15433",
      "title": "DynamicSound simulator for simulating moving sources and microphone arrays",
      "authors": [
        "Luca Barbisan",
        "Marco Levorato",
        "Fabrizio Riente"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Developing algorithms for sound classification, detection, and localization requires large amounts of flexible and realistic audio data, especially when leveraging modern machine learning and beamforming techniques. However, most existing acoustic simulators are tailored for indoor environments and are limited to static sound sources, making them unsuitable for scenarios involving moving sources, moving microphones, or long-distance propagation. This paper presents DynamicSound an open-source acoustic simulation framework for generating multichannel audio from one or more sound sources with the possibility to move them continuously in three-dimensional space and recorded by arbitrarily configured microphone arrays. The proposed model explicitly accounts for finite sound propagation delays, Doppler effects, distance-dependent attenuation, air absorption, and first-order reflections from planar surfaces, yielding temporally consistent spatial audio signals. Unlike conventional mono or stereo simulators, the proposed system synthesizes audio for an arbitrary number of virtual microphones, accurately reproducing inter-microphone time delays, level differences, and spectral coloration induced by the environment. Comparative evaluations with existing open-source tools demonstrate that the generated signals preserve high spatial fidelity across varying source positions and acoustic conditions. By enabling the generation of realistic multichannel audio under controlled and repeatable conditions, the proposed open framework provides a flexible and reproducible tool for the development, training, and evaluation of modern spatial audio and sound-source localization algorithms.",
      "url": "https://arxiv.org/abs/2601.15433",
      "pdfUrl": "https://arxiv.org/pdf/2601.15433.pdf",
      "titleJa": "移動音源とマイクアレイをシミュレートするDynamicSoundシミュレータ"
    },
    {
      "id": "2601.14227",
      "arxivId": "2601.14227",
      "title": "Transformer Architectures for Respiratory Sound Analysis and Multimodal Diagnosis",
      "authors": [
        "Theodore Aptekarev",
        "Vladimir Sokolovsky",
        "Gregory Furman"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Respiratory sound analysis is a crucial tool for screening asthma and other pulmonary pathologies, yet traditional auscultation remains subjective and experience-dependent. Our prior research established a CNN baseline using DenseNet201, which demonstrated high sensitivity in classifying respiratory sounds. In this work, we (i) adapt the Audio Spectrogram Transformer (AST) for respiratory sound analysis and (ii) evaluate a multimodal Vision-Language Model (VLM) that integrates spectrograms with structured patient metadata. AST is initialized from publicly available weights and fine-tuned on a medical dataset containing hundreds of recordings per diagnosis. The VLM experiment uses a compact Moondream-type model that processes spectrogram images alongside a structured text prompt (sex, age, recording site) to output a JSON-formatted diagnosis. Results indicate that AST achieves approximately 97% accuracy with an F1-score around 97% and ROC AUC of 0.98 for asthma detection, significantly outperforming both the internal CNN baseline and typical external benchmarks. The VLM reaches 86-87% accuracy, performing comparably to the CNN baseline while demonstrating the capability to integrate clinical context into the inference process. These results confirm the effectiveness of self-attention for acoustic screening and highlight the potential of multimodal architectures for holistic diagnostic tools.",
      "url": "https://arxiv.org/abs/2601.14227",
      "pdfUrl": "https://arxiv.org/pdf/2601.14227.pdf",
      "titleJa": "呼吸音解析とマルチモーダル診断のためのトランスフォーマーアーキテクチャ"
    },
    {
      "id": "2601.13849",
      "arxivId": "2601.13849",
      "title": "Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control",
      "authors": [
        "Ziyi Yang",
        "Li Rao",
        "Zhengding Luo",
        "Dongyuan Shi",
        "Qirui Huang",
        "Woon-Seng Gan"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.LG",
        "eess.SP"
      ],
      "abstract": "Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train.",
      "url": "https://arxiv.org/abs/2601.13849",
      "pdfUrl": "https://arxiv.org/pdf/2601.13849.pdf",
      "titleJa": "能動騒音制御のためのメタ学習による制御フィルタと二次パスの共初期化"
    },
    {
      "id": "2601.13847",
      "arxivId": "2601.13847",
      "title": "Emotion and Acoustics Should Agree: Cross-Level Inconsistency Analysis for Audio Deepfake Detection",
      "authors": [
        "Jinhua Zhang",
        "Zhenqi Jia",
        "Rui Liu"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio Deepfake Detection (ADD) aims to detect spoof speech from bonafide speech. Most prior studies assume that stronger correlations within or across acoustic and emotional features imply authenticity, and thus focus on enhancing or measuring such correlations. However, existing methods often treat acoustic and emotional features in isolation or rely on correlation metrics, which overlook subtle desynchronization between them and smooth out abrupt discontinuities. To address these issues, we propose EAI-ADD, which treats cross level emotion acoustic inconsistency as the primary detection signal. We first project emotional and acoustic representations into a comparable space. Then we progressively integrate frame level and utterance level emotion features with acoustic features to capture cross level emotion acoustic inconsistencies across different temporal granularities. Experimental results on the ASVspoof 2019LA and 2021LA datasets demonstrate that the proposed EAI-ADD outperforms baselines, providing a more effective solution for audio anti spoofing detection.",
      "url": "https://arxiv.org/abs/2601.13847",
      "pdfUrl": "https://arxiv.org/pdf/2601.13847.pdf",
      "titleJa": "感情と音響は一致するはず：オーディオディープフェイク検出のためのクロスレベル不整合分析"
    },
    {
      "id": "2601.13679",
      "arxivId": "2601.13679",
      "title": "Ultra-Lightweight Network for Ship-Radiated Sound Classification on Embedded Deployment",
      "authors": [
        "Sangwon Park",
        "Dongjun Kim",
        "Sung-Hoon Byun",
        "Sangwook Park"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This letter presents ShuffleFAC, a lightweight acoustic model for ship-radiated sound classification in resource-constrained maritime monitoring systems. ShuffleFAC integrates Frequency-Aware convolution into an efficiency-oriented backbone using separable convolution, point-wise group convolution, and channel shuffle, enabling frequency-sensitive feature extraction with low computational cost. Experiments on the DeepShip dataset show that ShuffleFAC achieves competitive performance with substantially reduced complexity. In particular, ShuffleFAC ($γ=16$) attains a macro F1-score of 71.45 $\\pm$ 1.18% using 39K parameters and 3.06M MACs, and achieves an inference latency of 6.05 $\\pm$ 0.95ms on a Raspberry Pi. Compared with MicroNet0, it improves macro F1-score by 1.82 % while reducing model size by 9.7x and latency by 2.5x. These results indicate that ShuffleFAC is suitable for real-time embedded UATR.",
      "url": "https://arxiv.org/abs/2601.13679",
      "pdfUrl": "https://arxiv.org/pdf/2601.13679.pdf",
      "titleJa": "組み込み型船舶放射音分類用超軽量ネットワーク"
    },
    {
      "id": "2601.13589",
      "arxivId": "2601.13589",
      "title": "Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification",
      "authors": [
        "HyeYoung Lee"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.",
      "url": "https://arxiv.org/abs/2601.13589",
      "pdfUrl": "https://arxiv.org/pdf/2601.13589.pdf",
      "titleJa": "リアルタイム安全性検証を備えたマルチエージェントAIシステムによる動作応答コンテンツ生成"
    },
    {
      "id": "2601.13513",
      "arxivId": "2601.13513",
      "title": "Event Classification by Physics-informed Inpainting for Distributed Multichannel Acoustic Sensor with Partially Degraded Channels",
      "authors": [
        "Noriyuki Tonami",
        "Wataru Kohno",
        "Yoshiyuki Yajima",
        "Sakiko Mishima",
        "Yumi Arai",
        "Reishi Kondo",
        "Tomoyuki Hino"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Distributed multichannel acoustic sensing (DMAS) enables large-scale sound event classification (SEC), but performance drops when many channels are degraded and when sensor layouts at test time differ from training layouts. We propose a learning-free, physics-informed inpainting frontend based on reverse time migration (RTM). In this approach, observed multichannel spectrograms are first back-propagated on a 3D grid using an analytic Green's function to form a scene-consistent image, and then forward-projected to reconstruct inpainted signals before log-mel feature extraction and Transformer-based classification. We evaluate the method on ESC-50 with 50 sensors and three layouts (circular, linear, right-angle), where per-channel SNRs are sampled from -30 to 0 dB. Compared with an AST baseline, scaling-sparsemax channel selection, and channel-swap augmentation, the proposed RTM frontend achieves the best or competitive accuracy across all layouts, improving accuracy by 13.1 points on the right-angle layout (from 9.7% to 22.8%). Correlation analyses show that spatial weights align more strongly with SNR than with channel--source distance, and that higher SNR--weight correlation corresponds to higher SEC accuracy. These results demonstrate that a reconstruct-then-project, physics-based preprocessing effectively complements learning-only methods for DMAS under layout-open configurations and severe channel degradation.",
      "url": "https://arxiv.org/abs/2601.13513",
      "pdfUrl": "https://arxiv.org/pdf/2601.13513.pdf",
      "titleJa": "部分的に劣化したチャネルを持つ分散型マルチチャネル音響センサーのための物理学に基づくインペインティングによるイベント分類"
    },
    {
      "id": "2601.12752",
      "arxivId": "2601.12752",
      "title": "SoundPlot: An Open-Source Framework for Birdsong Acoustic Analysis and Neural Synthesis with Interactive 3D Visualization",
      "authors": [
        "Naqcho Ali Mehdi",
        "Mohammad Adeel",
        "Aizaz Ali Larik"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "We present SoundPlot, an open-source framework for analyzing avian vocalizations through acoustic feature extraction, dimensionality reduction, and neural audio synthesis. The system transforms audio signals into a multi-dimensional acoustic feature space, enabling real-time visualization of temporal dynamics in 3D using web-based interactive graphics. Our framework implements a complete analysis-synthesis pipeline that extracts spectral features (centroid, bandwidth, contrast), pitch contours via probabilistic YIN (pYIN), and mel-frequency cepstral coefficients (MFCCs), mapping them to a unified timbre space for visualization. Audio reconstruction employs the Griffin-Lim phase estimation algorithm applied to mel spectrograms. The accompanying Three.js-based interface provides dual-viewport visualization comparing original and synthesized audio trajectories with independent playback controls. We demonstrate the framework's capabilities through comprehensive waveform analysis, spectrogram comparisons, and feature space evaluation using Principal Component Analysis (PCA). Quantitative evaluation shows mel spectrogram correlation scores exceeding 0.92, indicating high-fidelity preservation of perceptual acoustic structure. SoundPlot is released under the MIT License to facilitate research in bioacoustics, audio signal processing, and computational ethology.",
      "url": "https://arxiv.org/abs/2601.12752",
      "pdfUrl": "https://arxiv.org/pdf/2601.12752.pdf",
      "titleJa": "SoundPlot: インタラクティブな3D可視化による鳥のさえずりの音響分析とニューラルネットワーク合成のためのオープンソースフレームワーク"
    },
    {
      "id": "2601.12660",
      "arxivId": "2601.12660",
      "title": "Toward Faithful Explanations in Acoustic Anomaly Detection",
      "authors": [
        "Maab Elrashid",
        "Anthony Deschênes",
        "Cem Subakan",
        "Mirco Ravanelli",
        "Rémi Georges",
        "Michael Morin"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Interpretability is essential for user trust in real-world anomaly detection applications. However, deep learning models, despite their strong performance, often lack transparency. In this work, we study the interpretability of autoencoder-based models for audio anomaly detection, by comparing a standard autoencoder (AE) with a mask autoencoder (MAE) in terms of detection performance and interpretability. We applied several attribution methods, including error maps, saliency maps, SmoothGrad, Integrated Gradients, GradSHAP, and Grad-CAM. Although MAE shows a slightly lower detection, it consistently provides more faithful and temporally precise explanations, suggesting a better alignment with true anomalies. To assess the relevance of the regions highlighted by the explanation method, we propose a perturbation-based faithfulness metric that replaces them with their reconstructions to simulate normal input. Our findings, based on experiments in a real industrial scenario, highlight the importance of incorporating interpretability into anomaly detection pipelines and show that masked training improves explanation quality without compromising performance.",
      "url": "https://arxiv.org/abs/2601.12660",
      "pdfUrl": "https://arxiv.org/pdf/2601.12660.pdf",
      "titleJa": "音響異常検知における忠実な説明に向けて"
    },
    {
      "id": "2601.12600",
      "arxivId": "2601.12600",
      "title": "SSVD-O: Parameter-Efficient Fine-Tuning with Structured SVD for Speech Recognition",
      "authors": [
        "Pu Wang",
        "Shinji Watanabe",
        "Hugo Van hamme"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) is a scalable approach for adapting large speech foundation models to new domains. While methods such as LoRA and its state-of-the-art variants reduce adaptation costs, they typically allocate parameters uniformly across model subspaces, which limits their efficiency and scalability in speech applications. Building on our prior work, this paper introduces SSVD-Outer (SSVD-O), an extension of the structured SVD-guided (SSVD) fine-tuning method. SSVD-O combines input acoustic feature space-associated inner transformations with output semantic feature space-associated outer transformations to enable scalable and balanced adaptation. We conduct the first systematic analysis of parameter budget allocation across model subspaces in PEFT for automatic speech recognition (ASR), and investigate the trade-off between learning and forgetting under constrained resources. SSVD-O is benchmarked against LoRA, DoRA, PiSSA, and SSVD on domain-shifted ASR tasks, including child speech and regional accents, across model scales from 0.1B to 2B within the ESPnet framework. Experimental results show that SSVD-O consistently narrows the performance gap to full fine-tuning while improving generalization and mitigating catastrophic forgetting.",
      "url": "https://arxiv.org/abs/2601.12600",
      "pdfUrl": "https://arxiv.org/pdf/2601.12600.pdf",
      "titleJa": "SSVD-O: 音声認識のための構造化SVDによるパラメータ効率の高い微調整"
    },
    {
      "id": "2601.12494",
      "arxivId": "2601.12494",
      "title": "Harmonizing the Arabic Audio Space with Data Scheduling",
      "authors": [
        "Hunzalah Hassan Bhatti",
        "Firoj Alam",
        "Shammur Absar Chowdhury"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments.",
      "url": "https://arxiv.org/abs/2601.12494",
      "pdfUrl": "https://arxiv.org/pdf/2601.12494.pdf",
      "titleJa": "データスケジューリングによるアラビア語オーディオ空間の調和"
    },
    {
      "id": "2601.12480",
      "arxivId": "2601.12480",
      "title": "A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation",
      "authors": [
        "Hanchen Pei",
        "Shujie Liu",
        "Yanqing Liu",
        "Jianwei Yu",
        "Yuanhang Qian",
        "Gongping Huang",
        "Sheng Zhao",
        "Yan Lu"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Neural codec language models achieve impressive zero-shot Text-to-Speech (TTS) by fully imitating the acoustic characteristics of a short speech prompt, including timbre, prosody, and paralinguistic information. However, such holistic imitation limits their ability to isolate and control individual attributes. In this paper, we present a unified codec language model SpeechEdit that extends zero-shot TTS with a selective control mechanism. By default, SpeechEdit reproduces the complete acoustic profile inferred from the speech prompt, but it selectively overrides only the attributes specified by explicit control instructions. To enable controllable modeling, SpeechEdit is trained on our newly constructed LibriEdit dataset, which provides delta (difference-aware) training pairs derived from LibriHeavy. Experimental results show that our approach maintains naturalness and robustness while offering flexible and localized control over desired attributes. Audio samples are available at https://speech-editing.github.io/speech-editing/.",
      "url": "https://arxiv.org/abs/2601.12480",
      "pdfUrl": "https://arxiv.org/pdf/2601.12480.pdf",
      "titleJa": "選択的に編集可能なテキスト音声生成のための統合ニューラルコーデック言語モデル"
    },
    {
      "id": "2601.12354",
      "arxivId": "2601.12354",
      "title": "Bone-conduction Guided Multimodal Speech Enhancement with Conditional Diffusion Models",
      "authors": [
        "Sina Khanagha",
        "Bunlong Lay",
        "Timo Gerkmann"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Single-channel speech enhancement models face significant performance degradation in extremely noisy environments. While prior work has shown that complementary bone-conducted speech can guide enhancement, effective integration of this noise-immune modality remains a challenge. This paper introduces a novel multimodal speech enhancement framework that integrates bone-conduction sensors with air-conducted microphones using a conditional diffusion model. Our proposed model significantly outperforms previously established multimodal techniques and a powerful diffusion-based single-modal baseline across a wide range of acoustic conditions.",
      "url": "https://arxiv.org/abs/2601.12354",
      "pdfUrl": "https://arxiv.org/pdf/2601.12354.pdf",
      "titleJa": "条件付き拡散モデルを用いた骨伝導誘導マルチモーダル音声強調"
    }
  ],
  "lastUpdated": "2026-01-27T00:57:47.110779",
  "totalCount": 74
}