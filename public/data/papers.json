{
  "papers": [
    {
      "id": "2601.08764",
      "arxivId": "2601.08764",
      "title": "FusID: Modality-Fused Semantic IDs for Generative Music Recommendation",
      "authors": [
        "Haven Kim",
        "Yupeng Hou",
        "Julian McAuley"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.IR",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Generative recommendation systems have achieved significant advances by leveraging semantic IDs to represent items. However, existing approaches that tokenize each modality independently face two critical limitations: (1) redundancy across modalities that reduces efficiency, and (2) failure to capture inter-modal interactions that limits item representation. We introduce FusID, a modality-fused semantic ID framework that addresses these limitations through three key components: (i) multimodal fusion that learns unified representations by jointly encoding information across modalities, (ii) representation learning that brings frequently co-occurring item embeddings closer while maintaining distinctiveness and preventing feature redundancy, and (iii) product quantization that converts the fused continuous embeddings into multiple discrete tokens to mitigate ID conflict. Evaluated on a multimodal next-song recommendation (i.e., playlist continuation) benchmark, FusID achieves zero ID conflicts, ensuring that each token sequence maps to exactly one song, mitigates codebook underutilization, and outperforms baselines in terms of MRR and Recall@k (k = 1, 5, 10, 20).",
      "url": "https://arxiv.org/abs/2601.08764",
      "pdfUrl": "https://arxiv.org/pdf/2601.08764.pdf",
      "titleJa": "FusID: 生成的音楽推薦のためのモダリティ融合セマンティックID"
    },
    {
      "id": "2601.08516",
      "arxivId": "2601.08516",
      "title": "Robust CAPTCHA Using Audio Illusions in the Era of Large Language Models: from Evaluation to Advances",
      "authors": [
        "Ziqi Ding",
        "Yunfeng Wan",
        "Wei Song",
        "Yi Liu",
        "Gelei Deng",
        "Nan Sun",
        "Huadong Mo",
        "Jingling Xue",
        "Shidong Pan",
        "Yuekang Li"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.SD",
        "cs.CY",
        "eess.AS"
      ],
      "abstract": "CAPTCHAs are widely used by websites to block bots and spam by presenting challenges that are easy for humans but difficult for automated programs to solve. To improve accessibility, audio CAPTCHAs are designed to complement visual ones. However, the robustness of audio CAPTCHAs against advanced Large Audio Language Models (LALMs) and Automatic Speech Recognition (ASR) models remains unclear. In this paper, we introduce AI-CAPTCHA, a unified framework that offers (i) an evaluation framework, ACEval, which includes advanced LALM- and ASR-based solvers, and (ii) a novel audio CAPTCHA approach, IllusionAudio, leveraging audio illusions. Through extensive evaluations of seven widely deployed audio CAPTCHAs, we show that most existing methods can be solved with high success rates by advanced LALMs and ASR models, exposing critical security weaknesses. To address these vulnerabilities, we design a new audio CAPTCHA approach, IllusionAudio, which exploits perceptual illusion cues rooted in human auditory mechanisms. Extensive experiments demonstrate that our method defeats all tested LALM- and ASR-based attacks while achieving a 100% human pass rate, significantly outperforming existing audio CAPTCHA methods.",
      "url": "https://arxiv.org/abs/2601.08516",
      "pdfUrl": "https://arxiv.org/pdf/2601.08516.pdf",
      "titleJa": "大規模言語モデルの時代における音響錯覚を利用した堅牢なCAPTCHA：評価から発展まで"
    },
    {
      "id": "2601.08450",
      "arxivId": "2601.08450",
      "title": "Decoding Order Matters in Autoregressive Speech Synthesis",
      "authors": [
        "Minghui Zhao",
        "Anton Ragni"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Autoregressive speech synthesis often adopts a left-to-right order, yet generation order is a modelling choice. We investigate decoding order through masked diffusion framework, which progressively unmasks positions and allows arbitrary decoding orders during training and inference. By interpolating between identity and random permutations, we show that randomness in decoding order affects speech quality. We further compare fixed strategies, such as \\texttt{l2r} and \\texttt{r2l} with adaptive ones, such as Top-$K$, finding that fixed-order decoding, including the dominating left-to-right approach, is suboptimal, while adaptive decoding yields better performance. Finally, since masked diffusion requires discrete inputs, we quantise acoustic representations and find that even 1-bit quantisation can support reasonably high-quality speech.",
      "url": "https://arxiv.org/abs/2601.08450",
      "pdfUrl": "https://arxiv.org/pdf/2601.08450.pdf",
      "titleJa": "自己回帰音声合成におけるデコード順序の重要性"
    },
    {
      "id": "2601.08358",
      "arxivId": "2601.08358",
      "title": "Decodable but not structured: linear probing enables Underwater Acoustic Target Recognition with pretrained audio embeddings",
      "authors": [
        "Hilde I. Hummel",
        "Sandjai Bhulai",
        "Rob D. van der Mei",
        "Burooj Ghani"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Increasing levels of anthropogenic noise from ships contribute significantly to underwater sound pollution, posing risks to marine ecosystems. This makes monitoring crucial to understand and quantify the impact of the ship radiated noise. Passive Acoustic Monitoring (PAM) systems are widely deployed for this purpose, generating years of underwater recordings across diverse soundscapes. Manual analysis of such large-scale data is impractical, motivating the need for automated approaches based on machine learning. Recent advances in automatic Underwater Acoustic Target Recognition (UATR) have largely relied on supervised learning, which is constrained by the scarcity of labeled data. Transfer Learning (TL) offers a promising alternative to mitigate this limitation. In this work, we conduct the first empirical comparative study of transfer learning for UATR, evaluating multiple pretrained audio models originating from diverse audio domains. The pretrained model weights are frozen, and the resulting embeddings are analyzed through classification, clustering, and similarity-based evaluations. The analysis shows that the geometrical structure of the embedding space is largely dominated by recording-specific characteristics. However, a simple linear probe can effectively suppress this recording-specific information and isolate ship-type features from these embeddings. As a result, linear probing enables effective automatic UATR using pretrained audio models at low computational cost, significantly reducing the need for a large amounts of high-quality labeled ship recordings.",
      "url": "https://arxiv.org/abs/2601.08358",
      "pdfUrl": "https://arxiv.org/pdf/2601.08358.pdf",
      "titleJa": "デコード可能だが構造化されていない：線形プローブにより、事前学習済みのオーディオ埋め込みによる水中音響ターゲット認識が可能になる"
    },
    {
      "id": "2601.08074",
      "arxivId": "2601.08074",
      "title": "Elastic overtones: an equal temperament 12 tone music system with \"perfect\" fifths",
      "authors": [
        "X. Hernandez",
        "Luis Nasser",
        "Pablo Garcia-Valenzuela"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "physics.soc-ph",
        "cs.SD",
        "eess.AS",
        "physics.pop-ph"
      ],
      "abstract": "The impossibility of a transposable 12 semitone tuning of the octave arises from the mathematical fact that $2 \\times 2^{7/12} \\neq 3$ i.e., the second harmonic of the fifth can not exactly match the third harmonic of the fundamental. This in turn, stems from the whole number harmonic structure of western music, and the subsequent fundamental character of the octave interval as multiples of 2 in frequency, a property inherited by our music system from the physics of instruments with vibrating elements being to a good approximation one dimensional. In the current era of electronic music, one can relax the above assumptions to construct an analogous music system where all the structural properties of the standard music system are preserved, but where harmonics are not whole number multiples of the fundamental frequency, and the octave is no longer a factor of 2 in frequency. This now allows to construct a transposable 12 semitone music system where the second harmonic of the fifth exactly matches the third harmonic of the fundamental. The enhanced harmonic qualities of this system recover to a good approximation the musical qualities of Just Intonation, whilst retaining by construction all the versatility and modulating ability of 12TET.",
      "url": "https://arxiv.org/abs/2601.08074",
      "pdfUrl": "https://arxiv.org/pdf/2601.08074.pdf",
      "titleJa": "弾性倍音: 完全五度を含む平均律12音音楽システム"
    },
    {
      "id": "2601.07999",
      "arxivId": "2601.07999",
      "title": "VoxCog: Towards End-to-End Multilingual Cognitive Impairment Classification through Dialectal Knowledge",
      "authors": [
        "Tiantian Feng",
        "Anfeng Xu",
        "Jinkook Lee",
        "Shrikanth Narayanan"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "In this work, we present a novel perspective on cognitive impairment classification from speech by integrating speech foundation models that explicitly recognize speech dialects. Our motivation is based on the observation that individuals with Alzheimer's Disease (AD) or mild cognitive impairment (MCI) often produce measurable speech characteristics, such as slower articulation rate and lengthened sounds, in a manner similar to dialectal phonetic variations seen in speech. Building on this idea, we introduce VoxCog, an end-to-end framework that uses pre-trained dialect models to detect AD or MCI without relying on additional modalities such as text or images. Through experiments on multiple multilingual datasets for AD and MCI detection, we demonstrate that model initialization with a dialect classifier on top of speech foundation models consistently improves the predictive performance of AD or MCI. Our trained models yield similar or often better performance compared to previous approaches that ensembled several computational methods using different signal modalities. Particularly, our end-to-end speech-based model achieves 87.5% and 85.9% accuracy on the ADReSS 2020 challenge and ADReSSo 2021 challenge test sets, outperforming existing solutions that use multimodal ensemble-based computation or LLMs.",
      "url": "https://arxiv.org/abs/2601.07999",
      "pdfUrl": "https://arxiv.org/pdf/2601.07999.pdf",
      "titleJa": "VoxCog: 方言知識によるエンドツーエンドの多言語認知障害分類に向けて"
    },
    {
      "id": "2601.07969",
      "arxivId": "2601.07969",
      "title": "Tuberculosis Screening from Cough Audio: Baseline Models, Clinical Variables, and Uncertainty Quantification",
      "authors": [
        "George P. Kafentzis",
        "Efstratios Selisios"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "In this paper, we propose a standardized framework for automatic tuberculosis (TB) detection from cough audio and routinely collected clinical data using machine learning. While TB screening from audio has attracted growing interest, progress is difficult to measure because existing studies vary substantially in datasets, cohort definitions, feature representations, model families, validation protocols, and reported metrics. Consequently, reported gains are often not directly comparable, and it remains unclear whether improvements stem from modeling advances or from differences in data and evaluation. We address this gap by establishing a strong, well-documented baseline for TB prediction using cough recordings and accompanying clinical metadata from a recently compiled dataset from several countries. Our pipeline is reproducible end-to-end, covering feature extraction, multimodal fusion, cougher-independent evaluation, and uncertainty quantification, and it reports a consistent suite of clinically relevant metrics to enable fair comparison. We further quantify performance for cough audio-only and fused (audio + clinical metadata) models, and release the full experimental protocol to facilitate benchmarking. This baseline is intended to serve as a common reference point and to reduce methodological variance that currently holds back progress in the field.",
      "url": "https://arxiv.org/abs/2601.07969",
      "pdfUrl": "https://arxiv.org/pdf/2601.07969.pdf",
      "titleJa": "咳嗽音による結核スクリーニング：ベースラインモデル、臨床変数、不確実性の定量化"
    },
    {
      "id": "2601.07958",
      "arxivId": "2601.07958",
      "title": "LJ-Spoof: A Generatively Varied Corpus for Audio Anti-Spoofing and Synthesis Source Tracing",
      "authors": [
        "Surya Subramani",
        "Hashim Ali",
        "Hafiz Malik"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Speaker-specific anti-spoofing and synthesis-source tracing are central challenges in audio anti-spoofing. Progress has been hampered by the lack of datasets that systematically vary model architectures, synthesis pipelines, and generative parameters. To address this gap, we introduce LJ-Spoof, a speaker-specific, generatively diverse corpus that systematically varies prosody, vocoders, generative hyperparameters, bona fide prompt sources, training regimes, and neural post-processing. The corpus spans one speakers-including studio-quality recordings-30 TTS families, 500 generatively variant subsets, 10 bona fide neural-processing variants, and more than 3 million utterances. This variation-dense design enables robust speaker-conditioned anti-spoofing and fine-grained synthesis-source tracing. We further position this dataset as both a practical reference training resource and a benchmark evaluation suite for anti-spoofing and source tracing.",
      "url": "https://arxiv.org/abs/2601.07958",
      "pdfUrl": "https://arxiv.org/pdf/2601.07958.pdf",
      "titleJa": "LJ-Spoof: 音声スプーフィング防止と合成音源追跡のための生成的に多様なコーパス"
    },
    {
      "id": "2601.07367",
      "arxivId": "2601.07367",
      "title": "FOCAL: A Novel Benchmarking Technique for Multi-modal Agents",
      "authors": [
        "Aditya Choudhary",
        "Anupam Purwar"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD"
      ],
      "abstract": "With the recent advancements in reasoning capabilities, tool calling using MCP servers and Audio Language Models (ALMs), development and integration of multi-modal agents (with voice and text support) has come to the industry forefront. Cascading pipelines for voice agents still play a central role in the industry owing to their superior reasoning capabilities facilitated by LLMs. Although, cascading pipelines often present error propagation through the pipeline. We propose a framework, FOCAL to benchmark end-to-end reasoning, component-wise error propagation and error analysis for automated as well as human-assisted testing of multi-modal agents (voice to voice + text input). We also share two novel metrics viz. Reasoning and Semantic scores to evaluate efficacy of the agent in having meaningful conversations in voice mode.",
      "url": "https://arxiv.org/abs/2601.07367",
      "pdfUrl": "https://arxiv.org/pdf/2601.07367.pdf",
      "titleJa": "FOCAL: マルチモーダルエージェントのための新しいベンチマーク手法"
    },
    {
      "id": "2601.07331",
      "arxivId": "2601.07331",
      "title": "SEE: Signal Embedding Energy for Quantifying Noise Interference in Large Audio Language Models",
      "authors": [
        "Yuanhe Zhang",
        "Jiayu Tian",
        "Yibo Zhang",
        "Shilinlu Yan",
        "Liang Lin",
        "Zhenhong Zhou",
        "Li Sun",
        "Sen Su"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Large Audio Language Models (LALMs) have been widely applied in real-time scenarios, such as in-car assistants and online meeting comprehension. In practice, audio inputs are often corrupted by device and environmental noise, leading to performance degradation. However, existing LALM studies on noise lack quantitative analysis and rely mainly on intuition and empirical observation, thus failing to understand practical robustness. To address this issue, we introduce Signal Embedding Energy (SEE), a method for quantifying the impact of noise intensity on LALM inputs, enabling the differentiation of LALM robustness in real-world deployments. SEE introduces a perspective based on structured activation subspaces derived from the model's internal representations, which more accurately captures its perception of noise than raw audio features. Across experiments, SEE exhibits a strong correlation with LALM performance, achieving a correlation of 0.98. Surprisingly, traditional audio denoising methods are only marginally effective for LALMs, and, in some cases, even increase SEE and impair performance. This suggests a mismatch between speech-centric denoising objectives and the noise sensitivity of modern LALMs. Therefore, we propose a mitigation strategy derived from SEE to denoise LALM inputs, outperforming existing denoising methods. This paper introduces a novel metric for noise quantification in LALMs, providing guidance for robustness improvements in real-world deployments.",
      "url": "https://arxiv.org/abs/2601.07331",
      "pdfUrl": "https://arxiv.org/pdf/2601.07331.pdf",
      "titleJa": "参照: 大規模音声言語モデルにおけるノイズ干渉の定量化のための信号埋め込みエネルギー"
    },
    {
      "id": "2601.07303",
      "arxivId": "2601.07303",
      "title": "ESDD2: Environment-Aware Speech and Sound Deepfake Detection Challenge Evaluation Plan",
      "authors": [
        "Xueping Zhang",
        "Han Yin",
        "Yang Xiao",
        "Lin Zhang",
        "Ting Dang"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio recorded in real-world environments often contains a mixture of foreground speech and background environmental sounds. With rapid advances in text-to-speech, voice conversion, and other generation models, either component can now be modified independently. Such component-level manipulations are harder to detect, as the remaining unaltered component can mislead the systems designed for whole deepfake audio, and they often sound more natural to human listeners. To address this gap, we have proposed CompSpoofV2 dataset and a separation-enhanced joint learning framework. CompSpoofV2 is a large-scale curated dataset designed for component-level audio anti-spoofing, which contains over 250k audio samples, with a total duration of approximately 283 hours. Based on the CompSpoofV2 and the separation-enhanced joint learning framework, we launch the Environment-Aware Speech and Sound Deepfake Detection Challenge (ESDD2), focusing on component-level spoofing, where both speech and environmental sounds may be manipulated or synthesized, creating a more challenging and realistic detection scenario. The challenge will be held in conjunction with the IEEE International Conference on Multimedia and Expo 2026 (ICME 2026).",
      "url": "https://arxiv.org/abs/2601.07303",
      "pdfUrl": "https://arxiv.org/pdf/2601.07303.pdf",
      "titleJa": "ESDD2: 環境認識型音声ディープフェイク検出チャレンジ評価計画"
    },
    {
      "id": "2601.07237",
      "arxivId": "2601.07237",
      "title": "The ICASSP 2026 Automatic Song Aesthetics Evaluation Challenge",
      "authors": [
        "Guobin Ma",
        "Yuxuan Xia",
        "Jixun Yao",
        "Huixin Xue",
        "Hexin Liu",
        "Shuai Wang",
        "Hao Liu",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "This paper summarizes the ICASSP 2026 Automatic Song Aesthetics Evaluation (ASAE) Challenge, which focuses on predicting the subjective aesthetic scores of AI-generated songs. The challenge consists of two tracks: Track 1 targets the prediction of the overall musicality score, while Track 2 focuses on predicting five fine-grained aesthetic scores. The challenge attracted strong interest from the research community and received numerous submissions from both academia and industry. Top-performing systems significantly surpassed the official baseline, demonstrating substantial progress in aligning objective metrics with human aesthetic preferences. The outcomes establish a standardized benchmark and advance human-aligned evaluation methodologies for modern music generation systems.",
      "url": "https://arxiv.org/abs/2601.07237",
      "pdfUrl": "https://arxiv.org/pdf/2601.07237.pdf",
      "titleJa": "ICASSP 2026 自動歌曲美学評価チャレンジ"
    },
    {
      "id": "2601.07014",
      "arxivId": "2601.07014",
      "title": "DIVINE: Coordinating Multimodal Disentangled Representations for Oro-Facial Neurological Disorder Assessment",
      "authors": [
        "Mohd Mujtaba Akhtar",
        " Girish",
        "Muskaan Singh"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "In this study, we present a multimodal framework for predicting neuro-facial disorders by capturing both vocal and facial cues. We hypothesize that explicitly disentangling shared and modality-specific representations within multimodal foundation model embeddings can enhance clinical interpretability and generalization. To validate this hypothesis, we propose DIVINE a fully disentangled multimodal framework that operates on representations extracted from state-of-the-art (SOTA) audio and video foundation models, incorporating hierarchical variational bottlenecks, sparse gated fusion, and learnable symptom tokens. DIVINE operates in a multitask learning setup to jointly predict diagnostic categories (Healthy Control,ALS, Stroke) and severity levels (Mild, Moderate, Severe). The model is trained using synchronized audio and video inputs and evaluated on the Toronto NeuroFace dataset under full (audio-video) as well as single-modality (audio-only and video-only) test conditions. Our proposed approach, DIVINE achieves SOTA result, with the DeepSeek-VL2 and TRILLsson combination reaching 98.26% accuracy and 97.51% F1-score. Under modality-constrained scenarios, the framework performs well, showing strong generalization when tested with video-only or audio-only inputs. It consistently yields superior performance compared to unimodal models and baseline fusion techniques. To the best of our knowledge, DIVINE is the first framework that combines cross-modal disentanglement, adaptive fusion, and multitask learning to comprehensively assess neurological disorders using synchronized speech and facial video.",
      "url": "https://arxiv.org/abs/2601.07014",
      "pdfUrl": "https://arxiv.org/pdf/2601.07014.pdf",
      "titleJa": "DIVINE: 口腔顔面神経疾患評価のためのマルチモーダル分離表現の調整"
    },
    {
      "id": "2601.06981",
      "arxivId": "2601.06981",
      "title": "Directional Selective Fixed-Filter Active Noise Control Based on a Convolutional Neural Network in Reverberant Environments",
      "authors": [
        "Boxiang Wang",
        "Zhengding Luo",
        "Haowen Li",
        "Dongyuan Shi",
        "Junwei Ji",
        "Ziyi Yang",
        "Woon-Seng Gan"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "Selective fixed-filter active noise control (SFANC) is a novel approach capable of mitigating noise with varying frequency characteristics. It offers faster response and greater computational efficiency compared to traditional adaptive algorithms. However, spatial factors, particularly the influence of the noise source location, are often overlooked. Some existing studies have explored the impact of the direction-of-arrival (DoA) of the noise source on ANC performance, but they are mostly limited to free-field conditions and do not consider the more complex indoor reverberant environments. To address this gap, this paper proposes a learning-based directional SFANC method that incorporates the DoA of the noise source in reverberant environments. In this framework, multiple reference signals are processed by a convolutional neural network (CNN) to estimate the azimuth and elevation angles of the noise source, as well as to identify the most appropriate control filter for effective noise cancellation. Compared to traditional adaptive algorithms, the proposed approach achieves superior noise reduction with shorter response times, even in the presence of reverberations.",
      "url": "https://arxiv.org/abs/2601.06981",
      "pdfUrl": "https://arxiv.org/pdf/2601.06981.pdf",
      "titleJa": "残響環境における畳み込みニューラルネットワークに基づく方向選択固定フィルタアクティブノイズコントロール"
    },
    {
      "id": "2601.06829",
      "arxivId": "2601.06829",
      "title": "MoEScore: Mixture-of-Experts-Based Text-Audio Relevance Score Prediction for Text-to-Audio System Evaluation",
      "authors": [
        "Bochao Sun",
        "Yang Xiao",
        "Han Yin"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Recent advances in generative models have enabled modern Text-to-Audio (TTA) systems to synthesize audio with high perceptual quality. However, TTA systems often struggle to maintain semantic consistency with the input text, leading to mismatches in sound events, temporal tructures, or contextual relationships. Evaluating semantic fidelity in TTA remains a significant challenge. Traditional methods primarily rely on subjective human listening tests, which is time-consuming. To solve this, we propose an objective evaluator based on a Mixture of Experts (MoE) architecture with Sequential Cross-Attention (SeqCoAttn). Our model achieves the first rank in the XACLE Challenge, with an SRCC of 0.6402 (an improvement of 30.6% over the challenge baseline) on the test dataset. Code is available at: https://github.com/S-Orion/MOESCORE.",
      "url": "https://arxiv.org/abs/2601.06829",
      "pdfUrl": "https://arxiv.org/pdf/2601.06829.pdf",
      "titleJa": "MoEScore: テキスト音声変換システム評価のための専門家混合ベースのテキスト音声関連度スコア予測"
    },
    {
      "id": "2601.06662",
      "arxivId": "2601.06662",
      "title": "Dereverberation Filter by Deconvolution with Frequency Bin Specific Faded Impulse Response",
      "authors": [
        "Stefan Ciba"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "This work introduces a robust single-channel inverse filter for dereverberation of non-ideal recordings, validated on real audio. The developed method focuses on the calculation and modification of a discrete impulse response in order to filter the characteristics from a known digital single channel recording setup and room characteristics such as early reflections and reverberations. The aim is a dryer and clearer signal reconstruction, which ideally would be the direct-path signal. The time domain impulse response is calculated from the cepstral domain and faded by means of frequency bin specific exponential decay in the spectrum. The decay rates are obtained by using the blind estimates of reverberation time ratio between recorded output and test signals for each frequency bin. The modified impulse response does filter a recorded audio-signal by deconvolution. The blind estimation is well known and stands out for its robustness to noise and non-idealities. Estimation of a direct path signal is key to many applications.",
      "url": "https://arxiv.org/abs/2601.06662",
      "pdfUrl": "https://arxiv.org/pdf/2601.06662.pdf",
      "titleJa": "周波数ビン特定フェードインパルス応答によるデコンボリューションによる残響除去フィルタ"
    },
    {
      "id": "2601.06621",
      "arxivId": "2601.06621",
      "title": "Stereo Audio Rendering for Personal Sound Zones Using a Binaural Spatially Adaptive Neural Network (BSANN)",
      "authors": [
        "Hao Jiang",
        "Edgar Choueiri"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "A binaural rendering framework for personal sound zones (PSZs) is proposed to enable multiple head-tracked listeners to receive fully independent stereo audio programs. Current PSZ systems typically rely on monophonic rendering and therefore cannot control the left and right ears separately, which limits the quality and accuracy of spatial imaging. The proposed method employs a Binaural Spatially Adaptive Neural Network (BSANN) to generate ear-optimized loudspeaker filters that reconstruct the desired acoustic field at each ear of multiple listeners. The framework integrates anechoically measured loudspeaker frequency responses, analytically modeled transducer directivity, and rigid-sphere head-related transfer functions (HRTFs) to enhance acoustic accuracy and spatial rendering fidelity. An explicit active crosstalk cancellation (XTC) stage further improves three-dimensional spatial perception. Experiments show significant gains in measured objective performance metrics, including inter-zone isolation (IZI), inter-program isolation (IPI), and crosstalk cancellation (XTC), with log-frequency-weighted values of 10.23/10.03 dB (IZI), 11.11/9.16 dB (IPI), and 10.55/11.13 dB (XTC), respectively, over 100-20,000 Hz. The combined use of ear-wise control, accurate acoustic modeling, and integrated active XTC produces a unified rendering method that delivers greater isolation performance, increased robustness to room asymmetry, and more faithful spatial reproduction in real acoustic environments.",
      "url": "https://arxiv.org/abs/2601.06621",
      "pdfUrl": "https://arxiv.org/pdf/2601.06621.pdf",
      "titleJa": "バイノーラル空間適応型ニューラルネットワーク（BSANN）を用いたパーソナルサウンドゾーン向けステレオオーディオレンダリング"
    },
    {
      "id": "2601.06560",
      "arxivId": "2601.06560",
      "title": "Lightweight Resolution-Aware Audio Deepfake Detection via Cross-Scale Attention and Consistency Learning",
      "authors": [
        "K. A. Shahriar"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Audio deepfake detection has become increasingly challenging due to rapid advances in speech synthesis and voice conversion technologies, particularly under channel distortions, replay attacks, and real-world recording conditions. This paper proposes a resolution-aware audio deepfake detection framework that explicitly models and aligns multi-resolution spectral representations through cross-scale attention and consistency learning. Unlike conventional single-resolution or implicit feature-fusion approaches, the proposed method enforces agreement across complementary time--frequency scales. The proposed framework is evaluated on three representative benchmarks: ASVspoof 2019 (LA and PA), the Fake-or-Real (FoR) dataset, and the In-the-Wild Audio Deepfake dataset under a speaker-disjoint protocol. The method achieves near-perfect performance on ASVspoof LA (EER 0.16%), strong robustness on ASVspoof PA (EER 5.09%), FoR rerecorded audio (EER 4.54%), and in-the-wild deepfakes (AUC 0.98, EER 4.81%), significantly outperforming single-resolution and non-attention baselines under challenging conditions. The proposed model remains lightweight and efficient, requiring only 159k parameters and less than 1~GFLOP per inference, making it suitable for practical deployment. Comprehensive ablation studies confirm the critical contributions of cross-scale attention and consistency learning, while gradient-based interpretability analysis reveals that the model learns resolution-consistent and semantically meaningful spectral cues across diverse spoofing conditions. These results demonstrate that explicit cross-resolution modeling provides a principled, robust, and scalable foundation for next-generation audio deepfake detection systems.",
      "url": "https://arxiv.org/abs/2601.06560",
      "pdfUrl": "https://arxiv.org/pdf/2601.06560.pdf",
      "titleJa": "クロススケールアテンションと一貫性学習による軽量解像度認識オーディオディープフェイク検出"
    },
    {
      "id": "2601.06406",
      "arxivId": "2601.06406",
      "title": "Representing Sounds as Neural Amplitude Fields: A Benchmark of Coordinate-MLPs and A Fourier Kolmogorov-Arnold Framework",
      "authors": [
        "Linfei Li",
        "Lin Zhang",
        "Zhong Wang",
        "Fengyi Zhang",
        "Zelin Li",
        "Ying Shen"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Although Coordinate-MLP-based implicit neural representations have excelled in representing radiance fields, 3D shapes, and images, their application to audio signals remains underexplored. To fill this gap, we investigate existing implicit neural representations, from which we extract 3 types of positional encoding and 16 commonly used activation functions. Through combinatorial design, we establish the first benchmark for Coordinate-MLPs in audio signal representations. Our benchmark reveals that Coordinate-MLPs require complex hyperparameter tuning and frequency-dependent initialization, limiting their robustness. To address these issues, we propose Fourier-ASR, a novel framework based on the Fourier series theorem and the Kolmogorov-Arnold representation theorem. Fourier-ASR introduces Fourier Kolmogorov-Arnold Networks (Fourier-KAN), which leverage periodicity and strong nonlinearity to represent audio signals, eliminating the need for additional positional encoding. Furthermore, a Frequency-adaptive Learning Strategy (FaLS) is proposed to enhance the convergence of Fourier-KAN by capturing high-frequency components and preventing overfitting of low-frequency signals. Extensive experiments conducted on natural speech and music datasets reveal that: (1) well-designed positional encoding and activation functions in Coordinate-MLPs can effectively improve audio representation quality; and (2) Fourier-ASR can robustly represent complex audio signals without extensive hyperparameter tuning. Looking ahead, the continuity and infinite resolution of implicit audio representations make our research highly promising for tasks such as audio compression, synthesis, and generation. The source code will be released publicly to ensure reproducibility. The code is available at https://github.com/lif314/Fourier-ASR.",
      "url": "https://arxiv.org/abs/2601.06406",
      "pdfUrl": "https://arxiv.org/pdf/2601.06406.pdf",
      "titleJa": "音を神経振幅場として表現する：座標MLPのベンチマークとフーリエ・コルモゴロフ・アーノルド枠組み"
    },
    {
      "id": "2601.06006",
      "arxivId": "2601.06006",
      "title": "Discriminative-Generative Target Speaker Extraction with Decoder-Only Language Models",
      "authors": [
        "Bang Zeng",
        "Beilong Tang",
        "Wang Xiang",
        "Ming Li"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Target speaker extraction (TSE) aims to recover the speech signal of a desired speaker from a mixed audio recording, given a short enrollment utterance. Most existing TSE approaches are based on discriminative modeling paradigms. Although effective at suppressing interfering speakers, these methods often struggle to produce speech with high perceptual quality and naturalness. To address this limitation, we first propose LauraTSE, a generative TSE model built upon an auto-regressive decoder-only language model. However, purely generative approaches may suffer from hallucinations, content drift, and limited controllability, which may undermine their reliability in complex acoustic scenarios. To overcome these challenges, we further introduce a discriminative-generative TSE framework. In this framework, a discriminative front-end is employed to robustly extract the target speaker's speech, yielding stable and controllable intermediate representations. A generative back-end then operates in the neural audio codec representation space to reconstruct fine-grained speech details and enhance perceptual quality. This two-stage design effectively combines the robustness and controllability of discriminative models with the superior naturalness and quality enhancement capabilities of generative models. Moreover, we systematically investigate collaborative training strategies for the proposed framework, including freezing or fine-tuning the front-end, incorporating an auxiliary SI-SDR loss, and exploring both auto-regressive and non-auto-regressive inference mechanisms. Experimental results demonstrate that the proposed framework achieves a more favorable trade-off among speech quality, intelligibility, and speaker consistency.",
      "url": "https://arxiv.org/abs/2601.06006",
      "pdfUrl": "https://arxiv.org/pdf/2601.06006.pdf",
      "titleJa": "デコーダのみの言語モデルを用いた識別的・生成的ターゲット話者抽出"
    },
    {
      "id": "2601.08537",
      "arxivId": "2601.08537",
      "title": "Weakly Supervised Tabla Stroke Transcription via TI-SDRM: A Rhythm-Aware Lattice Rescoring Framework",
      "authors": [
        "Rahul Bapusaheb Kodag",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Tabla Stroke Transcription (TST) is central to the analysis of rhythmic structure in Hindustani classical music, yet remains challenging due to complex rhythmic organization and the scarcity of strongly annotated data. Existing approaches largely rely on fully supervised learning with onset-level annotations, which are costly and impractical at scale. This work addresses TST in a weakly supervised setting, using only symbolic stroke sequences without temporal alignment. We propose a framework that combines a CTC-based acoustic model with sequence-level rhythmic rescoring. The acoustic model produces a decoding lattice, which is refined using a \\textbf{$T\\bar{a}la$}-Independent Static--Dynamic Rhythmic Model (TI-SDRM) that integrates long-term rhythmic structure with short-term adaptive dynamics through an adaptive interpolation mechanism. We curate a new real-world tabla solo dataset and a complementary synthetic dataset, establishing the first benchmark for weakly supervised TST in Hindustani classical music. Experiments demonstrate consistent and substantial reductions in stroke error rate over acoustic-only decoding, confirming the importance of explicit rhythmic structure for accurate transcription.",
      "url": "https://arxiv.org/abs/2601.08537",
      "pdfUrl": "https://arxiv.org/pdf/2601.08537.pdf",
      "titleJa": "TI-SDRMによる弱教師付きタブラストローク転写：リズムを考慮したラティス再採点フレームワーク"
    },
    {
      "id": "2601.08480",
      "arxivId": "2601.08480",
      "title": "Quantitative Analysis of Proxy Tasks for Anomalous Sound Detection",
      "authors": [
        "Seunghyeon Shin",
        "Seokjin Lee"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Anomalous sound detection (ASD) typically involves self-supervised proxy tasks to learn feature representations from normal sound data, owing to the scarcity of anomalous samples. In ASD research, proxy tasks such as AutoEncoders operate under the explicit assumption that models trained on normal data will increase the reconstruction errors related to anomalies. A natural extension suggests that improved proxy task performance should improve ASD capability; however, this relationship has received little systematic attention. This study addresses this research gap by quantitatively analyzing the relationship between proxy task metrics and ASD performance across five configurations, namely, AutoEncoders, classification, source separation, contrastive learning, and pre-trained models. We evaluate the learned representations using linear probe (linear separability) and Mahalanobis distance (distributional compactness). Our experiments reveal that strong proxy performance does not necessarily improve anomalous sound detection performance. Specifically, classification tasks experience performance saturation owing to insufficient task difficulty, whereas contrastive learning fails to learn meaningful features owing to limited data diversity. Notably, source separation is the only task demonstrating a strong positive correlation, such that improved separation consistently improves anomaly detection. Based on these findings, we highlight the critical importance of task difficulty and objective alignment. Finally, we propose a three-stage alignment verification protocol to guide the design of highly effective proxy tasks for ASD systems.",
      "url": "https://arxiv.org/abs/2601.08480",
      "pdfUrl": "https://arxiv.org/pdf/2601.08480.pdf",
      "titleJa": "異常音検知のための代理タスクの定量分析"
    },
    {
      "id": "2601.07481",
      "arxivId": "2601.07481",
      "title": "Directional reflection modeling via wavenumber-domain reflection coefficient for 3D acoustic field simulation",
      "authors": [
        "Satoshi Hoshika",
        "Takahiro Iwami",
        "Akira Omoto"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This study proposes a framework for incorporating wavenumber-domain acoustic reflection coefficients into sound field analysis to characterize direction-dependent material reflection and scattering phenomena. The reflection coefficient is defined as the amplitude ratio between incident and reflected waves for each propagation direction and is estimated from spatial Fourier transforms of the incident and reflected sound fields. The resulting wavenumber-domain reflection coefficients are converted into an acoustic admittance representation that is directly compatible with numerical methods such as the Boundary Element Method (BEM), enabling simulation of reflections beyond simple specular components. Unlike conventional extended reaction models, the proposed approach avoids explicit modeling of the material interior. This significantly reduces computational cost while allowing direct use of measured data, empirical models, or user-defined directional reflection characteristics. The validity of the proposed formulation was previously demonstrated by the authors through two-dimensional sound field simulations, in which accurate reproduction of direction-dependent reflection behavior was confirmed. In the present work, the framework is extended to three-dimensional analysis, demonstrating its applicability to more realistic and complex acoustic environments. The proposed approach provides a practical and flexible tool for simulating direction-dependent acoustic reflections and scattering, with potential applications in architectural acoustics, material characterization, and noise control.",
      "url": "https://arxiv.org/abs/2601.07481",
      "pdfUrl": "https://arxiv.org/pdf/2601.07481.pdf",
      "titleJa": "3次元音響場シミュレーションのための波数領域反射係数による方向反射モデリング"
    },
    {
      "id": "2601.07064",
      "arxivId": "2601.07064",
      "title": "Bridging Attribution and Open-Set Detection using Graph-Augmented Instance Learning in Synthetic Speech",
      "authors": [
        "Mohd Mujtaba Akhtar",
        " Girish",
        "Farhan Sheth",
        "Muskaan Singh"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We propose a unified framework for not only attributing synthetic speech to its source but also for detecting speech generated by synthesizers that were not encountered during training. This requires methods that move beyond simple detection to support both detailed forensic analysis and open-set generalization. To address this, we introduce SIGNAL, a hybrid framework that combines speech foundation models (SFMs) with graph-based modeling and open-set-aware inference. Our framework integrates Graph Neural Networks (GNNs) and a k-Nearest Neighbor (KNN) classifier, allowing it to capture meaningful relationships between utterances and recognize speech that doesn`t belong to any known generator. It constructs a query-conditioned graph over generator class prototypes, enabling the GNN to reason over relationships among candidate generators, while the KNN branch supports open-set detection via confidence-based thresholding. We evaluate SIGNAL using the DiffSSD dataset, which offers a diverse mix of real speech and synthetic audio from both open-source and commercial diffusion-based TTS systems. To further assess generalization, we also test on the SingFake benchmark. Our results show that SIGNAL consistently improves performance across both tasks, with Mamba-based embeddings delivering especially strong results. To the best of our knowledge, this is the first study to unify graph-based learning and open-set detection for tracing synthetic speech back to its origin.",
      "url": "https://arxiv.org/abs/2601.07064",
      "pdfUrl": "https://arxiv.org/pdf/2601.07064.pdf",
      "titleJa": "合成音声におけるグラフ拡張インスタンス学習を用いた帰属とオープンセット検出の橋渡し"
    },
    {
      "id": "2601.06896",
      "arxivId": "2601.06896",
      "title": "TagSpeech: End-to-End Multi-Speaker ASR and Diarization with Fine-Grained Temporal Grounding",
      "authors": [
        "Mingyue Huo",
        "Yiwen Shao",
        "Yuheng Zhang"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "abstract": "We present TagSpeech, a unified LLM-based framework that utilizes Temporal Anchor Grounding for joint multi-speaker ASR and diarization. The framework is built on two key designs: (1) decoupled semantic and speaker streams fine-tuned via Serialized Output Training (SOT) to learn turn-taking dynamics; and (2) an interleaved time anchor mechanism that not only supports fine-grained timestamp prediction but also acts as a synchronization signal between semantic understanding and speaker tracking. Compared to previous works that primarily focus on speaker-attributed ASR or implicit diarization, TagSpeech addresses the challenge of fine-grained speaker-content alignment and explicitly models \"who spoke what and when\" in an end-to-end manner. Experiments on AMI and AliMeeting benchmarks demonstrate that our method achieves consistent improvements in Diarization Error Rate (DER) over strong end-to-end baselines, including Qwen-Omni and Gemini, particularly in handling complex speech overlaps. Moreover, TagSpeech employs a parameter-efficient training paradigm in which the LLM backbone is frozen and only lightweight projectors are trained, resulting in strong performance with low computational cost.",
      "url": "https://arxiv.org/abs/2601.06896",
      "pdfUrl": "https://arxiv.org/pdf/2601.06896.pdf",
      "titleJa": "TagSpeech: 細粒度時間グラウンディングによるエンドツーエンドのマルチスピーカーASRとダイアライゼーション"
    },
    {
      "id": "2601.06844",
      "arxivId": "2601.06844",
      "title": "Variational decomposition autoencoding improves disentanglement of latent representations",
      "authors": [
        "Ioannis Ziogas",
        "Aamna Al Shehhi",
        "Ahsan H. Khandoker",
        "Leontios J. Hadjileontiadis"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.AS",
        "eess.SP",
        "stat.ML"
      ],
      "abstract": "Understanding the structure of complex, nonstationary, high-dimensional time-evolving signals is a central challenge in scientific data analysis. In many domains, such as speech and biomedical signal processing, the ability to learn disentangled and interpretable representations is critical for uncovering latent generative mechanisms. Traditional approaches to unsupervised representation learning, including variational autoencoders (VAEs), often struggle to capture the temporal and spectral diversity inherent in such data. Here we introduce variational decomposition autoencoding (VDA), a framework that extends VAEs by incorporating a strong structural bias toward signal decomposition. VDA is instantiated through variational decomposition autoencoders (DecVAEs), i.e., encoder-only neural networks that combine a signal decomposition model, a contrastive self-supervised task, and variational prior approximation to learn multiple latent subspaces aligned with time-frequency characteristics. We demonstrate the effectiveness of DecVAEs on simulated data and three publicly available scientific datasets, spanning speech recognition, dysarthria severity evaluation, and emotional speech classification. Our results demonstrate that DecVAEs surpass state-of-the-art VAE-based methods in terms of disentanglement quality, generalization across tasks, and the interpretability of latent encodings. These findings suggest that decomposition-aware architectures can serve as robust tools for extracting structured representations from dynamic signals, with potential applications in clinical diagnostics, human-computer interaction, and adaptive neurotechnologies.",
      "url": "https://arxiv.org/abs/2601.06844",
      "pdfUrl": "https://arxiv.org/pdf/2601.06844.pdf",
      "titleJa": "変分分解オートエンコーディングは潜在表現の分離を改善する"
    },
    {
      "id": "2601.08829",
      "arxivId": "2601.08829",
      "title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System",
      "authors": [
        "Hsiang-Wei Huang",
        "Junbin Lu",
        "Kuang-Ming Chen",
        "Jenq-Neng Hwang"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.",
      "url": "https://arxiv.org/abs/2601.08829",
      "pdfUrl": "https://arxiv.org/pdf/2601.08829.pdf",
      "titleJa": "Eloランク付けレビューシステムにおけるLLMエージェントレビュー担当者のダイナミクスのモデリング"
    },
    {
      "id": "2601.08828",
      "arxivId": "2601.08828",
      "title": "Motion Attribution for Video Generation",
      "authors": [
        "Xindi Wu",
        "Despoina Paschalidou",
        "Jun Gao",
        "Antonio Torralba",
        "Laura Leal-Taixé",
        "Olga Russakovsky",
        "Sanja Fidler",
        "Jonathan Lorraine"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.RO"
      ],
      "abstract": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
      "url": "https://arxiv.org/abs/2601.08828",
      "pdfUrl": "https://arxiv.org/pdf/2601.08828.pdf",
      "titleJa": "ビデオ生成のためのモーションアトリビューション"
    },
    {
      "id": "2601.08816",
      "arxivId": "2601.08816",
      "title": "MemRec: Collaborative Memory-Augmented Agentic Recommender System",
      "authors": [
        "Weixin Chen",
        "Yuhan Zhao",
        "Jingyuan Huang",
        "Zihe Ye",
        "Clark Mingxuan Ju",
        "Tong Zhao",
        "Neil Shah",
        "Li Chen",
        "Yongfeng Zhang"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "abstract": "The evolution of recommender systems has shifted preference storage from rating matrices and dense embeddings to semantic memory in the agentic era. Yet existing agents rely on isolated memory, overlooking crucial collaborative signals. Bridging this gap is hindered by the dual challenges of distilling vast graph contexts without overwhelming reasoning agents with cognitive load, and evolving the collaborative memory efficiently without incurring prohibitive computational costs. To address this, we propose MemRec, a framework that architecturally decouples reasoning from memory management to enable efficient collaborative augmentation. MemRec introduces a dedicated, cost-effective LM_Mem to manage a dynamic collaborative memory graph, serving synthesized, high-signal context to a downstream LLM_Rec. The framework operates via a practical pipeline featuring efficient retrieval and cost-effective asynchronous graph propagation that evolves memory in the background. Extensive experiments on four benchmarks demonstrate that MemRec achieves state-of-the-art performance. Furthermore, architectural analysis confirms its flexibility, establishing a new Pareto frontier that balances reasoning quality, cost, and privacy through support for diverse deployments, including local open-source models. Code:https://github.com/rutgerswiselab/memrec and Homepage: https://memrec.weixinchen.com",
      "url": "https://arxiv.org/abs/2601.08816",
      "pdfUrl": "https://arxiv.org/pdf/2601.08816.pdf",
      "titleJa": "MemRec: 協調型メモリ拡張エージェントレコメンデーションシステム"
    },
    {
      "id": "2601.08811",
      "arxivId": "2601.08811",
      "title": "Reasoning Matters for 3D Visual Grounding",
      "authors": [
        "Hsiang-Wei Huang",
        "Kuang-Ming Chen",
        "Wenhao Chai",
        "Cheng-Yen Yang",
        "Jen-Hao Cheng",
        "Jenq-Neng Hwang"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.",
      "url": "https://arxiv.org/abs/2601.08811",
      "pdfUrl": "https://arxiv.org/pdf/2601.08811.pdf",
      "titleJa": "3Dビジュアルグラウンディングには推論が重要"
    },
    {
      "id": "2601.08808",
      "arxivId": "2601.08808",
      "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
      "authors": [
        "Yao Tang",
        "Li Dong",
        "Yaru Hao",
        "Qingxiu Dong",
        "Furu Wei",
        "Jiatao Gu"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.",
      "url": "https://arxiv.org/abs/2601.08808",
      "pdfUrl": "https://arxiv.org/pdf/2601.08808.pdf",
      "titleJa": "多重思考：トークンワイズ分岐・マージによる推論"
    },
    {
      "id": "2601.08807",
      "arxivId": "2601.08807",
      "title": "S3-CLIP: Video Super Resolution for Person-ReID",
      "authors": [
        "Tamas Endrei",
        "Gyorgy Cserey"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.",
      "url": "https://arxiv.org/abs/2601.08807",
      "pdfUrl": "https://arxiv.org/pdf/2601.08807.pdf",
      "titleJa": "S3-CLIP: 人物識別のためのビデオ超解像"
    },
    {
      "id": "2601.08806",
      "arxivId": "2601.08806",
      "title": "APEX-SWE",
      "authors": [
        "Abhi Kottamasu",
        "Akul Datta",
        "Aakash Barthwal",
        "Chirag Mahapatra",
        "Ajay Arun",
        "Adarsh Hiremath",
        "Brendan Foody",
        "Bertie Vidgen"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).",
      "url": "https://arxiv.org/abs/2601.08806",
      "pdfUrl": "https://arxiv.org/pdf/2601.08806.pdf",
      "titleJa": "APEX-SWE"
    },
    {
      "id": "2601.08785",
      "arxivId": "2601.08785",
      "title": "Uncovering Political Bias in Large Language Models using Parliamentary Voting Records",
      "authors": [
        "Jieying Chen",
        "Karen de Jong",
        "Andreas Poole",
        "Jan Burakowski",
        "Elena Elderson Nosti",
        "Joep Windt",
        "Chendi Wang"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.AI"
      ],
      "abstract": "As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.",
      "url": "https://arxiv.org/abs/2601.08785",
      "pdfUrl": "https://arxiv.org/pdf/2601.08785.pdf",
      "titleJa": "議会投票記録を用いた大規模言語モデルの政治的バイアスの解明"
    },
    {
      "id": "2601.08778",
      "arxivId": "2601.08778",
      "title": "Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards",
      "authors": [
        "Tengjun Jin",
        "Yoojin Choi",
        "Yuxuan Zhu",
        "Daniel Kang"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "abstract": "Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial. In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.",
      "url": "https://arxiv.org/abs/2601.08778",
      "pdfUrl": "https://arxiv.org/pdf/2601.08778.pdf",
      "titleJa": "広範囲にわたる注釈エラーにより、テキストから SQL へのベンチマークとリーダーボードが破壊される"
    },
    {
      "id": "2601.08777",
      "arxivId": "2601.08777",
      "title": "Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling",
      "authors": [
        "Yang Cai",
        "Weiqiang Zheng"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.GT"
      ],
      "abstract": "Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\\to 1$ as $k\\to\\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\\frac{k}{k+1}$, and no method can achieve a faster rate in general. We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.",
      "url": "https://arxiv.org/abs/2601.08777",
      "pdfUrl": "https://arxiv.org/pdf/2601.08777.pdf",
      "titleJa": "漸近的ユニバーサルアライメント：テスト時間スケーリングによる新しいアライメントフレームワーク"
    },
    {
      "id": "2601.08776",
      "arxivId": "2601.08776",
      "title": "Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN",
      "authors": [
        "Yanhua Zhao"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. We present a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. The method combines C01 and C02 fluorescence channels into RGB and learns a bidirectional mapping between fluorescence and H&E domains without paired training data. The architecture uses ResNet-based generators with residual blocks and PatchGAN discriminators, trained with adversarial, cycle-consistency, and identity losses. Experiments on fluorescence microscopy datasets show the model generates realistic pseudo H&E images that preserve morphological structures while adopting H&E-like color characteristics. This enables visualization of fluorescence data in a format familiar to pathologists and supports integration with existing H&E-based analysis pipelines.",
      "url": "https://arxiv.org/abs/2601.08776",
      "pdfUrl": "https://arxiv.org/pdf/2601.08776.pdf",
      "titleJa": "CycleGANを用いた光シート顕微鏡画像の仮想H&Eへの変換"
    },
    {
      "id": "2601.08773",
      "arxivId": "2601.08773",
      "title": "Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs",
      "authors": [
        "Manideep Reddy Chinthareddy"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "abstract": "Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal. Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.",
      "url": "https://arxiv.org/abs/2601.08773",
      "pdfUrl": "https://arxiv.org/pdf/2601.08773.pdf",
      "titleJa": "コードベースのための信頼性の高いGraph-RAG：ASTから導出されたグラフとLLMから抽出された知識グラフ"
    },
    {
      "id": "2601.08768",
      "arxivId": "2601.08768",
      "title": "AI as Entertainment",
      "authors": [
        "Cody Kommers",
        "Ari Holtzman"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "abstract": "Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose \"thick entertainment\" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about \"intelligence\" as social media is about social connection.",
      "url": "https://arxiv.org/abs/2601.08768",
      "pdfUrl": "https://arxiv.org/pdf/2601.08768.pdf",
      "titleJa": "エンターテインメントとしてのAI"
    },
    {
      "id": "2601.08753",
      "arxivId": "2601.08753",
      "title": "Grid-Aware Charging and Operational Optimization for Mixed-Fleet Public Transit",
      "authors": [
        "Rishav Sen",
        "Amutheezan Sivagnanam",
        "Aron Laszka",
        "Ayan Mukhopadhyay",
        "Abhishek Dubey"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "math.OC",
        "cs.AI",
        "eess.SY"
      ],
      "abstract": "The rapid growth of urban populations and the increasing need for sustainable transportation solutions have prompted a shift towards electric buses in public transit systems. However, the effective management of mixed fleets consisting of both electric and diesel buses poses significant operational challenges. One major challenge is coping with dynamic electricity pricing, where charging costs vary throughout the day. Transit agencies must optimize charging assignments in response to such dynamism while accounting for secondary considerations such as seating constraints. This paper presents a comprehensive mixed-integer linear programming (MILP) model to address these challenges by jointly optimizing charging schedules and trip assignments for mixed (electric and diesel bus) fleets while considering factors such as dynamic electricity pricing, vehicle capacity, and route constraints. We address the potential computational intractability of the MILP formulation, which can arise even with relatively small fleets, by employing a hierarchical approach tailored to the fleet composition. By using real-world data from the city of Chattanooga, Tennessee, USA, we show that our approach can result in significant savings in the operating costs of the mixed transit fleets.",
      "url": "https://arxiv.org/abs/2601.08753",
      "pdfUrl": "https://arxiv.org/pdf/2601.08753.pdf",
      "titleJa": "混合車両群の公共交通機関におけるグリッドアウェアな充電と運用の最適化"
    },
    {
      "id": "2601.08747",
      "arxivId": "2601.08747",
      "title": "To Retrieve or To Think? An Agentic Approach for Context Evolution",
      "authors": [
        "Rubing Chen",
        "Jian Wang",
        "Wenjie Li",
        "Xiao-Yong Wei",
        "Qing Li"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks.However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting.It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.",
      "url": "https://arxiv.org/abs/2601.08747",
      "pdfUrl": "https://arxiv.org/pdf/2601.08747.pdf",
      "titleJa": "検索するか考えるか？コンテキスト進化のためのエージェント的アプローチ"
    },
    {
      "id": "2601.08743",
      "arxivId": "2601.08743",
      "title": "TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL",
      "authors": [
        "Jinbo Su",
        "Yuxuan Hu",
        "Cuiping Li",
        "Hong Chen",
        "Jia Li",
        "Lintao Ma",
        "Jing Zhang"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "In Text-to-SQL tasks, existing LLM-based methods often include extensive database schemas in prompts, leading to long context lengths and increased prefilling latency. While user queries typically focus on recurrent table sets-offering an opportunity for KV cache sharing across queries-current inference engines, such as SGLang and vLLM, generate redundant prefix cache copies when processing user queries with varying table orders. To address this inefficiency, we propose precomputing table representations as KV caches offline and querying the required ones online. A key aspect of our approach is the computation of table caches while preserving primary foreign key relationships between tables. Additionally, we construct a Table Trie structure to facilitate efficient KV cache lookups during inference. To enhance cache performance, we introduce a cache management system with a query reranking strategy to improve cache hit rates and a computation loading pipeline for parallelizing model inference and cache loading. Experimental results show that our proposed TableCache achieves up to a 3.62x speedup in Time to First Token (TTFT) with negligible performance degradation.",
      "url": "https://arxiv.org/abs/2601.08743",
      "pdfUrl": "https://arxiv.org/pdf/2601.08743.pdf",
      "titleJa": "TableCache: 低レイテンシのテキストから SQL への変換を実現する主外部キー ガイド付き KV キャッシュ事前計算"
    },
    {
      "id": "2601.08734",
      "arxivId": "2601.08734",
      "title": "TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback",
      "authors": [
        "Prithwish Jana",
        "Sam Davidson",
        "Bhavana Bhasker",
        "Andrey Kan",
        "Anoop Deoras",
        "Laurent Callot"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "abstract": "Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.",
      "url": "https://arxiv.org/abs/2601.08734",
      "pdfUrl": "https://arxiv.org/pdf/2601.08734.pdf",
      "titleJa": "TerraFormer: ポリシーガイド検証フィードバックによって微調整された LLM を備えた自動化された Infrastructure-as-Code"
    },
    {
      "id": "2601.08732",
      "arxivId": "2601.08732",
      "title": "ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning",
      "authors": [
        "Vincent Roca",
        "Martin Bretzner",
        "Hilde Henon",
        "Laurent Puy",
        "Grégory Kuchcinski",
        "Renaud Lopes"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Accurate delineation of acute ischemic stroke lesions in MRI is a key component of stroke diagnosis and management. In recent years, deep learning models have been successfully applied to the automatic segmentation of such lesions. While most proposed architectures are based on the U-Net framework, they primarily differ in their choice of loss functions and in the use of deep supervision, residual connections, and attention mechanisms. Moreover, many implementations are not publicly available, and the optimal configuration for acute ischemic stroke (AIS) lesion segmentation remains unclear. In this work, we introduce ISLA (Ischemic Stroke Lesion Analyzer), a new deep learning model for AIS lesion segmentation from diffusion MRI, trained on three multicenter databases totaling more than 1500 AIS participants. Through systematic optimization of the loss function, convolutional architecture, deep supervision, and attention mechanisms, we developed a robust segmentation framework. We further investigated unsupervised domain adaptation to improve generalization to an external clinical dataset. ISLA outperformed two state-of-the-art approaches for AIS lesion segmentation on an external test set. Codes and trained models will be made publicly available to facilitate reuse and reproducibility.",
      "url": "https://arxiv.org/abs/2601.08732",
      "pdfUrl": "https://arxiv.org/pdf/2601.08732.pdf",
      "titleJa": "ISLA: 深い監視、注意、ドメイン適応、アンサンブル学習を備えたMRIベースの急性虚血性脳卒中病変セグメンテーションのためのU-Net"
    },
    {
      "id": "2601.08731",
      "arxivId": "2601.08731",
      "title": "Learning from Demonstrations via Capability-Aware Goal Sampling",
      "authors": [
        "Yuanlin Duan",
        "Yuning Wang",
        "Wenjie Qiu",
        "He Zhu"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.",
      "url": "https://arxiv.org/abs/2601.08731",
      "pdfUrl": "https://arxiv.org/pdf/2601.08731.pdf",
      "titleJa": "能力を考慮した目標サンプリングによるデモンストレーションからの学習"
    },
    {
      "id": "2601.08713",
      "arxivId": "2601.08713",
      "title": "Real-Time Localization Framework for Autonomous Basketball Robots",
      "authors": [
        "Naren Medarametla",
        "Sreejon Mondal"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "abstract": "Localization is a fundamental capability for autonomous robots, enabling them to operate effectively in dynamic environments. In Robocon 2025, accurate and reliable localization is crucial for improving shooting precision, avoiding collisions with other robots, and navigating the competition field efficiently. In this paper, we propose a hybrid localization algorithm that integrates classical techniques with learning based methods that rely solely on visual data from the court's floor to achieve self-localization on the basketball field.",
      "url": "https://arxiv.org/abs/2601.08713",
      "pdfUrl": "https://arxiv.org/pdf/2601.08713.pdf",
      "titleJa": "自律型バスケットボールロボットのためのリアルタイム位置推定フレームワーク"
    },
    {
      "id": "2601.04592",
      "arxivId": "2601.04592",
      "title": "Density Matrix RNN (DM-RNN): A Quantum Information Theoretic Framework for Modeling Musical Context and Polyphony",
      "authors": [
        "Joonwon Seo",
        "Mariana Montiel"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.LG",
        "cs.SD",
        "math-ph"
      ],
      "abstract": "Classical Recurrent Neural Networks (RNNs) summarize musical context into a deterministic hidden state vector, imposing an information bottleneck that fails to capture the inherent ambiguity in music. We propose the Density Matrix RNN (DM-RNN), a novel theoretical architecture utilizing the Density Matrix. This allows the model to maintain a statistical ensemble of musical interpretations (a mixed state), capturing both classical probabilities and quantum coherences. We rigorously define the temporal dynamics using Quantum Channels (CPTP maps). Crucially, we detail a parameterization strategy based on the Choi-Jamiolkowski isomorphism, ensuring the learned dynamics remain physically valid (CPTP) by construction. We introduce an analytical framework using Von Neumann Entropy to quantify musical uncertainty and Quantum Mutual Information (QMI) to measure entanglement between voices. The DM-RNN provides a mathematically rigorous framework for modeling complex, ambiguous musical structures.",
      "url": "https://arxiv.org/abs/2601.04592",
      "pdfUrl": "https://arxiv.org/pdf/2601.04592.pdf",
      "titleJa": "密度行列RNN（DM-RNN）：音楽的文脈とポリフォニーをモデル化する量子情報理論的枠組み"
    },
    {
      "id": "2601.04343",
      "arxivId": "2601.04343",
      "title": "Summary of The Inaugural Music Source Restoration Challenge",
      "authors": [
        "Yongyi Zang",
        "Jiarui Hai",
        "Wanying Ge",
        "Qiuqiang Kong",
        "Zheqi Dai",
        "Helin Wang",
        "Yuki Mitsufuji",
        "Mark D. Plumbley"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Music Source Restoration (MSR) aims to recover original, unprocessed instrument stems from professionally mixed and degraded audio, requiring the reversal of both production effects and real-world degradations. We present the inaugural MSR Challenge, which features objective evaluation on studio-produced mixtures using Multi-Mel-SNR, Zimtohrli, and FAD-CLAP, alongside subjective evaluation on real-world degraded recordings. Five teams participated in the challenge. The winning system achieved 4.46 dB Multi-Mel-SNR and 3.47 MOS-Overall, corresponding to relative improvements of 91% and 18% over the second-place system, respectively. Per-stem analysis reveals substantial variation in restoration difficulty across instruments, with bass averaging 4.59 dB across all teams, while percussion averages only 0.29 dB. The dataset, evaluation protocols, and baselines are available at https://msrchallenge.com/.",
      "url": "https://arxiv.org/abs/2601.04343",
      "pdfUrl": "https://arxiv.org/pdf/2601.04343.pdf",
      "titleJa": "第1回音楽ソース修復チャレンジの概要"
    },
    {
      "id": "2601.03973",
      "arxivId": "2601.03973",
      "title": "Muse: Towards Reproducible Long-Form Song Generation with Fine-Grained Style Control",
      "authors": [
        "Changhao Jiang",
        "Jiahao Chen",
        "Zhenghao Xiang",
        "Zhixiong Yang",
        "Hanchen Wang",
        "Jiabao Zhuang",
        "Xinmeng Che",
        "Jiajun Sun",
        "Hui Li",
        "Yifei Cao",
        "Shihan Dou",
        "Ming Zhang",
        "Junjie Ye",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Recent commercial systems such as Suno demonstrate strong capabilities in long-form song generation, while academic research remains largely non-reproducible due to the lack of publicly available training data, hindering fair comparison and progress. To this end, we release a fully open-source system for long-form song generation with fine-grained style conditioning, including a licensed synthetic dataset, training and evaluation pipelines, and Muse, an easy-to-deploy song generation model. The dataset consists of 116k fully licensed synthetic songs with automatically generated lyrics and style descriptions paired with audio synthesized by SunoV5. We train Muse via single-stage supervised finetuning of a Qwen-based language model extended with discrete audio tokens using MuCodec, without task-specific losses, auxiliary objectives, or additional architectural components. Our evaluations find that although Muse is trained with a modest data scale and model size, it achieves competitive performance on phoneme error rate, text--music style similarity, and audio aesthetic quality, while enabling controllable segment-level generation across different musical structures. All data, model weights, and training and evaluation pipelines will be publicly released, paving the way for continued progress in controllable long-form song generation research. The project repository is available at https://github.com/yuhui1038/Muse.",
      "url": "https://arxiv.org/abs/2601.03973",
      "pdfUrl": "https://arxiv.org/pdf/2601.03973.pdf",
      "titleJa": "Muse: きめ細かなスタイル制御による再現性の高い長編楽曲生成に向けて"
    },
    {
      "id": "2601.03626",
      "arxivId": "2601.03626",
      "title": "Learning from Limited Labels: Transductive Graph Label Propagation for Indian Music Analysis",
      "authors": [
        "Parampreet Singh",
        "Akshay Raina",
        "Sayeedul Islam Sheikh",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Supervised machine learning frameworks rely on extensive labeled datasets for robust performance on real-world tasks. However, there is a lack of large annotated datasets in audio and music domains, as annotating such recordings is resource-intensive, laborious, and often require expert domain knowledge. In this work, we explore the use of label propagation (LP), a graph-based semi-supervised learning technique, for automatically labeling the unlabeled set in an unsupervised manner. By constructing a similarity graph over audio embeddings, we propagate limited label information from a small annotated subset to a larger unlabeled corpus in a transductive, semi-supervised setting. We apply this method to two tasks in Indian Art Music (IAM): Raga identification and Instrument classification. For both these tasks, we integrate multiple public datasets along with additional recordings we acquire from Prasar Bharati Archives to perform LP. Our experiments demonstrate that LP significantly reduces labeling overhead and produces higher-quality annotations compared to conventional baseline methods, including those based on pretrained inductive models. These results highlight the potential of graph-based semi-supervised learning to democratize data annotation and accelerate progress in music information retrieval.",
      "url": "https://arxiv.org/abs/2601.03626",
      "pdfUrl": "https://arxiv.org/pdf/2601.03626.pdf",
      "titleJa": "限定ラベルからの学習：インド音楽分析のためのトランスダクティブグラフラベル伝播"
    },
    {
      "id": "2601.03612",
      "arxivId": "2601.03612",
      "title": "Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias",
      "authors": [
        "Joonwon Seo"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This monograph introduces a novel approach to polyphonic music generation by addressing the \"Missing Middle\" problem through structural inductive bias. Focusing on Beethoven's piano sonatas as a case study, we empirically verify the independence of pitch and hand attributes using normalized mutual information (NMI=0.167) and propose the Smart Embedding architecture, achieving a 48.30% reduction in parameters. We provide rigorous mathematical proofs using information theory (negligible loss bounded at 0.153 bits), Rademacher complexity (28.09% tighter generalization bound), and category theory to demonstrate improved stability and generalization. Empirical results show a 9.47% reduction in validation loss, confirmed by SVD analysis and an expert listening study (N=53). This dual theoretical and applied framework bridges gaps in AI music generation, offering verifiable insights for mathematically grounded deep learning.",
      "url": "https://arxiv.org/abs/2601.03612",
      "pdfUrl": "https://arxiv.org/pdf/2601.03612.pdf",
      "titleJa": "構造的帰納的バイアスによるポリフォニック音楽生成の数学的基礎"
    },
    {
      "id": "2601.03443",
      "arxivId": "2601.03443",
      "title": "Discriminating real and synthetic super-resolved audio samples using embedding-based classifiers",
      "authors": [
        "Mikhail Silaev",
        "Konstantinos Drossos",
        "Tuomas Virtanen"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "Generative adversarial networks (GANs) and diffusion models have recently achieved state-of-the-art performance in audio super-resolution (ADSR), producing perceptually convincing wideband audio from narrowband inputs. However, existing evaluations primarily rely on signal-level or perceptual metrics, leaving open the question of how closely the distributions of synthetic super-resolved and real wideband audio match. Here we address this problem by analyzing the separability of real and super-resolved audio in various embedding spaces. We consider both middle-band ($4\\to 16$~kHz) and full-band ($16\\to 48$~kHz) upsampling tasks for speech and music, training linear classifiers to distinguish real from synthetic samples based on multiple types of audio embeddings. Comparisons with objective metrics and subjective listening tests reveal that embedding-based classifiers achieve near-perfect separation, even when the generated audio attains high perceptual quality and state-of-the-art metric scores. This behavior is consistent across datasets and models, including recent diffusion-based approaches, highlighting a persistent gap between perceptual quality and true distributional fidelity in ADSR models.",
      "url": "https://arxiv.org/abs/2601.03443",
      "pdfUrl": "https://arxiv.org/pdf/2601.03443.pdf",
      "titleJa": "埋め込みベースの分類器を用いた実在および合成の超解像オーディオサンプルの識別"
    },
    {
      "id": "2601.02983",
      "arxivId": "2601.02983",
      "title": "Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning",
      "authors": [
        "Yuankun Xie",
        "Xiaoxuan Guo",
        "Jiayi Zhou",
        "Tao Wang",
        "Jian Liu",
        "Ruibo Fu",
        "Xiaopeng Wang",
        "Haonan Cheng",
        "Long Ye"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in audio large language models (ALLMs) have made high-quality synthetic audio widely accessible, increasing the risk of malicious audio deepfakes across speech, environmental sounds, singing voice, and music. Real-world audio deepfake detection (ADD) therefore requires all-type detectors that generalize across heterogeneous audio and provide interpretable decisions. Given the strong multi-task generalization ability of ALLMs, we first investigate their performance on all-type ADD under both supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). However, SFT using only binary real/fake labels tends to reduce the model to a black-box classifier, sacrificing interpretability. Meanwhile, vanilla RFT under sparse supervision is prone to reward hacking and can produce hallucinated, ungrounded rationales. To address this, we propose an automatic annotation and polishing pipeline that constructs Frequency-Time structured chain-of-thought (CoT) rationales, producing ~340K cold-start demonstrations. Building on CoT data, we propose Frequency Time-Group Relative Policy Optimization (FT-GRPO), a two-stage training paradigm that cold-starts ALLMs with SFT and then applies GRPO under rule-based frequency-time constraints. Experiments demonstrate that FT-GRPO achieves state-of-the-art performance on all-type ADD while producing interpretable, FT-grounded rationales. The data and code are available online.",
      "url": "https://arxiv.org/abs/2601.02983",
      "pdfUrl": "https://arxiv.org/pdf/2601.02983.pdf",
      "titleJa": "周波数時間強化学習によるオーディオLLMを用いた解釈可能な全タイプオーディオディープフェイク検出"
    },
    {
      "id": "2601.02967",
      "arxivId": "2601.02967",
      "title": "MoE Adapter for Large Audio Language Models: Sparsity, Disentanglement, and Gradient-Conflict-Free",
      "authors": [
        "Yishu Lei",
        "Shuwei He",
        "Jing Hu",
        "Dan Zhang",
        "Xianlong Luo",
        "Danxiang Zhu",
        "Shikun Feng",
        "Rui Liu",
        "Jingzhou He",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Extending the input modality of Large Language Models~(LLMs) to the audio domain is essential for achieving comprehensive multimodal perception. However, it is well-known that acoustic information is intrinsically \\textit{heterogeneous}, entangling attributes such as speech, music, and environmental context. Existing research is limited to a dense, parameter-shared adapter to model these diverse patterns, which induces \\textit{gradient conflict} during optimization, as parameter updates required for distinct attributes contradict each other. To address this limitation, we introduce the \\textit{\\textbf{MoE-Adapter}}, a sparse Mixture-of-Experts~(MoE) architecture designed to decouple acoustic information. Specifically, it employs a dynamic gating mechanism that routes audio tokens to specialized experts capturing complementary feature subspaces while retaining shared experts for global context, thereby mitigating gradient conflicts and enabling fine-grained feature learning. Comprehensive experiments show that the MoE-Adapter achieves superior performance on both audio semantic and paralinguistic tasks, consistently outperforming dense linear baselines with comparable computational costs. Furthermore, we will release the related code and models to facilitate future research.",
      "url": "https://arxiv.org/abs/2601.02967",
      "pdfUrl": "https://arxiv.org/pdf/2601.02967.pdf",
      "titleJa": "大規模音声言語モデルのためのMoEアダプタ：スパース性、分離、勾配衝突フリー"
    },
    {
      "id": "2601.02591",
      "arxivId": "2601.02591",
      "title": "A Music Information Retrieval Approach to Classify Sub-Genres in Role Playing Games",
      "authors": [
        "Daeun Hwang",
        "Xuyuan Cai",
        "Edward F. Melcer",
        "Elin Carstensdottir"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.IR"
      ],
      "abstract": "Video game music (VGM) is often studied under the same lens as film music, which largely focuses on its theoretical functionality with relation to the identified genres of the media. However, till date, we are unaware of any systematic approach that analyzes the quantifiable musical features in VGM across several identified game genres. Therefore, we extracted musical features from VGM in games from three sub-genres of Role-Playing Games (RPG), and then hypothesized how different musical features are correlated to the perceptions and portrayals of each genre. This observed correlation may be used to further suggest such features are relevant to the expected storytelling elements or play mechanics associated with the sub-genre.",
      "url": "https://arxiv.org/abs/2601.02591",
      "pdfUrl": "https://arxiv.org/pdf/2601.02591.pdf",
      "titleJa": "ロールプレイングゲームのサブジャンルを分類するための音楽情報検索アプローチ"
    },
    {
      "id": "2601.02586",
      "arxivId": "2601.02586",
      "title": "Understanding Human Perception of Music Plagiarism Through a Computational Approach",
      "authors": [
        "Daeun Hwang",
        "Hyeonbin Hwang"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.IR"
      ],
      "abstract": "There is a wide variety of music similarity detection algorithms, while discussions about music plagiarism in the real world are often based on audience perceptions. Therefore, we aim to conduct a study to examine the key criteria of human perception of music plagiarism, focusing on the three commonly used musical features in similarity analysis: melody, rhythm, and chord progression. After identifying the key features and levels of variation humans use in perceiving musical similarity, we propose a LLM-as-a-judge framework that applies a systematic, step-by-step approach, drawing on modules that extract such high-level attributes.",
      "url": "https://arxiv.org/abs/2601.02586",
      "pdfUrl": "https://arxiv.org/pdf/2601.02586.pdf",
      "titleJa": "計算論的アプローチによる音楽盗作に対する人間の認識の理解"
    },
    {
      "id": "2601.02357",
      "arxivId": "2601.02357",
      "title": "DARC: Drum accompaniment generation with fine-grained rhythm control",
      "authors": [
        "Trey Brosnan"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.",
      "url": "https://arxiv.org/abs/2601.02357",
      "pdfUrl": "https://arxiv.org/pdf/2601.02357.pdf",
      "titleJa": "DARC: きめ細かなリズムコントロールによるドラム伴奏生成"
    },
    {
      "id": "2601.02101",
      "arxivId": "2601.02101",
      "title": "A Mamba-Based Model for Automatic Chord Recognition",
      "authors": [
        "Chunyu Yuan",
        "Johanna Devaney"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In this work, we propose a new efficient solution, which is a Mamba-based model named BMACE (Bidirectional Mamba-based network, for Automatic Chord Estimation), which utilizes selective structured state-space models in a bidirectional Mamba layer to effectively model temporal dependencies. Our model achieves high prediction performance comparable to state-of-the-art models, with the advantage of requiring fewer parameters and lower computational resources",
      "url": "https://arxiv.org/abs/2601.02101",
      "pdfUrl": "https://arxiv.org/pdf/2601.02101.pdf",
      "titleJa": "自動コード認識のためのMambaベースのモデル"
    },
    {
      "id": "2601.02099",
      "arxivId": "2601.02099",
      "title": "BeatlesFC: Harmonic function annotations of Isophonics' The Beatles dataset",
      "authors": [
        "Ji Yeoung Sim",
        "Rebecca Moranis",
        "Johanna Devaney"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This paper presents BeatlesFC, a set of harmonic function annotations for Isophonics' The Beatles dataset. Harmonic function annotations characterize chord labels as stable (tonic) or unstable (predominant, dominant). They operate at the level of musical phrases, serving as a link between chord labels and higher-level formal structures.",
      "url": "https://arxiv.org/abs/2601.02099",
      "pdfUrl": "https://arxiv.org/pdf/2601.02099.pdf",
      "titleJa": "BeatlesFC: Isophonics の The Beatles データセットの調和関数注釈"
    },
    {
      "id": "2601.01294",
      "arxivId": "2601.01294",
      "title": "Diffusion Timbre Transfer Via Mutual Information Guided Inpainting",
      "authors": [
        "Ching Ho Lee",
        "Javier Nistal",
        "Stefan Lattner",
        "Marco Pasini",
        "George Fazekas"
      ],
      "publishedDate": "2026-01-03",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "We study timbre transfer as an inference-time editing problem for music audio. Starting from a strong pre-trained latent diffusion model, we introduce a lightweight procedure that requires no additional training: (i) a dimension-wise noise injection that targets latent channels most informative of instrument identity, and (ii) an early-step clamping mechanism that re-imposes the input's melodic and rhythmic structure during reverse diffusion. The method operates directly on audio latents and is compatible with text/audio conditioning (e.g., CLAP). We discuss design choices,analyze trade-offs between timbral change and structural preservation, and show that simple inference-time controls can meaningfully steer pre-trained models for style-transfer use cases.",
      "url": "https://arxiv.org/abs/2601.01294",
      "pdfUrl": "https://arxiv.org/pdf/2601.01294.pdf",
      "titleJa": "相互情報誘導による音色拡散転写"
    },
    {
      "id": "2601.00326",
      "arxivId": "2601.00326",
      "title": "MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality",
      "authors": [
        "Torin Hopkins",
        "Shih-Yu Ma",
        "Suibi Che-Chuan Weng",
        "Ming-Yuan Pai",
        "Ellen Yi-Luen Do",
        "Luca Turchet"
      ],
      "publishedDate": "2026-01-01",
      "categories": [
        "cs.HC",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Digital Audio Workstations (DAWs) are central to modern music production but often encumber the musician's workflow, tethering them to a desk and hindering natural interaction with their instrument. Furthermore, effective remote collaboration remains a significant challenge, with existing solutions hampered by network latency and asynchronous file sharing. This paper investigates the potential of Mixed Reality (MR) to overcome these barriers, creating an intuitive environment for real-time, remote musical collaboration. We employ qualitative and speculative design techniques to better understand: 1) how players currently use DAWs, and 2) to imagine a speculative future of collaborative MR-DAWs. To facilitate this discussion, we developed and evaluated the usability of a design probe, MR-DAW. An MR system enabling multiple, geographically dispersed users to control a single, shared DAW instance while moving freely in their local spaces. Our networked system enables each remote musician to use a physical foot pedal for collaborative looping, merging a familiar, hands-free interaction with a shared virtual session. Based on interviews and system evaluations with 20 musicians, we analyze current practices, report on the user experience with our MR system, and speculate on the future of musical collaboration in MR. Our results highlight the affordances of MR for unencumbered musical interaction and provide a speculative outlook on the future of remote collaborative DAWs in the Musical Metaverse.",
      "url": "https://arxiv.org/abs/2601.00326",
      "pdfUrl": "https://arxiv.org/pdf/2601.00326.pdf",
      "titleJa": "MR-DAW: 複合現実における協調型デジタルオーディオワークステーションに向けて"
    },
    {
      "id": "2601.05554",
      "arxivId": "2601.05554",
      "title": "SPAM: Style Prompt Adherence Metric for Prompt-based TTS",
      "authors": [
        "Chanhee Cho",
        "Nayeon Kim",
        "Bugeun Kim"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Prompt-based text-to-speech (TTS) aims to generate speech that adheres to fine-grained style cues provided in a text prompt. However, most prior works depend on neither plausible nor faithful measures to evaluate prompt adherence. That is, they cannot ensure whether the evaluation is grounded on the prompt and is similar to a human. Thus, we present a new automatic metric, the Style Prompt Adherence Metric, which explicitly satisfies both plausibility and faithfulness. Inspired by the CLAP, our approach factorizes speech into acoustic attributes and aligns them with the style prompt. Also, we trained the scorer with a supervised contrastive loss, which could provide a clearer distinction between different semantics. We conducted two experiments on two perspectives. The plausibility experiment showed that SPAM achieved a strong correlation with the mean opinion score (MOS). Also, the faithfulness experiment demonstrated that SPAM is successfully grounded to the given style prompt, as it can discriminate different semantics of the prompt. We believe that SPAM can provide a viable automatic solution for evaluating style prompt adherence of synthesized speech.",
      "url": "https://arxiv.org/abs/2601.05554",
      "pdfUrl": "https://arxiv.org/pdf/2601.05554.pdf",
      "titleJa": "SPAM: プロンプトベースの TTS におけるスタイルプロンプト遵守指標"
    },
    {
      "id": "2601.04744",
      "arxivId": "2601.04744",
      "title": "Semi-Supervised Diseased Detection from Speech Dialogues with Multi-Level Data Modeling",
      "authors": [
        "Xingyuan Li",
        "Mengyue Wu"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Detecting medical conditions from speech acoustics is fundamentally a weakly-supervised learning problem: a single, often noisy, session-level label must be linked to nuanced patterns within a long, complex audio recording. This task is further hampered by severe data scarcity and the subjective nature of clinical annotations. While semi-supervised learning (SSL) offers a viable path to leverage unlabeled data, existing audio methods often fail to address the core challenge that pathological traits are not uniformly expressed in a patient's speech. We propose a novel, audio-only SSL framework that explicitly models this hierarchy by jointly learning from frame-level, segment-level, and session-level representations within unsegmented clinical dialogues. Our end-to-end approach dynamically aggregates these multi-granularity features and generates high-quality pseudo-labels to efficiently utilize unlabeled data. Extensive experiments show the framework is model-agnostic, robust across languages and conditions, and highly data-efficient-achieving, for instance, 90\\% of fully-supervised performance using only 11 labeled samples. This work provides a principled approach to learning from weak, far-end supervision in medical speech analysis.",
      "url": "https://arxiv.org/abs/2601.04744",
      "pdfUrl": "https://arxiv.org/pdf/2601.04744.pdf",
      "titleJa": "多層データモデリングを用いた音声対話からの半教師付き疾患検出"
    },
    {
      "id": "2601.04564",
      "arxivId": "2601.04564",
      "title": "When Tone and Words Disagree: Towards Robust Speech Emotion Recognition under Acoustic-Semantic Conflict",
      "authors": [
        "Dawei Huang",
        "Yongjie Lv",
        "Ruijie Xiong",
        "Chunxiang Jin",
        "Xiaojiang Peng"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Speech Emotion Recognition (SER) systems often assume congruence between vocal emotion and lexical semantics. However, in real-world interactions, acoustic-semantic conflict is common yet overlooked, where the emotion conveyed by tone contradicts the literal meaning of spoken words. We show that state-of-the-art SER models, including ASR-based, self-supervised learning (SSL) approaches and Audio Language Models (ALMs), suffer performance degradation under such conflicts due to semantic bias or entangled acoustic-semantic representations. To address this, we propose the Fusion Acoustic-Semantic (FAS) framework, which explicitly disentangles acoustic and semantic pathways and bridges them through a lightweight, query-based attention module. To enable systematic evaluation, we introduce the Conflict in Acoustic-Semantic Emotion (CASE), the first dataset dominated by clear and interpretable acoustic-semantic conflicts in varied scenarios. Extensive experiments demonstrate that FAS consistently outperforms existing methods in both in-domain and zero-shot settings. Notably, on the CASE benchmark, conventional SER models fail dramatically, while FAS sets a new SOTA with 59.38% accuracy. Our code and datasets is available at https://github.com/24DavidHuang/FAS.",
      "url": "https://arxiv.org/abs/2601.04564",
      "pdfUrl": "https://arxiv.org/pdf/2601.04564.pdf",
      "titleJa": "音調と言葉が一致しないとき：音響的・意味的矛盾下におけるロバストな音声感情認識に向けて"
    },
    {
      "id": "2601.03712",
      "arxivId": "2601.03712",
      "title": "TellWhisper: Tell Whisper Who Speaks When",
      "authors": [
        "Yifan Hu",
        "Peiji Yang",
        "Zhisheng Wang",
        "Yicheng Zhong",
        "Rui Liu"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Multi-speaker automatic speech recognition (MASR) aims to predict ''who spoke when and what'' from multi-speaker speech, a key technology for multi-party dialogue understanding. However, most existing approaches decouple temporal modeling and speaker modeling when addressing ''when'' and ''who'': some inject speaker cues before encoding (e.g., speaker masking), which can cause irreversible information loss; others fuse identity by mixing speaker posteriors after encoding, which may entangle acoustic content with speaker identity. This separation is brittle under rapid turn-taking and overlapping speech, often leading to degraded performance. To address these limitations, we propose TellWhisper, a unified framework that jointly models speaker identity and temporal within the speech encoder. Specifically, we design TS-RoPE, a time-speaker rotary positional encoding: time coordinates are derived from frame indices, while speaker coordinates are derived from speaker activity and pause cues. By applying region-specific rotation angles, the model explicitly captures per-speaker continuity, speaker-turn transitions, and state dynamics, enabling the attention mechanism to simultaneously attend to ''when'' and ''who''. Moreover, to estimate frame-level speaker activity, we develop Hyper-SD, which casts speaker classification in hyperbolic space to enhance inter-class separation and refine speaker-activity estimates. Extensive experiments demonstrate the effectiveness of the proposed approach.",
      "url": "https://arxiv.org/abs/2601.03712",
      "pdfUrl": "https://arxiv.org/pdf/2601.03712.pdf",
      "titleJa": "TellWhisper: 誰がいつ話すかを知らせる"
    },
    {
      "id": "2601.03615",
      "arxivId": "2601.03615",
      "title": "Analyzing Reasoning Shifts in Audio Deepfake Detection under Adversarial Attacks: The Reasoning Tax versus Shield Bifurcation",
      "authors": [
        "Binh Nguyen",
        "Thai Le"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Audio Language Models (ALMs) offer a promising shift towards explainable audio deepfake detections (ADDs), moving beyond \\textit{black-box} classifiers by providing some level of transparency into their predictions via reasoning traces. This necessitates a new class of model robustness analysis: robustness of the predictive reasoning under adversarial attacks, which goes beyond existing paradigm that mainly focuses on the shifts of the final predictions (e.g., fake v.s. real). To analyze such reasoning shifts, we introduce a forensic auditing framework to evaluate the robustness of ALMs' reasoning under adversarial attacks in three inter-connected dimensions: acoustic perception, cognitive coherence, and cognitive dissonance. Our systematic analysis reveals that explicit reasoning does not universally enhance robustness. Instead, we observe a bifurcation: for models exhibiting robust acoustic perception, reasoning acts as a defensive \\textit{``shield''}, protecting them from adversarial attacks. However, for others, it imposes a performance \\textit{``tax''}, particularly under linguistic attacks which reduce cognitive coherence and increase attack success rate. Crucially, even when classification fails, high cognitive dissonance can serve as a \\textit{silent alarm}, flagging potential manipulation. Overall, this work provides a critical evaluation of the role of reasoning in forensic audio deepfake analysis and its vulnerabilities.",
      "url": "https://arxiv.org/abs/2601.03615",
      "pdfUrl": "https://arxiv.org/pdf/2601.03615.pdf",
      "titleJa": "敵対的攻撃下における音声ディープフェイク検出の推論シフトの分析：推論税とシールド分岐"
    },
    {
      "id": "2601.03610",
      "arxivId": "2601.03610",
      "title": "Investigation into respiratory sound classification for an imbalanced data set using hybrid LSTM-KAN architectures",
      "authors": [
        "Nithinkumar K.",
        "Anand R"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Respiratory sounds captured via auscultation contain critical clues for diagnosing pulmonary conditions. Automated classification of these sounds faces challenges due to subtle acoustic differences and severe class imbalance in clinical datasets. This study investigates respiratory sound classification with a focus on mitigating pronounced class imbalance. We propose a hybrid deep learning model that combines a Long Short-Term Memory (LSTM) network for sequential feature encoding with a Kolmogorov-Arnold Network (KAN) for classification. The model is integrated with a comprehensive feature extraction pipeline and targeted imbalance mitigation strategies. Experiments were conducted on a public respiratory sound database comprising six classes with a highly skewed distribution. Techniques such as focal loss, class-specific data augmentation, and Synthetic Minority Over-sampling Technique (SMOTE) were employed to enhance minority class recognition. The proposed Hybrid LSTM-KAN model achieves an overall accuracy of 94.6 percent and a macro-averaged F1 score of 0.703, despite the dominant COPD class accounting for over 86 percent of the data. Improved detection performance is observed for minority classes compared to baseline approaches, demonstrating the effectiveness of the proposed architecture for imbalanced respiratory sound classification.",
      "url": "https://arxiv.org/abs/2601.03610",
      "pdfUrl": "https://arxiv.org/pdf/2601.03610.pdf",
      "titleJa": "ハイブリッドLSTM-KANアーキテクチャを用いた不均衡なデータセットの呼吸音分類の調査"
    },
    {
      "id": "2601.02954",
      "arxivId": "2601.02954",
      "title": "The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models",
      "authors": [
        "Yuhuan You",
        "Lai Wei",
        "Xihong Wu",
        "Tianshu Qu"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Existing large audio-language models perceive the world as \"mono\" -- a single stream of audio that ignores the critical spatial dimension (\"where\") required for universal acoustic scene analysis. To bridge this gap, we first introduce a hierarchical framework for Auditory Scene Analysis (ASA). Guided by this framework, we introduce a system that enables models like Qwen2-Audio to understand and reason about the complex acoustic world. Our framework achieves this through three core contributions: First, we build a large-scale, synthesized binaural audio dataset to provide the rich spatial cues. Second, we design a hybrid feature projector, which leverages parallel semantic and spatial encoders to extract decoupled representations. These distinct streams are integrated via a dense fusion mechanism, ensuring the model receives a holistic view of the acoustic scene. Finally, we employ a progressive training curriculum, advancing from supervised fine-tuning (SFT) to reinforcement learning via Group Relative Policy Optimization (GRPO), to explicitly evolve the model's capabilities towards reasoning. On our comprehensive benchmark, the model demonstrates comparatively strong capability for spatial understanding. By enabling this spatial perception, our work provides a clear pathway for leveraging the powerful reasoning abilities of large models towards holistic acoustic scene analysis, advancing from \"mono\" semantic recognition to spatial intelligence.",
      "url": "https://arxiv.org/abs/2601.02954",
      "pdfUrl": "https://arxiv.org/pdf/2601.02954.pdf",
      "titleJa": "世界は単一ではない：大規模音声言語モデルにおける空間理解の実現"
    },
    {
      "id": "2601.02688",
      "arxivId": "2601.02688",
      "title": "Multi-channel multi-speaker transformer for speech recognition",
      "authors": [
        "Guo Yifan",
        "Tian Yao",
        "Suo Hongbin",
        "Wan Yulong"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "With the development of teleconferencing and in-vehicle voice assistants, far-field multi-speaker speech recognition has become a hot research topic. Recently, a multi-channel transformer (MCT) has been proposed, which demonstrates the ability of the transformer to model far-field acoustic environments. However, MCT cannot encode high-dimensional acoustic features for each speaker from mixed input audio because of the interference between speakers. Based on these, we propose the multi-channel multi-speaker transformer (M2Former) for far-field multi-speaker ASR in this paper. Experiments on the SMS-WSJ benchmark show that the M2Former outperforms the neural beamformer, MCT, dual-path RNN with transform-average-concatenate and multi-channel deep clustering based end-to-end systems by 9.2%, 14.3%, 24.9%, and 52.2% respectively, in terms of relative word error rate reduction.",
      "url": "https://arxiv.org/abs/2601.02688",
      "pdfUrl": "https://arxiv.org/pdf/2601.02688.pdf",
      "titleJa": "音声認識用マルチチャンネルマルチスピーカートランス"
    },
    {
      "id": "2601.02455",
      "arxivId": "2601.02455",
      "title": "Dynamic Quantization Error Propagation in Encoder-Decoder ASR Quantization",
      "authors": [
        "Xinyu Wang",
        "Yajie Luo",
        "Yihong Wu",
        "Liheng Ma",
        "Ziyu Zhao",
        "Jingrui Tian",
        "Lei Ding",
        "Yufei Cui",
        "Xiao-Wen Chang"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Running Automatic Speech Recognition (ASR) models on memory-constrained edge devices requires efficient compression. While layer-wise post-training quantization is effective, it suffers from error accumulation, especially in encoder-decoder architectures. Existing solutions like Quantization Error Propagation (QEP) are suboptimal for ASR due to the model's heterogeneity, processing acoustic features in the encoder while generating text in the decoder. To address this, we propose Fine-grained Alpha for Dynamic Quantization Error Propagation (FADE), which adaptively controls the trade-off between cross-layer error correction and local quantization. Experiments show that FADE significantly improves stability by reducing performance variance across runs, while simultaneously surpassing baselines in mean WER.",
      "url": "https://arxiv.org/abs/2601.02455",
      "pdfUrl": "https://arxiv.org/pdf/2601.02455.pdf",
      "titleJa": "エンコーダ・デコーダASR量子化における動的量子化誤差の伝播"
    },
    {
      "id": "2601.02444",
      "arxivId": "2601.02444",
      "title": "VocalBridge: Latent Diffusion-Bridge Purification for Defeating Perturbation-Based Voiceprint Defenses",
      "authors": [
        "Maryam Abbasihafshejani",
        "AHM Nazmus Sakib",
        "Murtuza Jadliwala"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "The rapid advancement of speech synthesis technologies, including text-to-speech (TTS) and voice conversion (VC), has intensified security and privacy concerns related to voice cloning. Recent defenses attempt to prevent unauthorized cloning by embedding protective perturbations into speech to obscure speaker identity while maintaining intelligibility. However, adversaries can apply advanced purification techniques to remove these perturbations, recover authentic acoustic characteristics, and regenerate cloneable voices. Despite the growing realism of such attacks, the robustness of existing defenses under adaptive purification remains insufficiently studied. Most existing purification methods are designed to counter adversarial noise in automatic speech recognition (ASR) systems rather than speaker verification or voice cloning pipelines. As a result, they fail to suppress the fine-grained acoustic cues that define speaker identity and are often ineffective against speaker verification attacks (SVA). To address these limitations, we propose Diffusion-Bridge (VocalBridge), a purification framework that learns a latent mapping from perturbed to clean speech in the EnCodec latent space. Using a time-conditioned 1D U-Net with a cosine noise schedule, the model enables efficient, transcript-free purification while preserving speaker-discriminative structure. We further introduce a Whisper-guided phoneme variant that incorporates lightweight temporal guidance without requiring ground-truth transcripts. Experimental results show that our approach consistently outperforms existing purification methods in recovering cloneable voices from protected speech. Our findings demonstrate the fragility of current perturbation-based defenses and highlight the need for more robust protection mechanisms against evolving voice-cloning and speaker verification threats.",
      "url": "https://arxiv.org/abs/2601.02444",
      "pdfUrl": "https://arxiv.org/pdf/2601.02444.pdf",
      "titleJa": "VocalBridge: 摂動ベースの声紋防御を破るための潜在的拡散ブリッジ浄化"
    },
    {
      "id": "2601.02432",
      "arxivId": "2601.02432",
      "title": "Quantifying Quanvolutional Neural Networks Robustness for Speech in Healthcare Applications",
      "authors": [
        "Ha Tran",
        "Bipasha Kashyap",
        "Pubudu N. Pathirana"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Speech-based machine learning systems are sensitive to noise, complicating reliable deployment in emotion recognition and voice pathology detection. We evaluate the robustness of a hybrid quantum machine learning model, quanvolutional neural networks (QNNs) against classical convolutional neural networks (CNNs) under four acoustic corruptions (Gaussian noise, pitch shift, temporal shift, and speed variation) in a clean-train/corrupted-test regime. Using AVFAD (voice pathology) and TESS (speech emotion), we compare three QNN models (Random, Basic, Strongly) to a simple CNN baseline (CNN-Base), ResNet-18 and VGG-16 using accuracy and corruption metrics (CE, mCE, RCE, RmCE), and analyze architectural factors (circuit complexity or depth, convergence) alongside per-emotion robustness. QNNs generally outperform the CNN-Base under pitch shift, temporal shift, and speed variation (up to 22% lower CE/RCE at severe temporal shift), while the CNN-Base remains more resilient to Gaussian noise. Among quantum circuits, QNN-Basic achieves the best overall robustness on AVFAD, and QNN-Random performs strongest on TESS. Emotion-wise, fear is most robust (80-90% accuracy under severe corruptions), neutral can collapse under strong Gaussian noise (5.5% accuracy), and happy is most vulnerable to pitch, temporal, and speed distortions. QNNs also converge up to six times faster than the CNN-Base. To our knowledge, this is a systematic study of QNN robustness for speech under common non-adversarial acoustic corruptions, indicating that shallow entangling quantum front-ends can improve noise resilience while sensitivity to additive noise remains a challenge.",
      "url": "https://arxiv.org/abs/2601.02432",
      "pdfUrl": "https://arxiv.org/pdf/2601.02432.pdf",
      "titleJa": "医療アプリケーションにおける音声認識のための量子畳み込みニューラルネットワークの堅牢性の定量化"
    },
    {
      "id": "2601.01568",
      "arxivId": "2601.01568",
      "title": "MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning",
      "authors": [
        "Chunyu Qiang",
        "Jun Wang",
        "Xiaopeng Wang",
        "Kang Yin",
        "Yuxin Guo"
      ],
      "publishedDate": "2026-01-04",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Joint audio-video generation aims to synthesize synchronized multisensory content, yet current unified models struggle with fine-grained acoustic control, particularly for identity-preserving speech. Existing approaches either suffer from temporal misalignment due to cascaded generation or lack the capability to perform zero-shot voice cloning within a joint synthesis framework. In this work, we present MM-Sonate, a multimodal flow-matching framework that unifies controllable audio-video joint generation with zero-shot voice cloning capabilities. Unlike prior works that rely on coarse semantic descriptions, MM-Sonate utilizes a unified instruction-phoneme input to enforce strict linguistic and temporal alignment. To enable zero-shot voice cloning, we introduce a timbre injection mechanism that effectively decouples speaker identity from linguistic content. Furthermore, addressing the limitations of standard classifier-free guidance in multimodal settings, we propose a noise-based negative conditioning strategy that utilizes natural noise priors to significantly enhance acoustic fidelity. Empirical evaluations demonstrate that MM-Sonate establishes new state-of-the-art performance in joint generation benchmarks, significantly outperforming baselines in lip synchronization and speech intelligibility, while achieving voice cloning fidelity comparable to specialized Text-to-Speech systems.",
      "url": "https://arxiv.org/abs/2601.01568",
      "pdfUrl": "https://arxiv.org/pdf/2601.01568.pdf",
      "titleJa": "MM-Sonate: ゼロショット音声クローニングによるマルチモーダル制御可能オーディオ・ビデオ生成"
    },
    {
      "id": "2601.01459",
      "arxivId": "2601.01459",
      "title": "OV-InstructTTS: Towards Open-Vocabulary Instruct Text-to-Speech",
      "authors": [
        "Yong Ren",
        "Jiangyan Yi",
        "Jianhua Tao",
        "Haiyang Sun",
        "Zhengqi Wen",
        "Hao Gu",
        "Le Xu",
        "Ye Bai"
      ],
      "publishedDate": "2026-01-04",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Instruct Text-to-Speech (InstructTTS) leverages natural language descriptions as style prompts to guide speech synthesis. However, existing InstructTTS methods mainly rely on a direct combination of audio-related labels or their diverse rephrasings, making it difficult to handle flexible, high-level instructions. Such rigid control is insufficient for users such as content creators who wish to steer generation with descriptive instructions. To address these constraints, we introduce OV-InstructTTS, a new paradigm for open-vocabulary InstructTTS. We propose a comprehensive solution comprising a newly curated dataset, OV-Speech, and a novel reasoning-driven framework. The OV-Speech dataset pairs speech with open-vocabulary instructions, each augmented with a reasoning process that connects high-level instructions to acoustic features. The reasoning-driven framework infers emotional, acoustic, and paralinguistic information from open-vocabulary instructions before synthesizing speech. Evaluations show that this reasoning-driven approach significantly improves instruction-following fidelity and speech expressiveness. We believe this work can inspire the next user-friendly InstructTTS systems with stronger generalization and real-world applicability. The dataset and demos are publicly available on our project page.",
      "url": "https://arxiv.org/abs/2601.01459",
      "pdfUrl": "https://arxiv.org/pdf/2601.01459.pdf",
      "titleJa": "OV-InstructTTS: オープン語彙指示テキスト読み上げに向けて"
    }
  ],
  "lastUpdated": "2026-01-14T13:00:03.107617",
  "totalCount": 74
}