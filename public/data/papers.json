{
  "papers": [
    {
      "id": "2602.20113",
      "arxivId": "2602.20113",
      "title": "StyleStream: Real-Time Zero-Shot Voice Style Conversion",
      "authors": [
        "Yisi Liu",
        "Nicholas Lee",
        "Gopala Anumanchipalli"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Voice style conversion aims to transform an input utterance to match a target speaker's timbre, accent, and emotion, with a central challenge being the disentanglement of linguistic content from style. While prior work has explored this problem, conversion quality remains limited, and real-time voice style conversion has not been addressed. We propose StyleStream, the first streamable zero-shot voice style conversion system that achieves state-of-the-art performance. StyleStream consists of two components: a Destylizer, which removes style attributes while preserving linguistic content, and a Stylizer, a diffusion transformer (DiT) that reintroduces target style conditioned on reference speech. Robust content-style disentanglement is enforced through text supervision and a highly constrained information bottleneck. This design enables a fully non-autoregressive architecture, achieving real-time voice style conversion with an end-to-end latency of 1 second. Samples and real-time demo: https://berkeley-speech-group.github.io/StyleStream/.",
      "url": "https://arxiv.org/abs/2602.20113",
      "pdfUrl": "https://arxiv.org/pdf/2602.20113.pdf",
      "titleJa": "StyleStream: リアルタイムゼロショット音声スタイル変換"
    },
    {
      "id": "2602.19976",
      "arxivId": "2602.19976",
      "title": "SongEcho: Towards Cover Song Generation via Instance-Adaptive Element-wise Linear Modulation",
      "authors": [
        "Sifei Li",
        "Yang Li",
        "Zizhou Wang",
        "Yuxin Zhang",
        "Fuzhang Wu",
        "Oliver Deussen",
        "Tong-Yee Lee",
        "Weiming Dong"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Cover songs constitute a vital aspect of musical culture, preserving the core melody of an original composition while reinterpreting it to infuse novel emotional depth and thematic emphasis. Although prior research has explored the reinterpretation of instrumental music through melody-conditioned text-to-music models, the task of cover song generation remains largely unaddressed. In this work, we reformulate our cover song generation as a conditional generation, which simultaneously generates new vocals and accompaniment conditioned on the original vocal melody and text prompts. To this end, we present SongEcho, which leverages Instance-Adaptive Element-wise Linear Modulation (IA-EiLM), a framework that incorporates controllable generation by improving both conditioning injection mechanism and conditional representation. To enhance the conditioning injection mechanism, we extend Feature-wise Linear Modulation (FiLM) to an Element-wise Linear Modulation (EiLM), to facilitate precise temporal alignment in melody control. For conditional representations, we propose Instance-Adaptive Condition Refinement (IACR), which refines conditioning features by interacting with the hidden states of the generative model, yielding instance-adaptive conditioning. Additionally, to address the scarcity of large-scale, open-source full-song datasets, we construct Suno70k, a high-quality AI song dataset enriched with comprehensive annotations. Experimental results across multiple datasets demonstrate that our approach generates superior cover songs compared to existing methods, while requiring fewer than 30% of the trainable parameters. The code, dataset, and demos are available at https://github.com/lsfhuihuiff/SongEcho_ICLR2026.",
      "url": "https://arxiv.org/abs/2602.19976",
      "pdfUrl": "https://arxiv.org/pdf/2602.19976.pdf",
      "titleJa": "SongEcho: インスタンス適応型要素単位線形変調によるカバー曲生成に向けて"
    },
    {
      "id": "2602.19825",
      "arxivId": "2602.19825",
      "title": "DTT-BSR: GAN-based DTTNet with RoPE Transformer Enhancement for Music Source Restoration",
      "authors": [
        "Shihong Tan",
        "Haoyu Wang",
        "Youran Ni",
        "Yingzhao Hou",
        "Jiayue Luo",
        "Zipei Hu",
        "Han Dou",
        "Zerui Han",
        "Ningning Pan",
        "Yuzhu Wang",
        "Gongping Huang"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Music source restoration (MSR) aims to recover unprocessed stems from mixed and mastered recordings. The challenge lies in both separating overlapping sources and reconstructing signals degraded by production effects such as compression and reverberation. We therefore propose DTT-BSR, a hybrid generative adversarial network (GAN) combining rotary positional embeddings (RoPE) transformer for long-term temporal modeling with dual-path band-split recurrent neural network (RNN) for multi-resolution spectral processing. Our model achieved 3rd place on the objective leaderboard and 4th place on the subjective leaderboard on the ICASSP 2026 MSR Challenge, demonstrating exceptional generation fidelity and semantic alignment with a compact size of 7.1M parameters.",
      "url": "https://arxiv.org/abs/2602.19825",
      "pdfUrl": "https://arxiv.org/pdf/2602.19825.pdf",
      "titleJa": "DTT-BSR: 音楽ソース復元のための RoPE トランスフォーマー強化を備えた GAN ベースの DTTNet"
    },
    {
      "id": "2602.19816",
      "arxivId": "2602.19816",
      "title": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling",
      "authors": [
        "Yungang Yi"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Long-context modeling is essential for symbolic music generation, since motif repetition and developmental variation can span thousands of musical events. However, practical composition and performance workflows frequently rely on resource-limited devices (e.g., electronic instruments and portable computers), making heavy memory and attention computation difficult to deploy. We introduce Depth-Structured Music Recurrence (DSMR), a recurrent long-context Transformer for full-piece symbolic music modeling that extends context beyond fixed-length excerpts via segment-level recurrence with detached cross-segment states, featuring a layer-wise memory-horizon schedule that budgets recurrent KV states across depth. DSMR is trained in a single left-to-right pass over each complete composition, akin to how a musician experiences it from beginning to end, while carrying recurrent cross-segment states forward. Within this recurrent framework, we systematically study how depth-wise horizon allocations affect optimization, best-checkpoint perplexity, and efficiency. By allocating different history-window lengths across layers while keeping the total recurrent-state budget fixed, DSMR creates depth-dependent temporal receptive fields within a recurrent attention stack without reducing compute depth. Our main instantiation is a two-scale DSMR schedule that allocates long history windows to lower layers and a uniform short window to the remaining layers. Experiments on the piano performance dataset MAESTRO demonstrate that two-scale DSMR provides a practical quality--efficiency recipe for full-length long-context symbolic music modeling with recurrent attention under limited computational resources.",
      "url": "https://arxiv.org/abs/2602.19816",
      "pdfUrl": "https://arxiv.org/pdf/2602.19816.pdf",
      "titleJa": "深度構造化音楽再帰：全曲記号音楽モデリングのための予算化された再帰的注意"
    },
    {
      "id": "2602.19778",
      "arxivId": "2602.19778",
      "title": "Enhancing Automatic Chord Recognition via Pseudo-Labeling and Knowledge Distillation",
      "authors": [
        "Nghia Phan",
        "Rong Jin",
        "Gang Liu",
        "Xiao Dong"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG",
        "cs.MM"
      ],
      "abstract": "Automatic Chord Recognition (ACR) is constrained by the scarcity of aligned chord labels, as well-aligned annotations are costly to acquire. At the same time, open-weight pre-trained models are currently more accessible than their proprietary training data. In this work, we present a two-stage training pipeline that leverages pre-trained models together with unlabeled audio. The proposed method decouples training into two stages. In the first stage, we use a pre-trained BTC model as a teacher to generate pseudo-labels for over 1,000 hours of diverse unlabeled audio and train a student model solely on these pseudo-labels. In the second stage, the student is continually trained on ground-truth labels as they become available, with selective knowledge distillation (KD) from the teacher applied as a regularizer to prevent catastrophic forgetting of the representations learned in the first stage. In our experiments, two models (BTC, 2E1D) were used as students. In stage 1, using only pseudo-labels, the BTC student achieves over 98% of the teacher's performance, while the 2E1D model achieves about 96% across seven standard mir_eval metrics. After a single training run for both students in stage 2, the resulting BTC student model surpasses the traditional supervised learning baseline by 2.5% and the original pre-trained teacher model by 1.55% on average across all metrics. And the resulting 2E1D student model improves from the traditional supervised learning baseline by 3.79% on average and achieves almost the same performance as the teacher. Both cases show the large gains on rare chord qualities.",
      "url": "https://arxiv.org/abs/2602.19778",
      "pdfUrl": "https://arxiv.org/pdf/2602.19778.pdf",
      "titleJa": "擬似ラベル付けと知識蒸留による自動コード認識の強化"
    },
    {
      "id": "2602.19674",
      "arxivId": "2602.19674",
      "title": "Continuous Telemonitoring of Heart Failure using Personalised Speech Dynamics",
      "authors": [
        "Yue Pan",
        "Xingyao Wang",
        "Hanyue Zhang",
        "Liwei Liu",
        "Changxin Li",
        "Gang Yang",
        "Rong Sheng",
        "Yili Xia",
        "Ming Chu"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Remote monitoring of heart failure (HF) via speech signals provides a non-invasive and cost-effective solution for long-term patient management. However, substantial inter-individual heterogeneity in vocal characteristics often limits the accuracy of traditional cross-sectional classification models. To address this, we propose a Longitudinal Intra-Patient Tracking (LIPT) scheme designed to capture the trajectory of relative symptomatic changes within individuals. Central to this framework is a Personalised Sequential Encoder (PSE), which transforms longitudinal speech recordings into context-aware latent representations. By incorporating historical data at each timestamp, the PSE facilitates a holistic assessment of the clinical trajectory rather than modelling discrete visits independently. Experimental results from a cohort of 225 patients demonstrate that the LIPT paradigm significantly outperforms the classic cross-sectional approaches, achieving a recognition accuracy of 99.7% for clinical status transitions. The model's high sensitivity was further corroborated by additional follow-up data, confirming its efficacy in predicting HF deterioration and its potential to secure patient safety in remote, home-based settings. Furthermore, this work addresses the gap in existing literature by providing a comprehensive analysis of different speech task designs and acoustic features. Taken together, the superior performance of the LIPT framework and PSE architecture validates their readiness for integration into long-term telemonitoring systems, offering a scalable solution for remote heart failure management.",
      "url": "https://arxiv.org/abs/2602.19674",
      "pdfUrl": "https://arxiv.org/pdf/2602.19674.pdf",
      "titleJa": "パーソナライズされた音声ダイナミクスを用いた心不全の継続的な遠隔モニタリング"
    },
    {
      "id": "2602.19574",
      "arxivId": "2602.19574",
      "title": "CTC-TTS: LLM-based dual-streaming text-to-speech with CTC alignment",
      "authors": [
        "Hanwen Liu",
        "Saierdaer Yusuyin",
        "Hao Huang",
        "Zhijian Ou"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Large-language-model (LLM)-based text-to-speech (TTS) systems can generate natural speech, but most are not designed for low-latency dual-streaming synthesis. High-quality dual-streaming TTS depends on accurate text--speech alignment and well-designed training sequences that balance synthesis quality and latency. Prior work often relies on GMM-HMM based forced-alignment toolkits (e.g., MFA), which are pipeline-heavy and less flexible than neural aligners; fixed-ratio interleaving of text and speech tokens struggles to capture text--speech alignment regularities. We propose CTC-TTS, which replaces MFA with a CTC based aligner and introduces a bi-word based interleaving strategy. Two variants are designed: CTC-TTS-L (token concatenation along the sequence length) for higher quality and CTC-TTS-F (embedding stacking along the feature dimension) for lower latency. Experiments show that CTC-TTS outperforms fixed-ratio interleaving and MFA-based baselines on streaming synthesis and zero-shot tasks. Speech samples are available at https://ctctts.github.io/.",
      "url": "https://arxiv.org/abs/2602.19574",
      "pdfUrl": "https://arxiv.org/pdf/2602.19574.pdf",
      "titleJa": "CTC-TTS: CTC アライメントを備えた LLM ベースのデュアル ストリーミング テキスト読み上げ"
    },
    {
      "id": "2602.19522",
      "arxivId": "2602.19522",
      "title": "An LLM-Enabled Frequency-Aware Flow Diffusion Model for Natural-Language-Guided Power System Scenario Generation",
      "authors": [
        "Zhenghao Zhou",
        "Yiyan Li",
        "Fei Xie",
        "Lu Wang",
        "Bo Wang",
        "Jiansheng Wang",
        "Zheng Yan",
        "Mo-Yuen Chow"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "eess.SP",
        "cs.SD"
      ],
      "abstract": "Diverse and controllable scenario generation (e.g., wind, solar, load, etc.) is critical for robust power system planning and operation. As AI-based scenario generation methods are becoming the mainstream, existing methods (e.g., Conditional Generative Adversarial Nets) mainly rely on a fixed-length numerical conditioning vector to control the generation results, facing challenges in user conveniency and generation flexibility. In this paper, a natural-language-guided scenario generation framework, named LLM-enabled Frequency-aware Flow Diffusion (LFFD), is proposed to enable users to generate desired scenarios using plain human language. First, a pretrained LLM module is introduced to convert generation requests described by unstructured natural languages into ordered semantic space. Second, instead of using standard diffusion models, a flow diffusion model employing a rectified flow matching objective is introduced to achieve efficient and high-quality scenario generation, taking the LLM output as the model input. During the model training process, a frequency-aware multi-objective optimization algorithm is introduced to mitigate the frequency-bias issue. Meanwhile, a dual-agent framework is designed to create text-scenario training sample pairs as well as to standardize semantic evaluation. Experiments based on large-scale photovoltaic and load datasets demonstrate the effectiveness of the proposed method.",
      "url": "https://arxiv.org/abs/2602.19522",
      "pdfUrl": "https://arxiv.org/pdf/2602.19522.pdf",
      "titleJa": "自然言語誘導による電力系統シナリオ生成のためのLLM対応周波数考慮フロー拡散モデル"
    },
    {
      "id": "2602.19409",
      "arxivId": "2602.19409",
      "title": "AuditoryHuM: Auditory Scene Label Generation and Clustering using Human-MLLM Collaboration",
      "authors": [
        "Henry Zhong",
        "Jörg M. Buchholz",
        "Julian Maclaren",
        "Simon Carlile",
        "Richard F. Lyon"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Manual annotation of audio datasets is labour intensive, and it is challenging to balance label granularity with acoustic separability. We introduce AuditoryHuM, a novel framework for the unsupervised discovery and clustering of auditory scene labels using a collaborative Human-Multimodal Large Language Model (MLLM) approach. By leveraging MLLMs (Gemma and Qwen) the framework generates contextually relevant labels for audio data. To ensure label quality and mitigate hallucinations, we employ zero-shot learning techniques (Human-CLAP) to quantify the alignment between generated text labels and raw audio content. A strategically targeted human-in-the-loop intervention is then used to refine the least aligned pairs. The discovered labels are grouped into thematically cohesive clusters using an adjusted silhouette score that incorporates a penalty parameter to balance cluster cohesion and thematic granularity. Evaluated across three diverse auditory scene datasets (ADVANCE, AHEAD-DS, and TAU 2019), AuditoryHuM provides a scalable, low-cost solution for creating standardised taxonomies. This solution facilitates the training of lightweight scene recognition models deployable to edge devices, such as hearing aids and smart home assistants. The project page and code: https://github.com/Australian-Future-Hearing-Initiative",
      "url": "https://arxiv.org/abs/2602.19409",
      "pdfUrl": "https://arxiv.org/pdf/2602.19409.pdf",
      "titleJa": "AuditoryHuM: 人間とMLLMのコラボレーションによる聴覚シーンラベル生成とクラスタリング"
    },
    {
      "id": "2602.19395",
      "arxivId": "2602.19395",
      "title": "DECAF: Dynamic Envelope Context-Aware Fusion for Speech-Envelope Reconstruction from EEG",
      "authors": [
        "Karan Thakkar",
        "Mounya Elhilali"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "Reconstructing the speech audio envelope from scalp neural recordings (EEG) is a central task for decoding a listener's attentional focus in applications like neuro-steered hearing aids. Current methods for this reconstruction, however, face challenges with fidelity and noise. Prevailing approaches treat it as a static regression problem, processing each EEG window in isolation and ignoring the rich temporal structure inherent in continuous speech. This study introduces a new, dynamic framework for envelope reconstruction that leverages this structure as a predictive temporal prior. We propose a state-space fusion model that combines direct neural estimates from EEG with predictions from recent speech context, using a learned gating mechanism to adaptively balance these cues. To validate this approach, we evaluate our model on the ICASSP 2023 Stimulus Reconstruction benchmark demonstrating significant improvements over static, EEG-only baselines. Our analyses reveal a powerful synergy between the neural and temporal information streams. Ultimately, this work reframes envelope reconstruction not as a simple mapping, but as a dynamic state-estimation problem, opening a new direction for developing more accurate and coherent neural decoding systems.",
      "url": "https://arxiv.org/abs/2602.19395",
      "pdfUrl": "https://arxiv.org/pdf/2602.19395.pdf",
      "titleJa": "DECAF: 脳波から音声エンベロープを再構成するための動的エンベロープコンテキスト認識融合"
    },
    {
      "id": "2602.19316",
      "arxivId": "2602.19316",
      "title": "Pay Attention to CTC: Fast and Robust Pseudo-Labelling for Unified Speech Recognition",
      "authors": [
        "Alexandros Haliassos",
        "Rodrigo Mira",
        "Stavros Petridis"
      ],
      "publishedDate": "2026-02-22",
      "categories": [
        "cs.CV",
        "cs.SD"
      ],
      "abstract": "Unified Speech Recognition (USR) has emerged as a semi-supervised framework for training a single model for audio, visual, and audiovisual speech recognition, achieving state-of-the-art results on in-distribution benchmarks. However, its reliance on autoregressive pseudo-labelling makes training expensive, while its decoupled supervision of CTC and attention branches increases susceptibility to self-reinforcing errors, particularly under distribution shifts involving longer sequences, noise, or unseen domains. We propose CTC-driven teacher forcing, where greedily decoded CTC pseudo-labels are fed into the decoder to generate attention targets in a single forward pass. Although these can be globally incoherent, in the pseudo-labelling setting they enable efficient and effective knowledge transfer. Because CTC and CTC-driven attention pseudo-labels have the same length, the decoder can predict both simultaneously, benefiting from the robustness of CTC and the expressiveness of attention without costly beam search. We further propose mixed sampling to mitigate the exposure bias of the decoder relying solely on CTC inputs. The resulting method, USR 2.0, halves training time, improves robustness to out-of-distribution inputs, and achieves state-of-the-art results on LRS3, LRS2, and WildVSR, surpassing USR and modality-specific self-supervised baselines.",
      "url": "https://arxiv.org/abs/2602.19316",
      "pdfUrl": "https://arxiv.org/pdf/2602.19316.pdf",
      "titleJa": "CTCに注目: 統合音声認識のための高速かつ堅牢な疑似ラベル付け"
    },
    {
      "id": "2602.19166",
      "arxivId": "2602.19166",
      "title": "CosyAccent: Duration-Controllable Accent Normalization Using Source-Synthesis Training Data",
      "authors": [
        "Qibing Bai",
        "Shuhao Shi",
        "Shuai Wang",
        "Yukai Ju",
        "Yannan Wang",
        "Haizhou Li"
      ],
      "publishedDate": "2026-02-22",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Accent normalization (AN) systems often struggle with unnatural outputs and undesired content distortion, stemming from both suboptimal training data and rigid duration modeling. In this paper, we propose a \"source-synthesis\" methodology for training data construction. By generating source L2 speech and using authentic native speech as the training target, our approach avoids learning from TTS artifacts and, crucially, requires no real L2 data in training. Alongside this data strategy, we introduce CosyAccent, a non-autoregressive model that resolves the trade-off between prosodic naturalness and duration control. CosyAccent implicitly models rhythm for flexibility yet offers explicit control over total output duration. Experiments show that, despite being trained without any real L2 speech, CosyAccent achieves significantly improved content preservation and superior naturalness compared to strong baselines trained on real-world data.",
      "url": "https://arxiv.org/abs/2602.19166",
      "pdfUrl": "https://arxiv.org/pdf/2602.19166.pdf",
      "titleJa": "CosyAccent: ソース合成トレーニングデータを用いた継続時間制御可能なアクセント正規化"
    },
    {
      "id": "2602.19163",
      "arxivId": "2602.19163",
      "title": "JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation",
      "authors": [
        "Kai Liu",
        "Yanhao Zheng",
        "Kai Wang",
        "Shengqiong Wu",
        "Rongjunchen Zhang",
        "Jiebo Luo",
        "Dimitrios Hatzinakos",
        "Ziwei Liu",
        "Hao Fei",
        "Tat-Seng Chua"
      ],
      "publishedDate": "2026-02-22",
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "AIGC has rapidly expanded from text-to-image generation toward high-quality multimodal synthesis across video and audio. Within this context, joint audio-video generation (JAVG) has emerged as a fundamental task that produces synchronized and semantically aligned sound and vision from textual descriptions. However, compared with advanced commercial models such as Veo3, existing open-source methods still suffer from limitations in generation quality, temporal synchrony, and alignment with human preferences. To bridge the gap, this paper presents JavisDiT++, a concise yet powerful framework for unified modeling and optimization of JAVG. First, we introduce a modality-specific mixture-of-experts (MS-MoE) design that enables cross-modal interaction efficacy while enhancing single-modal generation quality. Then, we propose a temporal-aligned RoPE (TA-RoPE) strategy to achieve explicit, frame-level synchronization between audio and video tokens. Besides, we develop an audio-video direct preference optimization (AV-DPO) method to align model outputs with human preference across quality, consistency, and synchrony dimensions. Built upon Wan2.1-1.3B-T2V, our model achieves state-of-the-art performance merely with around 1M public training entries, significantly outperforming prior approaches in both qualitative and quantitative evaluations. Comprehensive ablation studies have been conducted to validate the effectiveness of our proposed modules. All the code, model, and dataset are released at https://JavisVerse.github.io/JavisDiT2-page.",
      "url": "https://arxiv.org/abs/2602.19163",
      "pdfUrl": "https://arxiv.org/pdf/2602.19163.pdf",
      "titleJa": "JavisDiT++: オーディオとビデオの統合生成のための統合モデリングと最適化"
    },
    {
      "id": "2602.18899",
      "arxivId": "2602.18899",
      "title": "[b]=[d]-[t]+[p]: Self-supervised Speech Models Discover Phonological Vector Arithmetic",
      "authors": [
        "Kwanghee Choi",
        "Eunjung Yeo",
        "Cheol Jun Cho",
        "David Harwath",
        "David R. Mortensen"
      ],
      "publishedDate": "2026-02-21",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Self-supervised speech models (S3Ms) are known to encode rich phonetic information, yet how this information is structured remains underexplored. We conduct a comprehensive study across 96 languages to analyze the underlying structure of S3M representations, with particular attention to phonological vectors. We first show that there exist linear directions within the model's representation space that correspond to phonological features. We further demonstrate that the scale of these phonological vectors correlate to the degree of acoustic realization of their corresponding phonological features in a continuous manner. For example, the difference between [d] and [t] yields a voicing vector: adding this vector to [p] produces [b], while scaling it results in a continuum of voicing. Together, these findings indicate that S3Ms encode speech using phonologically interpretable and compositional vectors, demonstrating phonological vector arithmetic. All code and interactive demos are available at https://github.com/juice500ml/phonetic-arithmetic .",
      "url": "https://arxiv.org/abs/2602.18899",
      "pdfUrl": "https://arxiv.org/pdf/2602.18899.pdf",
      "titleJa": "[b]=[d]-[t]+[p]: 自己教師あり音声モデルが音韻ベクトル算術を発見"
    },
    {
      "id": "2602.18802",
      "arxivId": "2602.18802",
      "title": "Multi-Channel Speech Enhancement for Cocktail Party Speech Emotion Recognition",
      "authors": [
        "Youjun Chen",
        "Guinan Li",
        "Mengzhe Geng",
        "Xurong Xie",
        "Shujie Hu",
        "Huimeng Wang",
        "Haoning Xu",
        "Chengxi Deng",
        "Jiajun Deng",
        "Zhaoqing Li",
        "Mingyu Cui",
        "Xunying Liu"
      ],
      "publishedDate": "2026-02-21",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This paper highlights the critical importance of multi-channel speech enhancement (MCSE) for speech emotion recognition (ER) in cocktail party scenarios. A multi-channel speech dereverberation and separation front-end integrating DNN-WPE and mask-based MVDR is used to extract the target speaker's speech from the mixture speech, before being fed into the downstream ER back-end using HuBERT- and ViT-based speech and visual features. Experiments on mixture speech constructed using the IEMOCAP and MSP-FACE datasets suggest the MCSE output consistently outperforms domain fine-tuned single-channel speech representations produced by: a) Conformer-based metric GANs; and b) WavLM SSL features with optional SE-ER dual task fine-tuning. Statistically significant increases in weighted, unweighted accuracy and F1 measures by up to 9.5%, 8.5% and 9.1% absolute (17.1%, 14.7% and 16.0% relative) are obtained over the above single-channel baselines. The generalization of IEMOCAP trained MCSE front-ends are also shown when being zero-shot applied to out-of-domain MSP-FACE data.",
      "url": "https://arxiv.org/abs/2602.18802",
      "pdfUrl": "https://arxiv.org/pdf/2602.18802.pdf",
      "titleJa": "カクテルパーティーの音声感情認識のためのマルチチャンネル音声強化"
    },
    {
      "id": "2602.18777",
      "arxivId": "2602.18777",
      "title": "Mind the Gap: Detecting Cluster Exits for Robust Local Density-Based Score Normalization in Anomalous Sound Detection",
      "authors": [
        "Kevin Wilkinghoff",
        "Gordon Wichern",
        "Jonathan Le Roux",
        "Zheng-Hua Tan"
      ],
      "publishedDate": "2026-02-21",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Local density-based score normalization is an effective component of distance-based embedding methods for anomalous sound detection, particularly when data densities vary across conditions or domains. In practice, however, performance depends strongly on neighborhood size. Increasing it can degrade detection accuracy when neighborhood expansion crosses cluster boundaries, violating the locality assumption of local density estimation. This observation motivates adapting the neighborhood size based on locality preservation rather than fixing it in advance. We realize this by proposing cluster exit detection, a lightweight mechanism that identifies distance discontinuities and selects neighborhood sizes accordingly. Experiments across multiple embedding models and datasets show improved robustness to neighborhood-size selection and consistent performance gains.",
      "url": "https://arxiv.org/abs/2602.18777",
      "pdfUrl": "https://arxiv.org/pdf/2602.18777.pdf",
      "titleJa": "ギャップに注意：異常音検出における堅牢な局所密度ベースのスコア正規化のためのクラスター出口検出"
    },
    {
      "id": "2602.18635",
      "arxivId": "2602.18635",
      "title": "Musical Training, but not Mere Exposure to Music, Drives the Emergence of Chroma Equivalence in Artificial Neural Networks",
      "authors": [
        "Lukas Grasse",
        "Matthew S. Tata"
      ],
      "publishedDate": "2026-02-20",
      "categories": [
        "cs.SD",
        "cs.NE"
      ],
      "abstract": "Pitch is a fundamental aspect of auditory perception. Pitch perception is commonly described across two perceptual dimensions: pitch height is the sense that tones with varying frequencies seem to be higher or lower, and chroma equivalence is the cyclical similarity of notes octaves, corresponding to a doubling of fundamental frequency. Existing research is divided on whether chroma equivalence is a learned percept that varies according to musical experience and culture, or is an innate percept that develops automatically. Building on a recent framework that proposes to use ANNs to ask 'why' questions about the brain, we evaluated recent auditory ANNs using representational similarity analysis to test the emergence of pitch height and chroma equivalence in their learned representations. Additionally, we fine-tuned two models, Wav2Vec 2.0 and Data2Vec, on a self-supervised learning task using speech and music, and a supervised music transcription task. We found that all models exhibited varying degrees of pitch height representation, but that only models trained on the supervised music transcription task exhibited chroma equivalence. Mere exposure to music through self-supervised learning was not sufficient for chroma equivalence to emerge. This supports the view that chroma equivalence is a higher-order cognitive computation that emerges to support the specific task of music perception, distinct from other auditory perception such as speech listening. This work also highlights the usefulness of ANNs for probing the developmental conditions that give rise to perceptual representations in humans.",
      "url": "https://arxiv.org/abs/2602.18635",
      "pdfUrl": "https://arxiv.org/pdf/2602.18635.pdf",
      "titleJa": "音楽訓練は、単なる音楽への露出ではなく、人工ニューラルネットワークにおけるクロマ等価性の出現を促進する"
    },
    {
      "id": "2602.18535",
      "arxivId": "2602.18535",
      "title": "Fairness-Aware Partial-label Domain Adaptation for Voice Classification of Parkinson's and ALS",
      "authors": [
        "Arianna Francesconi",
        "Zhixiang Dai",
        "Arthur Stefano Moscheni",
        "Himesh Morgan Perera Kanattage",
        "Donato Cappetta",
        "Fabio Rebecchi",
        "Paolo Soda",
        "Valerio Guarrasi",
        "Rosa Sicilia",
        "Mary-Anne Hartley"
      ],
      "publishedDate": "2026-02-20",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Voice-based digital biomarkers can enable scalable, non-invasive screening and monitoring of Parkinson's disease (PD) and Amyotrophic Lateral Sclerosis (ALS). However, models trained on one cohort or device often fail on new acquisition settings due to cross-device and cross-cohort domain shift. This challenge is amplified in real-world scenarios with partial-label mismatch, where datasets may contain different disease labels and only partially overlap in class space. In addition, voice-based models may exploit demographic cues, raising concerns about gender-related unfairness, particularly when deployed across heterogeneous cohorts. To tackle these challenges, we propose a hybrid framework for unified three-class (healthy/PD/ALS) cross-domain voice classification from partially overlapping cohorts. The method combines style-based domain generalization with conditional adversarial alignment tailored to partial-label settings, reducing negative transfer. An additional adversarial gender branch promotes gender-invariant representations. We conduct a comprehensive evaluation across four heterogeneous sustained-vowel datasets, spanning distinct acquisition settings and devices, under both domain generalization and unsupervised domain adaptation protocols. The proposed approach is compared against twelve state-of-the-art machine learning and deep learning methods, and further evaluated through three targeted ablations, providing the first cross-cohort benchmark and end-to-end domain-adaptive framework for unified healthy/PD/ALS voice classification under partial-label mismatch and fairness constraints. Across all experimental settings, our method consistently achieves the best external generalization over the considered evaluation metrics, while maintaining reduced gender disparities. Notably, no competing method shows statistically significant gains in external performance.",
      "url": "https://arxiv.org/abs/2602.18535",
      "pdfUrl": "https://arxiv.org/pdf/2602.18535.pdf",
      "titleJa": "パーキンソン病およびALSの音声分類のための公平性を考慮した部分ラベル領域適応"
    },
    {
      "id": "2602.18104",
      "arxivId": "2602.18104",
      "title": "MeanVoiceFlow: One-step Nonparallel Voice Conversion with Mean Flows",
      "authors": [
        "Takuhiro Kaneko",
        "Hirokazu Kameoka",
        "Kou Tanaka",
        "Yuto Kondo"
      ],
      "publishedDate": "2026-02-20",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "In voice conversion (VC) applications, diffusion and flow-matching models have exhibited exceptional speech quality and speaker similarity performances. However, they are limited by slow conversion owing to their iterative inference. Consequently, we propose MeanVoiceFlow, a novel one-step nonparallel VC model based on mean flows, which can be trained from scratch without requiring pretraining or distillation. Unlike conventional flow matching that uses instantaneous velocity, mean flows employ average velocity to more accurately compute the time integral along the inference path in a single step. However, training the average velocity requires its derivative to compute the target velocity, which can cause instability. Therefore, we introduce a structural margin reconstruction loss as a zero-input constraint, which moderately regularizes the input-output behavior of the model without harmful statistical averaging. Furthermore, we propose conditional diffused-input training in which a mixture of noise and source data is used as input to the model during both training and inference. This enables the model to effectively leverage source information while maintaining consistency between training and inference. Experimental results validate the effectiveness of these techniques and demonstrate that MeanVoiceFlow achieves performance comparable to that of previous multi-step and distillation-based models, even when trained from scratch. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/meanvoiceflow/.",
      "url": "https://arxiv.org/abs/2602.18104",
      "pdfUrl": "https://arxiv.org/pdf/2602.18104.pdf",
      "titleJa": "MeanVoiceFlow: Mean Flows によるワンステップ非並列音声変換"
    },
    {
      "id": "2602.18030",
      "arxivId": "2602.18030",
      "title": "Methods for Pitch Analysis in Contemporary Popular Music: Multiphonic Tones Across Genres",
      "authors": [
        "Emmanuel Deruty",
        "David Meredith",
        "Yann Macé",
        "Luc Leroy",
        "Dima Tsypkin",
        "Pascal Arbez-Nicolas"
      ],
      "publishedDate": "2026-02-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This study argues that electronic tones routinely used in contemporary popular music - including 808-style bass and power chords - are structurally and perceptually equivalent to multiphonics in contemporary classical music. Using listening tests (n=10) and signal analysis, we show that both types of tones elicit multiple, listener-dependent pitch percepts arising from similar spectral and temporal features. These findings suggest that pitch ambiguity is not confined to experimental classical contexts but is also a feature of mainstream music production.",
      "url": "https://arxiv.org/abs/2602.18030",
      "pdfUrl": "https://arxiv.org/pdf/2602.18030.pdf",
      "titleJa": "現代ポピュラー音楽におけるピッチ分析の方法：ジャンルを超えた多重音"
    },
    {
      "id": "2602.18952",
      "arxivId": "2602.18952",
      "title": "MDM-ASR: Bridging Accuracy and Efficiency in ASR with Diffusion-Based Non-Autoregressive Decoding",
      "authors": [
        "Hao Yen",
        "Pin-Jui Ku",
        "Ante Jukić",
        "Sabato Marco Siniscalchi"
      ],
      "publishedDate": "2026-02-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "In sequence-to-sequence Transformer ASR, autoregressive (AR) models achieve strong accuracy but suffer from slow decoding, while non-autoregressive (NAR) models enable parallel decoding at the cost of degraded performance. We propose a principled NAR ASR framework based on Masked Diffusion Models to reduce this gap. A pre-trained speech encoder is coupled with a Transformer diffusion decoder conditioned on acoustic features and partially masked transcripts for parallel token prediction. To mitigate the training-inference mismatch, we introduce Iterative Self-Correction Training that exposes the model to its own intermediate predictions. We also design a Position-Biased Entropy-Bounded Confidence-based sampler with positional bias to further boost results. Experiments across multiple benchmarks demonstrate consistent gains over prior NAR models and competitive performance with strong AR baselines, while retaining parallel decoding efficiency.",
      "url": "https://arxiv.org/abs/2602.18952",
      "pdfUrl": "https://arxiv.org/pdf/2602.18952.pdf",
      "titleJa": "MDM-ASR: 拡散ベースの非自己回帰デコードによるASRの精度と効率の両立"
    },
    {
      "id": "2602.18721",
      "arxivId": "2602.18721",
      "title": "ReHear: Iterative Pseudo-Label Refinement for Semi-Supervised Speech Recognition via Audio Large Language Models",
      "authors": [
        "Zefang Liu",
        "Chenyang Zhu",
        "Sangwoo Cho",
        "Shi-Xiong Zhang"
      ],
      "publishedDate": "2026-02-21",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Semi-supervised learning in automatic speech recognition (ASR) typically relies on pseudo-labeling, which often suffers from confirmation bias and error accumulation due to noisy supervision. To address this limitation, we propose ReHear, a framework for iterative pseudo-label refinement that integrates an instruction-tuned, audio-aware large language model (LLM) into the self-training loop. Unlike conventional text-based correctors, our approach conditions the LLM on both the ASR hypothesis and the source audio, allowing it to recover phonetically accurate transcripts even from severe recognition errors. These refined pseudo-labels serve as high-fidelity targets for fine-tuning the ASR model in an iterative cycle. Experimental results across diverse benchmarks demonstrate that ReHear effectively mitigates error propagation, consistently outperforming both supervised and pseudo-labeling baselines.",
      "url": "https://arxiv.org/abs/2602.18721",
      "pdfUrl": "https://arxiv.org/pdf/2602.18721.pdf",
      "titleJa": "ReHear: 音声大規模言語モデルを用いた半教師あり音声認識のための反復擬似ラベル改良"
    },
    {
      "id": "2602.18355",
      "arxivId": "2602.18355",
      "title": "Rethinking Flow and Diffusion Bridge Models for Speech Enhancement",
      "authors": [
        "Dahan Wang",
        "Jun Gao",
        "Tong Lei",
        "Yuxiang Hu",
        "Changbao Zhu",
        "Kai Chen",
        "Jing Lu"
      ],
      "publishedDate": "2026-02-20",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Flow matching and diffusion bridge models have emerged as leading paradigms in generative speech enhancement, modeling stochastic processes between paired noisy and clean speech signals based on principles such as flow matching, score matching, and Schrödinger bridge. In this paper, we present a framework that unifies existing flow and diffusion bridge models by interpreting them as constructions of Gaussian probability paths with varying means and variances between paired data. Furthermore, we investigate the underlying consistency between the training/inference procedures of these generative models and conventional predictive models. Our analysis reveals that each sampling step of a well-trained flow or diffusion bridge model optimized with a data prediction loss is theoretically analogous to executing predictive speech enhancement. Motivated by this insight, we introduce an enhanced bridge model that integrates an effective probability path design with key elements from predictive paradigms, including improved network architecture, tailored loss functions, and optimized training strategies. Experiments on denoising and dereverberation tasks demonstrate that the proposed method outperforms existing flow and diffusion baselines with fewer parameters and reduced computational complexity. The results also highlight that the inherently predictive nature of this generative framework imposes limitations on its achievable upper-bound performance.",
      "url": "https://arxiv.org/abs/2602.18355",
      "pdfUrl": "https://arxiv.org/pdf/2602.18355.pdf",
      "titleJa": "音声強調のためのフローと拡散ブリッジモデルの再考"
    },
    {
      "id": "2602.17769",
      "arxivId": "2602.17769",
      "title": "MusicSem: A Semantically Rich Language--Audio Dataset of Natural Music Descriptions",
      "authors": [
        "Rebecca Salganik",
        "Teng Tu",
        "Fei-Yueh Chen",
        "Xiaohao Liu",
        "Keifeng Lu",
        "Ethan Luvisia",
        "Zhiyao Duan",
        "Guillaume Salha-Galvan",
        "Anson Kahng",
        "Yunshan Ma",
        "Jian Kang"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Music representation learning is central to music information retrieval and generation. While recent advances in multimodal learning have improved alignment between text and audio for tasks such as cross-modal music retrieval, text-to-music generation, and music-to-text generation, existing models often struggle to capture users' expressed intent in natural language descriptions of music. This observation suggests that the datasets used to train and evaluate these models do not fully reflect the broader and more natural forms of human discourse through which music is described. In this paper, we introduce MusicSem, a dataset of 32,493 language-audio pairs derived from organic music-related discussions on the social media platform Reddit. Compared to existing datasets, MusicSem captures a broader spectrum of musical semantics, reflecting how listeners naturally describe music in nuanced and human-centered ways. To structure these expressions, we propose a taxonomy of five semantic categories: descriptive, atmospheric, situational, metadata-related, and contextual. In addition to the construction, analysis, and release of MusicSem, we use the dataset to evaluate a wide range of multimodal models for retrieval and generation, highlighting the importance of modeling fine-grained semantics. Overall, MusicSem serves as a novel semantics-aware resource to support future research on human-aligned multimodal music representation learning.",
      "url": "https://arxiv.org/abs/2602.17769",
      "pdfUrl": "https://arxiv.org/pdf/2602.17769.pdf",
      "titleJa": "MusicSem: 意味的に豊かな言語 - 自然な音楽記述の音声データセット"
    },
    {
      "id": "2602.17598",
      "arxivId": "2602.17598",
      "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
      "authors": [
        "Jayadev Billa"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.",
      "url": "https://arxiv.org/abs/2602.17598",
      "pdfUrl": "https://arxiv.org/pdf/2602.17598.pdf",
      "titleJa": "カスケード等価性仮説: 音声 LLM が ASR$\\rightarrow$LLM パイプラインのように動作するのはどのような場合ですか?"
    },
    {
      "id": "2602.17749",
      "arxivId": "2602.17749",
      "title": "Detection and Classification of Cetacean Echolocation Clicks using Image-based Object Detection Methods applied to Advanced Wavelet-based Transformations",
      "authors": [
        "Christopher Hauer"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CV",
        "cs.SD"
      ],
      "abstract": "A challenge in marine bioacoustic analysis is the detection of animal signals, like calls, whistles and clicks, for behavioral studies. Manual labeling is too time-consuming to process sufficient data to get reasonable results. Thus, an automatic solution to overcome the time-consuming data analysis is necessary. Basic mathematical models can detect events in simple environments, but they struggle with complex scenarios, like differentiating signals with a low signal-to-noise ratio or distinguishing clicks from echoes. Deep Learning Neural Networks, such as ANIMAL-SPOT, are better suited for such tasks. DNNs process audio signals as image representations, often using spectrograms created by Short-Time Fourier Transform. However, spectrograms have limitations due to the uncertainty principle, which creates a tradeoff between time and frequency resolution. Alternatives like the wavelet, which provides better time resolution for high frequencies and improved frequency resolution for low frequencies, may offer advantages for feature extraction in complex bioacoustic environments. This thesis shows the efficacy of CLICK-SPOT on Norwegian Killer whale underwater recordings provided by the cetacean biologist Dr. Vester. Keywords: Bioacoustics, Deep Learning, Wavelet Transformation",
      "url": "https://arxiv.org/abs/2602.17749",
      "pdfUrl": "https://arxiv.org/pdf/2602.17749.pdf",
      "titleJa": "高度なウェーブレット変換を適用した画像ベースの物体検出法を用いたクジラ類のエコーロケーションクリックの検出と分類"
    },
    {
      "id": "2602.17157",
      "arxivId": "2602.17157",
      "title": "CC-G2PnP: Streaming Grapheme-to-Phoneme and prosody with Conformer-CTC for unsegmented languages",
      "authors": [
        "Yuma Shirahata",
        "Ryuichi Yamamoto"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We propose CC-G2PnP, a streaming grapheme-to-phoneme and prosody (G2PnP) model to connect large language model and text-to-speech in a streaming manner. CC-G2PnP is based on Conformer-CTC architecture. Specifically, the input grapheme tokens are processed chunk by chunk, which enables streaming inference of phonemic and prosodic (PnP) labels. By guaranteeing minimal look-ahead size to each input token, the proposed model can consider future context in each token, which leads to stable PnP label prediction. Unlike previous streaming methods that depend on explicit word boundaries, the CTC decoder in CC-G2PnP effectively learns the alignment between graphemes and phonemes during training, making it applicable to unsegmented languages. Experiments on a Japanese dataset, which has no explicit word boundaries, show that CC-G2PnP significantly outperforms the baseline streaming G2PnP model in the accuracy of PnP label prediction.",
      "url": "https://arxiv.org/abs/2602.17157",
      "pdfUrl": "https://arxiv.org/pdf/2602.17157.pdf",
      "titleJa": "CC-G2PnP: 非分節言語における Conformer-CTC を用いた書記素から音素への変換と韻律のストリーミング"
    },
    {
      "id": "2602.16687",
      "arxivId": "2602.16687",
      "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
      "authors": [
        "Potsawee Manakul",
        "Woody Haosheng Gan",
        "Martijn Bartelds",
        "Guangzhi Sun",
        "William Held",
        "Diyi Yang"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.",
      "url": "https://arxiv.org/abs/2602.16687",
      "pdfUrl": "https://arxiv.org/pdf/2602.16687.pdf",
      "titleJa": "インターリーブされたセマンティック、音響、テキストトークンによるオープン離散オーディオ基盤モデルのスケーリング"
    },
    {
      "id": "2602.17732",
      "arxivId": "2602.17732",
      "title": "SIRUP: A diffusion-based virtual upmixer of steering vectors for highly-directive spatialization with first-order ambisonics",
      "authors": [
        "Emilio Picard",
        "Diego Di Carlo",
        "Aditya Arie Nugraha",
        "Mathieu Fontaine",
        "Kazuyoshi Yoshii"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "This paper presents virtual upmixing of steering vectors captured by a fewer-channel spherical microphone array. This challenge has conventionally been addressed by recovering the directions and signals of sound sources from first-order ambisonics (FOA) data, and then rendering the higher-order ambisonics (HOA) data using a physics-based acoustic simulator. This approach, however, struggles to handle the mutual dependency between the spatial directivity of source estimation and the spatial resolution of FOA ambisonics data. Our method, named SIRUP, employs a latent diffusion model architecture. Specifically, a variational autoencoder (VAE) is used to learn a compact encoding of the HOA data in a latent space and a diffusion model is then trained to generate the HOA embeddings, conditioned by the FOA data. Experimental results showed that SIRUP achieved a significant improvement compared to FOA systems for steering vector upmixing, source localization, and speech denoising.",
      "url": "https://arxiv.org/abs/2602.17732",
      "pdfUrl": "https://arxiv.org/pdf/2602.17732.pdf",
      "titleJa": "SIRUP: 一次アンビソニックスによる高指向性空間化を実現するステアリングベクトルの拡散ベース仮想アップミキサー"
    },
    {
      "id": "2602.16442",
      "arxivId": "2602.16442",
      "title": "Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA",
      "authors": [
        "Kamil Jeziorek",
        "Piotr Wzorek",
        "Krzysztof Blachut",
        "Hiroshi Nakano",
        "Manon Dampfhoffer",
        "Thomas Mesquida",
        "Hiroaki Nishi",
        "Thomas Dalgaty",
        "Tomasz Kryjak"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA implementation of event-graph neural networks for audio processing. We utilise an artificial cochlea that converts time-series signals into sparse event data, reducing memory and computation costs. Our architecture was implemented on a SoC FPGA and evaluated on two open-source datasets. For classification task, our baseline floating-point model achieves 92.7% accuracy on SHD dataset - only 2.4% below the state of the art - while requiring over 10x and 67x fewer parameters. On SSC, our models achieve 66.9-71.0% accuracy. Compared to FPGA-based spiking neural networks, our quantised model reaches 92.3% accuracy, outperforming them by up to 19.3% while reducing resource usage and latency. For SSC, we report the first hardware-accelerated evaluation. We further demonstrate the first end-to-end FPGA implementation of event-audio keyword spotting, combining graph convolutional layers with recurrent sequence modelling. The system achieves up to 95% word-end detection accuracy, with only 10.53 microsecond latency and 1.18 W power consumption, establishing a strong benchmark for energy-efficient event-driven KWS.",
      "url": "https://arxiv.org/abs/2602.16442",
      "pdfUrl": "https://arxiv.org/pdf/2602.16442.pdf",
      "titleJa": "ハードウェアアクセラレーショングラフニューラルネットワーク：SoC FPGA上でのニューロモルフィックイベントベースのオーディオ分類とキーワードスポッティングの代替アプローチ"
    },
    {
      "id": "2602.16421",
      "arxivId": "2602.16421",
      "title": "SELEBI: Percussion-aware Time Stretching via Selective Magnitude Spectrogram Compression by Nonstationary Gabor Transform",
      "authors": [
        "Natsuki Akaishi",
        "Nicki Holighaus",
        "Kohei Yatabe"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Phase vocoder-based time-stretching is a widely used technique for the time-scale modification of audio signals. However, conventional implementations suffer from ``percussion smearing,'' a well-known artifact that significantly degrades the quality of percussive components. We attribute this artifact to a fundamental time-scale mismatch between the temporally smeared magnitude spectrogram and the localized, newly generated phase. To address this, we propose SELEBI, a signal-adaptive phase vocoder algorithm that significantly reduces percussion smearing while preserving stability and the perfect reconstruction property. Unlike conventional methods that rely on heuristic processing or component separation, our approach leverages the nonstationary Gabor transform. By dynamically adapting analysis window lengths to assign short windows to intervals containing significant energy associated with percussive components, we directly compute a temporally localized magnitude spectrogram from the time-domain signal. This approach ensures greater consistency between the temporal structures of the magnitude and phase. Furthermore, the perfect reconstruction property of the nonstationary Gabor transform guarantees stable, high-fidelity signal synthesis, in contrast to previous heuristic approaches. Experimental results demonstrate that the proposed method effectively mitigates percussion smearing and yields natural sound quality.",
      "url": "https://arxiv.org/abs/2602.16421",
      "pdfUrl": "https://arxiv.org/pdf/2602.16421.pdf",
      "titleJa": "SELEBI: 非定常ガボール変換による選択的振幅スペクトログラム圧縮による打楽器を考慮した時間伸縮"
    },
    {
      "id": "2602.16416",
      "arxivId": "2602.16416",
      "title": "Online Single-Channel Audio-Based Sound Speed Estimation for Robust Multi-Channel Audio Control",
      "authors": [
        "Andreas Jonas Fuglsig",
        "Mads Græsbøll Christensen",
        "Jesper Rindom Jensen"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Robust spatial audio control relies on accurate acoustic propagation models, yet environmental variations, especially changes in the speed of sound, cause systematic mismatches that degrade performance. Existing methods either assume known sound speed, require multiple microphones, or rely on separate calibration, making them impractical for systems with minimal sensing. We propose an online sound speed estimator that operates during general multichannel audio playback and requires only a single observation microphone. The method exploits the structured effect of sound speed on the reproduced signal and estimates it by minimizing the mismatch between the measured audio and a parametric acoustic model. Simulations show accurate tracking of sound speed for diverse input signals and improved spatial control performance when the estimates are used to compensate propagation errors in a sound zone control framework.",
      "url": "https://arxiv.org/abs/2602.16416",
      "pdfUrl": "https://arxiv.org/pdf/2602.16416.pdf",
      "titleJa": "堅牢なマルチチャンネルオーディオ制御のためのオンラインシングルチャンネルオーディオベースの音速推定"
    },
    {
      "id": "2602.16399",
      "arxivId": "2602.16399",
      "title": "Multi-Channel Replay Speech Detection using Acoustic Maps",
      "authors": [
        "Michael Neri",
        "Tuomas Virtanen"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Replay attacks remain a critical vulnerability for automatic speaker verification systems, particularly in real-time voice assistant applications. In this work, we propose acoustic maps as a novel spatial feature representation for replay speech detection from multi-channel recordings. Derived from classical beamforming over discrete azimuth and elevation grids, acoustic maps encode directional energy distributions that reflect physical differences between human speech radiation and loudspeaker-based replay. A lightweight convolutional neural network is designed to operate on this representation, achieving competitive performance on the ReMASC dataset with approximately 6k trainable parameters. Experimental results show that acoustic maps provide a compact and physically interpretable feature space for replay attack detection across different devices and acoustic environments.",
      "url": "https://arxiv.org/abs/2602.16399",
      "pdfUrl": "https://arxiv.org/pdf/2602.16399.pdf",
      "titleJa": "音響マップを用いたマルチチャンネル再生音声検出"
    },
    {
      "id": "2602.16256",
      "arxivId": "2602.16256",
      "title": "Color-based Emotion Representation for Speech Emotion Recognition",
      "authors": [
        "Ryotaro Nagase",
        "Ryoichi Takashima",
        "Yoichi Yamashita"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Speech emotion recognition (SER) has traditionally relied on categorical or dimensional labels. However, this technique is limited in representing both the diversity and interpretability of emotions. To overcome this limitation, we focus on color attributes, such as hue, saturation, and value, to represent emotions as continuous and interpretable scores. We annotated an emotional speech corpus with color attributes via crowdsourcing and analyzed them. Moreover, we built regression models for color attributes in SER using machine learning and deep learning, and explored the multitask learning of color attribute regression and emotion classification. As a result, we demonstrated the relationship between color attributes and emotions in speech, and successfully developed color attribute regression models for SER. We also showed that multitask learning improved the performance of each task.",
      "url": "https://arxiv.org/abs/2602.16256",
      "pdfUrl": "https://arxiv.org/pdf/2602.16256.pdf",
      "titleJa": "音声感情認識のための色彩に基づく感情表現"
    },
    {
      "id": "2602.20159",
      "arxivId": "2602.20159",
      "title": "A Very Big Video Reasoning Suite",
      "authors": [
        "Maijunxian Wang",
        "Ruisi Wang",
        "Juyi Lin",
        "Ran Ji",
        "Thaddäus Wiedemer",
        "Qingying Gao",
        "Dezhi Luo",
        "Yaoyao Qian",
        "Lianyu Huang",
        "Zelong Hong",
        "Jiahui Ge",
        "Qianli Ma",
        "Hang He",
        "Yifan Zhou",
        "Lingzi Guo",
        "Lantao Mei",
        "Jiachen Li",
        "Hanwen Xing",
        "Tianqi Zhao",
        "Fengyuan Yu",
        "Weihang Xiao",
        "Yizheng Jiao",
        "Jianheng Hou",
        "Danyang Zhang",
        "Pengcheng Xu",
        "Boyang Zhong",
        "Zehong Zhao",
        "Gaoyun Fang",
        "John Kitaoka",
        "Yile Xu",
        "Hua Xu",
        "Kenton Blacutt",
        "Tin Nguyen",
        "Siyuan Song",
        "Haoran Sun",
        "Shaoyue Wen",
        "Linyang He",
        "Runming Wang",
        "Yanzhi Wang",
        "Mengyue Yang",
        "Ziqiao Ma",
        "Raphaël Millière",
        "Freda Shi",
        "Nuno Vasconcelos",
        "Daniel Khashabi",
        "Alan Yuille",
        "Yilun Du",
        "Ziming Liu",
        "Bo Li",
        "Dahua Lin",
        "Ziwei Liu",
        "Vikash Kumar",
        "Yijiang Li",
        "Lei Yang",
        "Zhongang Cai",
        "Hokin Deng"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.RO"
      ],
      "abstract": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
      "url": "https://arxiv.org/abs/2602.20159",
      "pdfUrl": "https://arxiv.org/pdf/2602.20159.pdf",
      "titleJa": "非常に大規模なビデオ推論スイート"
    },
    {
      "id": "2602.20152",
      "arxivId": "2602.20152",
      "title": "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data",
      "authors": [
        "Zhenyao Ma",
        "Yue Liang",
        "Dongxu Li"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "abstract": "Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.",
      "url": "https://arxiv.org/abs/2602.20152",
      "pdfUrl": "https://arxiv.org/pdf/2602.20152.pdf",
      "titleJa": "行動学習（BL）：データから階層的最適化構造を学習する"
    },
    {
      "id": "2602.20144",
      "arxivId": "2602.20144",
      "title": "Agentic AI for Scalable and Robust Optical Systems Control",
      "authors": [
        "Zehao Wang",
        "Mingzhe Han",
        "Wei Cheng",
        "Yue-Kai Huang",
        "Philip Ji",
        "Denton Wu",
        "Mahdi Safari",
        "Flemming Holtorf",
        "Kenaish AlQubaisi",
        "Norbert M. Linke",
        "Danyang Zhuo",
        "Yiran Chen",
        "Ting Wang",
        "Dirk Englund",
        "Tingjun Chen"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.NI"
      ],
      "abstract": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems.",
      "url": "https://arxiv.org/abs/2602.20144",
      "pdfUrl": "https://arxiv.org/pdf/2602.20144.pdf",
      "titleJa": "スケーラブルで堅牢な光学システム制御のためのエージェントAI"
    },
    {
      "id": "2602.20141",
      "arxivId": "2602.20141",
      "title": "Recurrent Structural Policy Gradient for Partially Observable Mean Field Games",
      "authors": [
        "Clarisse Wibault",
        "Johannes Forkel",
        "Sebastian Towers",
        "Tiphaine Wibault",
        "Juan Duque",
        "George Whittle",
        "Andreas Schaab",
        "Yucheng Yang",
        "Chiyuan Wang",
        "Michael Osborne",
        "Benjamin Moll",
        "Jakob Foerster"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected return, conditioned on those samples. However, HSMs have not been scaled to Partially Observable settings. We propose Recurrent Structural Policy Gradient (RSPG), the first history-aware HSM for settings involving public information. We also introduce MFAX, our JAX-based framework for MFGs. By leveraging known transition dynamics, RSPG achieves state-of-the-art performance as well as an order-of-magnitude faster convergence and solves, for the first time, a macroeconomics MFG with heterogeneous agents, common noise and history-aware policies. MFAX is publicly available at: https://github.com/CWibault/mfax.",
      "url": "https://arxiv.org/abs/2602.20141",
      "pdfUrl": "https://arxiv.org/pdf/2602.20141.pdf",
      "titleJa": "部分観測平均場ゲームにおける再帰的構造政策勾配"
    },
    {
      "id": "2602.20135",
      "arxivId": "2602.20135",
      "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration",
      "authors": [
        "Mohammad Amanlou",
        "Erfan Shafiee Moghaddam",
        "Yasaman Amou Jafari",
        "Mahdi Noori",
        "Farhan Farsi",
        "Behnam Bahrak"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "abstract": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.",
      "url": "https://arxiv.org/abs/2602.20135",
      "pdfUrl": "https://arxiv.org/pdf/2602.20135.pdf",
      "titleJa": "KNIGHT: 適応的な難易度調整機能を備えた知識グラフ駆動型多肢選択問題生成"
    },
    {
      "id": "2602.20134",
      "arxivId": "2602.20134",
      "title": "Modeling Epidemiological Dynamics Under Adversarial Data and User Deception",
      "authors": [
        "Yiqi Su",
        "Christo Kurisummoottil Thomas",
        "Walid Saad",
        "Bud Mishra",
        "Naren Ramakrishnan"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "abstract": "Epidemiological models increasingly rely on self-reported behavioral data such as vaccination status, mask usage, and social distancing adherence to forecast disease transmission and assess the impact of non-pharmaceutical interventions (NPIs). While such data provide valuable real-time insights, they are often subject to strategic misreporting, driven by individual incentives to avoid penalties, access benefits, or express distrust in public health authorities. To account for such human behavior, in this paper, we introduce a game-theoretic framework that models the interaction between the population and a public health authority as a signaling game. Individuals (senders) choose how to report their behaviors, while the public health authority (receiver) updates their epidemiological model(s) based on potentially distorted signals. Focusing on deception around masking and vaccination, we characterize analytically game equilibrium outcomes and evaluate the degree to which deception can be tolerated while maintaining epidemic control through policy interventions. Our results show that separating equilibria-with minimal deception-drive infections to near zero over time. Remarkably, even under pervasive dishonesty in pooling equilibria, well-designed sender and receiver strategies can still maintain effective epidemic control. This work advances the understanding of adversarial data in epidemiology and offers tools for designing more robust public health models in the presence of strategic user behavior.",
      "url": "https://arxiv.org/abs/2602.20134",
      "pdfUrl": "https://arxiv.org/pdf/2602.20134.pdf",
      "titleJa": "敵対的データとユーザー欺瞞下における疫学的ダイナミクスのモデル化"
    },
    {
      "id": "2602.20133",
      "arxivId": "2602.20133",
      "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization",
      "authors": [
        "Mert Cemri",
        "Shubham Agrawal",
        "Akshat Gupta",
        "Shu Liu",
        "Audrey Cheng",
        "Qiuyang Mang",
        "Ashwin Naren",
        "Lutfi Eren Erdogan",
        "Koushik Sen",
        "Matei Zaharia",
        "Alex Dimakis",
        "Ion Stoica"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an \"accumulated improvement signal\" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.",
      "url": "https://arxiv.org/abs/2602.20133",
      "pdfUrl": "https://arxiv.org/pdf/2602.20133.pdf",
      "titleJa": "AdaEvolve: 適応型 LLM 駆動型ゼロ次最適化"
    },
    {
      "id": "2602.20130",
      "arxivId": "2602.20130",
      "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering",
      "authors": [
        "Zaifu Zhan",
        "Min Zeng",
        "Shuang Zhou",
        "Yiran Song",
        "Xiaoyi Chen",
        "Yu Hou",
        "Yifan Wu",
        "Yang Ruan",
        "Rui Zhang"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy. Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time. Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\\leq$4\\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost. Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability. Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.",
      "url": "https://arxiv.org/abs/2602.20130",
      "pdfUrl": "https://arxiv.org/pdf/2602.20130.pdf",
      "titleJa": "推論するかしないか：医療質問応答における選択的思考連鎖"
    },
    {
      "id": "2602.20122",
      "arxivId": "2602.20122",
      "title": "NanoKnow: How to Know What Your Language Model Knows",
      "authors": [
        "Lingwei Gu",
        "Nour Jedidi",
        "Jimmy Lin"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "abstract": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.",
      "url": "https://arxiv.org/abs/2602.20122",
      "pdfUrl": "https://arxiv.org/pdf/2602.20122.pdf",
      "titleJa": "NanoKnow: 言語モデルが何を知っているかを知る方法"
    },
    {
      "id": "2602.20119",
      "arxivId": "2602.20119",
      "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
      "authors": [
        "Jiahui Fu",
        "Junyu Nan",
        "Lingfeng Sun",
        "Hongyu Li",
        "Jianing Qian",
        "Jennifer L. Barry",
        "Kris Kitani",
        "George Konidaris"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "abstract": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/",
      "url": "https://arxiv.org/abs/2602.20119",
      "pdfUrl": "https://arxiv.org/pdf/2602.20119.pdf",
      "titleJa": "NovaPlan: 閉ループビデオ言語計画によるゼロショット長期ホライズン操作"
    },
    {
      "id": "2602.20117",
      "arxivId": "2602.20117",
      "title": "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models",
      "authors": [
        "Andre He",
        "Nathaniel Weir",
        "Kaj Bostrom",
        "Allen Nie",
        "Darion Cassel",
        "Sam Bayless",
        "Huzefa Rangwala"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs",
      "url": "https://arxiv.org/abs/2602.20117",
      "pdfUrl": "https://arxiv.org/pdf/2602.20117.pdf",
      "titleJa": "ReSyn: 推論モデルのための合成環境の自律的スケーリング"
    },
    {
      "id": "2602.20114",
      "arxivId": "2602.20114",
      "title": "Benchmarking Unlearning for Vision Transformers",
      "authors": [
        "Kairan Zhao",
        "Iurie Luca",
        "Peter Triantafillou"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.",
      "url": "https://arxiv.org/abs/2602.20114",
      "pdfUrl": "https://arxiv.org/pdf/2602.20114.pdf",
      "titleJa": "ビジョントランスフォーマーのためのアンラーニングのベンチマーク"
    },
    {
      "id": "2602.20104",
      "arxivId": "2602.20104",
      "title": "Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration",
      "authors": [
        "Hasan Amin",
        "Ming Yin",
        "Rajiv Khanna"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "abstract": "In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.",
      "url": "https://arxiv.org/abs/2602.20104",
      "pdfUrl": "https://arxiv.org/pdf/2602.20104.pdf",
      "titleJa": "望む時に協調し、必要な時に補完する！適応型人間-AIコラボレーションのための人間中心のアンサンブル"
    },
    {
      "id": "2602.20102",
      "arxivId": "2602.20102",
      "title": "BarrierSteer: LLM Safety via Learning Barrier Steering",
      "authors": [
        "Thanh Q. Tran",
        "Arun Verma",
        "Kiwan Wong",
        "Bryan Kian Hsiang Low",
        "Daniela Rus",
        "Wei Xiao"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.",
      "url": "https://arxiv.org/abs/2602.20102",
      "pdfUrl": "https://arxiv.org/pdf/2602.20102.pdf",
      "titleJa": "BarrierSteer: バリアステアリングの学習による LLM の安全性"
    },
    {
      "id": "2602.20100",
      "arxivId": "2602.20100",
      "title": "Transcending the Annotation Bottleneck: AI-Powered Discovery in Biology and Medicine",
      "authors": [
        "Soumick Chatterjee"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "abstract": "The dependence on expert annotation has long constituted the primary rate-limiting step in the application of artificial intelligence to biomedicine. While supervised learning drove the initial wave of clinical algorithms, a paradigm shift towards unsupervised and self-supervised learning (SSL) is currently unlocking the latent potential of biobank-scale datasets. By learning directly from the intrinsic structure of data - whether pixels in a magnetic resonance image (MRI), voxels in a volumetric scan, or tokens in a genomic sequence - these methods facilitate the discovery of novel phenotypes, the linkage of morphology to genetics, and the detection of anomalies without human bias. This article synthesises seminal and recent advances in \"learning without labels,\" highlighting how unsupervised frameworks can derive heritable cardiac traits, predict spatial gene expression in histology, and detect pathologies with performance that rivals or exceeds supervised counterparts.",
      "url": "https://arxiv.org/abs/2602.20100",
      "pdfUrl": "https://arxiv.org/pdf/2602.20100.pdf",
      "titleJa": "アノテーションのボトルネックを乗り越える：生物学と医学におけるAIを活用した発見"
    },
    {
      "id": "2602.20094",
      "arxivId": "2602.20094",
      "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching",
      "authors": [
        "Yuzhe Wang",
        "Yaochen Zhu",
        "Jundong Li"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.AI"
      ],
      "abstract": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.",
      "url": "https://arxiv.org/abs/2602.20094",
      "pdfUrl": "https://arxiv.org/pdf/2602.20094.pdf",
      "titleJa": "CausalFlip: セマンティックマッチングを超えたLLM因果判断のベンチマーク"
    },
    {
      "id": "2602.20089",
      "arxivId": "2602.20089",
      "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
      "authors": [
        "Zanxi Ruan",
        "Qiuyu Kong",
        "Songqun Gao",
        "Yiming Wang",
        "Marco Cristani"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them \"structure-centric\". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StructXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StructXLIP.",
      "url": "https://arxiv.org/abs/2602.20089",
      "pdfUrl": "https://arxiv.org/pdf/2602.20089.pdf",
      "titleJa": "StructXLIP: マルチモーダル構造手がかりによる視覚言語モデルの強化"
    },
    {
      "id": "2602.20078",
      "arxivId": "2602.20078",
      "title": "Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning",
      "authors": [
        "Shan Yang",
        "Yang Liu"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent's learning signal, so cross-agent noise grows with $N$. In the policy gradient setting, per-agent gradient estimate variance scales as $Θ(N)$, yielding sample complexity $\\mathcal{O}(N/ε)$. We observe that many domains -- cloud computing, transportation, power systems -- have differentiable analytical models that prescribe efficient system states. In this work, we propose Descent-Guided Policy Gradient (DG-PG), a framework that constructs noise-free per-agent guidance gradients from these analytical models, decoupling each agent's gradient from the actions of all others. We prove that DG-PG reduces gradient variance from $Θ(N)$ to $\\mathcal{O}(1)$, preserves the equilibria of the cooperative game, and achieves agent-independent sample complexity $\\mathcal{O}(1/ε)$. On a heterogeneous cloud scheduling task with up to 200 agents, DG-PG converges within 10 episodes at every tested scale -- from $N=5$ to $N=200$ -- directly confirming the predicted scale-invariant complexity, while MAPPO and IPPO fail to converge under identical architectures.",
      "url": "https://arxiv.org/abs/2602.20078",
      "pdfUrl": "https://arxiv.org/pdf/2602.20078.pdf",
      "titleJa": "スケーラブルな協調マルチエージェント学習のための降下誘導方策勾配"
    },
    {
      "id": "2602.20076",
      "arxivId": "2602.20076",
      "title": "Robust Taylor-Lagrange Control for Safety-Critical Systems",
      "authors": [
        "Wei Xiao",
        "Christos Cassandras",
        "Anni Li"
      ],
      "publishedDate": "2026-02-23",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.RO"
      ],
      "abstract": "Solving safety-critical control problem has widely adopted the Control Barrier Function (CBF) method. However, the existence of a CBF is only a sufficient condition for system safety. The recently proposed Taylor-Lagrange Control (TLC) method addresses this limitation, but is vulnerable to the feasibility preservation problem (e.g., inter-sampling effect). In this paper, we propose a robust TLC (rTLC) method to address the feasibility preservation problem. Specifically, the rTLC method expands the safety function at an order higher than the relative degree of the function using Taylor's expansion with Lagrange remainder, which allows the control to explicitly show up at the current time instead of the future time in the TLC method. The rTLC method naturally addresses the feasibility preservation problem with only one hyper-parameter (the discretization time interval size during implementation), which is much less than its counterparts. Finally, we illustrate the effectiveness of the proposed rTLC method through an adaptive cruise control problem, and compare it with existing safety-critical control methods.",
      "url": "https://arxiv.org/abs/2602.20076",
      "pdfUrl": "https://arxiv.org/pdf/2602.20076.pdf",
      "titleJa": "安全性が重要なシステムのためのロバストなテイラー・ラグランジュ制御"
    },
    {
      "id": "2602.18010",
      "arxivId": "2602.18010",
      "title": "Scaling Audio-Text Retrieval with Multimodal Large Language Models",
      "authors": [
        "Jilan Xu",
        "Carl Thomé",
        "Danijela Horak",
        "Weidi Xie",
        "Andrew Zisserman"
      ],
      "publishedDate": "2026-02-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio-text retrieval is crucial for bridging acoustic signals and natural language. While contrastive dual-encoder architectures like CLAP have shown promise, they are fundamentally limited by the capacity of small-scale encoders. Specifically, the text encoders struggle to understand complex queries that require reasoning or world knowledge. In this paper, we propose AuroLA, a novel contrastive language-audio pre-training framework that re-purposes Multimodal Large Language Models (MLLMs) as a unified backbone for retrieval. Specifically, we make three contributions: (i) we construct a scalable data pipeline that curates diverse audio from multiple sources and generates multi-granular captions, ranging from long descriptions to structured tags, via automated annotation; (ii) we adapt an MLLM for retrieval by prompting it to summarize the audio/text input and using the hidden state of a special token as audio/text embeddings. For model training, we devise a novel Hybrid-NCE loss, which employs multi-granular supervision and hard-negative reweighting to robustly align audio with diverse textual supervision; and (iii) we design an MLLM-based bidirectional re-ranking module that refines retrieval candidates through deep cross-modal interaction. Extensive experiments demonstrate that AuroLA consistently outperforms state-of-the-art models, including the recent PE-AV, while utilizing only approximately 1% of PE-AV's training data. Lastly, we observe clear scaling trends regarding dataset size and model capacity, validating the effectiveness of MLLM as a unified backbone for audio-text retrieval. Code is available at https://github.com/Jazzcharles/AuroLA.",
      "url": "https://arxiv.org/abs/2602.18010",
      "pdfUrl": "https://arxiv.org/pdf/2602.18010.pdf",
      "titleJa": "マルチモーダル大規模言語モデルによる音声テキスト検索のスケーリング"
    },
    {
      "id": "2602.18528",
      "arxivId": "2602.18528",
      "title": "Audio-Visual Continual Test-Time Adaptation without Forgetting",
      "authors": [
        "Sarthak Kumar Maharana",
        "Akshay Mehra",
        "Bhavya Ramakrishna",
        "Yunhui Guo",
        "Guan-Ming Su"
      ],
      "publishedDate": "2026-02-20",
      "categories": [
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Audio-visual continual test-time adaptation involves continually adapting a source audio-visual model at test-time, to unlabeled non-stationary domains, where either or both modalities can be distributionally shifted, which hampers online cross-modal learning and eventually leads to poor accuracy. While previous works have tackled this problem, we find that SOTA methods suffer from catastrophic forgetting, where the model's performance drops well below the source model due to continual parameter updates at test-time. In this work, we first show that adapting only the modality fusion layer to a target domain not only improves performance on that domain but can also enhance performance on subsequent domains. Based on this strong cross-task transferability of the fusion layer's parameters, we propose a method, $\\texttt{AV-CTTA}$, that improves test-time performance of the models without access to any source data. Our approach works by using a selective parameter retrieval mechanism that dynamically retrieves the best fusion layer parameters from a buffer using only a small batch of test data. These parameters are then integrated into the model, adapted to the current test distribution, and saved back for future use. Extensive experiments on benchmark datasets involving unimodal and bimodal corruptions show our proposed $\\texttt{AV-CTTA}$ significantly outperforms existing methods while minimizing catastrophic forgetting.",
      "url": "https://arxiv.org/abs/2602.18528",
      "pdfUrl": "https://arxiv.org/pdf/2602.18528.pdf",
      "titleJa": "忘れずに視聴覚の継続的なテスト時間の適応"
    },
    {
      "id": "2602.18527",
      "arxivId": "2602.18527",
      "title": "JAEGER: Joint 3D Audio-Visual Grounding and Reasoning in Simulated Physical Environments",
      "authors": [
        "Zhan Liu",
        "Changli Tang",
        "Yuxin Wang",
        "Zhiyuan Zhu",
        "Youjun Chen",
        "Yiwen Shao",
        "Tianzi Wang",
        "Lei Ke",
        "Zengrui Jin",
        "Chao Zhang"
      ],
      "publishedDate": "2026-02-20",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D perception, relying on RGB video and monaural audio. This design choice introduces a fundamental dimensionality mismatch that precludes reliable source localization and spatial reasoning in complex 3D environments. We address this limitation by presenting JAEGER, a framework that extends AV-LLMs to 3D space, to enable joint spatial grounding and reasoning through the integration of RGB-D observations and multi-channel first-order ambisonics. A core contribution of our work is the neural intensity vector (Neural IV), a learned spatial audio representation that encodes robust directional cues to enhance direction-of-arrival estimation, even in adverse acoustic scenarios with overlapping sources. To facilitate large-scale training and systematic evaluation, we propose SpatialSceneQA, a benchmark of 61k instruction-tuning samples curated from simulated physical environments. Extensive experiments demonstrate that our approach consistently surpasses 2D-centric baselines across diverse spatial perception and reasoning tasks, underscoring the necessity of explicit 3D modelling for advancing AI in physical environments. Our source code, pre-trained model checkpoints and datasets will be released upon acceptance.",
      "url": "https://arxiv.org/abs/2602.18527",
      "pdfUrl": "https://arxiv.org/pdf/2602.18527.pdf",
      "titleJa": "JAEGER: シミュレーションされた物理環境における共同3Dオーディオビジュアルグラウンディングと推論"
    },
    {
      "id": "2602.17599",
      "arxivId": "2602.17599",
      "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
      "authors": [
        "Ivan Rinaldi",
        "Matteo Mendula",
        "Nicola Fanelli",
        "Florence Levé",
        "Matteo Testi",
        "Giovanna Castellano",
        "Gennaro Vessio"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.",
      "url": "https://arxiv.org/abs/2602.17599",
      "pdfUrl": "https://arxiv.org/pdf/2602.17599.pdf",
      "titleJa": "Art2Mus: 視覚条件付けと大規模クロスモーダルアライメントによるアートワークから音楽への生成"
    },
    {
      "id": "2602.17818",
      "arxivId": "2602.17818",
      "title": "Lend me an Ear: Speech Enhancement Using a Robotic Arm with a Microphone Array",
      "authors": [
        "Zachary Turcotte",
        "François Grondin"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.RO",
        "cs.SD"
      ],
      "abstract": "Speech enhancement performance degrades significantly in noisy environments, limiting the deployment of speech-controlled technologies in industrial settings, such as manufacturing plants. Existing speech enhancement solutions primarly rely on advanced digital signal processing techniques, deep learning methods, or complex software optimization techniques. This paper introduces a novel enhancement strategy that incorporates a physical optimization stage by dynamically modifying the geometry of a microphone array to adapt to changing acoustic conditions. A sixteen-microphone array is mounted on a robotic arm manipulator with seven degrees of freedom, with microphones divided into four groups of four, including one group positioned near the end-effector. The system reconfigures the array by adjusting the manipulator joint angles to place the end-effector microphones closer to the target speaker, thereby improving the reference signal quality. This proposed method integrates sound source localization techniques, computer vision, inverse kinematics, minimum variance distortionless response beamformer and time-frequency masking using a deep neural network. Experimental results demonstrate that this approach outperforms other traditional recording configruations, achieving higher scale-invariant signal-to-distortion ratio and lower word error rate accross multiple input signal-to-noise ratio conditions.",
      "url": "https://arxiv.org/abs/2602.17818",
      "pdfUrl": "https://arxiv.org/pdf/2602.17818.pdf",
      "titleJa": "耳を貸してください：マイクアレイを備えたロボットアームによる音声強調"
    },
    {
      "id": "2602.17394",
      "arxivId": "2602.17394",
      "title": "Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks",
      "authors": [
        "Nuno Saavedra",
        "Pedro Ribeiro",
        "André Coelho",
        "Rui Campos"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "Unmanned Aerial Vehicle (UAV)-assisted networks are increasingly foreseen as a promising approach for emergency response, providing rapid, flexible, and resilient communications in environments where terrestrial infrastructure is degraded or unavailable. In such scenarios, voice radio communications remain essential for first responders due to their robustness; however, their unstructured nature prevents direct integration with automated UAV-assisted network management. This paper proposes SIREN, an AI-driven framework that enables voice-driven perception for UAV-assisted networks. By integrating Automatic Speech Recognition (ASR) with Large Language Model (LLM)-based semantic extraction and Natural Language Processing (NLP) validation, SIREN converts emergency voice traffic into structured, machine-readable information, including responding units, location references, emergency severity, and Quality-of-Service (QoS) requirements. SIREN is evaluated using synthetic emergency scenarios with controlled variations in language, speaker count, background noise, and message complexity. The results demonstrate robust transcription and reliable semantic extraction across diverse operating conditions, while highlighting speaker diarization and geographic ambiguity as the main limiting factors. These findings establish the feasibility of voice-driven situational awareness for UAV-assisted networks and show a practical foundation for human-in-the-loop decision support and adaptive network management in emergency response operations.",
      "url": "https://arxiv.org/abs/2602.17394",
      "pdfUrl": "https://arxiv.org/pdf/2602.17394.pdf",
      "titleJa": "UAV支援緊急ネットワークのための音声駆動型意味認識"
    },
    {
      "id": "2602.16008",
      "arxivId": "2602.16008",
      "title": "MAEB: Massive Audio Embedding Benchmark",
      "authors": [
        "Adnan El Assadi",
        "Isaac Chung",
        "Chenghao Xiao",
        "Roman Solomatin",
        "Animesh Jha",
        "Rahul Chand",
        "Silky Singh",
        "Kaitlyn Wang",
        "Ali Sartaz Khan",
        "Marc Moussa Nasser",
        "Sufen Fong",
        "Pengfei He",
        "Alan Xiao",
        "Ayush Sunil Munot",
        "Aditya Shrivastava",
        "Artem Gazizov",
        "Niklas Muennighoff",
        "Kenneth Enevoldsen"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.",
      "url": "https://arxiv.org/abs/2602.16008",
      "pdfUrl": "https://arxiv.org/pdf/2602.16008.pdf",
      "titleJa": "MAEB: 大規模オーディオ埋め込みベンチマーク"
    },
    {
      "id": "2602.15307",
      "arxivId": "2602.15307",
      "title": "What Do Neurons Listen To? A Neuron-level Dissection of a General-purpose Audio Model",
      "authors": [
        "Takao Kawamura",
        "Daisuke Niizumi",
        "Nobutaka Ono"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "In this paper, we analyze the internal representations of a general-purpose audio self-supervised learning (SSL) model from a neuron-level perspective. Despite their strong empirical performance as feature extractors, the internal mechanisms underlying the robust generalization of SSL audio models remain unclear. Drawing on the framework of mechanistic interpretability, we identify and examine class-specific neurons by analyzing conditional activation patterns across diverse tasks. Our analysis reveals that SSL models foster the emergence of class-specific neurons that provide extensive coverage across novel task classes. These neurons exhibit shared responses across different semantic categories and acoustic similarities, such as speech attributes and musical pitch. We also confirm that these neurons have a functional impact on classification performance. To our knowledge, this is the first systematic neuron-level analysis of a general-purpose audio SSL model, providing new insights into its internal representation.",
      "url": "https://arxiv.org/abs/2602.15307",
      "pdfUrl": "https://arxiv.org/pdf/2602.15307.pdf",
      "titleJa": "ニューロンは何を聞いているのか？汎用オーディオモデルのニューロンレベルの解析"
    },
    {
      "id": "2602.13928",
      "arxivId": "2602.13928",
      "title": "voice2mode: Phonation Mode Classification in Singing using Self-Supervised Speech Models",
      "authors": [
        "Aju Ani Justus",
        "Ruchit Agrawal",
        "Sudarsana Reddy Kadiri",
        "Shrikanth Narayanan"
      ],
      "publishedDate": "2026-02-14",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "We present voice2mode, a method for classification of four singing phonation modes (breathy, neutral (modal), flow, and pressed) using embeddings extracted from large self-supervised speech models. Prior work on singing phonation has relied on handcrafted signal features or task-specific neural nets; this work evaluates the transferability of speech foundation models to singing phonation classification. voice2mode extracts layer-wise representations from HuBERT and two wav2vec2 variants, applies global temporal pooling, and classifies the pooled embeddings with lightweight classifiers (SVM, XGBoost). Experiments on a publicly available soprano dataset (763 sustained vowel recordings, four labels) show that foundation-model features substantially outperform conventional spectral baselines (spectrogram, mel-spectrogram, MFCC). HuBERT embeddings obtained from early layers yield the best result (~95.7% accuracy with SVM), an absolute improvement of ~12-15% over the best traditional baseline. We also show layer-wise behaviour: lower layers, which retain acoustic/phonetic detail, are more effective than top layers specialized for Automatic Speech Recognition (ASR).",
      "url": "https://arxiv.org/abs/2602.13928",
      "pdfUrl": "https://arxiv.org/pdf/2602.13928.pdf",
      "titleJa": "voice2mode: 自己教師あり音声モデルを用いた歌唱における発声モードの分類"
    },
    {
      "id": "2602.11910",
      "arxivId": "2602.11910",
      "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
      "authors": [
        "Łukasz Staniszewski",
        "Katarzyna Zaleska",
        "Mateusz Modrzejewski",
        "Kamil Deja"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood.",
      "url": "https://arxiv.org/abs/2602.11910",
      "pdfUrl": "https://arxiv.org/pdf/2602.11910.pdf",
      "titleJa": "TADA! アクティベーションステアリングによるオーディオ拡散モデルのチューニング"
    },
    {
      "id": "2602.11896",
      "arxivId": "2602.11896",
      "title": "Musical Metamerism with Time--Frequency Scattering",
      "authors": [
        "Vincent Lostanlen",
        "Han Han"
      ],
      "publishedDate": "2026-02-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The concept of metamerism originates from colorimetry, where it describes a sensation of visual similarity between two colored lights despite significant differences in spectral content. Likewise, we propose to call ``musical metamerism'' the sensation of auditory similarity which is elicited by two music fragments which differ in terms of underlying waveforms. In this technical report, we describe a method to generate musical metamers from any audio recording. Our method is based on joint time--frequency scattering in Kymatio, an open-source software in Python which enables GPU computing and automatic differentiation. The advantage of our method is that it does not require any manual preprocessing, such as transcription, beat tracking, or source separation. We provide a mathematical description of JTFS as well as some excerpts from the Kymatio source code. Lastly, we review the prior work on JTFS and draw connections with closely related algorithms, such as spectrotemporal receptive fields (STRF), modulation power spectra (MPS), and Gabor filterbank (GBFB).",
      "url": "https://arxiv.org/abs/2602.11896",
      "pdfUrl": "https://arxiv.org/pdf/2602.11896.pdf",
      "titleJa": "時間周波数散乱を伴う音楽的メタメリズム"
    },
    {
      "id": "2602.10934",
      "arxivId": "2602.10934",
      "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
      "authors": [
        "Yitian Gong",
        "Kuangwei Chen",
        "Zhaoye Fei",
        "Xiaogui Yang",
        "Ke Chen",
        "Yang Wang",
        "Kexin Huang",
        "Mingshu Chen",
        "Ruixiao Li",
        "Qingyuan Cheng",
        "Shimin Li",
        "Xipeng Qiu"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.",
      "url": "https://arxiv.org/abs/2602.10934",
      "pdfUrl": "https://arxiv.org/pdf/2602.10934.pdf",
      "titleJa": "MOSS-Audio-Tokenizer: 将来のオーディオ基盤モデルに向けたオーディオトークナイザーのスケーリング"
    },
    {
      "id": "2602.12301",
      "arxivId": "2602.12301",
      "title": "Beyond Musical Descriptors: Extracting Preference-Bearing Intent in Music Queries",
      "authors": [
        "Marion Baranes",
        "Romain Hennequin",
        "Elena V. Epure"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.IR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Although annotated music descriptor datasets for user queries are increasingly common, few consider the user's intent behind these descriptors, which is essential for effectively meeting their needs. We introduce MusicRecoIntent, a manually annotated corpus of 2,291 Reddit music requests, labeling musical descriptors across seven categories with positive, negative, or referential preference-bearing roles. We then investigate how reliably large language models (LLMs) can extract these music descriptors, finding that they do capture explicit descriptors but struggle with context-dependent ones. This work can further serve as a benchmark for fine-grained modeling of user intent and for gaining insights into improving LLM-based music understanding systems.",
      "url": "https://arxiv.org/abs/2602.12301",
      "pdfUrl": "https://arxiv.org/pdf/2602.12301.pdf",
      "titleJa": "音楽記述子を超えて：音楽検索クエリにおける嗜好意図の抽出"
    },
    {
      "id": "2602.10656",
      "arxivId": "2602.10656",
      "title": "AudioRAG: A Challenging Benchmark for Audio Reasoning and Information Retrieval",
      "authors": [
        "Jingru Lin",
        "Chen Zhang",
        "Tianrui Wang",
        "Haizhou Li"
      ],
      "publishedDate": "2026-02-11",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Due to recent advancements in Large Audio-Language Models (LALMs) that demonstrate remarkable performance across a range of sound-, speech- and music-related tasks, there is a growing interest in proposing benchmarks to assess these models. Existing benchmarks generally focus only on reasoning with internal knowledge, neglecting real-world scenarios that require external information grounding. To bridge this gap, we introduce AudioRAG, a novel benchmark designed to evaluate audio-based reasoning augmented by information retrieval in realistic web environments. This benchmark comprises both LLM-generated and manually curated question-answer pairs. Our evaluations reveal that even the state-of-the-art LALMs struggle to answer these questions. We therefore propose an agentic pipeline that integrates audio reasoning with retrieval-augmented generation, providing a stronger baseline for future research.",
      "url": "https://arxiv.org/abs/2602.10656",
      "pdfUrl": "https://arxiv.org/pdf/2602.10656.pdf",
      "titleJa": "AudioRAG: オーディオ推論と情報検索のための挑戦的なベンチマーク"
    },
    {
      "id": "2602.10058",
      "arxivId": "2602.10058",
      "title": "Evaluating Disentangled Representations for Controllable Music Generation",
      "authors": [
        "Laura Ibáñez-Martínez",
        "Chukwuemeka Nkama",
        "Andrea Poltronieri",
        "Xavier Serra",
        "Martín Rocamora"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Recent approaches in music generation rely on disentangled representations, often labeled as structure and timbre or local and global, to enable controllable synthesis. Yet the underlying properties of these embeddings remain underexplored. In this work, we evaluate such disentangled representations in a set of music audio models for controllable generation using a probing-based framework that goes beyond standard downstream tasks. The selected models reflect diverse unsupervised disentanglement strategies, including inductive biases, data augmentations, adversarial objectives, and staged training procedures. We further isolate specific strategies to analyze their effect. Our analysis spans four key axes: informativeness, equivariance, invariance, and disentanglement, which are assessed across datasets, tasks, and controlled transformations. Our findings reveal inconsistencies between intended and actual semantics of the embeddings, suggesting that current strategies fall short of producing truly disentangled representations, and prompting a re-examination of how controllability is approached in music generation.",
      "url": "https://arxiv.org/abs/2602.10058",
      "pdfUrl": "https://arxiv.org/pdf/2602.10058.pdf",
      "titleJa": "制御可能な音楽生成のための分離表現の評価"
    },
    {
      "id": "2602.09891",
      "arxivId": "2602.09891",
      "title": "Stemphonic: All-at-once Flexible Multi-stem Music Generation",
      "authors": [
        "Shih-Lun Wu",
        "Ge Zhu",
        "Juan-Pablo Caceres",
        "Cheng-Zhi Anna Huang",
        "Nicholas J. Bryan"
      ],
      "publishedDate": "2026-02-10",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM"
      ],
      "abstract": "Music stem generation, the task of producing musically-synchronized and isolated instrument audio clips, offers the potential of greater user control and better alignment with musician workflows compared to conventional text-to-music models. Existing stem generation approaches, however, either rely on fixed architectures that output a predefined set of stems in parallel, or generate only one stem at a time, resulting in slow inference despite flexibility in stem combination. We propose Stemphonic, a diffusion-/flow-based framework that overcomes this trade-off and generates a variable set of synchronized stems in one inference pass. During training, we treat each stem as a batch element, group synchronized stems in a batch, and apply a shared noise latent to each group. At inference-time, we use a shared initial noise latent and stem-specific text inputs to generate synchronized multi-stem outputs in one pass. We further expand our approach to enable one-pass conditional multi-stem generation and stem-wise activity controls to empower users to iteratively generate and orchestrate the temporal layering of a mix. We benchmark our results on multiple open-source stem evaluation sets and show that Stemphonic produces higher-quality outputs while accelerating the full mix generation process by 25 to 50%. Demos at: https://stemphonic-demo.vercel.app.",
      "url": "https://arxiv.org/abs/2602.09891",
      "pdfUrl": "https://arxiv.org/pdf/2602.09891.pdf",
      "titleJa": "Stemphonic: 一度に柔軟なマルチステム音楽生成"
    },
    {
      "id": "2602.08794",
      "arxivId": "2602.08794",
      "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
      "authors": [
        "SII-OpenMOSS Team",
        " :",
        "Donghua Yu",
        "Mingshu Chen",
        "Qi Chen",
        "Qi Luo",
        "Qianyi Wu",
        "Qinyuan Cheng",
        "Ruixiao Li",
        "Tianyi Liang",
        "Wenbo Zhang",
        "Wenming Tu",
        "Xiangyu Peng",
        "Yang Gao",
        "Yanru Huo",
        "Ying Zhu",
        "Yinze Luo",
        "Yiyang Zhang",
        "Yuerong Song",
        "Zhe Xu",
        "Zhiyu Zhang",
        "Chenchen Yang",
        "Cheng Chang",
        "Chushu Zhou",
        "Hanfu Chen",
        "Hongnan Ma",
        "Jiaxi Li",
        "Jingqi Tong",
        "Junxi Liu",
        "Ke Chen",
        "Shimin Li",
        "Shiqi Jiang",
        "Songlin Wang",
        "Wei Jiang",
        "Zhaoye Fei",
        "Zhiyuan Ning",
        "Chunguo Li",
        "Chenhui Li",
        "Ziwei He",
        "Zengfeng Huang",
        "Xie Chen",
        "Xipeng Qiu"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.CV",
        "cs.SD"
      ],
      "abstract": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
      "url": "https://arxiv.org/abs/2602.08794",
      "pdfUrl": "https://arxiv.org/pdf/2602.08794.pdf",
      "titleJa": "MOVA: スケーラブルで同期したビデオ・オーディオ生成に向けて"
    },
    {
      "id": "2602.08671",
      "arxivId": "2602.08671",
      "title": "Input-Adaptive Spectral Feature Compression by Sequence Modeling for Source Separation",
      "authors": [
        "Kohei Saijo",
        "Yoshiaki Bando"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Time-frequency domain dual-path models have demonstrated strong performance and are widely used in source separation. Because their computational cost grows with the number of frequency bins, these models often use the band-split (BS) module in high-sampling-rate tasks such as music source separation (MSS) and cinematic audio source separation (CASS). The BS encoder compresses frequency information by encoding features for each predefined subband. It achieves effective compression by introducing an inductive bias that places greater emphasis on low-frequency parts. Despite its success, the BS module has two inherent limitations: (i) it is not input-adaptive, preventing the use of input-dependent information, and (ii) the parameter count is large, since each subband requires a dedicated module. To address these issues, we propose Spectral Feature Compression (SFC). SFC compresses the input using a single sequence modeling module, making it both input-adaptive and parameter-efficient. We investigate two variants of SFC, one based on cross-attention and the other on Mamba, and introduce inductive biases inspired by the BS module to make them suitable for frequency information compression. Experiments on MSS and CASS tasks demonstrate that the SFC module consistently outperforms the BS module across different separator sizes and compression ratios. We also provide an analysis showing that SFC adaptively captures frequency patterns from the input.",
      "url": "https://arxiv.org/abs/2602.08671",
      "pdfUrl": "https://arxiv.org/pdf/2602.08671.pdf",
      "titleJa": "音源分離のためのシーケンスモデリングによる入力適応型スペクトル特徴圧縮"
    },
    {
      "id": "2602.09070",
      "arxivId": "2602.09070",
      "title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
      "authors": [
        "Yufan Wen",
        "Zhaocheng Liu",
        "YeGuo Hua",
        "Ziyi Guo",
        "Lihua Zhang",
        "Chun Yuan",
        "Jian Wu"
      ],
      "publishedDate": "2026-02-09",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a \\textit{Global Semantic Anchor} ensures stylistic stability, while a surgical \\textit{Token-Level Affective Adapter} modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation.",
      "url": "https://arxiv.org/abs/2602.09070",
      "pdfUrl": "https://arxiv.org/pdf/2602.09070.pdf",
      "titleJa": "NarraScore: 階層的感情制御による視覚的物語と音楽的ダイナミクスの橋渡し"
    },
    {
      "id": "2602.17097",
      "arxivId": "2602.17097",
      "title": "AudioChat: Unified Audio Storytelling, Editing, and Understanding with Transfusion Forcing",
      "authors": [
        "William Chen",
        "Prem Seetharaman",
        "Rithesh Kumar",
        "Oriol Nieto",
        "Shinji Watanabe",
        "Justin Salamon",
        "Zeyu Jin"
      ],
      "publishedDate": "2026-02-19",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Despite recent breakthroughs, audio foundation models struggle in processing complex multi-source acoustic scenes. We refer to this challenging domain as audio stories, which can have multiple speakers and background/foreground sound effects. Compared to traditional audio processing tasks, audio stories introduce new layers of semantic, temporal, and physical complexity. To address this challenge, we propose AudioChat, a framework for developing audio foundation models that can generate, edit, and understand audio stories. AudioChat introduces a new paradigm in which LLM-based toolcalling agents simulate interactions between users and the system, and these simulated dialogues are used as training data. We also introduce a novel Audio Transfusion Forcing objective to train the AudioChat model, allowing it to simultaneously decompose high-level instructions via structured chain-of-thought reasoning and perform interactive multi-turn audio understanding/generation. To evaluate generation and editing performance, we develop three new metrics that directly measure task performance instead of relying upon distribution-based scoring. We highly encourage readers to visit our demo to better understand the capabilities of AudioChat: https://wanchichen.github.io/audiochat/.",
      "url": "https://arxiv.org/abs/2602.17097",
      "pdfUrl": "https://arxiv.org/pdf/2602.17097.pdf",
      "titleJa": "AudioChat: Transfusion Forcingによる統合オーディオストーリーテリング、編集、理解"
    },
    {
      "id": "2602.16118",
      "arxivId": "2602.16118",
      "title": "Real time fault detection in 3D printers using Convolutional Neural Networks and acoustic signals",
      "authors": [
        "Muhammad Fasih Waheed",
        "Shonda Bernadin"
      ],
      "publishedDate": "2026-02-18",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The reliability and quality of 3D printing processes are critically dependent on the timely detection of mechanical faults. Traditional monitoring methods often rely on visual inspection and hardware sensors, which can be both costly and limited in scope. This paper explores a scalable and contactless method for the use of real-time audio signal analysis for detecting mechanical faults in 3D printers. By capturing and classifying acoustic emissions during the printing process, we aim to identify common faults such as nozzle clogging, filament breakage, pully skipping and various other mechanical faults. Utilizing Convolutional neural networks, we implement algorithms capable of real-time audio classification to detect these faults promptly. Our methodology involves conducting a series of controlled experiments to gather audio data, followed by the application of advanced machine learning models for fault detection. Additionally, we review existing literature on audio-based fault detection in manufacturing and 3D printing to contextualize our research within the broader field. Preliminary results demonstrate that audio signals, when analyzed with machine learning techniques, provide a reliable and cost-effective means of enhancing real-time fault detection.",
      "url": "https://arxiv.org/abs/2602.16118",
      "pdfUrl": "https://arxiv.org/pdf/2602.16118.pdf",
      "titleJa": "畳み込みニューラルネットワークと音響信号を用いた3Dプリンターのリアルタイム故障検出"
    },
    {
      "id": "2602.15766",
      "arxivId": "2602.15766",
      "title": "TAC: Timestamped Audio Captioning",
      "authors": [
        "Sonal Kumar",
        "Prem Seetharaman",
        "Ke Chen",
        "Oriol Nieto",
        "Jiaqi Su",
        "Zhepei Wang",
        "Rithesh Kumar",
        "Dinesh Manocha",
        "Nicholas J. Bryan",
        "Zeyu Jin",
        "Justin Salamon"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Large Audio Language Models struggle to disentangle overlapping events in complex acoustic scenes, yielding temporally inconsistent captions and frequent hallucinations. We introduce Timestamped Audio Captioner (TAC), a model that produces temporally grounded audio descriptions at varying degrees of detail and resolution. TAC is trained with a synthetic data pipeline that constructs challenging and dynamic mixtures from real-world audio sources, enabling robust learning under realistic polyphonic conditions. Across event detection and dense captioning, TAC outperforms all competing methods, with a low hallucination rate and accurate temporal grounding. We also introduce TAC-V, an audio-visual pipeline to generate semantically rich audio-visual descriptions. We then show that TAC and TAC-V serves as a \"semantic bridge\" for a text-only reasoner: a simple TAC$\\rightarrow$LLM and TAC-V$\\rightarrow$LLM cascade achieves state-of-the-art scores on benchmarks for both audio (MMAU-Pro, MMSU, MMAR) and audio-visual (DailyOmni, VideoHolmes) understanding and reasoning respectively.",
      "url": "https://arxiv.org/abs/2602.15766",
      "pdfUrl": "https://arxiv.org/pdf/2602.15766.pdf",
      "titleJa": "TAC: タイムスタンプ付き音声字幕"
    },
    {
      "id": "2602.15519",
      "arxivId": "2602.15519",
      "title": "Enroll-on-Wakeup: A First Comparative Study of Target Speech Extraction for Seamless Interaction in Real Noisy Human-Machine Dialogue Scenarios",
      "authors": [
        "Yiming Yang",
        "Guangyong Wang",
        "Haixin Guan",
        "Yanhua Long"
      ],
      "publishedDate": "2026-02-17",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Target speech extraction (TSE) typically relies on pre-recorded high-quality enrollment speech, which disrupts user experience and limits feasibility in spontaneous interaction. In this paper, we propose Enroll-on-Wakeup (EoW), a novel framework where the wake-word segment, captured naturally during human-machine interaction, is automatically utilized as the enrollment reference. This eliminates the need for pre-collected speech to enable a seamless experience. We perform the first systematic study of EoW-TSE, evaluating advanced discriminative and generative models under real diverse acoustic conditions. Given the short and noisy nature of wake-word segments, we investigate enrollment augmentation using LLM-based TTS. Results show that while current TSE models face performance degradation in EoW-TSE, TTS-based assistance significantly enhances the listening experience, though gaps remain in speech recognition accuracy.",
      "url": "https://arxiv.org/abs/2602.15519",
      "pdfUrl": "https://arxiv.org/pdf/2602.15519.pdf",
      "titleJa": "エンロールオンウェイクアップ：実際のノイズ環境における人間と機械の対話シナリオにおけるシームレスなインタラクションのためのターゲット音声抽出に関する初の比較研究"
    },
    {
      "id": "2602.15909",
      "arxivId": "2602.15909",
      "title": "Resp-Agent: An Agent-Based System for Multimodal Respiratory Sound Generation and Disease Diagnosis",
      "authors": [
        "Pengfei Zhang",
        "Tianxin Xie",
        "Minghao Yang",
        "Li Liu"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.DB",
        "cs.HC",
        "cs.MA",
        "cs.SD"
      ],
      "abstract": "Deep learning-based respiratory auscultation is currently hindered by two fundamental challenges: (i) inherent information loss, as converting signals into spectrograms discards transient acoustic events and clinical context; (ii) limited data availability, exacerbated by severe class imbalance. To bridge these gaps, we present Resp-Agent, an autonomous multimodal system orchestrated by a novel Active Adversarial Curriculum Agent (Thinker-A$^2$CA). Unlike static pipelines, Thinker-A$^2$CA serves as a central controller that actively identifies diagnostic weaknesses and schedules targeted synthesis in a closed loop. To address the representation gap, we introduce a Modality-Weaving Diagnoser that weaves EHR data with audio tokens via Strategic Global Attention and sparse audio anchors, capturing both long-range clinical context and millisecond-level transients. To address the data gap, we design a Flow Matching Generator that adapts a text-only Large Language Model (LLM) via modality injection, decoupling pathological content from acoustic style to synthesize hard-to-diagnose samples. As a foundation for these efforts, we introduce Resp-229k, a benchmark corpus of 229k recordings paired with LLM-distilled clinical narratives. Extensive experiments demonstrate that Resp-Agent consistently outperforms prior approaches across diverse evaluation settings, improving diagnostic robustness under data scarcity and long-tailed class imbalance. Our code and data are available at https://github.com/zpforlove/Resp-Agent.",
      "url": "https://arxiv.org/abs/2602.15909",
      "pdfUrl": "https://arxiv.org/pdf/2602.15909.pdf",
      "titleJa": "Resp-Agent: マルチモーダル呼吸音生成と疾患診断のためのエージェントベースシステム"
    },
    {
      "id": "2602.14671",
      "arxivId": "2602.14671",
      "title": "Data Augmentation for Pathological Speech Enhancement",
      "authors": [
        "Mingchi Hou",
        "Enno Hermann",
        "Ina Kodrasi"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "eess.AS"
      ],
      "abstract": "The performance of state-of-the-art speech enhancement (SE) models considerably degrades for pathological speech due to atypical acoustic characteristics and limited data availability. This paper systematically investigates data augmentation (DA) strategies to improve SE performance for pathological speakers, evaluating both predictive and generative SE models. We examine three DA categories, i.e., transformative, generative, and noise augmentation, assessing their impact with objective SE metrics. Experimental results show that noise augmentation consistently delivers the largest and most robust gains, transformative augmentations provide moderate improvements, while generative augmentation yields limited benefits and can harm performance as the amount of synthetic data increases. Furthermore, we show that the effectiveness of DA varies depending on the SE model, with DA being more beneficial for predictive SE models. While our results demonstrate that DA improves SE performance for pathological speakers, a performance gap between neurotypical and pathological speech persists, highlighting the need for future research on targeted DA strategies for pathological speech.",
      "url": "https://arxiv.org/abs/2602.14671",
      "pdfUrl": "https://arxiv.org/pdf/2602.14671.pdf",
      "titleJa": "病的な音声強調のためのデータ拡張"
    },
    {
      "id": "2602.14664",
      "arxivId": "2602.14664",
      "title": "Probing Human Articulatory Constraints in End-to-End TTS with Reverse and Mismatched Speech-Text Directions",
      "authors": [
        "Parth Khadse",
        "Sunil Kumar Kopparapu"
      ],
      "publishedDate": "2026-02-16",
      "categories": [
        "cs.SD"
      ],
      "abstract": "An end-to-end (e2e) text-to-speech (TTS) system is a deep architecture that learns to associate a text string with acoustic speech patterns from a curated dataset. It is expected that all aspects associated with speech production, such as phone duration, speaker characteristics, and intonation among other things are captured in the trained TTS model to enable the synthesized speech to be natural and intelligible. Human speech is complex, involving smooth transitions between articulatory configurations (ACs). Due to anatomical constraints, some ACs are challenging to mimic or transition between. In this paper, we experimentally study if the constraints imposed by human anatomy have an implication on training an e2e-TTS systems. We experiment with two e2e-TTS architectures, namely, Tacotron-2 an autoregressive model and VITS-TTS a non-autoregressive model. In this study, we build TTS systems using (a) forward text, forward speech (conventional, e2e-TTS), (b) reverse text, reverse speech (r-e2e-TTS), and (c) reverse text, forward speech (rtfs-e2e-TTS). Experiments demonstrate that e2e-TTS systems are purely data-driven. Interestingly, the generated speech by r-e2e-TTS systems exhibits better fidelity, better perceptual intelligibility, and better naturalness",
      "url": "https://arxiv.org/abs/2602.14664",
      "pdfUrl": "https://arxiv.org/pdf/2602.14664.pdf",
      "titleJa": "逆方向および不一致な音声テキスト方向を持つエンドツーエンドTTSにおける人間の調音制約の調査"
    }
  ],
  "lastUpdated": "2026-02-25T01:10:26.747417",
  "totalCount": 79
}