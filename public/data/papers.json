{
  "papers": [
    {
      "id": "2601.09603",
      "arxivId": "2601.09603",
      "title": "Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer",
      "authors": [
        "Petros Vavaroutsos",
        "Theodoros Palamas",
        "Pantelis Vikatos"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "In recent years, foundation models have become very popular due to their exceptional performance, mainly in natural language (NLP) tasks where they were first introduced. These models usually consist of hundreds of millions, or even billions, of parameters, making them resource-intensive during training and in production systems, leading to increased costs. This paper focuses on the reduction of a foundation's model size when applied to music information retrieval (MIR) tasks. Our research combines the Branchformer architecture with SummaryMixing, which were first applied in speech recognition, along with a random quantization process. To facilitate reproducibility, we conduct pre-training on publicly available datasets, complemented by a proprietary dataset comparable in scale to other private datasets reported in the literature. We ensure robust evaluation by using a framework consisting of a variety of downstream MIR tasks. Our results show that our architecture achieves competitive performance when compared with other state-of-the-art models that use multi-head self-attention, while reducing the model size from 8.5% up to 12.3%.",
      "url": "https://arxiv.org/abs/2601.09603",
      "pdfUrl": "https://arxiv.org/pdf/2601.09603.pdf",
      "titleJa": "ランダム量子化器を用いた音楽理解のための線形複雑度自己教師学習"
    },
    {
      "id": "2601.09520",
      "arxivId": "2601.09520",
      "title": "Towards Realistic Synthetic Data for Automatic Drum Transcription",
      "authors": [
        "Pierfrancesco Melucci",
        "Paolo Merialdo",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Deep learning models define the state-of-the-art in Automatic Drum Transcription (ADT), yet their performance is contingent upon large-scale, paired audio-MIDI datasets, which are scarce. Existing workarounds that use synthetic data often introduce a significant domain gap, as they typically rely on low-fidelity SoundFont libraries that lack acoustic diversity. While high-quality one-shot samples offer a better alternative, they are not available in a standardized, large-scale format suitable for training. This paper introduces a new paradigm for ADT that circumvents the need for paired audio-MIDI training data. Our primary contribution is a semi-supervised method to automatically curate a large and diverse corpus of one-shot drum samples from unlabeled audio sources. We then use this corpus to synthesize a high-quality dataset from MIDI files alone, which we use to train a sequence-to-sequence transcription model. We evaluate our model on the ENST and MDB test sets, where it achieves new state-of-the-art results, significantly outperforming both fully supervised methods and previous synthetic-data approaches. The code for reproducing our experiments is publicly available at https://github.com/pier-maker92/ADT_STR",
      "url": "https://arxiv.org/abs/2601.09520",
      "pdfUrl": "https://arxiv.org/pdf/2601.09520.pdf",
      "titleJa": "自動ドラム転写のための現実的な合成データに向けて"
    },
    {
      "id": "2601.09461",
      "arxivId": "2601.09461",
      "title": "Analysis of the Maximum Prediction Gain of Short-Term Prediction on Sustained Speech",
      "authors": [
        "Reemt Hinrichs",
        "Muhamad Fadli Damara",
        "Stephan Preihs",
        "Jörn Ostermann"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Signal prediction is widely used in, e.g., economic forecasting, echo cancellation and in data compression, particularly in predictive coding of speech and music. Predictive coding algorithms reduce the bit-rate required for data transmission or storage by signal prediction. The prediction gain is a classic measure in applied signal coding of the quality of a predictor, as it links the mean-squared prediction error to the signal-to-quantization-noise of predictive coders. To evaluate predictor models, knowledge about the maximum achievable prediction gain independent of a predictor model is desirable. In this manuscript, Nadaraya-Watson kernel-regression (NWKR) and an information theoretic upper bound are applied to analyze the upper bound of the prediction gain on a newly recorded dataset of sustained speech/phonemes. It was found that for unvoiced speech a linear predictor always achieves the maximum prediction gain within at most 0.3 dB. On voiced speech, the optimum one-tap predictor was found to be linear but starting with two taps, the maximum achievable prediction gain was found to be about 2 dB to 6 dB above the prediction gain of the linear predictor. Significant differences between speakers/subjects were observed. The created dataset as well as the code can be obtained for research purpose upon request.",
      "url": "https://arxiv.org/abs/2601.09461",
      "pdfUrl": "https://arxiv.org/pdf/2601.09461.pdf",
      "titleJa": "持続音声における短期予測の最大予測利得の分析"
    },
    {
      "id": "2601.09448",
      "arxivId": "2601.09448",
      "title": "Population-Aligned Audio Reproduction With LLM-Based Equalizers",
      "authors": [
        "Ioannis Stylianou",
        "Jon Francombe",
        "Pablo Martinez-Nuevo",
        "Sven Ewan Shepstone",
        "Zheng-Hua Tan"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Conventional audio equalization is a static process that requires manual and cumbersome adjustments to adapt to changing listening contexts (e.g., mood, location, or social setting). In this paper, we introduce a Large Language Model (LLM)-based alternative that maps natural language text prompts to equalization settings. This enables a conversational approach to sound system control. By utilizing data collected from a controlled listening experiment, our models exploit in-context learning and parameter-efficient fine-tuning techniques to reliably align with population-preferred equalization settings. Our evaluation methods, which leverage distributional metrics that capture users' varied preferences, show statistically significant improvements in distributional alignment over random sampling and static preset baselines. These results indicate that LLMs could function as \"artificial equalizers,\" contributing to the development of more accessible, context-aware, and expert-level audio tuning methods.",
      "url": "https://arxiv.org/abs/2601.09448",
      "pdfUrl": "https://arxiv.org/pdf/2601.09448.pdf",
      "titleJa": "LLMベースのイコライザーによる人口整合オーディオ再生"
    },
    {
      "id": "2601.09413",
      "arxivId": "2601.09413",
      "title": "Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception",
      "authors": [
        "Zhen Wan",
        "Chao-Han Huck Yang",
        "Jinchuan Tian",
        "Hanrong Ye",
        "Ankita Pasad",
        "Szu-wei Fu",
        "Arushi Goel",
        "Ryo Hachiuma",
        "Shizhe Diao",
        "Kunal Dhawan",
        "Sreyan Ghosh",
        "Yusuke Hirota",
        "Zhehuai Chen",
        "Rafael Valle",
        "Ehsan Hosseini Asl",
        "Chenhui Chu",
        "Shinji Watanabe",
        "Yu-Chiang Frank Wang",
        "Boris Ginsburg"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MA",
        "eess.AS"
      ],
      "abstract": "We introduce a voice-agentic framework that learns one critical omni-understanding skill: knowing when to trust itself versus when to consult external audio perception. Our work is motivated by a crucial yet counterintuitive finding: naively fine-tuning an omni-model on both speech recognition and external sound understanding tasks often degrades performance, as the model can be easily misled by noisy hypotheses. To address this, our framework, Speech-Hands, recasts the problem as an explicit self-reflection decision. This learnable reflection primitive proves effective in preventing the model from being derailed by flawed external candidates. We show that this agentic action mechanism generalizes naturally from speech recognition to complex, multiple-choice audio reasoning. Across the OpenASR leaderboard, Speech-Hands consistently outperforms strong baselines by 12.1% WER on seven benchmarks. The model also achieves 77.37% accuracy and high F1 on audio QA decisions, showing robust generalization and reliability across diverse audio question answering datasets. By unifying perception and decision-making, our work offers a practical path toward more reliable and resilient audio intelligence.",
      "url": "https://arxiv.org/abs/2601.09413",
      "pdfUrl": "https://arxiv.org/pdf/2601.09413.pdf",
      "titleJa": "Speech-Hands: 全方位知覚による音声認識とオーディオ推論への自己反映型音声エージェントアプローチ"
    },
    {
      "id": "2601.09385",
      "arxivId": "2601.09385",
      "title": "SLAM-LLM: A Modular, Open-Source Multimodal Large Language Model Framework and Best Practice for Speech, Language, Audio and Music Processing",
      "authors": [
        "Ziyang Ma",
        "Guanrou Yang",
        "Wenxi Chen",
        "Zhifu Gao",
        "Yexing Du",
        "Xiquan Li",
        "Zhisheng Zheng",
        "Haina Zhu",
        "Jianheng Zhuo",
        "Zheshu Song",
        "Ruiyang Xu",
        "Tiranrui Wang",
        "Yifan Yang",
        "Yanqiao Zhu",
        "Zhikang Niu",
        "Liumeng Xue",
        "Yinghao Ma",
        "Ruibin Yuan",
        "Shiliang Zhang",
        "Kai Yu",
        "Eng Siong Chng",
        "Xie Chen"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.MM"
      ],
      "abstract": "The recent surge in open-source Multimodal Large Language Models (MLLM) frameworks, such as LLaVA, provides a convenient kickoff for artificial intelligence developers and researchers. However, most of the MLLM frameworks take vision as the main input modality, and provide limited in-depth support for the modality of speech, audio, and music. This situation hinders the development of audio-language models, and forces researchers to spend a lot of effort on code writing and hyperparameter tuning. We present SLAM-LLM, an open-source deep learning framework designed to train customized MLLMs, focused on speech, language, audio, and music processing. SLAM-LLM provides a modular configuration of different encoders, projectors, LLMs, and parameter-efficient fine-tuning plugins. SLAM-LLM also includes detailed training and inference recipes for mainstream tasks, along with high-performance checkpoints like LLM-based Automatic Speech Recognition (ASR), Automated Audio Captioning (AAC), and Music Captioning (MC). Some of these recipes have already reached or are nearing state-of-the-art performance, and some relevant techniques have also been accepted by academic papers. We hope SLAM-LLM will accelerate iteration, development, data engineering, and model training for researchers. We are committed to continually pushing forward audio-based MLLMs through this open-source framework, and call on the community to contribute to the LLM-based speech, audio and music processing.",
      "url": "https://arxiv.org/abs/2601.09385",
      "pdfUrl": "https://arxiv.org/pdf/2601.09385.pdf",
      "titleJa": "SLAM-LLM: 音声、言語、オーディオ、音楽処理のためのモジュール式オープンソースマルチモーダル大規模言語モデルフレームワークとベストプラクティス"
    },
    {
      "id": "2601.09333",
      "arxivId": "2601.09333",
      "title": "Research on Piano Timbre Transformation System Based on Diffusion Model",
      "authors": [
        "Chun-Chieh Hsu",
        "Tsai-Ling Hsu",
        "Chen-Chen Yeh",
        "Shao-Chien Lu",
        "Cheng-Han Wu",
        "Bing-Ze Liu",
        "Timothy K. Shih",
        "Yu-Cheng Lin"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.MM"
      ],
      "abstract": "We propose a timbre conversion model based on the Diffusion architecture de-signed to precisely translate music played by various instruments into piano ver-sions. The model employs a Pitch Encoder and Loudness Encoder to extract pitch and loudness features of the music, which serve as conditional inputs to the Dif-fusion Model's decoder, generating high-quality piano timbres. Case analysis re-sults show that the model performs excellently in terms of pitch accuracy and timbral similarity, maintaining stable conversion across different musical styles (classical, jazz, pop) and lengths (from short clips to full pieces). Particularly, the model maintains high sound quality and accuracy even when dealing with rapidly changing notes and complex musical structures, demonstrating good generaliza-tion capability. Additionally, the model has the potential for real-time musical conversion and is suitable for live performances and digital music creation tools. Future research will focus on enhancing the handling of loudness dynamics and incorporating additional musical features (such as timbral variations and rhythmic complexity) to improve the model's adaptability and expressiveness. We plan to explore the model's application potential in other timbre conversion tasks, such as converting vocals to instrumental sounds or integration with MIDI digital pianos, further expanding the application scope of the Diffusion-based timbre conversion model in the field of music generation.",
      "url": "https://arxiv.org/abs/2601.09333",
      "pdfUrl": "https://arxiv.org/pdf/2601.09333.pdf",
      "titleJa": "拡散モデルに基づくピアノ音色変換システムの研究"
    },
    {
      "id": "2601.09239",
      "arxivId": "2601.09239",
      "title": "DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion",
      "authors": [
        "Hanlin Zhang",
        "Daxin Tan",
        "Dehua Tao",
        "Xiao Chen",
        "Haochen Tan",
        "Yunhe Li",
        "Yuchen Cao",
        "Jianping Wang",
        "Linqi Song"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Speech tokenizers serve as the cornerstone of discrete Speech Large Language Models (Speech LLMs). Existing tokenizers either prioritize semantic encoding, fuse semantic content with acoustic style inseparably, or achieve incomplete semantic-acoustic disentanglement. To achieve better disentanglement, we propose DSA-Tokenizer, which explicitly disentangles speech into discrete semantic and acoustic tokens via distinct optimization constraints. Specifically, semantic tokens are supervised by ASR to capture linguistic content, while acoustic tokens focus on mel-spectrograms restoration to encode style. To eliminate rigid length constraints between the two sequences, we introduce a hierarchical Flow-Matching decoder that further improve speech generation quality.Furthermore, We employ a joint reconstruction-recombination training strategy to enforce this separation. DSA-Tokenizer enables high fidelity reconstruction and flexible recombination through robust disentanglement, facilitating controllable generation in speech LLMs. Our analysis highlights disentangled tokenization as a pivotal paradigm for future speech modeling. Audio samples are avaialble at https://anonymous.4open.science/w/DSA_Tokenizer_demo/. The code and model will be made publicly available after the paper has been accepted.",
      "url": "https://arxiv.org/abs/2601.09239",
      "pdfUrl": "https://arxiv.org/pdf/2601.09239.pdf",
      "titleJa": "DSA-Tokenizer: フローマッチングに基づく階層的融合による意味音響分離トークン化"
    },
    {
      "id": "2601.08764",
      "arxivId": "2601.08764",
      "title": "FusID: Modality-Fused Semantic IDs for Generative Music Recommendation",
      "authors": [
        "Haven Kim",
        "Yupeng Hou",
        "Julian McAuley"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.IR",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Generative recommendation systems have achieved significant advances by leveraging semantic IDs to represent items. However, existing approaches that tokenize each modality independently face two critical limitations: (1) redundancy across modalities that reduces efficiency, and (2) failure to capture inter-modal interactions that limits item representation. We introduce FusID, a modality-fused semantic ID framework that addresses these limitations through three key components: (i) multimodal fusion that learns unified representations by jointly encoding information across modalities, (ii) representation learning that brings frequently co-occurring item embeddings closer while maintaining distinctiveness and preventing feature redundancy, and (iii) product quantization that converts the fused continuous embeddings into multiple discrete tokens to mitigate ID conflict. Evaluated on a multimodal next-song recommendation (i.e., playlist continuation) benchmark, FusID achieves zero ID conflicts, ensuring that each token sequence maps to exactly one song, mitigates codebook underutilization, and outperforms baselines in terms of MRR and Recall@k (k = 1, 5, 10, 20).",
      "url": "https://arxiv.org/abs/2601.08764",
      "pdfUrl": "https://arxiv.org/pdf/2601.08764.pdf",
      "titleJa": "FusID: 生成的音楽推薦のためのモダリティ融合セマンティックID"
    },
    {
      "id": "2601.08516",
      "arxivId": "2601.08516",
      "title": "Robust CAPTCHA Using Audio Illusions in the Era of Large Language Models: from Evaluation to Advances",
      "authors": [
        "Ziqi Ding",
        "Yunfeng Wan",
        "Wei Song",
        "Yi Liu",
        "Gelei Deng",
        "Nan Sun",
        "Huadong Mo",
        "Jingling Xue",
        "Shidong Pan",
        "Yuekang Li"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.SD",
        "cs.CY",
        "eess.AS"
      ],
      "abstract": "CAPTCHAs are widely used by websites to block bots and spam by presenting challenges that are easy for humans but difficult for automated programs to solve. To improve accessibility, audio CAPTCHAs are designed to complement visual ones. However, the robustness of audio CAPTCHAs against advanced Large Audio Language Models (LALMs) and Automatic Speech Recognition (ASR) models remains unclear. In this paper, we introduce AI-CAPTCHA, a unified framework that offers (i) an evaluation framework, ACEval, which includes advanced LALM- and ASR-based solvers, and (ii) a novel audio CAPTCHA approach, IllusionAudio, leveraging audio illusions. Through extensive evaluations of seven widely deployed audio CAPTCHAs, we show that most existing methods can be solved with high success rates by advanced LALMs and ASR models, exposing critical security weaknesses. To address these vulnerabilities, we design a new audio CAPTCHA approach, IllusionAudio, which exploits perceptual illusion cues rooted in human auditory mechanisms. Extensive experiments demonstrate that our method defeats all tested LALM- and ASR-based attacks while achieving a 100% human pass rate, significantly outperforming existing audio CAPTCHA methods.",
      "url": "https://arxiv.org/abs/2601.08516",
      "pdfUrl": "https://arxiv.org/pdf/2601.08516.pdf",
      "titleJa": "大規模言語モデルの時代における音響錯覚を利用した堅牢なCAPTCHA：評価から発展まで"
    },
    {
      "id": "2601.08450",
      "arxivId": "2601.08450",
      "title": "Decoding Order Matters in Autoregressive Speech Synthesis",
      "authors": [
        "Minghui Zhao",
        "Anton Ragni"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Autoregressive speech synthesis often adopts a left-to-right order, yet generation order is a modelling choice. We investigate decoding order through masked diffusion framework, which progressively unmasks positions and allows arbitrary decoding orders during training and inference. By interpolating between identity and random permutations, we show that randomness in decoding order affects speech quality. We further compare fixed strategies, such as \\texttt{l2r} and \\texttt{r2l} with adaptive ones, such as Top-$K$, finding that fixed-order decoding, including the dominating left-to-right approach, is suboptimal, while adaptive decoding yields better performance. Finally, since masked diffusion requires discrete inputs, we quantise acoustic representations and find that even 1-bit quantisation can support reasonably high-quality speech.",
      "url": "https://arxiv.org/abs/2601.08450",
      "pdfUrl": "https://arxiv.org/pdf/2601.08450.pdf",
      "titleJa": "自己回帰音声合成におけるデコード順序の重要性"
    },
    {
      "id": "2601.08358",
      "arxivId": "2601.08358",
      "title": "Decodable but not structured: linear probing enables Underwater Acoustic Target Recognition with pretrained audio embeddings",
      "authors": [
        "Hilde I. Hummel",
        "Sandjai Bhulai",
        "Rob D. van der Mei",
        "Burooj Ghani"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Increasing levels of anthropogenic noise from ships contribute significantly to underwater sound pollution, posing risks to marine ecosystems. This makes monitoring crucial to understand and quantify the impact of the ship radiated noise. Passive Acoustic Monitoring (PAM) systems are widely deployed for this purpose, generating years of underwater recordings across diverse soundscapes. Manual analysis of such large-scale data is impractical, motivating the need for automated approaches based on machine learning. Recent advances in automatic Underwater Acoustic Target Recognition (UATR) have largely relied on supervised learning, which is constrained by the scarcity of labeled data. Transfer Learning (TL) offers a promising alternative to mitigate this limitation. In this work, we conduct the first empirical comparative study of transfer learning for UATR, evaluating multiple pretrained audio models originating from diverse audio domains. The pretrained model weights are frozen, and the resulting embeddings are analyzed through classification, clustering, and similarity-based evaluations. The analysis shows that the geometrical structure of the embedding space is largely dominated by recording-specific characteristics. However, a simple linear probe can effectively suppress this recording-specific information and isolate ship-type features from these embeddings. As a result, linear probing enables effective automatic UATR using pretrained audio models at low computational cost, significantly reducing the need for a large amounts of high-quality labeled ship recordings.",
      "url": "https://arxiv.org/abs/2601.08358",
      "pdfUrl": "https://arxiv.org/pdf/2601.08358.pdf",
      "titleJa": "デコード可能だが構造化されていない：線形プローブにより、事前学習済みのオーディオ埋め込みによる水中音響ターゲット認識が可能になる"
    },
    {
      "id": "2601.08074",
      "arxivId": "2601.08074",
      "title": "Elastic overtones: an equal temperament 12 tone music system with \"perfect\" fifths",
      "authors": [
        "X. Hernandez",
        "Luis Nasser",
        "Pablo Garcia-Valenzuela"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "physics.soc-ph",
        "cs.SD",
        "eess.AS",
        "physics.pop-ph"
      ],
      "abstract": "The impossibility of a transposable 12 semitone tuning of the octave arises from the mathematical fact that $2 \\times 2^{7/12} \\neq 3$ i.e., the second harmonic of the fifth can not exactly match the third harmonic of the fundamental. This in turn, stems from the whole number harmonic structure of western music, and the subsequent fundamental character of the octave interval as multiples of 2 in frequency, a property inherited by our music system from the physics of instruments with vibrating elements being to a good approximation one dimensional. In the current era of electronic music, one can relax the above assumptions to construct an analogous music system where all the structural properties of the standard music system are preserved, but where harmonics are not whole number multiples of the fundamental frequency, and the octave is no longer a factor of 2 in frequency. This now allows to construct a transposable 12 semitone music system where the second harmonic of the fifth exactly matches the third harmonic of the fundamental. The enhanced harmonic qualities of this system recover to a good approximation the musical qualities of Just Intonation, whilst retaining by construction all the versatility and modulating ability of 12TET.",
      "url": "https://arxiv.org/abs/2601.08074",
      "pdfUrl": "https://arxiv.org/pdf/2601.08074.pdf",
      "titleJa": "弾性倍音: 完全五度を含む平均律12音音楽システム"
    },
    {
      "id": "2601.07999",
      "arxivId": "2601.07999",
      "title": "VoxCog: Towards End-to-End Multilingual Cognitive Impairment Classification through Dialectal Knowledge",
      "authors": [
        "Tiantian Feng",
        "Anfeng Xu",
        "Jinkook Lee",
        "Shrikanth Narayanan"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "In this work, we present a novel perspective on cognitive impairment classification from speech by integrating speech foundation models that explicitly recognize speech dialects. Our motivation is based on the observation that individuals with Alzheimer's Disease (AD) or mild cognitive impairment (MCI) often produce measurable speech characteristics, such as slower articulation rate and lengthened sounds, in a manner similar to dialectal phonetic variations seen in speech. Building on this idea, we introduce VoxCog, an end-to-end framework that uses pre-trained dialect models to detect AD or MCI without relying on additional modalities such as text or images. Through experiments on multiple multilingual datasets for AD and MCI detection, we demonstrate that model initialization with a dialect classifier on top of speech foundation models consistently improves the predictive performance of AD or MCI. Our trained models yield similar or often better performance compared to previous approaches that ensembled several computational methods using different signal modalities. Particularly, our end-to-end speech-based model achieves 87.5% and 85.9% accuracy on the ADReSS 2020 challenge and ADReSSo 2021 challenge test sets, outperforming existing solutions that use multimodal ensemble-based computation or LLMs.",
      "url": "https://arxiv.org/abs/2601.07999",
      "pdfUrl": "https://arxiv.org/pdf/2601.07999.pdf",
      "titleJa": "VoxCog: 方言知識によるエンドツーエンドの多言語認知障害分類に向けて"
    },
    {
      "id": "2601.07969",
      "arxivId": "2601.07969",
      "title": "Tuberculosis Screening from Cough Audio: Baseline Models, Clinical Variables, and Uncertainty Quantification",
      "authors": [
        "George P. Kafentzis",
        "Efstratios Selisios"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "In this paper, we propose a standardized framework for automatic tuberculosis (TB) detection from cough audio and routinely collected clinical data using machine learning. While TB screening from audio has attracted growing interest, progress is difficult to measure because existing studies vary substantially in datasets, cohort definitions, feature representations, model families, validation protocols, and reported metrics. Consequently, reported gains are often not directly comparable, and it remains unclear whether improvements stem from modeling advances or from differences in data and evaluation. We address this gap by establishing a strong, well-documented baseline for TB prediction using cough recordings and accompanying clinical metadata from a recently compiled dataset from several countries. Our pipeline is reproducible end-to-end, covering feature extraction, multimodal fusion, cougher-independent evaluation, and uncertainty quantification, and it reports a consistent suite of clinically relevant metrics to enable fair comparison. We further quantify performance for cough audio-only and fused (audio + clinical metadata) models, and release the full experimental protocol to facilitate benchmarking. This baseline is intended to serve as a common reference point and to reduce methodological variance that currently holds back progress in the field.",
      "url": "https://arxiv.org/abs/2601.07969",
      "pdfUrl": "https://arxiv.org/pdf/2601.07969.pdf",
      "titleJa": "咳嗽音による結核スクリーニング：ベースラインモデル、臨床変数、不確実性の定量化"
    },
    {
      "id": "2601.07958",
      "arxivId": "2601.07958",
      "title": "LJ-Spoof: A Generatively Varied Corpus for Audio Anti-Spoofing and Synthesis Source Tracing",
      "authors": [
        "Surya Subramani",
        "Hashim Ali",
        "Hafiz Malik"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Speaker-specific anti-spoofing and synthesis-source tracing are central challenges in audio anti-spoofing. Progress has been hampered by the lack of datasets that systematically vary model architectures, synthesis pipelines, and generative parameters. To address this gap, we introduce LJ-Spoof, a speaker-specific, generatively diverse corpus that systematically varies prosody, vocoders, generative hyperparameters, bona fide prompt sources, training regimes, and neural post-processing. The corpus spans one speakers-including studio-quality recordings-30 TTS families, 500 generatively variant subsets, 10 bona fide neural-processing variants, and more than 3 million utterances. This variation-dense design enables robust speaker-conditioned anti-spoofing and fine-grained synthesis-source tracing. We further position this dataset as both a practical reference training resource and a benchmark evaluation suite for anti-spoofing and source tracing.",
      "url": "https://arxiv.org/abs/2601.07958",
      "pdfUrl": "https://arxiv.org/pdf/2601.07958.pdf",
      "titleJa": "LJ-Spoof: 音声スプーフィング防止と合成音源追跡のための生成的に多様なコーパス"
    },
    {
      "id": "2601.08879",
      "arxivId": "2601.08879",
      "title": "Echoes of Ideology: Toward an Audio Analysis Pipeline to Unveil Character Traits in Historical Nazi Propaganda Films",
      "authors": [
        "Nicolas Ruth",
        "Manuel Burghardt"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This study investigates the use of computational audio analysis to examine ideological narratives in Nazi propaganda films. Employing a three-step pipeline, speaker diarization, audio transcription and psycholinguistic analysis, it reveals ideological patterns in characters. Despite current issues with speaker diarization, the methodology provides insights into character traits and propaganda narratives, suggesting scalable applications.",
      "url": "https://arxiv.org/abs/2601.08879",
      "pdfUrl": "https://arxiv.org/pdf/2601.08879.pdf",
      "titleJa": "イデオロギーの響き：ナチスの歴史的プロパガンダ映画の登場人物の特徴を明らかにする音声分析パイプラインの構築に向けて"
    },
    {
      "id": "2601.07367",
      "arxivId": "2601.07367",
      "title": "FOCAL: A Novel Benchmarking Technique for Multi-modal Agents",
      "authors": [
        "Aditya Choudhary",
        "Anupam Purwar"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD"
      ],
      "abstract": "With the recent advancements in reasoning capabilities, tool calling using MCP servers and Audio Language Models (ALMs), development and integration of multi-modal agents (with voice and text support) has come to the industry forefront. Cascading pipelines for voice agents still play a central role in the industry owing to their superior reasoning capabilities facilitated by LLMs. Although, cascading pipelines often present error propagation through the pipeline. We propose a framework, FOCAL to benchmark end-to-end reasoning, component-wise error propagation and error analysis for automated as well as human-assisted testing of multi-modal agents (voice to voice + text input). We also share two novel metrics viz. Reasoning and Semantic scores to evaluate efficacy of the agent in having meaningful conversations in voice mode.",
      "url": "https://arxiv.org/abs/2601.07367",
      "pdfUrl": "https://arxiv.org/pdf/2601.07367.pdf",
      "titleJa": "FOCAL: マルチモーダルエージェントのための新しいベンチマーク手法"
    },
    {
      "id": "2601.07331",
      "arxivId": "2601.07331",
      "title": "SEE: Signal Embedding Energy for Quantifying Noise Interference in Large Audio Language Models",
      "authors": [
        "Yuanhe Zhang",
        "Jiayu Tian",
        "Yibo Zhang",
        "Shilinlu Yan",
        "Liang Lin",
        "Zhenhong Zhou",
        "Li Sun",
        "Sen Su"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Large Audio Language Models (LALMs) have been widely applied in real-time scenarios, such as in-car assistants and online meeting comprehension. In practice, audio inputs are often corrupted by device and environmental noise, leading to performance degradation. However, existing LALM studies on noise lack quantitative analysis and rely mainly on intuition and empirical observation, thus failing to understand practical robustness. To address this issue, we introduce Signal Embedding Energy (SEE), a method for quantifying the impact of noise intensity on LALM inputs, enabling the differentiation of LALM robustness in real-world deployments. SEE introduces a perspective based on structured activation subspaces derived from the model's internal representations, which more accurately captures its perception of noise than raw audio features. Across experiments, SEE exhibits a strong correlation with LALM performance, achieving a correlation of 0.98. Surprisingly, traditional audio denoising methods are only marginally effective for LALMs, and, in some cases, even increase SEE and impair performance. This suggests a mismatch between speech-centric denoising objectives and the noise sensitivity of modern LALMs. Therefore, we propose a mitigation strategy derived from SEE to denoise LALM inputs, outperforming existing denoising methods. This paper introduces a novel metric for noise quantification in LALMs, providing guidance for robustness improvements in real-world deployments.",
      "url": "https://arxiv.org/abs/2601.07331",
      "pdfUrl": "https://arxiv.org/pdf/2601.07331.pdf",
      "titleJa": "参照: 大規模音声言語モデルにおけるノイズ干渉の定量化のための信号埋め込みエネルギー"
    },
    {
      "id": "2601.07303",
      "arxivId": "2601.07303",
      "title": "ESDD2: Environment-Aware Speech and Sound Deepfake Detection Challenge Evaluation Plan",
      "authors": [
        "Xueping Zhang",
        "Han Yin",
        "Yang Xiao",
        "Lin Zhang",
        "Ting Dang"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio recorded in real-world environments often contains a mixture of foreground speech and background environmental sounds. With rapid advances in text-to-speech, voice conversion, and other generation models, either component can now be modified independently. Such component-level manipulations are harder to detect, as the remaining unaltered component can mislead the systems designed for whole deepfake audio, and they often sound more natural to human listeners. To address this gap, we have proposed CompSpoofV2 dataset and a separation-enhanced joint learning framework. CompSpoofV2 is a large-scale curated dataset designed for component-level audio anti-spoofing, which contains over 250k audio samples, with a total duration of approximately 283 hours. Based on the CompSpoofV2 and the separation-enhanced joint learning framework, we launch the Environment-Aware Speech and Sound Deepfake Detection Challenge (ESDD2), focusing on component-level spoofing, where both speech and environmental sounds may be manipulated or synthesized, creating a more challenging and realistic detection scenario. The challenge will be held in conjunction with the IEEE International Conference on Multimedia and Expo 2026 (ICME 2026).",
      "url": "https://arxiv.org/abs/2601.07303",
      "pdfUrl": "https://arxiv.org/pdf/2601.07303.pdf",
      "titleJa": "ESDD2: 環境認識型音声ディープフェイク検出チャレンジ評価計画"
    },
    {
      "id": "2601.08537",
      "arxivId": "2601.08537",
      "title": "Weakly Supervised Tabla Stroke Transcription via TI-SDRM: A Rhythm-Aware Lattice Rescoring Framework",
      "authors": [
        "Rahul Bapusaheb Kodag",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Tabla Stroke Transcription (TST) is central to the analysis of rhythmic structure in Hindustani classical music, yet remains challenging due to complex rhythmic organization and the scarcity of strongly annotated data. Existing approaches largely rely on fully supervised learning with onset-level annotations, which are costly and impractical at scale. This work addresses TST in a weakly supervised setting, using only symbolic stroke sequences without temporal alignment. We propose a framework that combines a CTC-based acoustic model with sequence-level rhythmic rescoring. The acoustic model produces a decoding lattice, which is refined using a \\textbf{$T\\bar{a}la$}-Independent Static--Dynamic Rhythmic Model (TI-SDRM) that integrates long-term rhythmic structure with short-term adaptive dynamics through an adaptive interpolation mechanism. We curate a new real-world tabla solo dataset and a complementary synthetic dataset, establishing the first benchmark for weakly supervised TST in Hindustani classical music. Experiments demonstrate consistent and substantial reductions in stroke error rate over acoustic-only decoding, confirming the importance of explicit rhythmic structure for accurate transcription.",
      "url": "https://arxiv.org/abs/2601.08537",
      "pdfUrl": "https://arxiv.org/pdf/2601.08537.pdf",
      "titleJa": "TI-SDRMによる弱教師付きタブラストローク転写：リズムを考慮したラティス再採点フレームワーク"
    },
    {
      "id": "2601.08480",
      "arxivId": "2601.08480",
      "title": "Quantitative Analysis of Proxy Tasks for Anomalous Sound Detection",
      "authors": [
        "Seunghyeon Shin",
        "Seokjin Lee"
      ],
      "publishedDate": "2026-01-13",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Anomalous sound detection (ASD) typically involves self-supervised proxy tasks to learn feature representations from normal sound data, owing to the scarcity of anomalous samples. In ASD research, proxy tasks such as AutoEncoders operate under the explicit assumption that models trained on normal data will increase the reconstruction errors related to anomalies. A natural extension suggests that improved proxy task performance should improve ASD capability; however, this relationship has received little systematic attention. This study addresses this research gap by quantitatively analyzing the relationship between proxy task metrics and ASD performance across five configurations, namely, AutoEncoders, classification, source separation, contrastive learning, and pre-trained models. We evaluate the learned representations using linear probe (linear separability) and Mahalanobis distance (distributional compactness). Our experiments reveal that strong proxy performance does not necessarily improve anomalous sound detection performance. Specifically, classification tasks experience performance saturation owing to insufficient task difficulty, whereas contrastive learning fails to learn meaningful features owing to limited data diversity. Notably, source separation is the only task demonstrating a strong positive correlation, such that improved separation consistently improves anomaly detection. Based on these findings, we highlight the critical importance of task difficulty and objective alignment. Finally, we propose a three-stage alignment verification protocol to guide the design of highly effective proxy tasks for ASD systems.",
      "url": "https://arxiv.org/abs/2601.08480",
      "pdfUrl": "https://arxiv.org/pdf/2601.08480.pdf",
      "titleJa": "異常音検知のための代理タスクの定量分析"
    },
    {
      "id": "2601.07481",
      "arxivId": "2601.07481",
      "title": "Directional reflection modeling via wavenumber-domain reflection coefficient for 3D acoustic field simulation",
      "authors": [
        "Satoshi Hoshika",
        "Takahiro Iwami",
        "Akira Omoto"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS"
      ],
      "abstract": "This study proposes a framework for incorporating wavenumber-domain acoustic reflection coefficients into sound field analysis to characterize direction-dependent material reflection and scattering phenomena. The reflection coefficient is defined as the amplitude ratio between incident and reflected waves for each propagation direction and is estimated from spatial Fourier transforms of the incident and reflected sound fields. The resulting wavenumber-domain reflection coefficients are converted into an acoustic admittance representation that is directly compatible with numerical methods such as the Boundary Element Method (BEM), enabling simulation of reflections beyond simple specular components. Unlike conventional extended reaction models, the proposed approach avoids explicit modeling of the material interior. This significantly reduces computational cost while allowing direct use of measured data, empirical models, or user-defined directional reflection characteristics. The validity of the proposed formulation was previously demonstrated by the authors through two-dimensional sound field simulations, in which accurate reproduction of direction-dependent reflection behavior was confirmed. In the present work, the framework is extended to three-dimensional analysis, demonstrating its applicability to more realistic and complex acoustic environments. The proposed approach provides a practical and flexible tool for simulating direction-dependent acoustic reflections and scattering, with potential applications in architectural acoustics, material characterization, and noise control.",
      "url": "https://arxiv.org/abs/2601.07481",
      "pdfUrl": "https://arxiv.org/pdf/2601.07481.pdf",
      "titleJa": "3次元音響場シミュレーションのための波数領域反射係数による方向反射モデリング"
    },
    {
      "id": "2601.07237",
      "arxivId": "2601.07237",
      "title": "The ICASSP 2026 Automatic Song Aesthetics Evaluation Challenge",
      "authors": [
        "Guobin Ma",
        "Yuxuan Xia",
        "Jixun Yao",
        "Huixin Xue",
        "Hexin Liu",
        "Shuai Wang",
        "Hao Liu",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "This paper summarizes the ICASSP 2026 Automatic Song Aesthetics Evaluation (ASAE) Challenge, which focuses on predicting the subjective aesthetic scores of AI-generated songs. The challenge consists of two tracks: Track 1 targets the prediction of the overall musicality score, while Track 2 focuses on predicting five fine-grained aesthetic scores. The challenge attracted strong interest from the research community and received numerous submissions from both academia and industry. Top-performing systems significantly surpassed the official baseline, demonstrating substantial progress in aligning objective metrics with human aesthetic preferences. The outcomes establish a standardized benchmark and advance human-aligned evaluation methodologies for modern music generation systems.",
      "url": "https://arxiv.org/abs/2601.07237",
      "pdfUrl": "https://arxiv.org/pdf/2601.07237.pdf",
      "titleJa": "ICASSP 2026 自動歌曲美学評価チャレンジ"
    },
    {
      "id": "2601.08871",
      "arxivId": "2601.08871",
      "title": "Semantic visually-guided acoustic highlighting with large vision-language models",
      "authors": [
        "Junhua Huang",
        "Chao Huang",
        "Chenliang Xu"
      ],
      "publishedDate": "2026-01-12",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Balancing dialogue, music, and sound effects with accompanying video is crucial for immersive storytelling, yet current audio mixing workflows remain largely manual and labor-intensive. While recent advancements have introduced the visually guided acoustic highlighting task, which implicitly rebalances audio sources using multimodal guidance, it remains unclear which visual aspects are most effective as conditioning signals.We address this gap through a systematic study of whether deep video understanding improves audio remixing. Using textual descriptions as a proxy for visual analysis, we prompt large vision-language models to extract six types of visual-semantic aspects, including object and character appearance, emotion, camera focus, tone, scene background, and inferred sound-related cues. Through extensive experiments, camera focus, tone, and scene background consistently yield the largest improvements in perceptual mix quality over state-of-the-art baselines. Our findings (i) identify which visual-semantic cues most strongly support coherent and visually aligned audio remixing, and (ii) outline a practical path toward automating cinema-grade sound design using lightweight guidance derived from large vision-language models.",
      "url": "https://arxiv.org/abs/2601.08871",
      "pdfUrl": "https://arxiv.org/pdf/2601.08871.pdf",
      "titleJa": "大規模視覚言語モデルを用いた意味的視覚誘導音響強調表示"
    },
    {
      "id": "2601.07064",
      "arxivId": "2601.07064",
      "title": "Bridging Attribution and Open-Set Detection using Graph-Augmented Instance Learning in Synthetic Speech",
      "authors": [
        "Mohd Mujtaba Akhtar",
        " Girish",
        "Farhan Sheth",
        "Muskaan Singh"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We propose a unified framework for not only attributing synthetic speech to its source but also for detecting speech generated by synthesizers that were not encountered during training. This requires methods that move beyond simple detection to support both detailed forensic analysis and open-set generalization. To address this, we introduce SIGNAL, a hybrid framework that combines speech foundation models (SFMs) with graph-based modeling and open-set-aware inference. Our framework integrates Graph Neural Networks (GNNs) and a k-Nearest Neighbor (KNN) classifier, allowing it to capture meaningful relationships between utterances and recognize speech that doesn`t belong to any known generator. It constructs a query-conditioned graph over generator class prototypes, enabling the GNN to reason over relationships among candidate generators, while the KNN branch supports open-set detection via confidence-based thresholding. We evaluate SIGNAL using the DiffSSD dataset, which offers a diverse mix of real speech and synthetic audio from both open-source and commercial diffusion-based TTS systems. To further assess generalization, we also test on the SingFake benchmark. Our results show that SIGNAL consistently improves performance across both tasks, with Mamba-based embeddings delivering especially strong results. To the best of our knowledge, this is the first study to unify graph-based learning and open-set detection for tracing synthetic speech back to its origin.",
      "url": "https://arxiv.org/abs/2601.07064",
      "pdfUrl": "https://arxiv.org/pdf/2601.07064.pdf",
      "titleJa": "合成音声におけるグラフ拡張インスタンス学習を用いた帰属とオープンセット検出の橋渡し"
    },
    {
      "id": "2601.07014",
      "arxivId": "2601.07014",
      "title": "DIVINE: Coordinating Multimodal Disentangled Representations for Oro-Facial Neurological Disorder Assessment",
      "authors": [
        "Mohd Mujtaba Akhtar",
        " Girish",
        "Muskaan Singh"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "In this study, we present a multimodal framework for predicting neuro-facial disorders by capturing both vocal and facial cues. We hypothesize that explicitly disentangling shared and modality-specific representations within multimodal foundation model embeddings can enhance clinical interpretability and generalization. To validate this hypothesis, we propose DIVINE a fully disentangled multimodal framework that operates on representations extracted from state-of-the-art (SOTA) audio and video foundation models, incorporating hierarchical variational bottlenecks, sparse gated fusion, and learnable symptom tokens. DIVINE operates in a multitask learning setup to jointly predict diagnostic categories (Healthy Control,ALS, Stroke) and severity levels (Mild, Moderate, Severe). The model is trained using synchronized audio and video inputs and evaluated on the Toronto NeuroFace dataset under full (audio-video) as well as single-modality (audio-only and video-only) test conditions. Our proposed approach, DIVINE achieves SOTA result, with the DeepSeek-VL2 and TRILLsson combination reaching 98.26% accuracy and 97.51% F1-score. Under modality-constrained scenarios, the framework performs well, showing strong generalization when tested with video-only or audio-only inputs. It consistently yields superior performance compared to unimodal models and baseline fusion techniques. To the best of our knowledge, DIVINE is the first framework that combines cross-modal disentanglement, adaptive fusion, and multitask learning to comprehensively assess neurological disorders using synchronized speech and facial video.",
      "url": "https://arxiv.org/abs/2601.07014",
      "pdfUrl": "https://arxiv.org/pdf/2601.07014.pdf",
      "titleJa": "DIVINE: 口腔顔面神経疾患評価のためのマルチモーダル分離表現の調整"
    },
    {
      "id": "2601.06981",
      "arxivId": "2601.06981",
      "title": "Directional Selective Fixed-Filter Active Noise Control Based on a Convolutional Neural Network in Reverberant Environments",
      "authors": [
        "Boxiang Wang",
        "Zhengding Luo",
        "Haowen Li",
        "Dongyuan Shi",
        "Junwei Ji",
        "Ziyi Yang",
        "Woon-Seng Gan"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "Selective fixed-filter active noise control (SFANC) is a novel approach capable of mitigating noise with varying frequency characteristics. It offers faster response and greater computational efficiency compared to traditional adaptive algorithms. However, spatial factors, particularly the influence of the noise source location, are often overlooked. Some existing studies have explored the impact of the direction-of-arrival (DoA) of the noise source on ANC performance, but they are mostly limited to free-field conditions and do not consider the more complex indoor reverberant environments. To address this gap, this paper proposes a learning-based directional SFANC method that incorporates the DoA of the noise source in reverberant environments. In this framework, multiple reference signals are processed by a convolutional neural network (CNN) to estimate the azimuth and elevation angles of the noise source, as well as to identify the most appropriate control filter for effective noise cancellation. Compared to traditional adaptive algorithms, the proposed approach achieves superior noise reduction with shorter response times, even in the presence of reverberations.",
      "url": "https://arxiv.org/abs/2601.06981",
      "pdfUrl": "https://arxiv.org/pdf/2601.06981.pdf",
      "titleJa": "残響環境における畳み込みニューラルネットワークに基づく方向選択固定フィルタアクティブノイズコントロール"
    },
    {
      "id": "2601.06896",
      "arxivId": "2601.06896",
      "title": "TagSpeech: End-to-End Multi-Speaker ASR and Diarization with Fine-Grained Temporal Grounding",
      "authors": [
        "Mingyue Huo",
        "Yiwen Shao",
        "Yuheng Zhang"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "abstract": "We present TagSpeech, a unified LLM-based framework that utilizes Temporal Anchor Grounding for joint multi-speaker ASR and diarization. The framework is built on two key designs: (1) decoupled semantic and speaker streams fine-tuned via Serialized Output Training (SOT) to learn turn-taking dynamics; and (2) an interleaved time anchor mechanism that not only supports fine-grained timestamp prediction but also acts as a synchronization signal between semantic understanding and speaker tracking. Compared to previous works that primarily focus on speaker-attributed ASR or implicit diarization, TagSpeech addresses the challenge of fine-grained speaker-content alignment and explicitly models \"who spoke what and when\" in an end-to-end manner. Experiments on AMI and AliMeeting benchmarks demonstrate that our method achieves consistent improvements in Diarization Error Rate (DER) over strong end-to-end baselines, including Qwen-Omni and Gemini, particularly in handling complex speech overlaps. Moreover, TagSpeech employs a parameter-efficient training paradigm in which the LLM backbone is frozen and only lightweight projectors are trained, resulting in strong performance with low computational cost.",
      "url": "https://arxiv.org/abs/2601.06896",
      "pdfUrl": "https://arxiv.org/pdf/2601.06896.pdf",
      "titleJa": "TagSpeech: 細粒度時間グラウンディングによるエンドツーエンドのマルチスピーカーASRとダイアライゼーション"
    },
    {
      "id": "2601.06844",
      "arxivId": "2601.06844",
      "title": "Variational decomposition autoencoding improves disentanglement of latent representations",
      "authors": [
        "Ioannis Ziogas",
        "Aamna Al Shehhi",
        "Ahsan H. Khandoker",
        "Leontios J. Hadjileontiadis"
      ],
      "publishedDate": "2026-01-11",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.AS",
        "eess.SP",
        "stat.ML"
      ],
      "abstract": "Understanding the structure of complex, nonstationary, high-dimensional time-evolving signals is a central challenge in scientific data analysis. In many domains, such as speech and biomedical signal processing, the ability to learn disentangled and interpretable representations is critical for uncovering latent generative mechanisms. Traditional approaches to unsupervised representation learning, including variational autoencoders (VAEs), often struggle to capture the temporal and spectral diversity inherent in such data. Here we introduce variational decomposition autoencoding (VDA), a framework that extends VAEs by incorporating a strong structural bias toward signal decomposition. VDA is instantiated through variational decomposition autoencoders (DecVAEs), i.e., encoder-only neural networks that combine a signal decomposition model, a contrastive self-supervised task, and variational prior approximation to learn multiple latent subspaces aligned with time-frequency characteristics. We demonstrate the effectiveness of DecVAEs on simulated data and three publicly available scientific datasets, spanning speech recognition, dysarthria severity evaluation, and emotional speech classification. Our results demonstrate that DecVAEs surpass state-of-the-art VAE-based methods in terms of disentanglement quality, generalization across tasks, and the interpretability of latent encodings. These findings suggest that decomposition-aware architectures can serve as robust tools for extracting structured representations from dynamic signals, with potential applications in clinical diagnostics, human-computer interaction, and adaptive neurotechnologies.",
      "url": "https://arxiv.org/abs/2601.06844",
      "pdfUrl": "https://arxiv.org/pdf/2601.06844.pdf",
      "titleJa": "変分分解オートエンコーディングは潜在表現の分離を改善する"
    },
    {
      "id": "2601.09708",
      "arxivId": "2601.09708",
      "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
      "authors": [
        "Chi-Pin Huang",
        "Yunze Man",
        "Zhiding Yu",
        "Min-Hung Chen",
        "Jan Kautz",
        "Yu-Chiang Frank Wang",
        "Fu-En Yang"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "abstract": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.",
      "url": "https://arxiv.org/abs/2601.09708",
      "pdfUrl": "https://arxiv.org/pdf/2601.09708.pdf",
      "titleJa": "Fast-ThinkAct: 言語化可能な潜在的計画による効率的な視覚・言語・行動推論"
    },
    {
      "id": "2601.09706",
      "arxivId": "2601.09706",
      "title": "Value-Aware Numerical Representations for Transformer Language Models",
      "authors": [
        "Andreea Dutulescu",
        "Stefan Ruseti",
        "Mihai Dascalu"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into the model's input space while remaining compatible with existing tokenizers and decoder-only Transformer architectures. Evaluation on arithmetic tasks shows that the proposed approach outperforms baselines across numerical formats, tasks, and operand lengths. These results indicate that explicitly encoding numerical value is an effective and efficient way to improve fundamental numerical robustness in language models.",
      "url": "https://arxiv.org/abs/2601.09706",
      "pdfUrl": "https://arxiv.org/pdf/2601.09706.pdf",
      "titleJa": "Transformer言語モデルのための値を考慮した数値表現"
    },
    {
      "id": "2601.09703",
      "arxivId": "2601.09703",
      "title": "ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation",
      "authors": [
        "Sicong Liu",
        "Yanxian Huang",
        "Mingwei Liu",
        "Jiachi Chen",
        "Ensheng Shi",
        "Yuchi Ma",
        "Hongyu Zhang",
        "Yin Zhang",
        "Yanlin Wang"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.",
      "url": "https://arxiv.org/abs/2601.09703",
      "pdfUrl": "https://arxiv.org/pdf/2601.09703.pdf",
      "titleJa": "ShortCoder: トークン効率の高いコード生成のための知識拡張構文最適化"
    },
    {
      "id": "2601.09694",
      "arxivId": "2601.09694",
      "title": "LLMs can Compress LLMs: Adaptive Pruning by Agents",
      "authors": [
        "Sai Varun Kodathala",
        "Rakesh Vunnam"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "abstract": "As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.",
      "url": "https://arxiv.org/abs/2601.09694",
      "pdfUrl": "https://arxiv.org/pdf/2601.09694.pdf",
      "titleJa": "LLMは圧縮可能 LLM: エージェントによる適応的プルーニング"
    },
    {
      "id": "2601.09692",
      "arxivId": "2601.09692",
      "title": "Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection",
      "authors": [
        "Tianyi Niu",
        "Justin Chih-Yao Chen",
        "Genta Indra Winata",
        "Shi-Xiong Zhang",
        "Supriyo Chakraborty",
        "Sambit Sahu",
        "Yue Zhang",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice, especially when user request distributions are heterogeneous and unknown. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs. We evaluate query-answer routers (using both queries and labels) and query-only routers across four diverse benchmarks and 12 models, finding that query-answer routers degrade faster than query-only routers as generator quality decreases. Our analysis reveals two crucial characteristics of effective generators: they must accurately respond to their own questions, and their questions must produce sufficient performance differentiation among the model pool. We then show how filtering for these characteristics can improve the quality of generated data. We further propose CASCAL, a novel query-only router that estimates model correctness through consensus voting and identifies model-specific skill niches via hierarchical clustering. CASCAL is substantially more robust to generator quality, outperforming the best query-answer router by 4.6% absolute accuracy when trained on weak generator data.",
      "url": "https://arxiv.org/abs/2601.09692",
      "pdfUrl": "https://arxiv.org/pdf/2601.09692.pdf",
      "titleJa": "生成データによるルーティング：アノテーションフリーのLLMスキル推定と専門家の選択"
    },
    {
      "id": "2601.09684",
      "arxivId": "2601.09684",
      "title": "Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection",
      "authors": [
        "Ziyu Yang",
        "Guibin Chen",
        "Yuxin Yang",
        "Aoxiong Zeng",
        "Xiangquan Yang"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Multi-Task Learning (MTL) combined with Low-Rank Adaptation (LoRA) has emerged as a promising direction for parameter-efficient deployment of Large Language Models (LLMs). By sharing a single adapter across multiple tasks, one can significantly reduce storage overhead. However, this approach suffers from negative transfer, where conflicting gradient updates from distinct tasks degrade the performance of individual tasks compared to single-task fine-tuning. This problem is exacerbated in LoRA due to the low-rank constraint, which limits the optimization landscape's capacity to accommodate diverse task requirements. In this paper, we propose Ortho-LoRA, a gradient projection method specifically tailored for the bipartite structure of LoRA. Ortho-LoRA dynamically projects conflicting task gradients onto the orthogonal complement of each other within the intrinsic LoRA subspace. Extensive experiments on the GLUE benchmark demonstrate that Ortho-LoRA effectively mitigates task interference, outperforming standard joint training and recovering 95\\% of the performance gap between multi-task and single-task baselines with negligible computational overhead.",
      "url": "https://arxiv.org/abs/2601.09684",
      "pdfUrl": "https://arxiv.org/pdf/2601.09684.pdf",
      "titleJa": "直交勾配射影によるマルチタスク LoRA におけるタスク競合の解消"
    },
    {
      "id": "2601.09680",
      "arxivId": "2601.09680",
      "title": "Automating Supply Chain Disruption Monitoring via an Agentic AI Approach",
      "authors": [
        "Sara AlMahri",
        "Liming Xu",
        "Alexandra Brintrup"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Modern supply chains are increasingly exposed to disruptions from geopolitical events, demand shocks, trade restrictions, to natural disasters. While many of these disruptions originate deep in the supply network, most companies still lack visibility beyond Tier-1 suppliers, leaving upstream vulnerabilities undetected until the impact cascades downstream. To overcome this blind-spot and move from reactive recovery to proactive resilience, we introduce a minimally supervised agentic AI framework that autonomously monitors, analyses, and responds to disruptions across extended supply networks. The architecture comprises seven specialised agents powered by large language models and deterministic tools that jointly detect disruption signals from unstructured news, map them to multi-tier supplier networks, evaluate exposure based on network structure, and recommend mitigations such as alternative sourcing options. \\rev{We evaluate the framework across 30 synthesised scenarios covering three automotive manufacturers and five disruption classes. The system achieves high accuracy across core tasks, with F1 scores between 0.962 and 0.991, and performs full end-to-end analyses in a mean of 3.83 minutes at a cost of \\$0.0836 per disruption. Relative to industry benchmarks of multi-day, analyst-driven assessments, this represents a reduction of more than three orders of magnitude in response time. A real-world case study of the 2022 Russia-Ukraine conflict further demonstrates operational applicability. This work establishes a foundational step toward building resilient, proactive, and autonomous supply chains capable of managing disruptions across deep-tier networks.",
      "url": "https://arxiv.org/abs/2601.09680",
      "pdfUrl": "https://arxiv.org/pdf/2601.09680.pdf",
      "titleJa": "エージェントAIアプローチによるサプライチェーンの混乱監視の自動化"
    },
    {
      "id": "2601.09667",
      "arxivId": "2601.09667",
      "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "authors": [
        "Zhiyuan Hu",
        "Yunhai Hu",
        "Juncheng Liu",
        "Shuyue Stella Li",
        "Yucheng Wang",
        "Zhen Xu",
        "See-Kiong Ng",
        "Anh Tuan Luu",
        "Xinxing Xu",
        "Bryan Hooi",
        "Cynthia Breazeal",
        "Hae Won Park"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.",
      "url": "https://arxiv.org/abs/2601.09667",
      "pdfUrl": "https://arxiv.org/pdf/2601.09667.pdf",
      "titleJa": "推論のための協調型マルチエージェントテスト時強化学習"
    },
    {
      "id": "2601.09636",
      "arxivId": "2601.09636",
      "title": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records",
      "authors": [
        "Yibo Lyu",
        "Gongwei Chen",
        "Rui Shao",
        "Weili Guan",
        "Liqiang Nie"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "abstract": "While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.",
      "url": "https://arxiv.org/abs/2601.09636",
      "pdfUrl": "https://arxiv.org/pdf/2601.09636.pdf",
      "titleJa": "PersonalAlign: 長期的なユーザー中心の記録を備えたパーソナライズされた GUI エージェントのための階層的暗黙的インテントアライメント"
    },
    {
      "id": "2601.09635",
      "arxivId": "2601.09635",
      "title": "LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach",
      "authors": [
        "Kuo Liang",
        "Yuhang Lu",
        "Jianming Mao",
        "Shuyi Sun",
        "Chunwei Yang",
        "Congcong Zeng",
        "Xiao Jin",
        "Hanzhang Qin",
        "Ruihao Zhu",
        "Chung-Piaw Teo"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.",
      "url": "https://arxiv.org/abs/2601.09635",
      "pdfUrl": "https://arxiv.org/pdf/2601.09635.pdf",
      "titleJa": "大規模最適化モデルの自動定式化のためのLLM：軽量な少数ショット学習アプローチ"
    },
    {
      "id": "2601.09626",
      "arxivId": "2601.09626",
      "title": "From Prompt to Protocol: Fast Charging Batteries with Large Language Models",
      "authors": [
        "Ge Lei",
        "Ferran Brosa Planella",
        "Sterling G. Baird",
        "Samuel J. Cooper"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "abstract": "Efficiently optimizing battery charging protocols is challenging because each evaluation is slow, costly, and non-differentiable. Many existing approaches address this difficulty by heavily constraining the protocol search space, which limits the diversity of protocols that can be explored, preventing the discovery of higher-performing solutions. We introduce two gradient-free, LLM-driven closed-loop methods: Prompt-to-Optimizer (P2O), which uses an LLM to propose the code for small neural-network-based protocols, which are then trained by an inner loop, and Prompt-to-Protocol (P2P), which simply writes an explicit function for the current and its scalar parameters. Across our case studies, LLM-guided P2O outperforms neural networks designed by Bayesian optimization, evolutionary algorithms, and random search. In a realistic fast charging scenario, both P2O and P2P yield around a 4.2 percent improvement in state of health (capacity retention based health metric under fast charging cycling) over a state-of-the-art multi-step constant current (CC) baseline, with P2P achieving this under matched evaluation budgets (same number of protocol evaluations). These results demonstrate that LLMs can expand the space of protocol functional forms, incorporate language-based constraints, and enable efficient optimization in high cost experimental settings.",
      "url": "https://arxiv.org/abs/2601.09626",
      "pdfUrl": "https://arxiv.org/pdf/2601.09626.pdf",
      "titleJa": "プロンプトからプロトコルへ: 大規模言語モデルによるバッテリーの急速充電"
    },
    {
      "id": "2601.09625",
      "arxivId": "2601.09625",
      "title": "The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware",
      "authors": [
        "Ben Nassi",
        "Bruce Schneier",
        "Oleg Brodt"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "abstract": "The rapid adoption of large language model (LLM)-based systems -- from chatbots to autonomous agents capable of executing code and financial transactions -- has created a new attack surface that existing security frameworks inadequately address. The dominant framing of these threats as \"prompt injection\" -- a catch-all phrase for security failures in LLM-based systems -- obscures a more complex reality: Attacks on LLM-based systems increasingly involve multi-step sequences that mirror traditional malware campaigns. In this paper, we propose that attacks targeting LLM-based applications constitute a distinct class of malware, which we term \\textit{promptware}, and introduce a five-step kill chain model for analyzing these threats. The framework comprises Initial Access (prompt injection), Privilege Escalation (jailbreaking), Persistence (memory and retrieval poisoning), Lateral Movement (cross-system and cross-user propagation), and Actions on Objective (ranging from data exfiltration to unauthorized transactions). By mapping recent attacks to this structure, we demonstrate that LLM-related attacks follow systematic sequences analogous to traditional malware campaigns. The promptware kill chain offers security practitioners a structured methodology for threat modeling and provides a common vocabulary for researchers across AI safety and cybersecurity to address a rapidly evolving threat landscape.",
      "url": "https://arxiv.org/abs/2601.09625",
      "pdfUrl": "https://arxiv.org/pdf/2601.09625.pdf",
      "titleJa": "プロンプトウェアのキルチェーン：プロンプトインジェクションが段階的にマルチステップマルウェアへと進化した経緯"
    },
    {
      "id": "2601.09624",
      "arxivId": "2601.09624",
      "title": "Toward Understanding Unlearning Difficulty: A Mechanistic Perspective and Circuit-Guided Difficulty Metric",
      "authors": [
        "Jiali Cheng",
        "Ziheng Chen",
        "Chirag Agarwal",
        "Hadi Amiri"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "abstract": "Machine unlearning is becoming essential for building trustworthy and compliant language models. Yet unlearning success varies considerably across individual samples: some are reliably erased, while others persist despite the same procedure. We argue that this disparity is not only a data-side phenomenon, but also reflects model-internal mechanisms that encode and protect memorized information. We study this problem from a mechanistic perspective based on model circuits--structured interaction pathways that govern how predictions are formed. We propose Circuit-guided Unlearning Difficulty (CUD), a {\\em pre-unlearning} metric that assigns each sample a continuous difficulty score using circuit-level signals. Extensive experiments demonstrate that CUD reliably separates intrinsically easy and hard samples, and remains stable across unlearning methods. We identify key circuit-level patterns that reveal a mechanistic signature of difficulty: easy-to-unlearn samples are associated with shorter, shallower interactions concentrated in earlier-to-intermediate parts of the original model, whereas hard samples rely on longer and deeper pathways closer to late-stage computation. Compared to existing qualitative studies, CUD takes a first step toward a principled, fine-grained, and interpretable analysis of unlearning difficulty; and motivates the development of unlearning methods grounded in model mechanisms.",
      "url": "https://arxiv.org/abs/2601.09624",
      "pdfUrl": "https://arxiv.org/pdf/2601.09624.pdf",
      "titleJa": "脱学習の困難さの理解に向けて：メカニズム的視点と回路誘導型困難度指標"
    },
    {
      "id": "2601.09620",
      "arxivId": "2601.09620",
      "title": "Full Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers' Trust",
      "authors": [
        "Pooja Prajod",
        "Hannes Cools",
        "Thomas Röggla",
        "Karthikeya Puttur Venkatraj",
        "Amber Kusters",
        "Alia ElKattan",
        "Pablo Cesar",
        "Abdallah El Ali"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "abstract": "As artificial intelligence (AI) is increasingly integrated into news production, calls for transparency about the use of AI have gained considerable traction. Recent studies suggest that AI disclosures can lead to a ``transparency dilemma'', where disclosure reduces readers' trust. However, little is known about how the \\textit{level of detail} in AI disclosures influences trust and contributes to this dilemma within the news context. In this 3$\\times$2$\\times$2 mixed factorial study with 40 participants, we investigate how three levels of AI disclosures (none, one-line, detailed) across two types of news (politics and lifestyle) and two levels of AI involvement (low and high) affect news readers' trust. We measured trust using the News Media Trust questionnaire, along with two decision behaviors: source-checking and subscription decisions. Questionnaire responses and subscription rates showed a decline in trust only for detailed AI disclosures, whereas source-checking behavior increased for both one-line and detailed disclosures, with the effect being more pronounced for detailed disclosures. Insights from semi-structured interviews suggest that source-checking behavior was primarily driven by interest in the topic, followed by trust, whereas trust was the main factor influencing subscription decisions. Around two-thirds of participants expressed a preference for detailed disclosures, while most participants who preferred one-line indicated a need for detail-on-demand disclosure formats. Our findings show that not all AI disclosures lead to a transparency dilemma, but instead reflect a trade-off between readers' desire for more transparency and their trust in AI-assisted news content.",
      "url": "https://arxiv.org/abs/2601.09620",
      "pdfUrl": "https://arxiv.org/pdf/2601.09620.pdf",
      "titleJa": "完全開示は信頼を低下させる？ニュース記事におけるAI活用の詳細度が読者の信頼に及ぼす影響"
    },
    {
      "id": "2601.09613",
      "arxivId": "2601.09613",
      "title": "CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems",
      "authors": [
        "Yonglin Tian",
        "Qiyao Zhang",
        "Wei Xu",
        "Yutong Wang",
        "Yihao Wu",
        "Xinyi Li",
        "Xingyuan Dai",
        "Hui Zhang",
        "Zhiyong Cui",
        "Baoqing Guo",
        "Zujun Yu",
        "Yisheng Lv"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.",
      "url": "https://arxiv.org/abs/2601.09613",
      "pdfUrl": "https://arxiv.org/pdf/2601.09613.pdf",
      "titleJa": "CogRail: インテリジェント鉄道輸送システムにおける認知侵入検知における VLM のベンチマーク"
    },
    {
      "id": "2601.09609",
      "arxivId": "2601.09609",
      "title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing",
      "authors": [
        "Qian Cao",
        "Yahui Liu",
        "Wei Bi",
        "Yi Zhao",
        "Ruihua Song",
        "Xiting Wang",
        "Ruiming Tang",
        "Guorui Zhou",
        "Han Li"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.",
      "url": "https://arxiv.org/abs/2601.09609",
      "pdfUrl": "https://arxiv.org/pdf/2601.09609.pdf",
      "titleJa": "DPWriter: クリエイティブライティングのための多様なプランニング分岐を備えた強化学習"
    },
    {
      "id": "2601.09605",
      "arxivId": "2601.09605",
      "title": "Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets",
      "authors": [
        "Jeremiah Coholich",
        "Justin Wit",
        "Robert Azarcon",
        "Zsolt Kira"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "abstract": "Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose MANGO -- an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss. We find that these elements are crucial for maintaining viewpoint consistency during sim2real translation. When training MANGO, we only require a small amount of fixed-camera data from the real world, but show that our method can generate diverse unseen viewpoints by translating simulated observations. In this domain, MANGO outperforms all other image translation methods we tested. Imitation-learning policies trained on data augmented by MANGO are able to achieve success rates as high as 60\\% on views that the non-augmented policy fails completely on.",
      "url": "https://arxiv.org/abs/2601.09605",
      "pdfUrl": "https://arxiv.org/pdf/2601.09605.pdf",
      "titleJa": "Sim2real画像変換により固定カメラデータセットからの視点ロバストなポリシーを実現"
    },
    {
      "id": "2601.09600",
      "arxivId": "2601.09600",
      "title": "Information Access of the Oppressed: A Problem-Posing Framework for Envisioning Emancipatory Information Access Platforms",
      "authors": [
        "Bhaskar Mitra",
        "Nicola Neophytou",
        "Sireesh Gururaja"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ],
      "abstract": "Online information access (IA) platforms are targets of authoritarian capture. These concerns are particularly serious and urgent today in light of the rising levels of democratic erosion worldwide, the emerging capabilities of generative AI technologies such as AI persuasion, and the increasing concentration of economic and political power in the hands of Big Tech. This raises the question of what alternative IA infrastructure we must reimagine and build to mitigate the risks of authoritarian capture of our information ecosystems. We explore this question through the lens of Paulo Freire's theories of emancipatory pedagogy. Freire's theories provide a radically different lens for exploring IA's sociotechnical concerns relative to the current dominating frames of fairness, accountability, confidentiality, transparency, and safety. We make explicit, with the intention to challenge, the dichotomy of how we relate to technology as either technologists (who envision and build technology) and its users. We posit that this mirrors the teacher-student relationship in Freire's analysis. By extending Freire's analysis to IA, we challenge the notion that it is the burden of the (altruistic) technologists to come up with interventions to mitigate the risks that emerging technologies pose to marginalized communities. Instead, we advocate that the first task for the technologists is to pose these as problems to the marginalized communities, to encourage them to make and unmake the technology as part of their material struggle against oppression. Their second task is to redesign our online technology stacks to structurally expose spaces for community members to co-opt and co-construct the technology in aid of their emancipatory struggles. We operationalize Freire's theories to develop a problem-posing framework for envisioning emancipatory IA platforms of the future.",
      "url": "https://arxiv.org/abs/2601.09600",
      "pdfUrl": "https://arxiv.org/pdf/2601.09600.pdf",
      "titleJa": "抑圧された人々の情報アクセス：解放的な情報アクセス・プラットフォームを構想するための問題提起的枠組み"
    },
    {
      "id": "2601.09566",
      "arxivId": "2601.09566",
      "title": "Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese Language Modeling",
      "authors": [
        "Shuyang Xiang",
        "Hao Guan"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "abstract": "Large language models typically represent Chinese characters as discrete index-based tokens, largely ignoring their visual form. For logographic scripts, visual structure carries semantic and phonetic information, which may aid prediction. We investigate whether low-resolution visual inputs can serve as an alternative for character-level modeling. Instead of token IDs, our decoder receives grayscale images of individual characters, with resolutions as low as $8 \\times 8$ pixels. Remarkably, these inputs achieve 39.2\\% accuracy, comparable to the index-based baseline of 39.1\\%. Such low-resource settings also exhibit a pronounced \\emph{hot-start} effect: by 0.4\\% of total training, accuracy reaches above 12\\%, while index-based models lag at below 6\\%. Overall, our results demonstrate that minimal visual structure can provide a robust and efficient signal for Chinese language modeling, offering an alternative perspective on character representation that complements traditional index-based approaches.",
      "url": "https://arxiv.org/abs/2601.09566",
      "pdfUrl": "https://arxiv.org/pdf/2601.09566.pdf",
      "titleJa": "ピクセルからのホットスタート：中国語言語モデリングのための低解像度ビジュアルトークン"
    },
    {
      "id": "2601.06406",
      "arxivId": "2601.06406",
      "title": "Representing Sounds as Neural Amplitude Fields: A Benchmark of Coordinate-MLPs and A Fourier Kolmogorov-Arnold Framework",
      "authors": [
        "Linfei Li",
        "Lin Zhang",
        "Zhong Wang",
        "Fengyi Zhang",
        "Zelin Li",
        "Ying Shen"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Although Coordinate-MLP-based implicit neural representations have excelled in representing radiance fields, 3D shapes, and images, their application to audio signals remains underexplored. To fill this gap, we investigate existing implicit neural representations, from which we extract 3 types of positional encoding and 16 commonly used activation functions. Through combinatorial design, we establish the first benchmark for Coordinate-MLPs in audio signal representations. Our benchmark reveals that Coordinate-MLPs require complex hyperparameter tuning and frequency-dependent initialization, limiting their robustness. To address these issues, we propose Fourier-ASR, a novel framework based on the Fourier series theorem and the Kolmogorov-Arnold representation theorem. Fourier-ASR introduces Fourier Kolmogorov-Arnold Networks (Fourier-KAN), which leverage periodicity and strong nonlinearity to represent audio signals, eliminating the need for additional positional encoding. Furthermore, a Frequency-adaptive Learning Strategy (FaLS) is proposed to enhance the convergence of Fourier-KAN by capturing high-frequency components and preventing overfitting of low-frequency signals. Extensive experiments conducted on natural speech and music datasets reveal that: (1) well-designed positional encoding and activation functions in Coordinate-MLPs can effectively improve audio representation quality; and (2) Fourier-ASR can robustly represent complex audio signals without extensive hyperparameter tuning. Looking ahead, the continuity and infinite resolution of implicit audio representations make our research highly promising for tasks such as audio compression, synthesis, and generation. The source code will be released publicly to ensure reproducibility. The code is available at https://github.com/lif314/Fourier-ASR.",
      "url": "https://arxiv.org/abs/2601.06406",
      "pdfUrl": "https://arxiv.org/pdf/2601.06406.pdf",
      "titleJa": "音を神経振幅場として表現する：座標MLPのベンチマークとフーリエ・コルモゴロフ・アーノルド枠組み"
    },
    {
      "id": "2601.04592",
      "arxivId": "2601.04592",
      "title": "Density Matrix RNN (DM-RNN): A Quantum Information Theoretic Framework for Modeling Musical Context and Polyphony",
      "authors": [
        "Joonwon Seo",
        "Mariana Montiel"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.LG",
        "cs.SD",
        "math-ph"
      ],
      "abstract": "Classical Recurrent Neural Networks (RNNs) summarize musical context into a deterministic hidden state vector, imposing an information bottleneck that fails to capture the inherent ambiguity in music. We propose the Density Matrix RNN (DM-RNN), a novel theoretical architecture utilizing the Density Matrix. This allows the model to maintain a statistical ensemble of musical interpretations (a mixed state), capturing both classical probabilities and quantum coherences. We rigorously define the temporal dynamics using Quantum Channels (CPTP maps). Crucially, we detail a parameterization strategy based on the Choi-Jamiolkowski isomorphism, ensuring the learned dynamics remain physically valid (CPTP) by construction. We introduce an analytical framework using Von Neumann Entropy to quantify musical uncertainty and Quantum Mutual Information (QMI) to measure entanglement between voices. The DM-RNN provides a mathematically rigorous framework for modeling complex, ambiguous musical structures.",
      "url": "https://arxiv.org/abs/2601.04592",
      "pdfUrl": "https://arxiv.org/pdf/2601.04592.pdf",
      "titleJa": "密度行列RNN（DM-RNN）：音楽的文脈とポリフォニーをモデル化する量子情報理論的枠組み"
    },
    {
      "id": "2601.04343",
      "arxivId": "2601.04343",
      "title": "Summary of The Inaugural Music Source Restoration Challenge",
      "authors": [
        "Yongyi Zang",
        "Jiarui Hai",
        "Wanying Ge",
        "Qiuqiang Kong",
        "Zheqi Dai",
        "Helin Wang",
        "Yuki Mitsufuji",
        "Mark D. Plumbley"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Music Source Restoration (MSR) aims to recover original, unprocessed instrument stems from professionally mixed and degraded audio, requiring the reversal of both production effects and real-world degradations. We present the inaugural MSR Challenge, which features objective evaluation on studio-produced mixtures using Multi-Mel-SNR, Zimtohrli, and FAD-CLAP, alongside subjective evaluation on real-world degraded recordings. Five teams participated in the challenge. The winning system achieved 4.46 dB Multi-Mel-SNR and 3.47 MOS-Overall, corresponding to relative improvements of 91% and 18% over the second-place system, respectively. Per-stem analysis reveals substantial variation in restoration difficulty across instruments, with bass averaging 4.59 dB across all teams, while percussion averages only 0.29 dB. The dataset, evaluation protocols, and baselines are available at https://msrchallenge.com/.",
      "url": "https://arxiv.org/abs/2601.04343",
      "pdfUrl": "https://arxiv.org/pdf/2601.04343.pdf",
      "titleJa": "第1回音楽ソース修復チャレンジの概要"
    },
    {
      "id": "2601.03973",
      "arxivId": "2601.03973",
      "title": "Muse: Towards Reproducible Long-Form Song Generation with Fine-Grained Style Control",
      "authors": [
        "Changhao Jiang",
        "Jiahao Chen",
        "Zhenghao Xiang",
        "Zhixiong Yang",
        "Hanchen Wang",
        "Jiabao Zhuang",
        "Xinmeng Che",
        "Jiajun Sun",
        "Hui Li",
        "Yifei Cao",
        "Shihan Dou",
        "Ming Zhang",
        "Junjie Ye",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Recent commercial systems such as Suno demonstrate strong capabilities in long-form song generation, while academic research remains largely non-reproducible due to the lack of publicly available training data, hindering fair comparison and progress. To this end, we release a fully open-source system for long-form song generation with fine-grained style conditioning, including a licensed synthetic dataset, training and evaluation pipelines, and Muse, an easy-to-deploy song generation model. The dataset consists of 116k fully licensed synthetic songs with automatically generated lyrics and style descriptions paired with audio synthesized by SunoV5. We train Muse via single-stage supervised finetuning of a Qwen-based language model extended with discrete audio tokens using MuCodec, without task-specific losses, auxiliary objectives, or additional architectural components. Our evaluations find that although Muse is trained with a modest data scale and model size, it achieves competitive performance on phoneme error rate, text--music style similarity, and audio aesthetic quality, while enabling controllable segment-level generation across different musical structures. All data, model weights, and training and evaluation pipelines will be publicly released, paving the way for continued progress in controllable long-form song generation research. The project repository is available at https://github.com/yuhui1038/Muse.",
      "url": "https://arxiv.org/abs/2601.03973",
      "pdfUrl": "https://arxiv.org/pdf/2601.03973.pdf",
      "titleJa": "Muse: きめ細かなスタイル制御による再現性の高い長編楽曲生成に向けて"
    },
    {
      "id": "2601.03626",
      "arxivId": "2601.03626",
      "title": "Learning from Limited Labels: Transductive Graph Label Propagation for Indian Music Analysis",
      "authors": [
        "Parampreet Singh",
        "Akshay Raina",
        "Sayeedul Islam Sheikh",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Supervised machine learning frameworks rely on extensive labeled datasets for robust performance on real-world tasks. However, there is a lack of large annotated datasets in audio and music domains, as annotating such recordings is resource-intensive, laborious, and often require expert domain knowledge. In this work, we explore the use of label propagation (LP), a graph-based semi-supervised learning technique, for automatically labeling the unlabeled set in an unsupervised manner. By constructing a similarity graph over audio embeddings, we propagate limited label information from a small annotated subset to a larger unlabeled corpus in a transductive, semi-supervised setting. We apply this method to two tasks in Indian Art Music (IAM): Raga identification and Instrument classification. For both these tasks, we integrate multiple public datasets along with additional recordings we acquire from Prasar Bharati Archives to perform LP. Our experiments demonstrate that LP significantly reduces labeling overhead and produces higher-quality annotations compared to conventional baseline methods, including those based on pretrained inductive models. These results highlight the potential of graph-based semi-supervised learning to democratize data annotation and accelerate progress in music information retrieval.",
      "url": "https://arxiv.org/abs/2601.03626",
      "pdfUrl": "https://arxiv.org/pdf/2601.03626.pdf",
      "titleJa": "限定ラベルからの学習：インド音楽分析のためのトランスダクティブグラフラベル伝播"
    },
    {
      "id": "2601.03612",
      "arxivId": "2601.03612",
      "title": "Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias",
      "authors": [
        "Joonwon Seo"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "This monograph introduces a novel approach to polyphonic music generation by addressing the \"Missing Middle\" problem through structural inductive bias. Focusing on Beethoven's piano sonatas as a case study, we empirically verify the independence of pitch and hand attributes using normalized mutual information (NMI=0.167) and propose the Smart Embedding architecture, achieving a 48.30% reduction in parameters. We provide rigorous mathematical proofs using information theory (negligible loss bounded at 0.153 bits), Rademacher complexity (28.09% tighter generalization bound), and category theory to demonstrate improved stability and generalization. Empirical results show a 9.47% reduction in validation loss, confirmed by SVD analysis and an expert listening study (N=53). This dual theoretical and applied framework bridges gaps in AI music generation, offering verifiable insights for mathematically grounded deep learning.",
      "url": "https://arxiv.org/abs/2601.03612",
      "pdfUrl": "https://arxiv.org/pdf/2601.03612.pdf",
      "titleJa": "構造的帰納的バイアスによるポリフォニック音楽生成の数学的基礎"
    },
    {
      "id": "2601.03443",
      "arxivId": "2601.03443",
      "title": "Discriminating real and synthetic super-resolved audio samples using embedding-based classifiers",
      "authors": [
        "Mikhail Silaev",
        "Konstantinos Drossos",
        "Tuomas Virtanen"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "Generative adversarial networks (GANs) and diffusion models have recently achieved state-of-the-art performance in audio super-resolution (ADSR), producing perceptually convincing wideband audio from narrowband inputs. However, existing evaluations primarily rely on signal-level or perceptual metrics, leaving open the question of how closely the distributions of synthetic super-resolved and real wideband audio match. Here we address this problem by analyzing the separability of real and super-resolved audio in various embedding spaces. We consider both middle-band ($4\\to 16$~kHz) and full-band ($16\\to 48$~kHz) upsampling tasks for speech and music, training linear classifiers to distinguish real from synthetic samples based on multiple types of audio embeddings. Comparisons with objective metrics and subjective listening tests reveal that embedding-based classifiers achieve near-perfect separation, even when the generated audio attains high perceptual quality and state-of-the-art metric scores. This behavior is consistent across datasets and models, including recent diffusion-based approaches, highlighting a persistent gap between perceptual quality and true distributional fidelity in ADSR models.",
      "url": "https://arxiv.org/abs/2601.03443",
      "pdfUrl": "https://arxiv.org/pdf/2601.03443.pdf",
      "titleJa": "埋め込みベースの分類器を用いた実在および合成の超解像オーディオサンプルの識別"
    },
    {
      "id": "2601.02983",
      "arxivId": "2601.02983",
      "title": "Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning",
      "authors": [
        "Yuankun Xie",
        "Xiaoxuan Guo",
        "Jiayi Zhou",
        "Tao Wang",
        "Jian Liu",
        "Ruibo Fu",
        "Xiaopeng Wang",
        "Haonan Cheng",
        "Long Ye"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in audio large language models (ALLMs) have made high-quality synthetic audio widely accessible, increasing the risk of malicious audio deepfakes across speech, environmental sounds, singing voice, and music. Real-world audio deepfake detection (ADD) therefore requires all-type detectors that generalize across heterogeneous audio and provide interpretable decisions. Given the strong multi-task generalization ability of ALLMs, we first investigate their performance on all-type ADD under both supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). However, SFT using only binary real/fake labels tends to reduce the model to a black-box classifier, sacrificing interpretability. Meanwhile, vanilla RFT under sparse supervision is prone to reward hacking and can produce hallucinated, ungrounded rationales. To address this, we propose an automatic annotation and polishing pipeline that constructs Frequency-Time structured chain-of-thought (CoT) rationales, producing ~340K cold-start demonstrations. Building on CoT data, we propose Frequency Time-Group Relative Policy Optimization (FT-GRPO), a two-stage training paradigm that cold-starts ALLMs with SFT and then applies GRPO under rule-based frequency-time constraints. Experiments demonstrate that FT-GRPO achieves state-of-the-art performance on all-type ADD while producing interpretable, FT-grounded rationales. The data and code are available online.",
      "url": "https://arxiv.org/abs/2601.02983",
      "pdfUrl": "https://arxiv.org/pdf/2601.02983.pdf",
      "titleJa": "周波数時間強化学習によるオーディオLLMを用いた解釈可能な全タイプオーディオディープフェイク検出"
    },
    {
      "id": "2601.02967",
      "arxivId": "2601.02967",
      "title": "MoE Adapter for Large Audio Language Models: Sparsity, Disentanglement, and Gradient-Conflict-Free",
      "authors": [
        "Yishu Lei",
        "Shuwei He",
        "Jing Hu",
        "Dan Zhang",
        "Xianlong Luo",
        "Danxiang Zhu",
        "Shikun Feng",
        "Rui Liu",
        "Jingzhou He",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Extending the input modality of Large Language Models~(LLMs) to the audio domain is essential for achieving comprehensive multimodal perception. However, it is well-known that acoustic information is intrinsically \\textit{heterogeneous}, entangling attributes such as speech, music, and environmental context. Existing research is limited to a dense, parameter-shared adapter to model these diverse patterns, which induces \\textit{gradient conflict} during optimization, as parameter updates required for distinct attributes contradict each other. To address this limitation, we introduce the \\textit{\\textbf{MoE-Adapter}}, a sparse Mixture-of-Experts~(MoE) architecture designed to decouple acoustic information. Specifically, it employs a dynamic gating mechanism that routes audio tokens to specialized experts capturing complementary feature subspaces while retaining shared experts for global context, thereby mitigating gradient conflicts and enabling fine-grained feature learning. Comprehensive experiments show that the MoE-Adapter achieves superior performance on both audio semantic and paralinguistic tasks, consistently outperforming dense linear baselines with comparable computational costs. Furthermore, we will release the related code and models to facilitate future research.",
      "url": "https://arxiv.org/abs/2601.02967",
      "pdfUrl": "https://arxiv.org/pdf/2601.02967.pdf",
      "titleJa": "大規模音声言語モデルのためのMoEアダプタ：スパース性、分離、勾配衝突フリー"
    },
    {
      "id": "2601.02591",
      "arxivId": "2601.02591",
      "title": "A Music Information Retrieval Approach to Classify Sub-Genres in Role Playing Games",
      "authors": [
        "Daeun Hwang",
        "Xuyuan Cai",
        "Edward F. Melcer",
        "Elin Carstensdottir"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.IR"
      ],
      "abstract": "Video game music (VGM) is often studied under the same lens as film music, which largely focuses on its theoretical functionality with relation to the identified genres of the media. However, till date, we are unaware of any systematic approach that analyzes the quantifiable musical features in VGM across several identified game genres. Therefore, we extracted musical features from VGM in games from three sub-genres of Role-Playing Games (RPG), and then hypothesized how different musical features are correlated to the perceptions and portrayals of each genre. This observed correlation may be used to further suggest such features are relevant to the expected storytelling elements or play mechanics associated with the sub-genre.",
      "url": "https://arxiv.org/abs/2601.02591",
      "pdfUrl": "https://arxiv.org/pdf/2601.02591.pdf",
      "titleJa": "ロールプレイングゲームのサブジャンルを分類するための音楽情報検索アプローチ"
    },
    {
      "id": "2601.02586",
      "arxivId": "2601.02586",
      "title": "Understanding Human Perception of Music Plagiarism Through a Computational Approach",
      "authors": [
        "Daeun Hwang",
        "Hyeonbin Hwang"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.IR"
      ],
      "abstract": "There is a wide variety of music similarity detection algorithms, while discussions about music plagiarism in the real world are often based on audience perceptions. Therefore, we aim to conduct a study to examine the key criteria of human perception of music plagiarism, focusing on the three commonly used musical features in similarity analysis: melody, rhythm, and chord progression. After identifying the key features and levels of variation humans use in perceiving musical similarity, we propose a LLM-as-a-judge framework that applies a systematic, step-by-step approach, drawing on modules that extract such high-level attributes.",
      "url": "https://arxiv.org/abs/2601.02586",
      "pdfUrl": "https://arxiv.org/pdf/2601.02586.pdf",
      "titleJa": "計算論的アプローチによる音楽盗作に対する人間の認識の理解"
    },
    {
      "id": "2601.06621",
      "arxivId": "2601.06621",
      "title": "Stereo Audio Rendering for Personal Sound Zones Using a Binaural Spatially Adaptive Neural Network (BSANN)",
      "authors": [
        "Hao Jiang",
        "Edgar Choueiri"
      ],
      "publishedDate": "2026-01-10",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "A binaural rendering framework for personal sound zones (PSZs) is proposed to enable multiple head-tracked listeners to receive fully independent stereo audio programs. Current PSZ systems typically rely on monophonic rendering and therefore cannot control the left and right ears separately, which limits the quality and accuracy of spatial imaging. The proposed method employs a Binaural Spatially Adaptive Neural Network (BSANN) to generate ear-optimized loudspeaker filters that reconstruct the desired acoustic field at each ear of multiple listeners. The framework integrates anechoically measured loudspeaker frequency responses, analytically modeled transducer directivity, and rigid-sphere head-related transfer functions (HRTFs) to enhance acoustic accuracy and spatial rendering fidelity. An explicit active crosstalk cancellation (XTC) stage further improves three-dimensional spatial perception. Experiments show significant gains in measured objective performance metrics, including inter-zone isolation (IZI), inter-program isolation (IPI), and crosstalk cancellation (XTC), with log-frequency-weighted values of 10.23/10.03 dB (IZI), 11.11/9.16 dB (IPI), and 10.55/11.13 dB (XTC), respectively, over 100-20,000 Hz. The combined use of ear-wise control, accurate acoustic modeling, and integrated active XTC produces a unified rendering method that delivers greater isolation performance, increased robustness to room asymmetry, and more faithful spatial reproduction in real acoustic environments.",
      "url": "https://arxiv.org/abs/2601.06621",
      "pdfUrl": "https://arxiv.org/pdf/2601.06621.pdf",
      "titleJa": "バイノーラル空間適応型ニューラルネットワーク（BSANN）を用いたパーソナルサウンドゾーン向けステレオオーディオレンダリング"
    },
    {
      "id": "2601.06006",
      "arxivId": "2601.06006",
      "title": "Discriminative-Generative Target Speaker Extraction with Decoder-Only Language Models",
      "authors": [
        "Bang Zeng",
        "Beilong Tang",
        "Wang Xiang",
        "Ming Li"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Target speaker extraction (TSE) aims to recover the speech signal of a desired speaker from a mixed audio recording, given a short enrollment utterance. Most existing TSE approaches are based on discriminative modeling paradigms. Although effective at suppressing interfering speakers, these methods often struggle to produce speech with high perceptual quality and naturalness. To address this limitation, we first propose LauraTSE, a generative TSE model built upon an auto-regressive decoder-only language model. However, purely generative approaches may suffer from hallucinations, content drift, and limited controllability, which may undermine their reliability in complex acoustic scenarios. To overcome these challenges, we further introduce a discriminative-generative TSE framework. In this framework, a discriminative front-end is employed to robustly extract the target speaker's speech, yielding stable and controllable intermediate representations. A generative back-end then operates in the neural audio codec representation space to reconstruct fine-grained speech details and enhance perceptual quality. This two-stage design effectively combines the robustness and controllability of discriminative models with the superior naturalness and quality enhancement capabilities of generative models. Moreover, we systematically investigate collaborative training strategies for the proposed framework, including freezing or fine-tuning the front-end, incorporating an auxiliary SI-SDR loss, and exploring both auto-regressive and non-auto-regressive inference mechanisms. Experimental results demonstrate that the proposed framework achieves a more favorable trade-off among speech quality, intelligibility, and speaker consistency.",
      "url": "https://arxiv.org/abs/2601.06006",
      "pdfUrl": "https://arxiv.org/pdf/2601.06006.pdf",
      "titleJa": "デコーダのみの言語モデルを用いた識別的・生成的ターゲット話者抽出"
    },
    {
      "id": "2601.05554",
      "arxivId": "2601.05554",
      "title": "SPAM: Style Prompt Adherence Metric for Prompt-based TTS",
      "authors": [
        "Chanhee Cho",
        "Nayeon Kim",
        "Bugeun Kim"
      ],
      "publishedDate": "2026-01-09",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Prompt-based text-to-speech (TTS) aims to generate speech that adheres to fine-grained style cues provided in a text prompt. However, most prior works depend on neither plausible nor faithful measures to evaluate prompt adherence. That is, they cannot ensure whether the evaluation is grounded on the prompt and is similar to a human. Thus, we present a new automatic metric, the Style Prompt Adherence Metric, which explicitly satisfies both plausibility and faithfulness. Inspired by the CLAP, our approach factorizes speech into acoustic attributes and aligns them with the style prompt. Also, we trained the scorer with a supervised contrastive loss, which could provide a clearer distinction between different semantics. We conducted two experiments on two perspectives. The plausibility experiment showed that SPAM achieved a strong correlation with the mean opinion score (MOS). Also, the faithfulness experiment demonstrated that SPAM is successfully grounded to the given style prompt, as it can discriminate different semantics of the prompt. We believe that SPAM can provide a viable automatic solution for evaluating style prompt adherence of synthesized speech.",
      "url": "https://arxiv.org/abs/2601.05554",
      "pdfUrl": "https://arxiv.org/pdf/2601.05554.pdf",
      "titleJa": "SPAM: プロンプトベースの TTS におけるスタイルプロンプト遵守指標"
    },
    {
      "id": "2601.04744",
      "arxivId": "2601.04744",
      "title": "Semi-Supervised Diseased Detection from Speech Dialogues with Multi-Level Data Modeling",
      "authors": [
        "Xingyuan Li",
        "Mengyue Wu"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Detecting medical conditions from speech acoustics is fundamentally a weakly-supervised learning problem: a single, often noisy, session-level label must be linked to nuanced patterns within a long, complex audio recording. This task is further hampered by severe data scarcity and the subjective nature of clinical annotations. While semi-supervised learning (SSL) offers a viable path to leverage unlabeled data, existing audio methods often fail to address the core challenge that pathological traits are not uniformly expressed in a patient's speech. We propose a novel, audio-only SSL framework that explicitly models this hierarchy by jointly learning from frame-level, segment-level, and session-level representations within unsegmented clinical dialogues. Our end-to-end approach dynamically aggregates these multi-granularity features and generates high-quality pseudo-labels to efficiently utilize unlabeled data. Extensive experiments show the framework is model-agnostic, robust across languages and conditions, and highly data-efficient-achieving, for instance, 90\\% of fully-supervised performance using only 11 labeled samples. This work provides a principled approach to learning from weak, far-end supervision in medical speech analysis.",
      "url": "https://arxiv.org/abs/2601.04744",
      "pdfUrl": "https://arxiv.org/pdf/2601.04744.pdf",
      "titleJa": "多層データモデリングを用いた音声対話からの半教師付き疾患検出"
    },
    {
      "id": "2601.04564",
      "arxivId": "2601.04564",
      "title": "When Tone and Words Disagree: Towards Robust Speech Emotion Recognition under Acoustic-Semantic Conflict",
      "authors": [
        "Dawei Huang",
        "Yongjie Lv",
        "Ruijie Xiong",
        "Chunxiang Jin",
        "Xiaojiang Peng"
      ],
      "publishedDate": "2026-01-08",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Speech Emotion Recognition (SER) systems often assume congruence between vocal emotion and lexical semantics. However, in real-world interactions, acoustic-semantic conflict is common yet overlooked, where the emotion conveyed by tone contradicts the literal meaning of spoken words. We show that state-of-the-art SER models, including ASR-based, self-supervised learning (SSL) approaches and Audio Language Models (ALMs), suffer performance degradation under such conflicts due to semantic bias or entangled acoustic-semantic representations. To address this, we propose the Fusion Acoustic-Semantic (FAS) framework, which explicitly disentangles acoustic and semantic pathways and bridges them through a lightweight, query-based attention module. To enable systematic evaluation, we introduce the Conflict in Acoustic-Semantic Emotion (CASE), the first dataset dominated by clear and interpretable acoustic-semantic conflicts in varied scenarios. Extensive experiments demonstrate that FAS consistently outperforms existing methods in both in-domain and zero-shot settings. Notably, on the CASE benchmark, conventional SER models fail dramatically, while FAS sets a new SOTA with 59.38% accuracy. Our code and datasets is available at https://github.com/24DavidHuang/FAS.",
      "url": "https://arxiv.org/abs/2601.04564",
      "pdfUrl": "https://arxiv.org/pdf/2601.04564.pdf",
      "titleJa": "音調と言葉が一致しないとき：音響的・意味的矛盾下におけるロバストな音声感情認識に向けて"
    },
    {
      "id": "2601.03712",
      "arxivId": "2601.03712",
      "title": "TellWhisper: Tell Whisper Who Speaks When",
      "authors": [
        "Yifan Hu",
        "Peiji Yang",
        "Zhisheng Wang",
        "Yicheng Zhong",
        "Rui Liu"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Multi-speaker automatic speech recognition (MASR) aims to predict ''who spoke when and what'' from multi-speaker speech, a key technology for multi-party dialogue understanding. However, most existing approaches decouple temporal modeling and speaker modeling when addressing ''when'' and ''who'': some inject speaker cues before encoding (e.g., speaker masking), which can cause irreversible information loss; others fuse identity by mixing speaker posteriors after encoding, which may entangle acoustic content with speaker identity. This separation is brittle under rapid turn-taking and overlapping speech, often leading to degraded performance. To address these limitations, we propose TellWhisper, a unified framework that jointly models speaker identity and temporal within the speech encoder. Specifically, we design TS-RoPE, a time-speaker rotary positional encoding: time coordinates are derived from frame indices, while speaker coordinates are derived from speaker activity and pause cues. By applying region-specific rotation angles, the model explicitly captures per-speaker continuity, speaker-turn transitions, and state dynamics, enabling the attention mechanism to simultaneously attend to ''when'' and ''who''. Moreover, to estimate frame-level speaker activity, we develop Hyper-SD, which casts speaker classification in hyperbolic space to enhance inter-class separation and refine speaker-activity estimates. Extensive experiments demonstrate the effectiveness of the proposed approach.",
      "url": "https://arxiv.org/abs/2601.03712",
      "pdfUrl": "https://arxiv.org/pdf/2601.03712.pdf",
      "titleJa": "TellWhisper: 誰がいつ話すかを知らせる"
    },
    {
      "id": "2601.03615",
      "arxivId": "2601.03615",
      "title": "Analyzing Reasoning Shifts in Audio Deepfake Detection under Adversarial Attacks: The Reasoning Tax versus Shield Bifurcation",
      "authors": [
        "Binh Nguyen",
        "Thai Le"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Audio Language Models (ALMs) offer a promising shift towards explainable audio deepfake detections (ADDs), moving beyond \\textit{black-box} classifiers by providing some level of transparency into their predictions via reasoning traces. This necessitates a new class of model robustness analysis: robustness of the predictive reasoning under adversarial attacks, which goes beyond existing paradigm that mainly focuses on the shifts of the final predictions (e.g., fake v.s. real). To analyze such reasoning shifts, we introduce a forensic auditing framework to evaluate the robustness of ALMs' reasoning under adversarial attacks in three inter-connected dimensions: acoustic perception, cognitive coherence, and cognitive dissonance. Our systematic analysis reveals that explicit reasoning does not universally enhance robustness. Instead, we observe a bifurcation: for models exhibiting robust acoustic perception, reasoning acts as a defensive \\textit{``shield''}, protecting them from adversarial attacks. However, for others, it imposes a performance \\textit{``tax''}, particularly under linguistic attacks which reduce cognitive coherence and increase attack success rate. Crucially, even when classification fails, high cognitive dissonance can serve as a \\textit{silent alarm}, flagging potential manipulation. Overall, this work provides a critical evaluation of the role of reasoning in forensic audio deepfake analysis and its vulnerabilities.",
      "url": "https://arxiv.org/abs/2601.03615",
      "pdfUrl": "https://arxiv.org/pdf/2601.03615.pdf",
      "titleJa": "敵対的攻撃下における音声ディープフェイク検出の推論シフトの分析：推論税とシールド分岐"
    },
    {
      "id": "2601.03610",
      "arxivId": "2601.03610",
      "title": "Investigation into respiratory sound classification for an imbalanced data set using hybrid LSTM-KAN architectures",
      "authors": [
        "Nithinkumar K.",
        "Anand R"
      ],
      "publishedDate": "2026-01-07",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "Respiratory sounds captured via auscultation contain critical clues for diagnosing pulmonary conditions. Automated classification of these sounds faces challenges due to subtle acoustic differences and severe class imbalance in clinical datasets. This study investigates respiratory sound classification with a focus on mitigating pronounced class imbalance. We propose a hybrid deep learning model that combines a Long Short-Term Memory (LSTM) network for sequential feature encoding with a Kolmogorov-Arnold Network (KAN) for classification. The model is integrated with a comprehensive feature extraction pipeline and targeted imbalance mitigation strategies. Experiments were conducted on a public respiratory sound database comprising six classes with a highly skewed distribution. Techniques such as focal loss, class-specific data augmentation, and Synthetic Minority Over-sampling Technique (SMOTE) were employed to enhance minority class recognition. The proposed Hybrid LSTM-KAN model achieves an overall accuracy of 94.6 percent and a macro-averaged F1 score of 0.703, despite the dominant COPD class accounting for over 86 percent of the data. Improved detection performance is observed for minority classes compared to baseline approaches, demonstrating the effectiveness of the proposed architecture for imbalanced respiratory sound classification.",
      "url": "https://arxiv.org/abs/2601.03610",
      "pdfUrl": "https://arxiv.org/pdf/2601.03610.pdf",
      "titleJa": "ハイブリッドLSTM-KANアーキテクチャを用いた不均衡なデータセットの呼吸音分類の調査"
    },
    {
      "id": "2601.02954",
      "arxivId": "2601.02954",
      "title": "The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models",
      "authors": [
        "Yuhuan You",
        "Lai Wei",
        "Xihong Wu",
        "Tianshu Qu"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Existing large audio-language models perceive the world as \"mono\" -- a single stream of audio that ignores the critical spatial dimension (\"where\") required for universal acoustic scene analysis. To bridge this gap, we first introduce a hierarchical framework for Auditory Scene Analysis (ASA). Guided by this framework, we introduce a system that enables models like Qwen2-Audio to understand and reason about the complex acoustic world. Our framework achieves this through three core contributions: First, we build a large-scale, synthesized binaural audio dataset to provide the rich spatial cues. Second, we design a hybrid feature projector, which leverages parallel semantic and spatial encoders to extract decoupled representations. These distinct streams are integrated via a dense fusion mechanism, ensuring the model receives a holistic view of the acoustic scene. Finally, we employ a progressive training curriculum, advancing from supervised fine-tuning (SFT) to reinforcement learning via Group Relative Policy Optimization (GRPO), to explicitly evolve the model's capabilities towards reasoning. On our comprehensive benchmark, the model demonstrates comparatively strong capability for spatial understanding. By enabling this spatial perception, our work provides a clear pathway for leveraging the powerful reasoning abilities of large models towards holistic acoustic scene analysis, advancing from \"mono\" semantic recognition to spatial intelligence.",
      "url": "https://arxiv.org/abs/2601.02954",
      "pdfUrl": "https://arxiv.org/pdf/2601.02954.pdf",
      "titleJa": "世界は単一ではない：大規模音声言語モデルにおける空間理解の実現"
    },
    {
      "id": "2601.02688",
      "arxivId": "2601.02688",
      "title": "Multi-channel multi-speaker transformer for speech recognition",
      "authors": [
        "Guo Yifan",
        "Tian Yao",
        "Suo Hongbin",
        "Wan Yulong"
      ],
      "publishedDate": "2026-01-06",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "With the development of teleconferencing and in-vehicle voice assistants, far-field multi-speaker speech recognition has become a hot research topic. Recently, a multi-channel transformer (MCT) has been proposed, which demonstrates the ability of the transformer to model far-field acoustic environments. However, MCT cannot encode high-dimensional acoustic features for each speaker from mixed input audio because of the interference between speakers. Based on these, we propose the multi-channel multi-speaker transformer (M2Former) for far-field multi-speaker ASR in this paper. Experiments on the SMS-WSJ benchmark show that the M2Former outperforms the neural beamformer, MCT, dual-path RNN with transform-average-concatenate and multi-channel deep clustering based end-to-end systems by 9.2%, 14.3%, 24.9%, and 52.2% respectively, in terms of relative word error rate reduction.",
      "url": "https://arxiv.org/abs/2601.02688",
      "pdfUrl": "https://arxiv.org/pdf/2601.02688.pdf",
      "titleJa": "音声認識用マルチチャンネルマルチスピーカートランス"
    },
    {
      "id": "2601.02455",
      "arxivId": "2601.02455",
      "title": "Dynamic Quantization Error Propagation in Encoder-Decoder ASR Quantization",
      "authors": [
        "Xinyu Wang",
        "Yajie Luo",
        "Yihong Wu",
        "Liheng Ma",
        "Ziyu Zhao",
        "Jingrui Tian",
        "Lei Ding",
        "Yufei Cui",
        "Xiao-Wen Chang"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Running Automatic Speech Recognition (ASR) models on memory-constrained edge devices requires efficient compression. While layer-wise post-training quantization is effective, it suffers from error accumulation, especially in encoder-decoder architectures. Existing solutions like Quantization Error Propagation (QEP) are suboptimal for ASR due to the model's heterogeneity, processing acoustic features in the encoder while generating text in the decoder. To address this, we propose Fine-grained Alpha for Dynamic Quantization Error Propagation (FADE), which adaptively controls the trade-off between cross-layer error correction and local quantization. Experiments show that FADE significantly improves stability by reducing performance variance across runs, while simultaneously surpassing baselines in mean WER.",
      "url": "https://arxiv.org/abs/2601.02455",
      "pdfUrl": "https://arxiv.org/pdf/2601.02455.pdf",
      "titleJa": "エンコーダ・デコーダASR量子化における動的量子化誤差の伝播"
    },
    {
      "id": "2601.02444",
      "arxivId": "2601.02444",
      "title": "VocalBridge: Latent Diffusion-Bridge Purification for Defeating Perturbation-Based Voiceprint Defenses",
      "authors": [
        "Maryam Abbasihafshejani",
        "AHM Nazmus Sakib",
        "Murtuza Jadliwala"
      ],
      "publishedDate": "2026-01-05",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "The rapid advancement of speech synthesis technologies, including text-to-speech (TTS) and voice conversion (VC), has intensified security and privacy concerns related to voice cloning. Recent defenses attempt to prevent unauthorized cloning by embedding protective perturbations into speech to obscure speaker identity while maintaining intelligibility. However, adversaries can apply advanced purification techniques to remove these perturbations, recover authentic acoustic characteristics, and regenerate cloneable voices. Despite the growing realism of such attacks, the robustness of existing defenses under adaptive purification remains insufficiently studied. Most existing purification methods are designed to counter adversarial noise in automatic speech recognition (ASR) systems rather than speaker verification or voice cloning pipelines. As a result, they fail to suppress the fine-grained acoustic cues that define speaker identity and are often ineffective against speaker verification attacks (SVA). To address these limitations, we propose Diffusion-Bridge (VocalBridge), a purification framework that learns a latent mapping from perturbed to clean speech in the EnCodec latent space. Using a time-conditioned 1D U-Net with a cosine noise schedule, the model enables efficient, transcript-free purification while preserving speaker-discriminative structure. We further introduce a Whisper-guided phoneme variant that incorporates lightweight temporal guidance without requiring ground-truth transcripts. Experimental results show that our approach consistently outperforms existing purification methods in recovering cloneable voices from protected speech. Our findings demonstrate the fragility of current perturbation-based defenses and highlight the need for more robust protection mechanisms against evolving voice-cloning and speaker verification threats.",
      "url": "https://arxiv.org/abs/2601.02444",
      "pdfUrl": "https://arxiv.org/pdf/2601.02444.pdf",
      "titleJa": "VocalBridge: 摂動ベースの声紋防御を破るための潜在的拡散ブリッジ浄化"
    }
  ],
  "lastUpdated": "2026-01-16T00:55:00.127312",
  "totalCount": 72
}