{
  "papers": [
    {
      "id": "2601.15240",
      "arxivId": "2601.15240",
      "title": "WeDefense: A Toolkit to Defend Against Fake Audio",
      "authors": [
        "Lin Zhang",
        "Johan Rohdin",
        "Xin Wang",
        "Junyi Peng",
        "Tianchi Liu",
        "You Zhang",
        "Hieu-Thi Luong",
        "Shuai Wang",
        "Chengdong Liang",
        "Anna Silnova",
        "Nicholas Evans"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The advances in generative AI have enabled the creation of synthetic audio which is perceptually indistinguishable from real, genuine audio. Although this stellar progress enables many positive applications, it also raises risks of misuse, such as for impersonation, disinformation and fraud. Despite a growing number of open-source fake audio detection codes released through numerous challenges and initiatives, most are tailored to specific competitions, datasets or models. A standardized and unified toolkit that supports the fair benchmarking and comparison of competing solutions with not just common databases, protocols, metrics, but also a shared codebase, is missing. To address this, we propose WeDefense, the first open-source toolkit to support both fake audio detection and localization. Beyond model training, WeDefense emphasizes critical yet often overlooked components: flexible input and augmentation, calibration, score fusion, standardized evaluation metrics, and analysis tools for deeper understanding and interpretation. The toolkit is publicly available at https://github.com/zlin0/wedefense with interactive demos for fake audio detection and localization.",
      "url": "https://arxiv.org/abs/2601.15240",
      "pdfUrl": "https://arxiv.org/pdf/2601.15240.pdf",
      "titleJa": "WeDefense: 偽の音声から身を守るツールキット"
    },
    {
      "id": "2601.15118",
      "arxivId": "2601.15118",
      "title": "WavLink: Compact Audio--Text Embeddings with a Global Whisper Token",
      "authors": [
        "Gokul Karthik Kumar",
        "Ludovick Lepauloux",
        "Hakim Hacid"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "Whisper has become the de-facto encoder for extracting general-purpose audio features in large audio-language models, where a 30-second clip is typically represented by 1500 frame features projected into an LLM. In contrast, audio-text embedding models like CLAP-based models have largely relied on alternative audio encoders (e.g., HTS-AT, PaSST), and have not leveraged Whisper effectively. We present WavLink, a compact audio-text embedding model that augments Whisper encoder with a learnable global token, trained jointly with a text encoder. Through a systematic study of design choices, including pretrained text encoders, loss functions, training modes, and data mixtures, we identify configurations that yield state-of-the-art retrieval performance. Our two-stage training recipe across three model sizes, combined with Matryoshka-style supervision, improves scalability, enabling 8x smaller embeddings with minimal performance drop. WavLink also demonstrates competitive performance on AIR-Bench with MCQs and zero-shot classification.",
      "url": "https://arxiv.org/abs/2601.15118",
      "pdfUrl": "https://arxiv.org/pdf/2601.15118.pdf",
      "titleJa": "WavLink: コンパクトオーディオ - グローバルウィスパートークンによるテキスト埋め込み"
    },
    {
      "id": "2601.15097",
      "arxivId": "2601.15097",
      "title": "Neural Tracking of Sustained Attention, Attention Switching, and Natural Conversation in Audiovisual Environments using Mobile EEG",
      "authors": [
        "Johanna Wilroth",
        "Oskar Keding",
        "Martin A. Skoglund",
        "Maria Sandsten",
        "Martin Enqvist",
        "Emina Alickovic"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Everyday communication is dynamic and multisensory, often involving shifting attention, overlapping speech and visual cues. Yet, most neural attention tracking studies are still limited to highly controlled lab settings, using clean, often audio-only stimuli and requiring sustained attention to a single talker. This work addresses that gap by introducing a novel dataset from 24 normal-hearing participants. We used a mobile electroencephalography (EEG) system (44 scalp electrodes and 20 cEEGrid electrodes) in an audiovisual (AV) paradigm with three conditions: sustained attention to a single talker in a two-talker environment, attention switching between two talkers, and unscripted two-talker conversations with a competing single talker. Analysis included temporal response functions (TRFs) modeling, optimal lag analysis, selective attention classification with decision windows ranging from 1.1s to 35s, and comparisons of TRFs for attention to AV conversations versus side audio-only talkers. Key findings show significant differences in the attention-related P2-peak between attended and ignored speech across conditions for scalp EEG. No significant change in performance between switching and sustained attention suggests robustness for attention switches. Optimal lag analysis revealed narrower peak for conversation compared to single-talker AV stimuli, reflecting the additional complexity of multi-talker processing. Classification of selective attention was consistently above chance (55-70% accuracy) for scalp EEG, while cEEGrid data yielded lower correlations, highlighting the need for further methodological improvements. These results demonstrate that mobile EEG can reliably track selective attention in dynamic, multisensory listening scenarios and provide guidance for designing future AV paradigms and real-world attention tracking applications.",
      "url": "https://arxiv.org/abs/2601.15097",
      "pdfUrl": "https://arxiv.org/pdf/2601.15097.pdf",
      "titleJa": "モバイルEEGを用いた視聴覚環境における持続的注意、注意の切り替え、自然な会話の神経追跡"
    },
    {
      "id": "2601.15083",
      "arxivId": "2601.15083",
      "title": "Bangla Music Genre Classification Using Bidirectional LSTMS",
      "authors": [
        "Muntakimur Rahaman",
        "Md Mahmudul Hoque",
        "Md Mehedi Hassain"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Bangla music is enrich in its own music cultures. Now a days music genre classification is very significant because of the exponential increase in available music, both in digital and physical formats. It is necessary to index them accordingly to facilitate improved retrieval. Automatically classifying Bangla music by genre is essential for efficiently locating specific pieces within a vast and diverse music library. Prevailing methods for genre classification predominantly employ conventional machine learning or deep learning approaches. This work introduces a novel music dataset comprising ten distinct genres of Bangla music. For the task of audio classification, we utilize a recurrent neural network (RNN) architecture. Specifically, a Long Short-Term Memory (LSTM) network is implemented to train the model and perform the classification. Feature extraction represents a foundational stage in audio data processing. This study utilizes Mel-Frequency Cepstral Coefficients (MFCCs) to transform raw audio waveforms into a compact and representative set of features. The proposed framework facilitates music genre classification by leveraging these extracted features. Experimental results demonstrate a classification accuracy of 78%, indicating the system's strong potential to enhance and streamline the organization of Bangla music genres.",
      "url": "https://arxiv.org/abs/2601.15083",
      "pdfUrl": "https://arxiv.org/pdf/2601.15083.pdf",
      "titleJa": "双方向LSTMSを用いたバングラ音楽のジャンル分類"
    },
    {
      "id": "2601.14960",
      "arxivId": "2601.14960",
      "title": "VCNAC: A Variable-Channel Neural Audio Codec for Mono, Stereo, and Surround Sound",
      "authors": [
        "Florian Grötschla",
        "Arunasish Sen",
        "Alessandro Lombardi",
        "Guillermo Cámbara",
        "Andreas Schwarz"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We present VCNAC, a variable channel neural audio codec. Our approach features a single encoder and decoder parametrization that enables native inference for different channel setups, from mono speech to cinematic 5.1 channel surround audio. Channel compatibility objectives ensure that multi-channel content maintains perceptual quality when decoded to fewer channels. The shared representation enables training of generative language models on a single set of codebooks while supporting inference-time scalability across modalities and channel configurations. Evaluation using objective spatial audio metrics and subjective listening tests demonstrates that our unified approach maintains high reconstruction quality across mono, stereo, and surround audio configurations.",
      "url": "https://arxiv.org/abs/2601.14960",
      "pdfUrl": "https://arxiv.org/pdf/2601.14960.pdf",
      "titleJa": "VCNAC: モノラル、ステレオ、サラウンドサウンド用の可変チャネルニューラルオーディオコーデック"
    },
    {
      "id": "2601.14931",
      "arxivId": "2601.14931",
      "title": "Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali",
      "authors": [
        "Nouhoum Coulibaly",
        "Ousmane Ly",
        "Michael Leventhal",
        "Ousmane Goro"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "This study explores the capacity of generative artificial intelligence (Gen AI) to contribute to the construction of peace narratives and the revitalization of musical heritage in Mali. The study has been made in a political and social context where inter-community tensions and social fractures motivate a search for new symbolic frameworks for reconciliation. The study empirically explores three questions: (1) how Gen AI can be used as a tool for musical creation rooted in national languages and traditions; (2) to what extent Gen AI systems enable a balanced hybridization between technological innovation and cultural authenticity; and (3) how AI-assisted musical co-creation can strengthen social cohesion and cultural sovereignty. The experimental results suggest that Gen AI, embedded in a culturally conscious participatory framework, can act as a catalyst for symbolic diplomacy, amplifying local voices instead of standardizing them. However, challenges persist regarding the availability of linguistic corpora, algorithmic censorship, and the ethics of generating compositions derived from copyrighted sources.",
      "url": "https://arxiv.org/abs/2601.14931",
      "pdfUrl": "https://arxiv.org/pdf/2601.14931.pdf",
      "titleJa": "生成型人工知能、音楽遺産、そして平和物語の構築：マリにおける事例研究"
    },
    {
      "id": "2601.14850",
      "arxivId": "2601.14850",
      "title": "Multi-Tast Transformer for Explainable Speech Deepfake Detection via Formant Modeling",
      "authors": [
        "Viola Negroni",
        "Luca Cuccovillo",
        "Paolo Bestagini",
        "Patrick Aichroth",
        "Stefano Tubaro"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In this work, we introduce a multi-task transformer for speech deepfake detection, capable of predicting formant trajectories and voicing patterns over time, ultimately classifying speech as real or fake, and highlighting whether its decisions rely more on voiced or unvoiced regions. Building on a prior speaker-formant transformer architecture, we streamline the model with an improved input segmentation strategy, redesign the decoding process, and integrate built-in explainability. Compared to the baseline, our model requires fewer parameters, trains faster, and provides better interpretability, without sacrificing prediction performance.",
      "url": "https://arxiv.org/abs/2601.14850",
      "pdfUrl": "https://arxiv.org/pdf/2601.14850.pdf",
      "titleJa": "フォルマントモデリングによる説明可能な音声ディープフェイク検出のためのマルチテイストトランスフォーマー"
    },
    {
      "id": "2601.14786",
      "arxivId": "2601.14786",
      "title": "Training-Efficient Text-to-Music Generation with State-Space Modeling",
      "authors": [
        "Wei-Jaw Lee",
        "Fang-Chih Hsieh",
        "Xuanjun Chen",
        "Fang-Duo Tsai",
        "Yi-Hsuan Yang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Recent advances in text-to-music generation (TTM) have yielded high-quality results, but often at the cost of extensive compute and the use of large proprietary internal data. To improve the affordability and openness of TTM training, an open-source generative model backbone that is more training- and data-efficient is needed. In this paper, we constrain the number of trainable parameters in the generative model to match that of the MusicGen-small benchmark (with about 300M parameters), and replace its Transformer backbone with the emerging class of state-space models (SSMs). Specifically, we explore different SSM variants for sequence modeling, and compare a single-stage SSM-based design with a decomposable two-stage SSM/diffusion hybrid design. All proposed models are trained from scratch on a purely public dataset comprising 457 hours of CC-licensed music, ensuring full openness. Our experimental findings are three-fold. First, we show that SSMs exhibit superior training efficiency compared to the Transformer counterpart. Second, despite using only 9% of the FLOPs and 2% of the training data size compared to the MusicGen-small benchmark, our model achieves competitive performance in both objective metrics and subjective listening tests based on MusicCaps captions. Finally, our scaling-down experiment demonstrates that SSMs can maintain competitive performance relative to the Transformer baseline even at the same training budget (measured in iterations), when the model size is reduced to four times smaller. To facilitate the democratization of TTM research, the processed captions, model checkpoints, and source code are available on GitHub via the project page: https://lonian6.github.io/ssmttm/.",
      "url": "https://arxiv.org/abs/2601.14786",
      "pdfUrl": "https://arxiv.org/pdf/2601.14786.pdf",
      "titleJa": "状態空間モデリングによる効率的なテキストから音楽への生成"
    },
    {
      "id": "2601.14744",
      "arxivId": "2601.14744",
      "title": "Unlocking Large Audio-Language Models for Interactive Language Learning",
      "authors": [
        "Hongfu Liu",
        "Zhouying Cui",
        "Xiangming Gu",
        "Ye Wang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Achieving pronunciation proficiency in a second language (L2) remains a challenge, despite the development of Computer-Assisted Pronunciation Training (CAPT) systems. Traditional CAPT systems often provide unintuitive feedback that lacks actionable guidance, limiting its effectiveness. Recent advancements in audio-language models (ALMs) offer the potential to enhance these systems by providing more user-friendly feedback. In this work, we investigate ALMs for chat-based pronunciation training by introducing L2-Arctic-plus, an English dataset with detailed error explanations and actionable suggestions for improvement. We benchmark cascaded ASR+LLMs and existing ALMs on this dataset, specifically in detecting mispronunciation and generating actionable feedback. To improve the performance, we further propose to instruction-tune ALMs on L2-Arctic-plus. Experimental results demonstrate that our instruction-tuned models significantly outperform existing baselines on mispronunciation detection and suggestion generation in terms of both objective and human evaluation, highlighting the value of the proposed dataset.",
      "url": "https://arxiv.org/abs/2601.14744",
      "pdfUrl": "https://arxiv.org/pdf/2601.14744.pdf",
      "titleJa": "インタラクティブな言語学習のための大規模音声言語モデルの解放"
    },
    {
      "id": "2601.14728",
      "arxivId": "2601.14728",
      "title": "AQAScore: Evaluating Semantic Alignment in Text-to-Audio Generation via Audio Question Answering",
      "authors": [
        "Chun-Yi Kuan",
        "Kai-Wei Chang",
        "Hung-yi Lee"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Although text-to-audio generation has made remarkable progress in realism and diversity, the development of evaluation metrics has not kept pace. Widely-adopted approaches, typically based on embedding similarity like CLAPScore, effectively measure general relevance but remain limited in fine-grained semantic alignment and compositional reasoning. To address this, we introduce AQAScore, a backbone-agnostic evaluation framework that leverages the reasoning capabilities of audio-aware large language models (ALLMs). AQAScore reformulates assessment as a probabilistic semantic verification task; rather than relying on open-ended text generation, it estimates alignment by computing the exact log-probability of a \"Yes\" answer to targeted semantic queries. We evaluate AQAScore across multiple benchmarks, including human-rated relevance, pairwise comparison, and compositional reasoning tasks. Experimental results show that AQAScore consistently achieves higher correlation with human judgments than similarity-based metrics and generative prompting baselines, showing its effectiveness in capturing subtle semantic inconsistencies and scaling with the capability of underlying ALLMs.",
      "url": "https://arxiv.org/abs/2601.14728",
      "pdfUrl": "https://arxiv.org/pdf/2601.14728.pdf",
      "titleJa": "AQAScore: 音声質問応答によるテキスト音声生成における意味的整合の評価"
    },
    {
      "id": "2601.14684",
      "arxivId": "2601.14684",
      "title": "Dissecting Performance Degradation in Audio Source Separation under Sampling Frequency Mismatch",
      "authors": [
        "Kanami Imamura",
        "Tomohiko Nakamura",
        "Kohei Yatabe",
        "Hiroshi Saruwatari"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio processing methods based on deep neural networks are typically trained at a single sampling frequency (SF). To handle untrained SFs, signal resampling is commonly employed, but it can degrade performance, particularly when the input SF is lower than the trained SF. This paper investigates the causes of this degradation through two hypotheses: (i) the lack of high-frequency components introduced by up-sampling, and (ii) the greater importance of their presence than their precise representation. To examine these hypotheses, we compare conventional resampling with three alternatives: post-resampling noise addition, which adds Gaussian noise to the resampled signal; noisy-kernel resampling, which perturbs the kernel with Gaussian noise to enrich high-frequency components; and trainable-kernel resampling, which adapts the interpolation kernel through training. Experiments on music source separation show that noisy-kernel and trainable-kernel resampling alleviate the degradation observed with conventional resampling. We further demonstrate that noisy-kernel resampling is effective across diverse models, highlighting it as a simple yet practical option.",
      "url": "https://arxiv.org/abs/2601.14684",
      "pdfUrl": "https://arxiv.org/pdf/2601.14684.pdf",
      "titleJa": "サンプリング周波数の不一致による音源分離の性能劣化の分析"
    },
    {
      "id": "2601.14651",
      "arxivId": "2601.14651",
      "title": "READ-Net: Clarifying Emotional Ambiguity via Adaptive Feature Recalibration for Audio-Visual Depression Detection",
      "authors": [
        "Chenglizhao Chen",
        "Boze Li",
        "Mengke Song",
        "Dehao Feng",
        "Xinyu Liu",
        "Shanchen Pang",
        "Jufeng Yang",
        "Hui Yu"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Depression is a severe global mental health issue that impairs daily functioning and overall quality of life. Although recent audio-visual approaches have improved automatic depression detection, methods that ignore emotional cues often fail to capture subtle depressive signals hidden within emotional expressions. Conversely, those incorporating emotions frequently confuse transient emotional expressions with stable depressive symptoms in feature representations, a phenomenon termed \\emph{Emotional Ambiguity}, thereby leading to detection errors. To address this critical issue, we propose READ-Net, the first audio-visual depression detection framework explicitly designed to resolve Emotional Ambiguity through Adaptive Feature Recalibration (AFR). The core insight of AFR is to dynamically adjust the weights of emotional features to enhance depression-related signals. Rather than merely overlooking or naively combining emotional information, READ-Net innovatively identifies and preserves depressive-relevant cues within emotional features, while adaptively filtering out irrelevant emotional noise. This recalibration strategy significantly clarifies feature representations, and effectively mitigates the persistent challenge of emotional interference. Additionally, READ-Net can be easily integrated into existing frameworks for improved performance. Extensive evaluations on three publicly available datasets show that READ-Net outperforms state-of-the-art methods, with average gains of 4.55\\% in accuracy and 1.26\\% in F1-score, demonstrating its robustness to emotional disturbances and improving audio-visual depression detection.",
      "url": "https://arxiv.org/abs/2601.14651",
      "pdfUrl": "https://arxiv.org/pdf/2601.14651.pdf",
      "titleJa": "READ-Net: 適応的特徴再調整による感情的曖昧性の明確化による視聴覚うつ病検出"
    },
    {
      "id": "2601.14620",
      "arxivId": "2601.14620",
      "title": "Scaling Ambiguity: Augmenting Human Annotation in Speech Emotion Recognition with Audio-Language Models",
      "authors": [
        "Wenda Zhang",
        "Hongyu Jin",
        "Siyi Wang",
        "Zhiqiang Wei",
        "Ting Dang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Speech Emotion Recognition models typically use single categorical labels, overlooking the inherent ambiguity of human emotions. Ambiguous Emotion Recognition addresses this by representing emotions as probability distributions, but progress is limited by unreliable ground-truth distributions inferred from sparse human annotations. This paper explores whether Large Audio-Language Models (ALMs) can mitigate the annotation bottleneck by generating high-quality synthetic annotations. We introduce a framework leveraging ALMs to create Synthetic Perceptual Proxies, augmenting human annotations to improve ground-truth distribution reliability. We validate these proxies through statistical analysis of their alignment with human distributions and evaluate their impact by fine-tuning ALMs with the augmented emotion distributions. Furthermore, to address class imbalance and enable unbiased evaluation, we propose DiME-Aug, a Distribution-aware Multimodal Emotion Augmentation strategy. Experiments on IEMOCAP and MSP-Podcast show that synthetic annotations enhance emotion distribution, especially in low-ambiguity regions where annotation agreement is high. However, benefits diminish for highly ambiguous emotions with greater human disagreement. This work provides the first evidence that ALMs could address annotation scarcity in ambiguous emotion recognition, but highlights the need for more advanced prompting or generation strategies to handle highly ambiguous cases.",
      "url": "https://arxiv.org/abs/2601.14620",
      "pdfUrl": "https://arxiv.org/pdf/2601.14620.pdf",
      "titleJa": "曖昧さのスケーリング：音声言語モデルによる音声感情認識における人間の注釈の強化"
    },
    {
      "id": "2601.14516",
      "arxivId": "2601.14516",
      "title": "Towards noise-robust speech inversion through multi-task learning with speech enhancement",
      "authors": [
        "Saba Tabatabaee",
        "Carol Espy-Wilson"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Recent studies demonstrate the effectiveness of Self Supervised Learning (SSL) speech representations for Speech Inversion (SI). However, applying SI in real-world scenarios remains challenging due to the pervasive presence of background noise. We propose a unified framework that integrates Speech Enhancement (SE) and SI models through shared SSL-based speech representations. In this framework, the SSL model is trained not only to support the SE module in suppressing noise but also to produce representations that are more informative for the SI task, allowing both modules to benefit from joint training. At a Signal-to-Noise Ratio of -5 db, our method for the SI task achieves relative improvements over the baseline of 80.95% under babble noise and 38.98% under non-babble noise, as measured by the average Pearson product-moment correlation across all estimated parameters.",
      "url": "https://arxiv.org/abs/2601.14516",
      "pdfUrl": "https://arxiv.org/pdf/2601.14516.pdf",
      "titleJa": "音声強調を用いたマルチタスク学習によるノイズに強い音声反転の実現"
    },
    {
      "id": "2601.14472",
      "arxivId": "2601.14472",
      "title": "Prosody-Guided Harmonic Attention for Phase-Coherent Neural Vocoding in the Complex Spectrum",
      "authors": [
        "Mohammed Salah Al-Radhi",
        "Riad Larbi",
        "Mátyás Bartalis",
        "Géza Németh"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Neural vocoders are central to speech synthesis; despite their success, most still suffer from limited prosody modeling and inaccurate phase reconstruction. We propose a vocoder that introduces prosody-guided harmonic attention to enhance voiced segment encoding and directly predicts complex spectral components for waveform synthesis via inverse STFT. Unlike mel-spectrogram-based approaches, our design jointly models magnitude and phase, ensuring phase coherence and improved pitch fidelity. To further align with perceptual quality, we adopt a multi-objective training strategy that integrates adversarial, spectral, and phase-aware losses. Experiments on benchmark datasets demonstrate consistent gains over HiFi-GAN and AutoVocoder: F0 RMSE reduced by 22 percent, voiced/unvoiced error lowered by 18 percent, and MOS scores improved by 0.15. These results show that prosody-guided attention combined with direct complex spectrum modeling yields more natural, pitch-accurate, and robust synthetic speech, setting a strong foundation for expressive neural vocoding.",
      "url": "https://arxiv.org/abs/2601.14472",
      "pdfUrl": "https://arxiv.org/pdf/2601.14472.pdf",
      "titleJa": "複素スペクトルにおける位相コヒーレントニューラルボコーディングのための韻律誘導ハーモニックアテンション"
    },
    {
      "id": "2601.14227",
      "arxivId": "2601.14227",
      "title": "Transformer Architectures for Respiratory Sound Analysis and Multimodal Diagnosis",
      "authors": [
        "Theodore Aptekarev",
        "Vladimir Sokolovsky",
        "Gregory Furman"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Respiratory sound analysis is a crucial tool for screening asthma and other pulmonary pathologies, yet traditional auscultation remains subjective and experience-dependent. Our prior research established a CNN baseline using DenseNet201, which demonstrated high sensitivity in classifying respiratory sounds. In this work, we (i) adapt the Audio Spectrogram Transformer (AST) for respiratory sound analysis and (ii) evaluate a multimodal Vision-Language Model (VLM) that integrates spectrograms with structured patient metadata. AST is initialized from publicly available weights and fine-tuned on a medical dataset containing hundreds of recordings per diagnosis. The VLM experiment uses a compact Moondream-type model that processes spectrogram images alongside a structured text prompt (sex, age, recording site) to output a JSON-formatted diagnosis. Results indicate that AST achieves approximately 97% accuracy with an F1-score around 97% and ROC AUC of 0.98 for asthma detection, significantly outperforming both the internal CNN baseline and typical external benchmarks. The VLM reaches 86-87% accuracy, performing comparably to the CNN baseline while demonstrating the capability to integrate clinical context into the inference process. These results confirm the effectiveness of self-attention for acoustic screening and highlight the potential of multimodal architectures for holistic diagnostic tools.",
      "url": "https://arxiv.org/abs/2601.14227",
      "pdfUrl": "https://arxiv.org/pdf/2601.14227.pdf",
      "titleJa": "呼吸音解析とマルチモーダル診断のためのトランスフォーマーアーキテクチャ"
    },
    {
      "id": "2601.14356",
      "arxivId": "2601.14356",
      "title": "Single-step Controllable Music Bandwidth Extension With Flow Matching",
      "authors": [
        "Carlos Hernandez-Olivan",
        "Hendrik Vincent Koops",
        "Hao Hao Tan",
        "Elio Quinton"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio restoration consists in inverting degradations of a digital audio signal to recover what would have been the pristine quality signal before the degradation occurred. This is valuable in contexts such as archives of music recordings, particularly those of precious historical value, for which a clean version may have been lost or simply does not exist. Recent work applied generative models to audio restoration, showing promising improvement over previous methods, and opening the door to the ability to perform restoration operations that were not possible before. However, making these models finely controllable remains a challenge. In this paper, we propose an extension of FLowHigh and introduce the Dynamic Spectral Contour (DSC) as a control signal for bandwidth extension via classifier-free guidance. Our experiments show competitive model performance, and indicate that DSC is a promising feature to support fine-grained conditioning.",
      "url": "https://arxiv.org/abs/2601.14356",
      "pdfUrl": "https://arxiv.org/pdf/2601.14356.pdf",
      "titleJa": "フローマッチングによるシングルステップ制御可能な音楽帯域幅拡張"
    },
    {
      "id": "2601.14157",
      "arxivId": "2601.14157",
      "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models",
      "authors": [
        "Bruno Sienkiewicz",
        "Łukasz Neumann",
        "Mateusz Modrzejewski"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.",
      "url": "https://arxiv.org/abs/2601.14157",
      "pdfUrl": "https://arxiv.org/pdf/2601.14157.pdf",
      "titleJa": "ConceptCaps - 音楽モデルの解釈可能性のための蒸留概念データセット"
    },
    {
      "id": "2601.14046",
      "arxivId": "2601.14046",
      "title": "PRiSM: Benchmarking Phone Realization in Speech Models",
      "authors": [
        "Shikhar Bharadwaj",
        "Chin-Jou Li",
        "Yoonjae Kim",
        "Kwanghee Choi",
        "Eunjung Yeo",
        "Ryan Soh-Eun Shim",
        "Hanyu Zhou",
        "Brendon Boldt",
        "Karen Rosero Jacome",
        "Kalvin Chang",
        "Darsh Agrawal",
        "Keer Xu",
        "Chao-Han Huck Yang",
        "Jian Zhu",
        "Shinji Watanabe",
        "David R. Mortensen"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.",
      "url": "https://arxiv.org/abs/2601.14046",
      "pdfUrl": "https://arxiv.org/pdf/2601.14046.pdf",
      "titleJa": "PRiSM: 音声モデルにおける音素実現のベンチマーク"
    },
    {
      "id": "2601.13931",
      "arxivId": "2601.13931",
      "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music",
      "authors": [
        "Yannis Vasilakis",
        "Rachel Bittner",
        "Johan Pauwels"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG"
      ],
      "abstract": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.",
      "url": "https://arxiv.org/abs/2601.13931",
      "pdfUrl": "https://arxiv.org/pdf/2601.13931.pdf",
      "titleJa": "音楽のための音声テキスト統合モデルにおける効果的な否定モデリングに向けて"
    },
    {
      "id": "2601.14925",
      "arxivId": "2601.14925",
      "title": "Fast-ULCNet: A fast and ultra low complexity network for single-channel speech enhancement",
      "authors": [
        "Nicolás Arrieta Larraza",
        "Niels de Koeijer"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "Single-channel speech enhancement algorithms are often used in resource-constrained embedded devices, where low latency and low complexity designs gain more importance. In recent years, researchers have proposed a wide variety of novel solutions to this problem. In particular, a recent deep learning model named ULCNet is among the state-of-the-art approaches in this domain. This paper proposes an adaptation of ULCNet, by replacing its GRU layers with FastGRNNs, to reduce both computational latency and complexity. Furthermore, this paper shows empirical evidence on the performance decay of FastGRNNs in long audio signals during inference due to internal state drifting, and proposes a novel approach based on a trainable complementary filter to mitigate it. The resulting model, Fast-ULCNet, performs on par with the state-of-the-art original ULCNet architecture on a speech enhancement task, while reducing its model size by more than half and decreasing its latency by 34% on average.",
      "url": "https://arxiv.org/abs/2601.14925",
      "pdfUrl": "https://arxiv.org/pdf/2601.14925.pdf",
      "titleJa": "Fast-ULCNet: 単一チャネル音声強調のための高速かつ超低複雑性ネットワーク"
    },
    {
      "id": "2601.14770",
      "arxivId": "2601.14770",
      "title": "Test-Time Adaptation For Speech Enhancement Via Mask Polarization",
      "authors": [
        "Tobias Raichle",
        "Erfan Amini",
        "Bin Yang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Adapting speech enhancement (SE) models to unseen environments is crucial for practical deployments, yet test-time adaptation (TTA) for SE remains largely under-explored due to a lack of understanding of how SE models degrade under domain shifts. We observe that mask-based SE models lose confidence under domain shifts, with predicted masks becoming flattened and losing decisive speech preservation and noise suppression. Based on this insight, we propose mask polarization (MPol), a lightweight TTA method that restores mask bimodality through distribution comparison using the Wasserstein distance. MPol requires no additional parameters beyond the trained model, making it suitable for resource-constrained edge deployments. Experimental results across diverse domain shifts and architectures demonstrate that MPol achieves very consistent gains that are competitive with significantly more complex approaches.",
      "url": "https://arxiv.org/abs/2601.14770",
      "pdfUrl": "https://arxiv.org/pdf/2601.14770.pdf",
      "titleJa": "マスク偏光による音声強調のためのテスト時適応"
    },
    {
      "id": "2601.14751",
      "arxivId": "2601.14751",
      "title": "Inverse-Hessian Regularization for Continual Learning in ASR",
      "authors": [
        "Steven Vander Eeckt",
        "Hugo Van hamme"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Catastrophic forgetting remains a major challenge for continual learning (CL) in automatic speech recognition (ASR), where models must adapt to new domains without losing performance on previously learned conditions. Several CL methods have been proposed for ASR, and, recently, weight averaging - where models are averaged in a merging step after fine-tuning - has proven effective as a simple memory-free strategy. However, it is heuristic in nature and ignores the underlying loss landscapes of the tasks, hindering adaptability. In this work, we propose Inverse Hessian Regularization (IHR), a memory-free approach for CL in ASR that incorporates curvature information into the merging step. After fine-tuning on a new task, the adaptation is adjusted through a Kronecker-factored inverse Hessian approximation of the previous task, ensuring that the model moves primarily in directions less harmful to past performance, while keeping the method lightweight. We evaluate IHR on two CL benchmarks and show that it significantly outperforms state-of-the-art baselines, reducing forgetting while improving adaptability. Ablation studies and analyses further confirm its effectiveness.",
      "url": "https://arxiv.org/abs/2601.14751",
      "pdfUrl": "https://arxiv.org/pdf/2601.14751.pdf",
      "titleJa": "ASRにおける継続学習のための逆ヘッセ行列正則化"
    },
    {
      "id": "2601.14721",
      "arxivId": "2601.14721",
      "title": "NLP-Based Review for Toxic Comment Detection Tailored to the Chinese Cyberspace",
      "authors": [
        "Ruixing Ren",
        "Junhui Zhao",
        "Xiaoke Sun",
        "Qiuping Li"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "With the in-depth integration of mobile Internet and widespread adoption of social platforms, user-generated content in the Chinese cyberspace has witnessed explosive growth. Among this content, the proliferation of toxic comments poses severe challenges to individual mental health, community atmosphere and social trust. Owing to the strong context dependence, cultural specificity and rapid evolution of Chinese cyber language, toxic expressions are often conveyed through complex forms such as homophones and metaphors, imposing notable limitations on traditional detection methods. To address this issue, this review focuses on the core topic of natural language processing based toxic comment detection in the Chinese cyberspace, systematically collating and critically analyzing the research progress and key challenges in this field. This review first defines the connotation and characteristics of Chinese toxic comments, and analyzes the platform ecology and transmission mechanisms they rely on. It then comprehensively reviews the construction methods and limitations of existing public datasets, and proposes a novel fine-grained and scalable framework for toxic comment definition and classification, along with corresponding data annotation and quality assessment strategies. We systematically summarize the evolutionary path of detection models from traditional methods to deep learning, with special emphasis on the importance of interpretability in model design. Finally, we thoroughly discuss the open challenges faced by current research and provide forward-looking suggestions for future research directions.",
      "url": "https://arxiv.org/abs/2601.14721",
      "pdfUrl": "https://arxiv.org/pdf/2601.14721.pdf",
      "titleJa": "中国のサイバースペースに合わせたNLPベースの有害コメント検出レビュー"
    },
    {
      "id": "2601.14699",
      "arxivId": "2601.14699",
      "title": "Triage knowledge distillation for speaker verification",
      "authors": [
        "Ju-ho Kim",
        "Youngmoon Jung",
        "Joon-Young Yang",
        "Jaeyoung Roh",
        "Chang Woo Han",
        "Hoon-Young Cho"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Deploying speaker verification on resource-constrained devices remains challenging due to the computational cost of high-capacity models; knowledge distillation (KD) offers a remedy. Classical KD entangles target confidence with non-target structure in a Kullback-Leibler term, limiting the transfer of relational information. Decoupled KD separates these signals into target and non-target terms, yet treats non-targets uniformly and remains vulnerable to the long tail of low-probability classes in large-class settings. We introduce Triage KD (TRKD), a distillation scheme that operationalizes assess-prioritize-focus. TRKD introduces a cumulative-probability cutoff $τ$ to assess per-example difficulty and partition the teacher posterior into three groups: the target class, a high-probability non-target confusion-set, and a background-set. To prioritize informative signals, TRKD distills the confusion-set conditional distribution and discards the background. Concurrently, it transfers a three-mass (target/confusion/background) that capture sample difficulty and inter-class confusion. Finally, TRKD focuses learning via a curriculum on $τ$: training begins with a larger $τ$ to convey broad non-target context, then $τ$ is progressively decreased to shrink the confusion-set, concentrating supervision on the most confusable classes. In extensive experiments on VoxCeleb1 with both homogeneous and heterogeneous teacher-student pairs, TRKD was consistently superior to recent KD variants and attained the lowest EER across all protocols.",
      "url": "https://arxiv.org/abs/2601.14699",
      "pdfUrl": "https://arxiv.org/pdf/2601.14699.pdf",
      "titleJa": "話者認証のためのトリアージ知識抽出"
    },
    {
      "id": "2601.14012",
      "arxivId": "2601.14012",
      "title": "MATE: Matryoshka Audio-Text Embeddings for Open-Vocabulary Keyword Spotting",
      "authors": [
        "Youngmoon Jung",
        "Myunghun Jung",
        "Joon-Young Yang",
        "Yong-Hyeok Lee",
        "Jaeyoung Roh",
        "Hoon-Young Cho"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "Open-vocabulary keyword spotting (KWS) with text-based enrollment has emerged as a flexible alternative to fixed-phrase triggers. Prior utterance-level matching methods, from an embedding-learning standpoint, learn embeddings at a single fixed dimensionality. We depart from this design and propose Matryoshka Audio-Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings (\"prefixes\"). Specifically, we introduce a PCA-guided prefix alignment: PCA-compressed versions of the full text embedding for each prefix size serve as teacher targets to align both audio and text prefixes. This alignment concentrates salient keyword cues in lower-dimensional prefixes, while higher dimensions add detail. MATE is trained with standard deep metric learning objectives for audio-text KWS, and is loss-agnostic. To our knowledge, this is the first application of matryoshka-style embeddings to KWS, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead.",
      "url": "https://arxiv.org/abs/2601.14012",
      "pdfUrl": "https://arxiv.org/pdf/2601.14012.pdf",
      "titleJa": "MATE: オープン語彙キーワードスポッティングのためのマトリョーシカ音声テキスト埋め込み"
    },
    {
      "id": "2601.13999",
      "arxivId": "2601.13999",
      "title": "DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification",
      "authors": [
        "Youngmoon Jung",
        "Joon-Young Yang",
        "Ju-ho Kim",
        "Jaeyoung Roh",
        "Chang Woo Han",
        "Hoon-Young Cho"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. While existing methods focus on enhancing speaker encoders, the embedding learning strategy still forces a single fixed-dimensional representation reused for utterances of any length, leaving capacity misaligned with the information available at different durations. We propose Duration-Aware Matryoshka Embedding (DAME), a model-agnostic framework that builds a nested hierarchy of sub-embeddings aligned to utterance durations: lower-dimensional representations capture compact speaker traits from short utterances, while higher dimensions encode richer details from longer speech. DAME supports both training from scratch and fine-tuning, and serves as a direct alternative to conventional large-margin fine-tuning, consistently improving performance across durations. On the VoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently reduces the equal error rate on 1-s and other short-duration trials, while maintaining full-length performance with no additional inference cost. These gains generalize across various speaker encoder architectures under both general training and fine-tuning setups.",
      "url": "https://arxiv.org/abs/2601.13999",
      "pdfUrl": "https://arxiv.org/pdf/2601.13999.pdf",
      "titleJa": "DAME: 継続時間を考慮したマトリョーシカ埋め込みによる継続時間ロバストな話者検証"
    },
    {
      "id": "2601.13948",
      "arxivId": "2601.13948",
      "title": "Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models",
      "authors": [
        "Nikita Kuzmin",
        "Songting Liu",
        "Kong Aik Lee",
        "Eng Siong Chng"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.",
      "url": "https://arxiv.org/abs/2601.13948",
      "pdfUrl": "https://arxiv.org/pdf/2601.13948.pdf",
      "titleJa": "Stream-Voice-Anon: ニューラルオーディオコーデックと言語モデルによるリアルタイム話者匿名化の有用性向上"
    },
    {
      "id": "2601.13910",
      "arxivId": "2601.13910",
      "title": "Synthetic Singers: A Review of Deep-Learning-based Singing Voice Synthesis Approaches",
      "authors": [
        "Changhao Pan",
        "Dongyu Yao",
        "Yu Zhang",
        "Wenxiang Guo",
        "Jingyu Lu",
        "Zhiyuan Zhu",
        "Zhou Zhao"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Recent advances in singing voice synthesis (SVS) have attracted substantial attention from both academia and industry. With the advent of large language models and novel generative paradigms, producing controllable, high-fidelity singing voices has become an attainable goal. Yet the field still lacks a comprehensive survey that systematically analyzes deep-learning-based singing voice synthesis systems and their enabling technologies. To address the aforementioned issue, this survey first categorizes existing systems by task type and then organizes current architectures into two major paradigms: cascaded and end-to-end approaches. Moreover, we provide an in-depth analysis of core technologies, covering singing modeling and control techniques. Finally, we review relevant datasets, annotation tools, and evaluation benchmarks that support training and assessment. In appendix, we introduce training strategies and further discussion of SVS. This survey provides an up-to-date review of the literature on SVS models, which would be a useful reference for both researchers and engineers. Related materials are available at https://github.com/David-Pigeon/SyntheticSingers.",
      "url": "https://arxiv.org/abs/2601.13910",
      "pdfUrl": "https://arxiv.org/pdf/2601.13910.pdf",
      "titleJa": "合成歌手：ディープラーニングに基づく歌声合成アプローチのレビュー"
    },
    {
      "id": "2601.13849",
      "arxivId": "2601.13849",
      "title": "Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control",
      "authors": [
        "Ziyi Yang",
        "Li Rao",
        "Zhengding Luo",
        "Dongyuan Shi",
        "Qirui Huang",
        "Woon-Seng Gan"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.LG",
        "eess.SP"
      ],
      "abstract": "Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train.",
      "url": "https://arxiv.org/abs/2601.13849",
      "pdfUrl": "https://arxiv.org/pdf/2601.13849.pdf",
      "titleJa": "能動騒音制御のためのメタ学習による制御フィルタと二次パスの共初期化"
    },
    {
      "id": "2601.13802",
      "arxivId": "2601.13802",
      "title": "Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis",
      "authors": [
        "Yushen Chen",
        "Junzhe Liu",
        "Yujie Tu",
        "Zhikang Niu",
        "Yuzhe Liang",
        "Kai Yu",
        "Chunyu Qiang",
        "Chen Zhang",
        "Xie Chen"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "A notable gap persists in speech synthesis research and development for Arabic dialects, particularly from a unified modeling perspective. Despite its high practical value, the inherent linguistic complexity of Arabic dialects, further compounded by a lack of standardized data, benchmarks, and evaluation guidelines, steers researchers toward safer ground. To bridge this divide, we present Habibi, a suite of specialized and unified text-to-speech models that harnesses existing open-source ASR corpora to support a wide range of high- to low-resource Arabic dialects through linguistically-informed curriculum learning. Our approach outperforms the leading commercial service in generation quality, while maintaining extensibility through effective in-context learning, without requiring text diacritization. We are committed to open-sourcing the model, along with creating the first systematic benchmark for multi-dialect Arabic speech synthesis. Furthermore, by identifying the key challenges in and establishing evaluation standards for the process, we aim to provide a solid groundwork for subsequent research. Resources at https://SWivid.github.io/Habibi/ .",
      "url": "https://arxiv.org/abs/2601.13802",
      "pdfUrl": "https://arxiv.org/pdf/2601.13802.pdf",
      "titleJa": "Habibi: 統一方言アラビア語音声合成のオープンソース基盤の構築"
    },
    {
      "id": "2601.13704",
      "arxivId": "2601.13704",
      "title": "Performance and Complexity Trade-off Optimization of Speech Models During Training",
      "authors": [
        "Esteban Gómez",
        "Tom Bäckström"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "In speech machine learning, neural network models are typically designed by choosing an architecture with fixed layer sizes and structure. These models are then trained to maximize performance on metrics aligned with the task's objective. While the overall architecture is usually guided by prior knowledge of the task, the sizes of individual layers are often chosen heuristically. However, this approach does not guarantee an optimal trade-off between performance and computational complexity; consequently, post hoc methods such as weight quantization or model pruning are typically employed to reduce computational cost. This occurs because stochastic gradient descent (SGD) methods can only optimize differentiable functions, while factors influencing computational complexity, such as layer sizes and floating-point operations per second (FLOP/s), are non-differentiable and require modifying the model structure during training. We propose a reparameterization technique based on feature noise injection that enables joint optimization of performance and computational complexity during training using SGD-based methods. Unlike traditional pruning methods, our approach allows the model size to be dynamically optimized for a target performance-complexity trade-off, without relying on heuristic criteria to select which weights or structures to remove. We demonstrate the effectiveness of our method through three case studies, including a synthetic example and two practical real-world applications: voice activity detection and audio anti-spoofing. The code related to our work is publicly available to encourage further research.",
      "url": "https://arxiv.org/abs/2601.13704",
      "pdfUrl": "https://arxiv.org/pdf/2601.13704.pdf",
      "titleJa": "学習中の音声モデルの性能と複雑さのトレードオフ最適化"
    },
    {
      "id": "2601.13629",
      "arxivId": "2601.13629",
      "title": "S$^2$Voice: Style-Aware Autoregressive Modeling with Enhanced Conditioning for Singing Style Conversion",
      "authors": [
        "Ziqian Wang",
        "Xianjun Xia",
        "Chuanzeng Huang",
        "Lei Xie"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "We present S$^2$Voice, the winning system of the Singing Voice Conversion Challenge (SVCC) 2025 for both the in-domain and zero-shot singing style conversion tracks. Built on the strong two-stage Vevo baseline, S$^2$Voice advances style control and robustness through several contributions. First, we integrate style embeddings into the autoregressive large language model (AR LLM) via a FiLM-style layer-norm conditioning and a style-aware cross-attention for enhanced fine-grained style modeling. Second, we introduce a global speaker embedding into the flow-matching transformer to improve timbre similarity. Third, we curate a large, high-quality singing corpus via an automated pipeline for web harvesting, vocal separation, and transcript refinement. Finally, we employ a multi-stage training strategy combining supervised fine-tuning (SFT) and direct preference optimization (DPO). Subjective listening tests confirm our system's superior performance: leading in style similarity and singer similarity for Task 1, and across naturalness, style similarity, and singer similarity for Task 2. Ablation studies demonstrate the effectiveness of our contributions in enhancing style fidelity, timbre preservation, and generalization. Audio samples are available~\\footnote{https://honee-w.github.io/SVC-Challenge-Demo/}.",
      "url": "https://arxiv.org/abs/2601.13629",
      "pdfUrl": "https://arxiv.org/pdf/2601.13629.pdf",
      "titleJa": "S$^2$Voice: 歌唱スタイル変換のための強化された条件付けによるスタイルを考慮した自己回帰モデリング"
    },
    {
      "id": "2601.15286",
      "arxivId": "2601.15286",
      "title": "Iterative Refinement Improves Compositional Image Generation",
      "authors": [
        "Shantanu Jaiswal",
        "Mihir Prabhudesai",
        "Nikash Bhardwaj",
        "Zheyang Qin",
        "Amir Zadeh",
        "Chuan Li",
        "Katerina Fragkiadaki",
        "Deepak Pathak"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "abstract": "Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/",
      "url": "https://arxiv.org/abs/2601.15286",
      "pdfUrl": "https://arxiv.org/pdf/2601.15286.pdf",
      "titleJa": "反復的な改良により構成画像生成が向上"
    },
    {
      "id": "2601.15282",
      "arxivId": "2601.15282",
      "title": "Rethinking Video Generation Model for the Embodied World",
      "authors": [
        "Yufan Deng",
        "Zilin Pan",
        "Hongyu Zhang",
        "Xiaojie Li",
        "Ruoqing Hu",
        "Yufei Ding",
        "Yiming Zou",
        "Yan Zeng",
        "Daquan Zhou"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "abstract": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
      "url": "https://arxiv.org/abs/2601.15282",
      "pdfUrl": "https://arxiv.org/pdf/2601.15282.pdf",
      "titleJa": "身体化された世界のためのビデオ生成モデルの再考"
    },
    {
      "id": "2601.15279",
      "arxivId": "2601.15279",
      "title": "MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs",
      "authors": [
        "Christoph Bartmann",
        "Johannes Schimunek",
        "Mykyta Ielanskyi",
        "Philipp Seidl",
        "Günter Klambauer",
        "Sohvi Luukkonen"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "A molecule's properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. MolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and reveals capability patterns that localize model failures to specific tasks and molecular structures. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure.",
      "url": "https://arxiv.org/abs/2601.15279",
      "pdfUrl": "https://arxiv.org/pdf/2601.15279.pdf",
      "titleJa": "MolecularIQ: 分子グラフの記号検証による化学推論能力の特性評価"
    },
    {
      "id": "2601.15267",
      "arxivId": "2601.15267",
      "title": "Evaluation of Large Language Models in Legal Applications: Challenges, Methods, and Future Directions",
      "authors": [
        "Yiran Hu",
        "Huanghai Liu",
        "Chong Wang",
        "Kunran Li",
        "Tien-Hsuan Wu",
        "Haitao Li",
        "Xinran Xu",
        "Siqing Huo",
        "Weihang Su",
        "Ning Zheng",
        "Siyuan Zheng",
        "Qingyao Ai",
        "Yun Liu",
        "Renjun Bian",
        "Yiqun Liu",
        "Charles L. A. Clarke",
        "Weixing Shen",
        "Ben Kao"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Large language models (LLMs) are being increasingly integrated into legal applications, including judicial decision support, legal practice assistance, and public-facing legal services. While LLMs show strong potential in handling legal knowledge and tasks, their deployment in real-world legal settings raises critical concerns beyond surface-level accuracy, involving the soundness of legal reasoning processes and trustworthy issues such as fairness and reliability. Systematic evaluation of LLM performance in legal tasks has therefore become essential for their responsible adoption. This survey identifies key challenges in evaluating LLMs for legal tasks grounded in real-world legal practice. We analyze the major difficulties involved in assessing LLM performance in the legal domain, including outcome correctness, reasoning reliability, and trustworthiness. Building on these challenges, we review and categorize existing evaluation methods and benchmarks according to their task design, datasets, and evaluation metrics. We further discuss the extent to which current approaches address these challenges, highlight their limitations, and outline future research directions toward more realistic, reliable, and legally grounded evaluation frameworks for LLMs in legal domains.",
      "url": "https://arxiv.org/abs/2601.15267",
      "pdfUrl": "https://arxiv.org/pdf/2601.15267.pdf",
      "titleJa": "法務アプリケーションにおける大規模言語モデルの評価：課題、方法、そして将来の方向性"
    },
    {
      "id": "2601.15254",
      "arxivId": "2601.15254",
      "title": "Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?",
      "authors": [
        "Felix Schur",
        "Niklas Pfister",
        "Peng Ding",
        "Sach Mukherjee",
        "Jonas Peters"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\\ell_1$-regularized estimation and post-selection refitting.",
      "url": "https://arxiv.org/abs/2601.15254",
      "pdfUrl": "https://arxiv.org/pdf/2601.15254.pdf",
      "titleJa": "多くの実験、少ない繰り返し、対になっていないデータ、スパースな効果: 因果推論は可能か?"
    },
    {
      "id": "2601.15249",
      "arxivId": "2601.15249",
      "title": "Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism",
      "authors": [
        "Garrett G. Wen",
        "Buxin Su",
        "Natalie Collina",
        "Zhun Deng",
        "Weijie Su"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "stat.ME"
      ],
      "abstract": "Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors' assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions' ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.",
      "url": "https://arxiv.org/abs/2601.15249",
      "pdfUrl": "https://arxiv.org/pdf/2601.15249.pdf",
      "titleJa": "アイソトニックメカニズムによる ML/AI カンファレンスのベストペーパー賞の推薦"
    },
    {
      "id": "2601.15241",
      "arxivId": "2601.15241",
      "title": "Feasibility Preservation under Monotone Retrieval Truncation",
      "authors": [
        "Sean Plummer"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "abstract": "Retrieval-based systems approximate access to a corpus by exposing only a truncated subset of available evidence. Even when relevant information exists in the corpus, truncation can prevent compatible evidence from co-occurring, leading to failures that are not captured by relevance-based evaluation. This paper studies retrieval from a structural perspective, modeling query answering as a feasibility problem under truncation. We formalize retrieval as a sequence of candidate evidence sets and characterize conditions under which feasibility in the limit implies feasibility at finite retrieval depth. We show that monotone truncation suffices to guarantee finite witnessability for individual queries. For classes of queries, we identify finite generation of witness certificates as the additional condition required to obtain a uniform retrieval bound, and we show that this condition is necessary. We further exhibit sharp counterexamples demonstrating failure under non-monotone truncation, non-finitely-generated query classes, and purely slotwise coverage. Together, these results isolate feasibility preservation as a correctness criterion for retrieval independent of relevance scoring or optimization, and clarify structural limitations inherent to truncation-based retrieval.",
      "url": "https://arxiv.org/abs/2601.15241",
      "pdfUrl": "https://arxiv.org/pdf/2601.15241.pdf",
      "titleJa": "単調検索打ち切りにおける実現可能性の保持"
    },
    {
      "id": "2601.15235",
      "arxivId": "2601.15235",
      "title": "Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to Cervical Spine Fracture Identification",
      "authors": [
        "Fabi Nahian Madhurja",
        "Rusab Sarmun",
        "Muhammad E. H. Chowdhury",
        "Adam Mushtak",
        "Israa Al-Hashimi",
        "Sohaib Bassam Zoghoul"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of 94.45 percent. This projection-based localization strategy reduces computational complexity compared to traditional 3D segmentation methods while maintaining high performance. It is followed by a DenseNet121-Unet-based multi-label segmentation leveraging variance- and energy-based projections, achieving a Dice score of 87.86 percent. Strategic approximation of 3D vertebral masks from these 2D segmentation masks enables the extraction of individual vertebra volumes. The volumes are analyzed for fractures using an ensemble of 2.5D Spatio-Sequential models incorporating both raw slices and projections per vertebra for complementary evaluation. This ensemble achieves vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. We further validate our approach through an explainability study that provides saliency map visualizations highlighting anatomical regions relevant for diagnosis, and an interobserver variability analysis comparing our model's performance with expert radiologists, demonstrating competitive results.",
      "url": "https://arxiv.org/abs/2601.15235",
      "pdfUrl": "https://arxiv.org/pdf/2601.15235.pdf",
      "titleJa": "2Dストロークで3D解剖学をトレース：頸椎骨折同定のための多段階投影駆動型アプローチ"
    },
    {
      "id": "2601.15209",
      "arxivId": "2601.15209",
      "title": "Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface",
      "authors": [
        "Paige S. DeVries",
        "Michaela Okosi",
        "Ming Li",
        "Nora Dunphy. Gidey Gezae",
        "Dante Conway",
        "Abraham Glasser",
        "Raja Kushalnagar",
        "Christian Vogler"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "abstract": "We investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken English; with Alexa's automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against that of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through an LLM-powered \"task prompter,\" which integrated the user's history and smart environment to suggest contextually-appropriate commands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch. Qualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust deaf-accented speech recognized natively by IPAs.",
      "url": "https://arxiv.org/abs/2601.15209",
      "pdfUrl": "https://arxiv.org/pdf/2601.15209.pdf",
      "titleJa": "聴覚障害者および難聴者のためのインテリジェントパーソナルアシスタントへのアクセス：音声ベースのオプションとLLM搭載タッチインターフェースの比較"
    },
    {
      "id": "2601.15197",
      "arxivId": "2601.15197",
      "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
      "authors": [
        "Shijie Lian",
        "Bin Yu",
        "Xiaopeng Lin",
        "Laurence T. Yang",
        "Zhaolong Shen",
        "Changti Wu",
        "Yuzhuo Miao",
        "Cong Huang",
        "Kai Chen"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.RO"
      ],
      "abstract": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \\mid v)$ and a language-conditioned posterior $π(a \\mid v, \\ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.",
      "url": "https://arxiv.org/abs/2601.15197",
      "pdfUrl": "https://arxiv.org/pdf/2601.15197.pdf",
      "titleJa": "BayesianVLA: 潜在行動クエリによる視覚言語行動モデルのベイズ分解"
    },
    {
      "id": "2601.15195",
      "arxivId": "2601.15195",
      "title": "Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub",
      "authors": [
        "Ramtin Ehsani",
        "Sakshi Pathak",
        "Shriya Rawal",
        "Abdullah Al Mujahid",
        "Mia Mohammad Imran",
        "Preetha Chatterjee"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "abstract": "AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project's CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.",
      "url": "https://arxiv.org/abs/2601.15195",
      "pdfUrl": "https://arxiv.org/pdf/2601.15195.pdf",
      "titleJa": "AIコーディングエージェントはどこで失敗するのか？GitHubにおけるエージェントプルリクエストの失敗に関する実証的研究"
    },
    {
      "id": "2601.15188",
      "arxivId": "2601.15188",
      "title": "Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback",
      "authors": [
        "Stephan Wallraven",
        "Tim Köhne",
        "Hartmut Westenberger",
        "Andreas Moser"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "abstract": "This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.",
      "url": "https://arxiv.org/abs/2601.15188",
      "pdfUrl": "https://arxiv.org/pdf/2601.15188.pdf",
      "titleJa": "ABAPコード生成のための大規模言語モデルのベンチマーク：コンパイラフィードバックによる反復的改善に関する実証的研究"
    },
    {
      "id": "2601.15177",
      "arxivId": "2601.15177",
      "title": "Dynamic Management of a Deep Learning-Based Anomaly Detection System for 5G Networks",
      "authors": [
        "Lorenzo Fernández Maimó",
        "Alberto Huertas Celdrán",
        "Manuel Gil Pérez",
        "Félix J. García Clemente",
        "Gregorio Martínez Pérez"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "abstract": "Fog and mobile edge computing (MEC) will play a key role in the upcoming fifth generation (5G) mobile networks to support decentralized applications, data analytics and management into the network itself by using a highly distributed compute model. Furthermore, increasing attention is paid to providing user-centric cybersecurity solutions, which particularly require collecting, processing and analyzing significantly large amount of data traffic and huge number of network connections in 5G networks. In this regard, this paper proposes a MEC-oriented solution in 5G mobile networks to detect network anomalies in real-time and in autonomic way. Our proposal uses deep learning techniques to analyze network flows and to detect network anomalies. Moreover, it uses policies in order to provide an efficient and dynamic management system of the computing resources used in the anomaly detection process. The paper presents relevant aspects of the deployment of the proposal and experimental results to show its performance.",
      "url": "https://arxiv.org/abs/2601.15177",
      "pdfUrl": "https://arxiv.org/pdf/2601.15177.pdf",
      "titleJa": "5Gネットワーク向けディープラーニングベースの異常検知システムの動的管理"
    },
    {
      "id": "2601.15165",
      "arxivId": "2601.15165",
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "authors": [
        "Zanlin Ni",
        "Shenzhi Wang",
        "Yang Yue",
        "Tianyu Yu",
        "Weilin Zhao",
        "Yeguo Hua",
        "Tianyi Chen",
        "Jun Song",
        "Cheng Yu",
        "Bo Zheng",
        "Gao Huang"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap",
      "url": "https://arxiv.org/abs/2601.15165",
      "pdfUrl": "https://arxiv.org/pdf/2601.15165.pdf",
      "titleJa": "柔軟性の罠：拡散言語モデルにおける任意の順序が推論の可能性を制限する理由"
    },
    {
      "id": "2601.15164",
      "arxivId": "2601.15164",
      "title": "V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks",
      "authors": [
        "Yaru Liu",
        "Ao-bo Wang",
        "Nanyang Ye"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "abstract": "Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently \"succeed\" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., \"get ready for work\") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out \"silent failures\" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.",
      "url": "https://arxiv.org/abs/2601.15164",
      "pdfUrl": "https://arxiv.org/pdf/2601.15164.pdf",
      "titleJa": "V-CAGE: スケーラブルな長期的身体化タスクのためのコンテキスト認識型生成と検証"
    },
    {
      "id": "2601.15161",
      "arxivId": "2601.15161",
      "title": "Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems",
      "authors": [
        "Yinzhu Chen",
        "Abdine Maiga",
        "Hossein A. Rahmani",
        "Emine Yilmaz"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta ($μ_Δ = 8.658$) and an AUROC of 0.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0% to 68.2%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.",
      "url": "https://arxiv.org/abs/2601.15161",
      "pdfUrl": "https://arxiv.org/pdf/2601.15161.pdf",
      "titleJa": "医療対話システムの信頼性の高い評価のための自動ルーブリック"
    },
    {
      "id": "2601.15160",
      "arxivId": "2601.15160",
      "title": "Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning",
      "authors": [
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a \"compositional bridge\", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.",
      "url": "https://arxiv.org/abs/2601.15160",
      "pdfUrl": "https://arxiv.org/pdf/2601.15160.pdf",
      "titleJa": "知識グラフは暗黙の報酬モデルである：パスから得られるシグナルは構成的推論を可能にする"
    },
    {
      "id": "2601.15158",
      "arxivId": "2601.15158",
      "title": "Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data",
      "authors": [
        "Yuval Ran-Milo",
        "Yotam Alexander",
        "Shahar Mendel",
        "Nadav Cohen"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of \"simple examples\": instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.",
      "url": "https://arxiv.org/abs/2601.15158",
      "pdfUrl": "https://arxiv.org/pdf/2601.15158.pdf",
      "titleJa": "結果ベースの強化学習は、適切なデータがある場合に限り、トランスフォーマーを推論へと導くことが証明されている"
    },
    {
      "id": "2601.15153",
      "arxivId": "2601.15153",
      "title": "How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework",
      "authors": [
        "Choro Ulan uulu",
        "Mikhail Kulyabin",
        "Iris Fuhrmann",
        "Jan Joosten",
        "Nuno Miguel Martins Pacheco",
        "Filippos Petridis",
        "Rebecca Johnson",
        "Jan Bosch",
        "Helena Holmström Olsson"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline's poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.",
      "url": "https://arxiv.org/abs/2601.15153",
      "pdfUrl": "https://arxiv.org/pdf/2601.15153.pdf",
      "titleJa": "人間の専門分野の知識を体系的に体系化したLLMでAIエージェントを構築する方法とは？ソフトウェアエンジニアリングフレームワーク"
    },
    {
      "id": "2601.15131",
      "arxivId": "2601.15131",
      "title": "Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning with Improved Network Embedding",
      "authors": [
        "Ayan Maity",
        "Sudeshna Sarkar"
      ],
      "publishedDate": "2026-01-21",
      "categories": [
        "cs.AI"
      ],
      "abstract": "In this paper, we study the vehicle routing problem with a finite time horizon. In this routing problem, the objective is to maximize the number of customer requests served within a finite time horizon. We present a novel routing network embedding module which creates local node embedding vectors and a context-aware global graph representation. The proposed Markov decision process for the vehicle routing problem incorporates the node features, the network adjacency matrix and the edge features as components of the state space. We incorporate the remaining finite time horizon into the network embedding module to provide a proper routing context to the embedding module. We integrate our embedding module with a policy gradient-based deep Reinforcement Learning framework to solve the vehicle routing problem with finite time horizon. We trained and validated our proposed routing method on real-world routing networks, as well as synthetically generated Euclidean networks. Our experimental results show that our method achieves a higher customer service rate than the existing routing methods. Additionally, the solution time of our method is significantly lower than that of the existing methods.",
      "url": "https://arxiv.org/abs/2601.15131",
      "pdfUrl": "https://arxiv.org/pdf/2601.15131.pdf",
      "titleJa": "改良ネットワーク埋め込みによる深層強化学習を用いた有限時間ホライズンでの車両ルーティング"
    },
    {
      "id": "2601.13647",
      "arxivId": "2601.13647",
      "title": "Fusion Segment Transformer: Bi-Directional Attention Guided Fusion Network for AI-Generated Music Detection",
      "authors": [
        "Yumin Kim",
        "Seonghyeon Go"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "With the rise of generative AI technology, anyone can now easily create and deploy AI-generated music, which has heightened the need for technical solutions to address copyright and ownership issues. While existing works mainly focused on short-audio, the challenge of full-audio detection, which requires modeling long-term structure and context, remains insufficiently explored. To address this, we propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. As in our previous work, we extract content embeddings from short music segments using diverse feature extractors. Furthermore, we enhance the architecture for full-audio AI-generated music detection by introducing a Gated Fusion Layer that effectively integrates content and structural information, enabling the capture of long-term context. Experiments on the SONICS and AIME datasets show that our approach outperforms the previous model and recent baselines, achieving state-of-the-art results in AI-generated music detection.",
      "url": "https://arxiv.org/abs/2601.13647",
      "pdfUrl": "https://arxiv.org/pdf/2601.13647.pdf",
      "titleJa": "Fusion Segment Transformer: AI生成音楽検出のための双方向アテンションガイド融合ネットワーク"
    },
    {
      "id": "2601.12961",
      "arxivId": "2601.12961",
      "title": "Supervised Learning for Game Music Segmentation",
      "authors": [
        "Shangxuan Luo",
        "Joshua Reiss"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD"
      ],
      "abstract": "At present, neural network-based models, including transformers, struggle to generate memorable and readily comprehensible music from unified and repetitive musical material due to a lack of understanding of musical structure. Consequently, these models are rarely employed by the games industry. It is hypothesised by many scholars that the modelling of musical structure may inform models at a higher level, thereby enhancing the quality of music generation. The aim of this study is to explore the performance of supervised learning methods in the task of structural segmentation, which is the initial step in music structure modelling. An audio game music dataset with 309 structural annotations was created to train the proposed method, which combines convolutional neural networks and recurrent neural networks, achieving performance comparable to the state-of-the-art unsupervised learning methods with fewer training resources.",
      "url": "https://arxiv.org/abs/2601.12961",
      "pdfUrl": "https://arxiv.org/pdf/2601.12961.pdf",
      "titleJa": "ゲーム音楽セグメンテーションのための教師あり学習"
    },
    {
      "id": "2601.12802",
      "arxivId": "2601.12802",
      "title": "UNMIXX: Untangling Highly Correlated Singing Voices Mixtures",
      "authors": [
        "Jihoo Jung",
        "Ji-Hoon Kim",
        "Doyeop Kwak",
        "Junwon Lee",
        "Juhan Nam",
        "Joon Son Chung"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "We introduce UNMIXX, a novel framework for multiple singing voices separation (MSVS). While related to speech separation, MSVS faces unique challenges: data scarcity and the highly correlated nature of singing voices mixture. To address these issues, we propose UNMIXX with three key components: (1) musically informed mixing strategy to construct highly correlated, music-like mixtures, (2) cross-source attention that drives representations of two singers apart via reverse attention, and (3) magnitude penalty loss penalizing erroneously assigned interfering energy. UNMIXX not only addresses data scarcity by simulating realistic training data, but also excels at separating highly correlated mixtures through cross-source interactions at both the architectural and loss levels. Our extensive experiments demonstrate that UNMIXX greatly enhances performance, with SDRi gains exceeding 2.2 dB over prior work.",
      "url": "https://arxiv.org/abs/2601.12802",
      "pdfUrl": "https://arxiv.org/pdf/2601.12802.pdf",
      "titleJa": "UNMIXX: 相関性の高い歌声ミックスを解きほぐす"
    },
    {
      "id": "2601.12314",
      "arxivId": "2601.12314",
      "title": "A Similarity Network for Correlating Musical Structure to Military Strategy",
      "authors": [
        "Yiwen Zhang",
        "Hui Zhang",
        "Fanqin Meng"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Music perception, a multi-sensory process based on the synesthesia effect, is an essential component of music aesthetic education. Understanding music structure helps both perception and aesthetic education. Music structure incorporates a range of information, the coordination of which forms the melody, just as different military actions cooperate to produce a military strategy. However, there are a few ways for assessing music perception from the perspectives of system operation and information management. In this paper, we explore the similarities between music structure and military strategy while creating the Music Clips Correlation Network (MCCN) based on Mel-frequency Cepstral Coefficients (MFCCs). The inspiration comes from the comparison between a concert conductor's musical score and a military war commander's sand table exercise. Specifically, we create MCCNs for various kinds of war movie soundtracks, then relate military tactics (Sun Tzu's Art of War, etc.) and political institutions to military operations networks. Our primary findings suggest a few similarities, implying that music perception and aesthetic education can be approached from a military strategy and management perspective through this interdisciplinary research. Similarly, we can discover similarities between the art of military scheming and the art of musical structure based on network analysis in order to facilitate the understanding of the relationship between technology and art.",
      "url": "https://arxiv.org/abs/2601.12314",
      "pdfUrl": "https://arxiv.org/pdf/2601.12314.pdf",
      "titleJa": "音楽構造と軍事戦略を相関させる類似性ネットワーク"
    },
    {
      "id": "2601.12245",
      "arxivId": "2601.12245",
      "title": "Sound2Hap: Learning Audio-to-Vibrotactile Haptic Generation from Human Ratings",
      "authors": [
        "Yinan Li",
        "Hasti Seifi"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Environmental sounds like footsteps, keyboard typing, or dog barking carry rich information and emotional context, making them valuable for designing haptics in user applications. Existing audio-to-vibration methods, however, rely on signal-processing rules tuned for music or games and often fail to generalize across diverse sounds. To address this, we first investigated user perception of four existing audio-to-haptic algorithms, then created a data-driven model for environmental sounds. In Study 1, 34 participants rated vibrations generated by the four algorithms for 1,000 sounds, revealing no consistent algorithm preferences. Using this dataset, we trained Sound2Hap, a CNN-based autoencoder, to generate perceptually meaningful vibrations from diverse sounds with low latency. In Study 2, 15 participants rated its output higher than signal-processing baselines on both audio-vibration match and Haptic Experience Index (HXI), finding it more harmonious with diverse sounds. This work demonstrates a perceptually validated approach to audio-haptic translation, broadening the reach of sound-driven haptics.",
      "url": "https://arxiv.org/abs/2601.12245",
      "pdfUrl": "https://arxiv.org/pdf/2601.12245.pdf",
      "titleJa": "Sound2Hap: 人間の評価から音声から振動触覚への触覚生成を学習する"
    },
    {
      "id": "2601.12222",
      "arxivId": "2601.12222",
      "title": "Song Aesthetics Evaluation with Multi-Stem Attention and Hierarchical Uncertainty Modeling",
      "authors": [
        "Yishan Lv",
        "Jing Luo",
        "Boyuan Ju",
        "Yang Zhang",
        "Xinda Wu",
        "Bo Yuan",
        "Xinyu Yang"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "abstract": "Music generative artificial intelligence (AI) is rapidly expanding music content, necessitating automated song aesthetics evaluation. However, existing studies largely focus on speech, audio or singing quality, leaving song aesthetics underexplored. Moreover, conventional approaches often predict a precise Mean Opinion Score (MOS) value directly, which struggles to capture the nuances of human perception in song aesthetics evaluation. This paper proposes a song-oriented aesthetics evaluation framework, featuring two novel modules: 1) Multi-Stem Attention Fusion (MSAF) builds bidirectional cross-attention between mixture-vocal and mixture-accompaniment pairs, fusing them to capture complex musical features; 2) Hierarchical Granularity-Aware Interval Aggregation (HiGIA) learns multi-granularity score probability distributions, aggregates them into a score interval, and applies a regression within the interval to produce the final score. We evaluated on two datasets of full-length songs: SongEval dataset (AI-generated) and an internal aesthetics dataset (human-created), and compared with two state-of-the-art (SOTA) models. Results show that the proposed method achieves stronger performance for multi-dimensional song aesthetics evaluation.",
      "url": "https://arxiv.org/abs/2601.12222",
      "pdfUrl": "https://arxiv.org/pdf/2601.12222.pdf",
      "titleJa": "マルチステムアテンションと階層的不確実性モデリングによる歌の美的評価"
    },
    {
      "id": "2601.12205",
      "arxivId": "2601.12205",
      "title": "Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks",
      "authors": [
        "Shih-Heng Wang",
        "Jiatong Shi",
        "Jinchuan Tian",
        "Haibin Wu",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "abstract": "This paper investigates three crucial yet underexplored aspects of the generalization capabilities of neural audio codecs (NACs): (i) whether NACs can generalize to unseen languages during pre-training, (ii) whether speech-only pre-trained NACs can effectively generalize to non-speech applications such as environmental sounds, music, and animal vocalizations, and (iii) whether incorporating non-speech data during pre-training can improve performance on both speech and non-speech tasks. Existing studies typically rely on off-the-shelf NACs for comparison, which limits insight due to variations in implementation. In this work, we train NACs from scratch using strictly controlled configurations and carefully curated pre-training data to enable fair comparisons. We conduct a comprehensive evaluation of NAC performance on both signal reconstruction quality and downstream applications using 11 metrics. Our results show that NACs can generalize to unseen languages during pre-training, speech-only pre-trained NACs exhibit degraded performance on non-speech tasks, and incorporating non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks.",
      "url": "https://arxiv.org/abs/2601.12205",
      "pdfUrl": "https://arxiv.org/pdf/2601.12205.pdf",
      "titleJa": "ニューラルコーデックは一般化するか？未知の言語と非音声タスクを対象とした対照研究"
    },
    {
      "id": "2601.12180",
      "arxivId": "2601.12180",
      "title": "VidTune: Creating Video Soundtracks with Generative Music and Contextual Thumbnails",
      "authors": [
        "Mina Huh",
        "Ailie C. Fraser",
        "Dingzeyu Li",
        "Mira Dontcheva",
        "Bryan Wang"
      ],
      "publishedDate": "2026-01-17",
      "categories": [
        "cs.HC",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Music shapes the tone of videos, yet creators often struggle to find soundtracks that match their video's mood and narrative. Recent text-to-music models let creators generate music from text prompts, but our formative study (N=8) shows creators struggle to construct diverse prompts, quickly review and compare tracks, and understand their impact on the video. We present VidTune, a system that supports soundtrack creation by generating diverse music options from a creator's prompt and producing contextual thumbnails for rapid review. VidTune extracts representative video subjects to ground thumbnails in context, maps each track's valence and energy onto visual cues like color and brightness, and depicts prominent genres and instruments. Creators can refine tracks through natural language edits, which VidTune expands into new generations. In a controlled user study (N=12) and an exploratory case study (N=6), participants found VidTune helpful for efficiently reviewing and comparing music options and described the process as playful and enriching.",
      "url": "https://arxiv.org/abs/2601.12180",
      "pdfUrl": "https://arxiv.org/pdf/2601.12180.pdf",
      "titleJa": "VidTune: ジェネレーティブミュージックとコンテキストサムネイルを使ったビデオサウンドトラックの作成"
    },
    {
      "id": "2601.11968",
      "arxivId": "2601.11968",
      "title": "MuseAgent-1: Interactive Grounded Multimodal Understanding of Music Scores and Performance Audio",
      "authors": [
        "Qihao Zhao",
        "Yunqi Cao",
        "Yangyu Huang",
        "Hui Yi Leong",
        "Fan Zhang",
        "Kim-Hui Yap",
        "Wei Hu"
      ],
      "publishedDate": "2026-01-17",
      "categories": [
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Despite recent advances in multimodal large language models (MLLMs), their ability to understand and interact with music remains limited. Music understanding requires grounded reasoning over symbolic scores and expressive performance audio, which general-purpose MLLMs often fail to handle due to insufficient perceptual grounding. We introduce MuseAgent, a music-centric multimodal agent that augments language models with structured symbolic representations derived from sheet music images and performance audio. By integrating optical music recognition and automatic music transcription modules, MuseAgent enables multi-step reasoning and interaction over fine-grained musical content. To systematically evaluate music understanding capabilities, we further propose MuseBench, a benchmark covering music theory reasoning, score interpretation, and performance-level analysis across text, image, and audio modalities. Experiments show that existing MLLMs perform poorly on these tasks, while MuseAgent achieves substantial improvements, highlighting the importance of structured multimodal grounding for interactive music understanding.",
      "url": "https://arxiv.org/abs/2601.11968",
      "pdfUrl": "https://arxiv.org/pdf/2601.11968.pdf",
      "titleJa": "MuseAgent-1: 楽譜と演奏音声のインタラクティブなマルチモーダル理解"
    },
    {
      "id": "2601.11768",
      "arxivId": "2601.11768",
      "title": "Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music",
      "authors": [
        "Venkat Suprabath Bitra",
        "Homayoon Beigi"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "abstract": "Reliable fundamental frequency (F 0) and voicing estimation is essential for neural synthesis, yet many pitch extractors depend on large labeled corpora and degrade under realistic recording artifacts. We propose a lightweight, fully self-supervised framework for joint F 0 estimation and voicing inference, designed for rapid single-instrument training from limited audio. Using transposition-equivariant learning on CQT features, we introduce an EM-style iterative reweighting scheme that uses Shift Cross-Entropy (SCE) consistency as a reliability signal to suppress uninformative noisy/unvoiced frames. The resulting weights provide confidence scores that enable pseudo-labeling for a separate lightweight voicing classifier without manual annotations. Trained on MedleyDB and evaluated on MDB-stem-synth ground truth, our method achieves competitive cross-corpus performance (RPA 95.84, RCA 96.24) and demonstrates cross-instrument generalization.",
      "url": "https://arxiv.org/abs/2601.11768",
      "pdfUrl": "https://arxiv.org/pdf/2601.11768.pdf",
      "titleJa": "モノフォニック音楽における基本周波数と正確な有声音確率の軽量自己教師検出"
    },
    {
      "id": "2601.11262",
      "arxivId": "2601.11262",
      "title": "Scalable Music Cover Retrieval Using Lyrics-Aligned Audio Embeddings",
      "authors": [
        "Joanne Affolter",
        "Benjamin Martin",
        "Elena V. Epure",
        "Gabriel Meseguer-Brocal",
        "Frédéric Kaplan"
      ],
      "publishedDate": "2026-01-16",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG"
      ],
      "abstract": "Music Cover Retrieval, also known as Version Identification, aims to recognize distinct renditions of the same underlying musical work, a task central to catalog management, copyright enforcement, and music retrieval. State-of-the-art approaches have largely focused on harmonic and melodic features, employing increasingly complex audio pipelines designed to be invariant to musical attributes that often vary widely across covers. While effective, these methods demand substantial training time and computational resources. By contrast, lyrics constitute a strong invariant across covers, though their use has been limited by the difficulty of extracting them accurately and efficiently from polyphonic audio. Early methods relied on simple frameworks that limited downstream performance, while more recent systems deliver stronger results but require large models integrated within complex multimodal architectures. We introduce LIVI (Lyrics-Informed Version Identification), an approach that seeks to balance retrieval accuracy with computational efficiency. First, LIVI leverages supervision from state-of-the-art transcription and text embedding models during training to achieve retrieval accuracy on par with--or superior to--harmonic-based systems. Second, LIVI remains lightweight and efficient by removing the transcription step at inference, challenging the dominance of complexity-heavy pipelines.",
      "url": "https://arxiv.org/abs/2601.11262",
      "pdfUrl": "https://arxiv.org/pdf/2601.11262.pdf",
      "titleJa": "歌詞に合わせたオーディオ埋め込みを用いたスケーラブルな音楽カバー検索"
    },
    {
      "id": "2601.10547",
      "arxivId": "2601.10547",
      "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
      "authors": [
        "Dongchao Yang",
        "Yuxin Xie",
        "Yuguo Yin",
        "Zheyu Wang",
        "Xiaoyu Yi",
        "Gongxi Zhu",
        "Xiaolong Weng",
        "Zihan Xiong",
        "Yingzhe Ma",
        "Dading Cong",
        "Jingliang Liu",
        "Zihang Huang",
        "Jinghan Ru",
        "Rongjie Huang",
        "Haoran Wan",
        "Peixu Wang",
        "Kuoxi Yu",
        "Helin Wang",
        "Liming Liang",
        "Xianwei Zhuang",
        "Yuanyuan Wang",
        "Haohan Guo",
        "Junjie Cao",
        "Zeqian Ju",
        "Songxiang Liu",
        "Yuewen Cao",
        "Heming Weng",
        "Yuexian Zou"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD"
      ],
      "abstract": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio-text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "url": "https://arxiv.org/abs/2601.10547",
      "pdfUrl": "https://arxiv.org/pdf/2601.10547.pdf",
      "titleJa": "HeartMuLa: オープンソースの音楽基盤モデルファミリー"
    },
    {
      "id": "2601.09603",
      "arxivId": "2601.09603",
      "title": "Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer",
      "authors": [
        "Petros Vavaroutsos",
        "Theodoros Palamas",
        "Pantelis Vikatos"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "abstract": "In recent years, foundation models have become very popular due to their exceptional performance, mainly in natural language (NLP) tasks where they were first introduced. These models usually consist of hundreds of millions, or even billions, of parameters, making them resource-intensive during training and in production systems, leading to increased costs. This paper focuses on the reduction of a foundation's model size when applied to music information retrieval (MIR) tasks. Our research combines the Branchformer architecture with SummaryMixing, which were first applied in speech recognition, along with a random quantization process. To facilitate reproducibility, we conduct pre-training on publicly available datasets, complemented by a proprietary dataset comparable in scale to other private datasets reported in the literature. We ensure robust evaluation by using a framework consisting of a variety of downstream MIR tasks. Our results show that our architecture achieves competitive performance when compared with other state-of-the-art models that use multi-head self-attention, while reducing the model size from 8.5% up to 12.3%.",
      "url": "https://arxiv.org/abs/2601.09603",
      "pdfUrl": "https://arxiv.org/pdf/2601.09603.pdf",
      "titleJa": "ランダム量子化器を用いた音楽理解のための線形複雑度自己教師学習"
    },
    {
      "id": "2601.13847",
      "arxivId": "2601.13847",
      "title": "Emotion and Acoustics Should Agree: Cross-Level Inconsistency Analysis for Audio Deepfake Detection",
      "authors": [
        "Jinhua Zhang",
        "Zhenqi Jia",
        "Rui Liu"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio Deepfake Detection (ADD) aims to detect spoof speech from bonafide speech. Most prior studies assume that stronger correlations within or across acoustic and emotional features imply authenticity, and thus focus on enhancing or measuring such correlations. However, existing methods often treat acoustic and emotional features in isolation or rely on correlation metrics, which overlook subtle desynchronization between them and smooth out abrupt discontinuities. To address these issues, we propose EAI-ADD, which treats cross level emotion acoustic inconsistency as the primary detection signal. We first project emotional and acoustic representations into a comparable space. Then we progressively integrate frame level and utterance level emotion features with acoustic features to capture cross level emotion acoustic inconsistencies across different temporal granularities. Experimental results on the ASVspoof 2019LA and 2021LA datasets demonstrate that the proposed EAI-ADD outperforms baselines, providing a more effective solution for audio anti spoofing detection.",
      "url": "https://arxiv.org/abs/2601.13847",
      "pdfUrl": "https://arxiv.org/pdf/2601.13847.pdf",
      "titleJa": "感情と音響は一致するはず：オーディオディープフェイク検出のためのクロスレベル不整合分析"
    },
    {
      "id": "2601.13679",
      "arxivId": "2601.13679",
      "title": "Ultra-Lightweight Network for Ship-Radiated Sound Classification on Embedded Deployment",
      "authors": [
        "Sangwon Park",
        "Dongjun Kim",
        "Sung-Hoon Byun",
        "Sangwook Park"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This letter presents ShuffleFAC, a lightweight acoustic model for ship-radiated sound classification in resource-constrained maritime monitoring systems. ShuffleFAC integrates Frequency-Aware convolution into an efficiency-oriented backbone using separable convolution, point-wise group convolution, and channel shuffle, enabling frequency-sensitive feature extraction with low computational cost. Experiments on the DeepShip dataset show that ShuffleFAC achieves competitive performance with substantially reduced complexity. In particular, ShuffleFAC ($γ=16$) attains a macro F1-score of 71.45 $\\pm$ 1.18% using 39K parameters and 3.06M MACs, and achieves an inference latency of 6.05 $\\pm$ 0.95ms on a Raspberry Pi. Compared with MicroNet0, it improves macro F1-score by 1.82 % while reducing model size by 9.7x and latency by 2.5x. These results indicate that ShuffleFAC is suitable for real-time embedded UATR.",
      "url": "https://arxiv.org/abs/2601.13679",
      "pdfUrl": "https://arxiv.org/pdf/2601.13679.pdf",
      "titleJa": "組み込み型船舶放射音分類用超軽量ネットワーク"
    },
    {
      "id": "2601.13589",
      "arxivId": "2601.13589",
      "title": "Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification",
      "authors": [
        "HyeYoung Lee"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.AI",
        "cs.SD"
      ],
      "abstract": "This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.",
      "url": "https://arxiv.org/abs/2601.13589",
      "pdfUrl": "https://arxiv.org/pdf/2601.13589.pdf",
      "titleJa": "リアルタイム安全性検証を備えたマルチエージェントAIシステムによる動作応答コンテンツ生成"
    },
    {
      "id": "2601.13513",
      "arxivId": "2601.13513",
      "title": "Event Classification by Physics-informed Inpainting for Distributed Multichannel Acoustic Sensor with Partially Degraded Channels",
      "authors": [
        "Noriyuki Tonami",
        "Wataru Kohno",
        "Yoshiyuki Yajima",
        "Sakiko Mishima",
        "Yumi Arai",
        "Reishi Kondo",
        "Tomoyuki Hino"
      ],
      "publishedDate": "2026-01-20",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Distributed multichannel acoustic sensing (DMAS) enables large-scale sound event classification (SEC), but performance drops when many channels are degraded and when sensor layouts at test time differ from training layouts. We propose a learning-free, physics-informed inpainting frontend based on reverse time migration (RTM). In this approach, observed multichannel spectrograms are first back-propagated on a 3D grid using an analytic Green's function to form a scene-consistent image, and then forward-projected to reconstruct inpainted signals before log-mel feature extraction and Transformer-based classification. We evaluate the method on ESC-50 with 50 sensors and three layouts (circular, linear, right-angle), where per-channel SNRs are sampled from -30 to 0 dB. Compared with an AST baseline, scaling-sparsemax channel selection, and channel-swap augmentation, the proposed RTM frontend achieves the best or competitive accuracy across all layouts, improving accuracy by 13.1 points on the right-angle layout (from 9.7% to 22.8%). Correlation analyses show that spatial weights align more strongly with SNR than with channel--source distance, and that higher SNR--weight correlation corresponds to higher SEC accuracy. These results demonstrate that a reconstruct-then-project, physics-based preprocessing effectively complements learning-only methods for DMAS under layout-open configurations and severe channel degradation.",
      "url": "https://arxiv.org/abs/2601.13513",
      "pdfUrl": "https://arxiv.org/pdf/2601.13513.pdf",
      "titleJa": "部分的に劣化したチャネルを持つ分散型マルチチャネル音響センサーのための物理学に基づくインペインティングによるイベント分類"
    },
    {
      "id": "2601.12752",
      "arxivId": "2601.12752",
      "title": "SoundPlot: An Open-Source Framework for Birdsong Acoustic Analysis and Neural Synthesis with Interactive 3D Visualization",
      "authors": [
        "Naqcho Ali Mehdi",
        "Mohammad Adeel",
        "Aizaz Ali Larik"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "We present SoundPlot, an open-source framework for analyzing avian vocalizations through acoustic feature extraction, dimensionality reduction, and neural audio synthesis. The system transforms audio signals into a multi-dimensional acoustic feature space, enabling real-time visualization of temporal dynamics in 3D using web-based interactive graphics. Our framework implements a complete analysis-synthesis pipeline that extracts spectral features (centroid, bandwidth, contrast), pitch contours via probabilistic YIN (pYIN), and mel-frequency cepstral coefficients (MFCCs), mapping them to a unified timbre space for visualization. Audio reconstruction employs the Griffin-Lim phase estimation algorithm applied to mel spectrograms. The accompanying Three.js-based interface provides dual-viewport visualization comparing original and synthesized audio trajectories with independent playback controls. We demonstrate the framework's capabilities through comprehensive waveform analysis, spectrogram comparisons, and feature space evaluation using Principal Component Analysis (PCA). Quantitative evaluation shows mel spectrogram correlation scores exceeding 0.92, indicating high-fidelity preservation of perceptual acoustic structure. SoundPlot is released under the MIT License to facilitate research in bioacoustics, audio signal processing, and computational ethology.",
      "url": "https://arxiv.org/abs/2601.12752",
      "pdfUrl": "https://arxiv.org/pdf/2601.12752.pdf",
      "titleJa": "SoundPlot: インタラクティブな3D可視化による鳥のさえずりの音響分析とニューラルネットワーク合成のためのオープンソースフレームワーク"
    },
    {
      "id": "2601.12660",
      "arxivId": "2601.12660",
      "title": "Toward Faithful Explanations in Acoustic Anomaly Detection",
      "authors": [
        "Maab Elrashid",
        "Anthony Deschênes",
        "Cem Subakan",
        "Mirco Ravanelli",
        "Rémi Georges",
        "Michael Morin"
      ],
      "publishedDate": "2026-01-19",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Interpretability is essential for user trust in real-world anomaly detection applications. However, deep learning models, despite their strong performance, often lack transparency. In this work, we study the interpretability of autoencoder-based models for audio anomaly detection, by comparing a standard autoencoder (AE) with a mask autoencoder (MAE) in terms of detection performance and interpretability. We applied several attribution methods, including error maps, saliency maps, SmoothGrad, Integrated Gradients, GradSHAP, and Grad-CAM. Although MAE shows a slightly lower detection, it consistently provides more faithful and temporally precise explanations, suggesting a better alignment with true anomalies. To assess the relevance of the regions highlighted by the explanation method, we propose a perturbation-based faithfulness metric that replaces them with their reconstructions to simulate normal input. Our findings, based on experiments in a real industrial scenario, highlight the importance of incorporating interpretability into anomaly detection pipelines and show that masked training improves explanation quality without compromising performance.",
      "url": "https://arxiv.org/abs/2601.12660",
      "pdfUrl": "https://arxiv.org/pdf/2601.12660.pdf",
      "titleJa": "音響異常検知における忠実な説明に向けて"
    },
    {
      "id": "2601.12600",
      "arxivId": "2601.12600",
      "title": "SSVD-O: Parameter-Efficient Fine-Tuning with Structured SVD for Speech Recognition",
      "authors": [
        "Pu Wang",
        "Shinji Watanabe",
        "Hugo Van hamme"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) is a scalable approach for adapting large speech foundation models to new domains. While methods such as LoRA and its state-of-the-art variants reduce adaptation costs, they typically allocate parameters uniformly across model subspaces, which limits their efficiency and scalability in speech applications. Building on our prior work, this paper introduces SSVD-Outer (SSVD-O), an extension of the structured SVD-guided (SSVD) fine-tuning method. SSVD-O combines input acoustic feature space-associated inner transformations with output semantic feature space-associated outer transformations to enable scalable and balanced adaptation. We conduct the first systematic analysis of parameter budget allocation across model subspaces in PEFT for automatic speech recognition (ASR), and investigate the trade-off between learning and forgetting under constrained resources. SSVD-O is benchmarked against LoRA, DoRA, PiSSA, and SSVD on domain-shifted ASR tasks, including child speech and regional accents, across model scales from 0.1B to 2B within the ESPnet framework. Experimental results show that SSVD-O consistently narrows the performance gap to full fine-tuning while improving generalization and mitigating catastrophic forgetting.",
      "url": "https://arxiv.org/abs/2601.12600",
      "pdfUrl": "https://arxiv.org/pdf/2601.12600.pdf",
      "titleJa": "SSVD-O: 音声認識のための構造化SVDによるパラメータ効率の高い微調整"
    },
    {
      "id": "2601.12494",
      "arxivId": "2601.12494",
      "title": "Harmonizing the Arabic Audio Space with Data Scheduling",
      "authors": [
        "Hunzalah Hassan Bhatti",
        "Firoj Alam",
        "Shammur Absar Chowdhury"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments.",
      "url": "https://arxiv.org/abs/2601.12494",
      "pdfUrl": "https://arxiv.org/pdf/2601.12494.pdf",
      "titleJa": "データスケジューリングによるアラビア語オーディオ空間の調和"
    },
    {
      "id": "2601.12480",
      "arxivId": "2601.12480",
      "title": "A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation",
      "authors": [
        "Hanchen Pei",
        "Shujie Liu",
        "Yanqing Liu",
        "Jianwei Yu",
        "Yuanhang Qian",
        "Gongping Huang",
        "Sheng Zhao",
        "Yan Lu"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Neural codec language models achieve impressive zero-shot Text-to-Speech (TTS) by fully imitating the acoustic characteristics of a short speech prompt, including timbre, prosody, and paralinguistic information. However, such holistic imitation limits their ability to isolate and control individual attributes. In this paper, we present a unified codec language model SpeechEdit that extends zero-shot TTS with a selective control mechanism. By default, SpeechEdit reproduces the complete acoustic profile inferred from the speech prompt, but it selectively overrides only the attributes specified by explicit control instructions. To enable controllable modeling, SpeechEdit is trained on our newly constructed LibriEdit dataset, which provides delta (difference-aware) training pairs derived from LibriHeavy. Experimental results show that our approach maintains naturalness and robustness while offering flexible and localized control over desired attributes. Audio samples are available at https://speech-editing.github.io/speech-editing/.",
      "url": "https://arxiv.org/abs/2601.12480",
      "pdfUrl": "https://arxiv.org/pdf/2601.12480.pdf",
      "titleJa": "選択的に編集可能なテキスト音声生成のための統合ニューラルコーデック言語モデル"
    },
    {
      "id": "2601.12354",
      "arxivId": "2601.12354",
      "title": "Bone-conduction Guided Multimodal Speech Enhancement with Conditional Diffusion Models",
      "authors": [
        "Sina Khanagha",
        "Bunlong Lay",
        "Timo Gerkmann"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Single-channel speech enhancement models face significant performance degradation in extremely noisy environments. While prior work has shown that complementary bone-conducted speech can guide enhancement, effective integration of this noise-immune modality remains a challenge. This paper introduces a novel multimodal speech enhancement framework that integrates bone-conduction sensors with air-conducted microphones using a conditional diffusion model. Our proposed model significantly outperforms previously established multimodal techniques and a powerful diffusion-based single-modal baseline across a wide range of acoustic conditions.",
      "url": "https://arxiv.org/abs/2601.12354",
      "pdfUrl": "https://arxiv.org/pdf/2601.12354.pdf",
      "titleJa": "条件付き拡散モデルを用いた骨伝導誘導マルチモーダル音声強調"
    },
    {
      "id": "2601.12345",
      "arxivId": "2601.12345",
      "title": "Adaptive Rotary Steering with Joint Autoregression for Robust Extraction of Closely Moving Speakers in Dynamic Scenarios",
      "authors": [
        "Jakob Kienegger",
        "Timo Gerkmann"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Latest advances in deep spatial filtering for Ambisonics demonstrate strong performance in stationary multi-speaker scenarios by rotating the sound field toward a target speaker prior to multi-channel enhancement. For applicability in dynamic acoustic conditions with moving speakers, we propose to automate this rotary steering using an interleaved tracking algorithm conditioned on the target's initial direction. However, for nearby or crossing speakers, robust tracking becomes difficult and spatial cues less effective for enhancement. By incorporating the processed recording as additional guide into both algorithms, our novel joint autoregressive framework leverages temporal-spectral correlations of speech to resolve spatially challenging speaker constellations. Consequently, our proposed method significantly improves tracking and enhancement of closely spaced speakers, consistently outperforming comparable non-autoregressive methods on a synthetic dataset. Real-world recordings complement these findings in complex scenarios with multiple speaker crossings and varying speaker-to-array distances.",
      "url": "https://arxiv.org/abs/2601.12345",
      "pdfUrl": "https://arxiv.org/pdf/2601.12345.pdf",
      "titleJa": "動的シナリオにおける近接して移動する話者のロバストな抽出のためのジョイント自己回帰を用いた適応型ロータリーステアリング"
    },
    {
      "id": "2601.12203",
      "arxivId": "2601.12203",
      "title": "Embryonic Exposure to VPA Influences Chick Vocalisations: A Computational Study",
      "authors": [
        "Antonella M. C. Torrisi",
        "Inês Nolasco",
        "Paola Sgadò",
        "Elisabetta Versace",
        "Emmanouil Benetos"
      ],
      "publishedDate": "2026-01-18",
      "categories": [
        "cs.SD"
      ],
      "abstract": "In young animals like poultry chicks (Gallus gallus), vocalisations convey information about affective and behavioural states. Traditional approaches to vocalisation analysis, relying on manual annotation and predefined categories, introduce biases, limit scalability, and fail to capture the full complexity of vocal repertoires. We introduce a computational framework for the automated detection, acoustic feature extraction, and unsupervised learning of chick vocalisations. Applying this framework to a dataset of newly hatched chicks, we identified two primary vocal clusters. We then tested our computational framework on an independent dataset of chicks exposed during embryonic development to vehicle or Valproic Acid (VPA), a compound that disrupts neural development and is linked to autistic-like symptoms. Clustering analysis on the experimental dataset confirmed two primary vocal clusters and revealed systematic differences between groups. VPA-exposed chicks showed an altered repertoire, with a relative increase in softer calls. VPA differentially affected call clusters, modulating temporal, frequency, and energy domain features. Overall, VPA-exposed chicks produced vocalisations with shorter duration, reduced pitch variability, and modified energy profiles, with the strongest alterations observed in louder calls. This study provides a computational framework for analysing animal vocalisations, advancing knowledge of early-life communication in typical and atypical vocal development.",
      "url": "https://arxiv.org/abs/2601.12203",
      "pdfUrl": "https://arxiv.org/pdf/2601.12203.pdf",
      "titleJa": "胎児期のVPA曝露がニワトリの発声に影響を与える：計算論的研究"
    },
    {
      "id": "2601.12153",
      "arxivId": "2601.12153",
      "title": "A Survey on 30+ Years of Automatic Singing Assessment and Singing Information Processing",
      "authors": [
        "Arthur N. dos Santos",
        "Bruno S. Masiero"
      ],
      "publishedDate": "2026-01-17",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Automatic Singing Assessment and Singing Information Processing have evolved over the past three decades to support singing pedagogy, performance analysis, and vocal training. While the first approach objectively evaluates a singer's performance through computational metrics ranging from real-time visual feedback and acoustical biofeedback to sophisticated pitch tracking and spectral analysis, the latter method compares a predictor vocal signal with a target reference to capture nuanced data embedded in the singing voice. Notable advancements include the development of interactive systems that have significantly improved real-time visual feedback, and the integration of machine learning and deep neural network architectures that enhance the precision of vocal signal processing. This survey critically examines the literature to map the historical evolution of these technologies, while identifying and discussing key gaps. The analysis reveals persistent challenges, such as the lack of standardized evaluation frameworks, difficulties in reliably separating vocal signals from various noise sources, and the underutilization of advanced digital signal processing and artificial intelligence methodologies for capturing artistic expressivity. By detailing these limitations and the corresponding technological advances, this review demonstrates how addressing these issues can bridge the gap between objective computational assessments and subjective human-like evaluations of singing performance, ultimately enhancing both the technical accuracy and pedagogical relevance of automated singing evaluation systems.",
      "url": "https://arxiv.org/abs/2601.12153",
      "pdfUrl": "https://arxiv.org/pdf/2601.12153.pdf",
      "titleJa": "30年以上にわたる自動歌唱評価と歌唱情報処理に関する調査"
    },
    {
      "id": "2601.10384",
      "arxivId": "2601.10384",
      "title": "RSA-Bench: Benchmarking Audio Large Models in Real-World Acoustic Scenarios",
      "authors": [
        "Yibo Zhang",
        "Liang Lin",
        "Kaiwen Luo",
        "Shilinlu Yan",
        "Jin Wang",
        "Yaoqi Guo",
        "Yitian Chen",
        "Yalan Qin",
        "Zhenhong Zhou",
        "Kun Wang",
        "Li Sun"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD"
      ],
      "abstract": "While Audio Large Models (ALMs) have achieved remarkable proficiency, their robustness remains brittle in real-world deployment. Existing evaluations largely rely on synthetic Gaussian noise or simplistic single-source interference, failing to capture the intricate, multi-layered acoustic dynamics -- or ``Acoustic Ecology'' -- that characterize authentic physical environments. To bridge this ecological gap, we introduce \\textbf{RSA-Bench}, a comprehensive robustness benchmark designed to stress-test ALLMs through high-fidelity auditory scene simulations. Unlike traditional methods, we construct evaluation samples by naturally superimposing diverse environmental soundscapes -- spanning \\textit{Pasture}, \\textit{Extreme Weather}, \\textit{Classroom}, and \\textit{Outdoors} -- onto clean speech signals across a spectrum of interference intensities. By evaluating models on six core tasks ranging from fundamental perception to complex reasoning, our study unveils three macro-level insights: \\textbf{(I) The Perception-Cognition Gap:} Models maintain relative resilience in low-level recognition but suffer a \\textbf{functional collapse} in high-order reasoning tasks under stress; \\textbf{(II) Scenario Sensitivity:} ``Vocal-like'' interference (e.g., background laughter) proves significantly more destructive than mechanical noise, challenging the model's auditory attention mechanisms; and \\textbf{(III) The Denoising Paradox:} Standard speech enhancement often exacerbates performance degradation, as ALLMs prove highly sensitive to the semantic distortions introduced by denoising artifacts.",
      "url": "https://arxiv.org/abs/2601.10384",
      "pdfUrl": "https://arxiv.org/pdf/2601.10384.pdf",
      "titleJa": "RSA-Bench: 実世界の音響シナリオにおける大規模オーディオモデルのベンチマーク"
    },
    {
      "id": "2601.10345",
      "arxivId": "2601.10345",
      "title": "Self-supervised restoration of singing voice degraded by pitch shifting using shallow diffusion",
      "authors": [
        "Yunyi Liu",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-15",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Pitch shifting has been an essential feature in singing voice production. However, conventional signal processing approaches exhibit well known trade offs such as formant shifts and robotic coloration that becomes more severe at larger transposition jumps. This paper targets high quality pitch shifting for singing by reframing it as a restoration problem: given an audio track that has been pitch shifted (and thus contaminated by artifacts), we recover a natural sounding performance while preserving its melody and timing. Specifically, we use a lightweight, mel space diffusion model driven by frame level acoustic features such as f0, volume, and content features. We construct training pairs in a self supervised manner by applying pitch shifts and reversing them to simulate realistic artifacts while retaining ground truth. On a curated singing set, the proposed approach substantially reduces pitch shift artifacts compared to representative classical baselines, as measured by both statistical metrics and pairwise acoustic measures. The results suggest that restoration based pitch shifting could be a viable approach towards artifact resistant transposition in vocal production workflows.",
      "url": "https://arxiv.org/abs/2601.10345",
      "pdfUrl": "https://arxiv.org/pdf/2601.10345.pdf",
      "titleJa": "浅い拡散を用いたピッチシフトによって劣化した歌声の自己教師あり復元"
    },
    {
      "id": "2601.09931",
      "arxivId": "2601.09931",
      "title": "Diffusion-based Frameworks for Unsupervised Speech Enhancement",
      "authors": [
        "Jean-Eudes Ayilo",
        "Mostafa Sadeghi",
        "Romain Serizel",
        "Xavier Alameda-Pineda"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD"
      ],
      "abstract": "This paper addresses $\\textit{unsupervised}$ diffusion-based single-channel speech enhancement (SE). Prior work in this direction combines a score-based diffusion model trained on clean speech with a Gaussian noise model whose covariance is structured by non-negative matrix factorization (NMF). This combination is used within an iterative expectation-maximization (EM) scheme, in which a diffusion-based posterior-sampling E-step estimates the clean speech. We first revisit this framework and propose to explicitly model both speech and acoustic noise as latent variables, jointly sampling them in the E-step instead of sampling speech alone as in previous approaches. We then introduce a new unsupervised SE framework that replaces the NMF noise prior with a diffusion-based noise model, learned jointly with the speech prior in a single conditional score model. Within this framework, we derive two variants: one that implicitly accounts for noise and one that explicitly treats noise as a latent variable. Experiments on WSJ0-QUT and VoiceBank-DEMAND show that explicit noise modeling systematically improves SE performance for both NMF-based and diffusion-based noise priors. Under matched conditions, the diffusion-based noise model attains the best overall quality and intelligibility among unsupervised methods, while under mismatched conditions the proposed NMF-based explicit-noise framework is more robust and suffers less degradation than several supervised baselines. Our code will be publicly available on this $\\href{https://github.com/jeaneudesAyilo/enudiffuse}{URL}$.",
      "url": "https://arxiv.org/abs/2601.09931",
      "pdfUrl": "https://arxiv.org/pdf/2601.09931.pdf",
      "titleJa": "教師なし音声強調のための拡散ベースフレームワーク"
    },
    {
      "id": "2601.09520",
      "arxivId": "2601.09520",
      "title": "Towards Realistic Synthetic Data for Automatic Drum Transcription",
      "authors": [
        "Pierfrancesco Melucci",
        "Paolo Merialdo",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Deep learning models define the state-of-the-art in Automatic Drum Transcription (ADT), yet their performance is contingent upon large-scale, paired audio-MIDI datasets, which are scarce. Existing workarounds that use synthetic data often introduce a significant domain gap, as they typically rely on low-fidelity SoundFont libraries that lack acoustic diversity. While high-quality one-shot samples offer a better alternative, they are not available in a standardized, large-scale format suitable for training. This paper introduces a new paradigm for ADT that circumvents the need for paired audio-MIDI training data. Our primary contribution is a semi-supervised method to automatically curate a large and diverse corpus of one-shot drum samples from unlabeled audio sources. We then use this corpus to synthesize a high-quality dataset from MIDI files alone, which we use to train a sequence-to-sequence transcription model. We evaluate our model on the ENST and MDB test sets, where it achieves new state-of-the-art results, significantly outperforming both fully supervised methods and previous synthetic-data approaches. The code for reproducing our experiments is publicly available at https://github.com/pier-maker92/ADT_STR",
      "url": "https://arxiv.org/abs/2601.09520",
      "pdfUrl": "https://arxiv.org/pdf/2601.09520.pdf",
      "titleJa": "自動ドラム転写のための現実的な合成データに向けて"
    }
  ],
  "lastUpdated": "2026-01-23T00:55:32.998403",
  "totalCount": 83
}