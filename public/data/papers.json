{
  "papers": [
    {
      "id": "2602.04796",
      "arxivId": "2602.04796",
      "title": "LALM-as-a-Judge: Benchmarking Large Audio-Language Models for Safety Evaluation in Multi-Turn Spoken Dialogues",
      "authors": [
        "Amir Ivry",
        "Shinji Watanabe"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "Spoken dialogues with and between voice agents are becoming increasingly common, yet assessing them for their socially harmful content such as violence, harassment, and hate remains text-centric and fails to account for audio-specific cues and transcription errors. We present LALM-as-a-Judge, the first controlled benchmark and systematic study of large audio-language models (LALMs) as safety judges for multi-turn spoken dialogues. We generate 24,000 unsafe and synthetic spoken dialogues in English that consist of 3-10 turns, by having a single dialogue turn including content with one of 8 harmful categories (e.g., violence) and on one of 5 grades, from very mild to severe. On 160 dialogues, 5 human raters confirmed reliable unsafe detection and a meaningful severity scale. We benchmark three open-source LALMs: Qwen2-Audio, Audio Flamingo 3, and MERaLiON as zero-shot judges that output a scalar safety score in [0,1] across audio-only, transcription-only, or multimodal inputs, along with a transcription-only LLaMA baseline. We measure the judges' sensitivity to detecting unsafe content, the specificity in ordering severity levels, and the stability of the score in dialogue turns. Results reveal architecture- and modality-dependent trade-offs: the most sensitive judge is also the least stable across turns, while stable configurations sacrifice detection of mild harmful content. Transcription quality is a key bottleneck: Whisper-Large may significantly reduce sensitivity for transcription-only modes, while largely preserving severity ordering. Audio becomes crucial when paralinguistic cues or transcription fidelity are category-critical. We summarize all findings and provide actionable guidance for practitioners.",
      "url": "https://arxiv.org/abs/2602.04796",
      "pdfUrl": "https://arxiv.org/pdf/2602.04796.pdf",
      "titleJa": "LALMを審査員として活用：複数ターンの音声対話における安全性評価のための大規模音声言語モデルのベンチマーク"
    },
    {
      "id": "2602.04776",
      "arxivId": "2602.04776",
      "title": "Speaker-Aware Simulation Improves Conversational Speech Recognition",
      "authors": [
        "Máté Gedeon",
        "Péter Mihajlik"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Automatic speech recognition (ASR) for conversational speech remains challenging due to the limited availability of large-scale, well-annotated multi-speaker dialogue data and the complex temporal dynamics of natural interactions. Speaker-aware simulated conversations (SASC) offer an effective data augmentation strategy by transforming single-speaker recordings into realistic multi-speaker dialogues. However, prior work has primarily focused on English data, leaving questions about the applicability to lower-resource languages. In this paper, we adapt and implement the SASC framework for Hungarian conversational ASR. We further propose C-SASC, an extended variant that incorporates pause modeling conditioned on utterance duration, enabling a more faithful representation of local temporal dependencies observed in human conversation while retaining the simplicity and efficiency of the original approach. We generate synthetic Hungarian dialogues from the BEA-Large corpus and combine them with real conversational data for ASR training. Both SASC and C-SASC are evaluated extensively under a wide range of simulation configurations, using conversational statistics derived from CallHome, BEA-Dialogue, and GRASS corpora. Experimental results show that speaker-aware conversational simulation consistently improves recognition performance over naive concatenation-based augmentation. While the additional duration conditioning in C-SASC yields modest but systematic gains--most notably in character-level error rates--its effectiveness depends on the match between source conversational statistics and the target domain. Overall, our findings confirm the robustness of speaker-aware conversational simulation for Hungarian ASR and highlight the benefits and limitations of increasingly detailed temporal modeling in synthetic dialogue generation.",
      "url": "https://arxiv.org/abs/2602.04776",
      "pdfUrl": "https://arxiv.org/pdf/2602.04776.pdf",
      "titleJa": "話者認識シミュレーションが会話音声認識を向上"
    },
    {
      "id": "2602.04702",
      "arxivId": "2602.04702",
      "title": "Fine-Grained Frame Modeling in Multi-head Self-Attention for Speech Deepfake Detection",
      "authors": [
        "Tuan Dat Phuong",
        "Duc-Tuan Truong",
        "Long-Vu Hoang",
        "Trang Nguyen Thi Thu"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Transformer-based models have shown strong performance in speech deepfake detection, largely due to the effectiveness of the multi-head self-attention (MHSA) mechanism. MHSA provides frame-level attention scores, which are particularly valuable because deepfake artifacts often occur in small, localized regions along the temporal dimension of speech. This makes fine-grained frame modeling essential for accurately detecting subtle spoofing cues. In this work, we propose fine-grained frame modeling (FGFM) for MHSA-based speech deepfake detection, where the most informative frames are first selected through a multi-head voting (MHV) module. These selected frames are then refined via a cross-layer refinement (CLR) module to enhance the model's ability to learn subtle spoofing cues. Experimental results demonstrate that our method outperforms the baseline model and achieves Equal Error Rate (EER) of 0.90%, 1.88%, and 6.64% on the LA21, DF21, and ITW datasets, respectively. These consistent improvements across multiple benchmarks highlight the effectiveness of our fine-grained modeling for robust speech deepfake detection.",
      "url": "https://arxiv.org/abs/2602.04702",
      "pdfUrl": "https://arxiv.org/pdf/2602.04702.pdf",
      "titleJa": "音声ディープフェイク検出のためのマルチヘッド自己注意における細粒度フレームモデリング"
    },
    {
      "id": "2602.04683",
      "arxivId": "2602.04683",
      "title": "UniAudio 2.0: A Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization",
      "authors": [
        "Dongchao Yang",
        "Yuanyuan Wang",
        "Dading Chong",
        "Songxiang Liu",
        "Xixin Wu",
        "Helen Meng"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.SD"
      ],
      "abstract": "We study two foundational problems in audio language models: (1) how to design an audio tokenizer that can serve as an intermediate representation for both understanding and generation; and (2) how to build an audio foundation model that generalizes in few-shot and zero-shot settings, analogous to large language models. To this end, we make the following two contributions. First, we propose ReasoningCodec, a discrete audio codec that factorizes audio into (i) reasoning tokens, which encode text-aligned, high-level analysis and planning representations for audio understanding and hierarchical generation, and (ii) reconstruction tokens, which encode semantic-rich acoustic cues for high-fidelity waveform reconstruction. This design achieves understanding performance comparable to strong continuous representations while improving generation quality and reconstruction fidelity over prior discrete tokenizers. Second, we introduce a unified autoregressive architecture for text and audio, together with multi-stage training and multi-task data construction. Using this framework, we train UniAudio 2.0 on 100B text tokens and 60B audio tokens. Across a wide range of speech, sound, and music tasks, UniAudio 2.0 performs competitively on in-domain evaluations and demonstrates strong few-shot and zero-shot generalization to unseen tasks. Demo, code, and checkpoints will be available at \\href{https://dongchaoyang.top/UniAudio2Demo/}{https://dongchaoyang.top/UniAudio2Demo/}.",
      "url": "https://arxiv.org/abs/2602.04683",
      "pdfUrl": "https://arxiv.org/pdf/2602.04683.pdf",
      "titleJa": "UniAudio 2.0: テキスト整合されたファクタライズされたオーディオトークン化を備えた統合オーディオ言語モデル"
    },
    {
      "id": "2602.04680",
      "arxivId": "2602.04680",
      "title": "Audio ControlNet for Fine-Grained Audio Generation and Editing",
      "authors": [
        "Haina Zhu",
        "Yao Xiao",
        "Xiquan Li",
        "Ziyang Ma",
        "Jianwei Yu",
        "Bowen Zhang",
        "Mingqi Yang",
        "Xie Chen"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "abstract": "We study the fine-grained text-to-audio (T2A) generation task. While recent models can synthesize high-quality audio from text descriptions, they often lack precise control over attributes such as loudness, pitch, and sound events. Unlike prior approaches that retrain models for specific control types, we propose to train ControlNet models on top of pre-trained T2A backbones to achieve controllable generation over loudness, pitch, and event roll. We introduce two designs, T2A-ControlNet and T2A-Adapter, and show that the T2A-Adapter model offers a more efficient structure with strong control ability. With only 38M additional parameters, T2A-Adapter achieves state-of-the-art performance on the AudioSet-Strong in both event-level and segment-level F1 scores. We further extend this framework to audio editing, proposing T2A-Editor for removing and inserting audio events at time locations specified by instructions. Models, code, dataset pipelines, and benchmarks will be released to support future research on controllable audio generation and editing.",
      "url": "https://arxiv.org/abs/2602.04680",
      "pdfUrl": "https://arxiv.org/pdf/2602.04680.pdf",
      "titleJa": "きめ細かなオーディオ生成と編集のための Audio ControlNet"
    },
    {
      "id": "2602.04535",
      "arxivId": "2602.04535",
      "title": "HoliAntiSpoof: Audio LLM for Holistic Speech Anti-Spoofing",
      "authors": [
        "Xuenan Xu",
        "Yiming Ren",
        "Liwei Liu",
        "Wen Wu",
        "Baoxiang Li",
        "Chaochao Lu",
        "Shuai Wang",
        "Chao Zhang"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Recent advances in speech synthesis and editing have made speech spoofing increasingly challenging. However, most existing methods treat spoofing as binary classification, overlooking that diverse spoofing techniques manipulate multiple, coupled speech attributes and their semantic effects. In this paper, we introduce HoliAntiSpoof, the first audio large language model (ALLM) framework for holistic speech anti-spoofing analysis. HoliAntiSpoof reformulates spoofing analysis as a unified text generation task, enabling joint reasoning over spoofing methods, affected speech attributes, and their semantic impacts. To support semantic-level analysis, we introduce DailyTalkEdit, a new anti-spoofing benchmark that simulates realistic conversational manipulations and provides annotations of semantic influence. Extensive experiments demonstrate that HoliAntiSpoof outperforms conventional baselines across multiple settings, while preliminary results show that in-context learning further improves out-of-domain generalization. These findings indicate that ALLMs not only enhance speech spoofing detection performance but also enable interpretable analysis of spoofing behaviors and their semantic effects, pointing towards more trustworthy and explainable speech security. Data and code are publicly available.",
      "url": "https://arxiv.org/abs/2602.04535",
      "pdfUrl": "https://arxiv.org/pdf/2602.04535.pdf",
      "titleJa": "HoliAntiSpoof: 総合的な音声なりすまし対策のためのオーディオ LLM"
    },
    {
      "id": "2602.04307",
      "arxivId": "2602.04307",
      "title": "Universal Robust Speech Adaptation for Cross-Domain Speech Recognition and Enhancement",
      "authors": [
        "Chien-Chun Wang",
        "Hung-Shin Lee",
        "Hsin-Min Wang",
        "Berlin Chen"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "Pre-trained models for automatic speech recognition (ASR) and speech enhancement (SE) have exhibited remarkable capabilities under matched noise and channel conditions. However, these models often suffer from severe performance degradation when confronted with domain shifts, particularly in the presence of unseen noise and channel distortions. In view of this, we in this paper present URSA-GAN, a unified and domain-aware generative framework specifically designed to mitigate mismatches in both noise and channel conditions. URSA-GAN leverages a dual-embedding architecture that consists of a noise encoder and a channel encoder, each pre-trained with limited in-domain data to capture domain-relevant representations. These embeddings condition a GAN-based speech generator, facilitating the synthesis of speech that is acoustically aligned with the target domain while preserving phonetic content. To enhance generalization further, we propose dynamic stochastic perturbation, a novel regularization technique that introduces controlled variability into the embeddings during generation, promoting robustness to unseen domains. Empirical results demonstrate that URSA-GAN effectively reduces character error rates in ASR and improves perceptual metrics in SE across diverse noisy and mismatched channel scenarios. Notably, evaluations on compound test conditions with both channel and noise degradations confirm the generalization ability of URSA-GAN, yielding relative improvements of 16.16% in ASR performance and 15.58% in SE metrics.",
      "url": "https://arxiv.org/abs/2602.04307",
      "pdfUrl": "https://arxiv.org/pdf/2602.04307.pdf",
      "titleJa": "クロスドメイン音声認識と拡張のためのユニバーサルロバスト音声適応"
    },
    {
      "id": "2602.04247",
      "arxivId": "2602.04247",
      "title": "DementiaBank-Emotion: A Multi-Rater Emotion Annotation Corpus for Alzheimer's Disease Speech (Version 1.0)",
      "authors": [
        "Cheonkam Jeong",
        "Jessica Liao",
        "Audrey Lu",
        "Yutong Song",
        "Christopher Rashidian",
        "Donna Krogh",
        "Erik Krogh",
        "Mahkameh Rasouli",
        "Jung-Ah Lee",
        "Nikil Dutt",
        "Lisa M Gibbs",
        "David Sultzer",
        "Julie Rousseau",
        "Jocelyn Ludlow",
        "Margaret Galvez",
        "Alexander Nuth",
        "Chet Khay",
        "Sabine Brunswicker",
        "Adeline Nyamathi"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "abstract": "We present DementiaBank-Emotion, the first multi-rater emotion annotation corpus for Alzheimer's disease (AD) speech. Annotating 1,492 utterances from 108 speakers for Ekman's six basic emotions and neutral, we find that AD patients express significantly more non-neutral emotions (16.9%) than healthy controls (5.7%; p < .001). Exploratory acoustic analysis suggests a possible dissociation: control speakers showed substantial F0 modulation for sadness (Delta = -3.45 semitones from baseline), whereas AD speakers showed minimal change (Delta = +0.11 semitones; interaction p = .023), though this finding is based on limited samples (sadness: n=5 control, n=15 AD) and requires replication. Within AD speech, loudness differentiates emotion categories, indicating partially preserved emotion-prosody mappings. We release the corpus, annotation guidelines, and calibration workshop materials to support research on emotion recognition in clinical populations.",
      "url": "https://arxiv.org/abs/2602.04247",
      "pdfUrl": "https://arxiv.org/pdf/2602.04247.pdf",
      "titleJa": "DementiaBank-Emotion: アルツハイマー病音声の多評価者感情アノテーションコーパス（バージョン1.0）"
    },
    {
      "id": "2602.04217",
      "arxivId": "2602.04217",
      "title": "Frontend Token Enhancement for Token-Based Speech Recognition",
      "authors": [
        "Takanori Ashihara",
        "Shota Horiguchi",
        "Kohei Matsuura",
        "Tsubasa Ochiai",
        "Marc Delcroix"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Discretized representations of speech signals are efficient alternatives to continuous features for various speech applications, including automatic speech recognition (ASR) and speech language models. However, these representations, such as semantic or phonetic tokens derived from clustering outputs of self-supervised learning (SSL) speech models, are susceptible to environmental noise, which can degrade backend task performance. In this work, we introduce a frontend system that estimates clean speech tokens from noisy speech and evaluate it on an ASR backend using semantic tokens. We consider four types of enhancement models based on their input/output domains: wave-to-wave, token-to-token, continuous SSL features-to-token, and wave-to-token. These models are trained independently of ASR backends. Experiments on the CHiME-4 dataset demonstrate that wave-to-token enhancement achieves the best performance among the frontends. Moreover, it mostly outperforms the ASR system based on continuous SSL features.",
      "url": "https://arxiv.org/abs/2602.04217",
      "pdfUrl": "https://arxiv.org/pdf/2602.04217.pdf",
      "titleJa": "トークンベースの音声認識のためのフロントエンドトークンの強化"
    },
    {
      "id": "2602.04160",
      "arxivId": "2602.04160",
      "title": "PFluxTTS: Hybrid Flow-Matching TTS with Robust Cross-Lingual Voice Cloning and Inference-Time Model Fusion",
      "authors": [
        "Vikentii Pankov",
        "Artem Gribul",
        "Oktai Tatanov",
        "Vladislav Proskurov",
        "Yuliya Korotkova",
        "Darima Mylzenova",
        "Dmitrii Vypirailenko"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.SD"
      ],
      "abstract": "We present PFluxTTS, a hybrid text-to-speech system addressing three gaps in flow-matching TTS: the stability-naturalness trade-off, weak cross-lingual voice cloning, and limited audio quality from low-rate mel features. Our contributions are: (1) a dual-decoder design combining duration-guided and alignment-free models through inference-time vector-field fusion; (2) robust cloning using a sequence of speech-prompt embeddings in a FLUX-based decoder, preserving speaker traits across languages without prompt transcripts; and (3) a modified PeriodWave vocoder with super-resolution to 48 kHz. On cross-lingual in-the-wild data, PFluxTTS clearly outperforms F5-TTS, FishSpeech, and SparkTTS, matches ChatterBox in naturalness (MOS 4.11) while achieving 23% lower WER (6.9% vs. 9.0%), and surpasses ElevenLabs in speaker similarity (+0.32 SMOS). The system remains robust in challenging scenarios where most open-source models fail, while requiring only short reference audio and no extra training. Audio demos are available at https://braskai.github.io/pfluxtts/",
      "url": "https://arxiv.org/abs/2602.04160",
      "pdfUrl": "https://arxiv.org/pdf/2602.04160.pdf",
      "titleJa": "PFluxTTS: 堅牢なクロスリンガル音声クローニングと推論時間モデル融合を備えたハイブリッドフローマッチングTTS"
    },
    {
      "id": "2602.04085",
      "arxivId": "2602.04085",
      "title": "BASS: Benchmarking Audio LMs for Musical Structure and Semantic Reasoning",
      "authors": [
        "Min Jang",
        "Orevaoghene Ahia",
        "Nazif Tamer",
        "Sachin Kumar",
        "Yulia Tsvetkov",
        "Noah A. Smith"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Music understanding is a complex task that often requires reasoning over both structural and semantic elements of audio. We introduce BASS, designed to evaluate music understanding and reasoning in audio language models across four broad categories: structural segmentation, lyric transcription, musicological analysis, and artist collaboration. BASS comprises 2658 questions spanning 12 tasks, 1993 unique songs and covering over 138 hours of music from a wide range of genres and tracks, crafted to assess musicological knowledge and reasoning in real-world scenarios. We evaluate 14 open-source and frontier multimodal LMs, finding that even state-of-the-art models struggle on higher-level reasoning tasks such as structural segmentation and artist collaboration, while performing best on lyric transcription. Our analysis reveals that current models leverage linguistic priors effectively but remain limited in reasoning over musical structure, vocal, and musicological attributes. BASS provides an evaluation framework with widespread applications in music recommendation and search and has the potential to guide the development of audio LMs.",
      "url": "https://arxiv.org/abs/2602.04085",
      "pdfUrl": "https://arxiv.org/pdf/2602.04085.pdf",
      "titleJa": "BASS: 音楽構造と意味論的推論のためのオーディオ LM のベンチマーク"
    },
    {
      "id": "2602.03817",
      "arxivId": "2602.03817",
      "title": "Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion",
      "authors": [
        "Oscar Ovanger",
        "Levi Harris",
        "Timothy H. Keitt"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce \\textbf{F}usion under \\textbf{IN}dependent \\textbf{C}onditional \\textbf{H}ypotheses (\\textbf{FINCH}), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family \\emph{contains} the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \\texttt{\\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md}{anonymous-repository}}",
      "url": "https://arxiv.org/abs/2602.03817",
      "pdfUrl": "https://arxiv.org/pdf/2602.03817.pdf",
      "titleJa": "音声時空間融合のための適応的証拠重み付け"
    },
    {
      "id": "2602.03624",
      "arxivId": "2602.03624",
      "title": "A Multi-decoder Neural Tracking Method for Accurately Predicting Speech Intelligibility",
      "authors": [
        "Rien Sonck",
        "Bernd Accou",
        "Tom Francart",
        "Jonas Vanthornhout"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "eess.SP",
        "cs.SD"
      ],
      "abstract": "Objective: EEG-based methods can predict speech intelligibility, but their accuracy and robustness lag behind behavioral tests, which typically show test-retest differences under 1 dB. We introduce the multi-decoder method to predict speech reception thresholds (SRTs) from EEG recordings, enabling objective assessment for populations unable to perform behavioral tests; such as those with disorders of consciousness or during hearing aid fitting. Approach: The method aggregates data from hundreds of decoders, each trained on different speech features and EEG preprocessing setups to quantify neural tracking (NT) of speech signals. Using data from 39 participants (ages 18-24), we recorded 29 minutes of EEG per person while they listened to speech at six signal-to-noise ratios and a quiet story. NT values were combined into a high-dimensional feature vector per subject, and a support vector regression model was trained to predict SRTs from these vectors. Main Result: Predictions correlated significantly with behavioral SRTs (r = 0.647, p < 0.001; NRMSE = 0.19), with all differences under 1 dB. SHAP analysis showed theta/delta bands and early lags had slightly greater influence. Using pretrained subject-independent decoders reduced required EEG data collection to 15 minutes (3 minutes of story, 12 minutes across six SNR conditions) without losing accuracy.",
      "url": "https://arxiv.org/abs/2602.03624",
      "pdfUrl": "https://arxiv.org/pdf/2602.03624.pdf",
      "titleJa": "音声明瞭度を正確に予測するマルチデコーダーニューラルトラッキング法"
    },
    {
      "id": "2602.03549",
      "arxivId": "2602.03549",
      "title": "EarResp-ANS : Audio-Based On-Device Respiration Rate Estimation on Earphones with Adaptive Noise Suppression",
      "authors": [
        "Michael Küttner",
        "Valeria Zitz",
        "Supraja Ramesh",
        "Michael Beigl",
        "Tobias Röddiger"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.HC"
      ],
      "abstract": "Respiratory rate (RR) is a key vital sign for clinical assessment and mental well-being, yet it is rarely monitored in everyday life due to the lack of unobtrusive sensing technologies. In-ear audio sensing is promising due to its high social acceptance and the amplification of physiological sounds caused by the occlusion effect; however, existing approaches often fail under real-world noise or rely on computationally expensive models. We present EarResp-ANS, the first system enabling fully on-device, real-time RR estimation on commercial earphones. The system employs LMS-based adaptive noise suppression (ANS) to attenuate ambient noise while preserving respiration-related acoustic components, without requiring neural networks or audio streaming, thereby explicitly addressing the energy and privacy constraints of wearable devices. We evaluate EarResp-ANS in a study with 18 participants under realistic acoustic conditions, including music, cafeteria noise, and white noise up to 80 dB SPL. EarResp-ANS achieves robust performance with a global MAE of 0.84 CPM , reduced to 0.47 CPM via automatic outlier rejection, while operating with less than 2% processor load directly on the earphone.",
      "url": "https://arxiv.org/abs/2602.03549",
      "pdfUrl": "https://arxiv.org/pdf/2602.03549.pdf",
      "titleJa": "EarResp-ANS：適応型ノイズ抑制機能を備えたイヤホンにおける音声ベースのデバイス内呼吸数推定"
    },
    {
      "id": "2602.03523",
      "arxivId": "2602.03523",
      "title": "D3PIA: A Discrete Denoising Diffusion Model for Piano Accompaniment Generation From Lead sheet",
      "authors": [
        "Eunjin Choi",
        "Hounsu Kim",
        "Hayeon Bang",
        "Taegyun Kwon",
        "Juhan Nam"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM"
      ],
      "abstract": "Generating piano accompaniments in the symbolic music domain is a challenging task that requires producing a complete piece of piano music from given melody and chord constraints, such as those provided by a lead sheet. In this paper, we propose a discrete diffusion-based piano accompaniment generation model, D3PIA, leveraging local alignment between lead sheet and accompaniment in piano-roll representation. D3PIA incorporates Neighborhood Attention (NA) to both encode the lead sheet and condition it for predicting note states in the piano accompaniment. This design enhances local contextual modeling by efficiently attending to nearby melody and chord conditions. We evaluate our model using the POP909 dataset, a widely used benchmark for piano accompaniment generation. Objective evaluation results demonstrate that D3PIA preserves chord conditions more faithfully compared to continuous diffusion-based and Transformer-based baselines. Furthermore, a subjective listening test indicates that D3PIA generates more musically coherent accompaniments than the comparison models.",
      "url": "https://arxiv.org/abs/2602.03523",
      "pdfUrl": "https://arxiv.org/pdf/2602.03523.pdf",
      "titleJa": "D3PIA: リードシートからのピアノ伴奏生成のための離散ノイズ除去拡散モデル"
    },
    {
      "id": "2602.03420",
      "arxivId": "2602.03420",
      "title": "CoCoEmo: Composable and Controllable Human-Like Emotional TTS via Activation Steering",
      "authors": [
        "Siyi Wang",
        "Shihong Tan",
        "Siyi Liu",
        "Hong Jia",
        "Gongping Huang",
        "James Bailey",
        "Ting Dang"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Emotional expression in human speech is nuanced and compositional, often involving multiple, sometimes conflicting, affective cues that may diverge from linguistic content. In contrast, most expressive text-to-speech systems enforce a single utterance-level emotion, collapsing affective diversity and suppressing mixed or text-emotion-misaligned expression. While activation steering via latent direction vectors offers a promising solution, it remains unclear whether emotion representations are linearly steerable in TTS, where steering should be applied within hybrid TTS architectures, and how such complex emotion behaviors should be evaluated. This paper presents the first systematic analysis of activation steering for emotional control in hybrid TTS models, introducing a quantitative, controllable steering framework, and multi-rater evaluation protocols that enable composable mixed-emotion synthesis and reliable text-emotion mismatch synthesis. Our results demonstrate, for the first time, that emotional prosody and expressive variability are primarily synthesized by the TTS language module instead of the flow-matching module, and also provide a lightweight steering approach for generating natural, human-like emotional speech.",
      "url": "https://arxiv.org/abs/2602.03420",
      "pdfUrl": "https://arxiv.org/pdf/2602.03420.pdf",
      "titleJa": "CoCoEmo: アクティベーションステアリングによる構成可能かつ制御可能な人間のような感情表現TTS"
    },
    {
      "id": "2602.03355",
      "arxivId": "2602.03355",
      "title": "PACE: Pretrained Audio Continual Learning",
      "authors": [
        "Chang Li",
        "Kanglei Zhou",
        "Liyuan Wang"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Audio is a fundamental modality for analyzing speech, music, and environmental sounds. Although pretrained audio models have significantly advanced audio understanding, they remain fragile in real-world settings where data distributions shift over time. In this work, we present the first systematic benchmark for audio continual learning (CL) with pretrained models (PTMs), together with a comprehensive analysis of its unique challenges. Unlike in vision, where parameter-efficient fine-tuning (PEFT) has proven effective for CL, directly transferring such strategies to audio leads to poor performance. This stems from a fundamental property of audio backbones: they focus on low-level spectral details rather than structured semantics, causing severe upstream-downstream misalignment. Through extensive empirical study, we identify analytic classifiers with first-session adaptation (FSA) as a promising direction, but also reveal two major limitations: representation saturation in coarse-grained scenarios and representation drift in fine-grained scenarios. To address these challenges, we propose PACE, a novel method that enhances FSA via a regularized analytic classifier and enables multi-session adaptation through adaptive subspace-orthogonal PEFT for improved semantic alignment. In addition, we introduce spectrogram-based boundary-aware perturbations to mitigate representation overlap and improve stability. Experiments on six diverse audio CL benchmarks demonstrate that PACE substantially outperforms state-of-the-art baselines, marking an important step toward robust and scalable audio continual learning with PTMs.",
      "url": "https://arxiv.org/abs/2602.03355",
      "pdfUrl": "https://arxiv.org/pdf/2602.03355.pdf",
      "titleJa": "PACE: 事前学習済み音声継続学習"
    },
    {
      "id": "2602.03307",
      "arxivId": "2602.03307",
      "title": "GRAM: Spatial general-purpose audio representations for real-world environments",
      "authors": [
        "Goksenin Yuksel",
        "Marcel van Gerven",
        "Kiki van der Heijden"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Audio foundation models learn general-purpose audio representations that facilitate a wide range of downstream tasks. While the performance of these models has greatly increased for conventional single-channel, dry audio clips, their success in real-world acoustic environments with reverberation and noise is limited. Furthermore, most audio foundation models ignore the spatial dimension of real-world acoustic environments, ruling out tasks involving sound localization. To address these limitations, we propose GRAM: a general-purpose real-world audio model that employs a multi-channel masked autoencoder to efficiently learn spatial audio representations. We evaluated GRAM and other audio foundation models in a standardized manner on high-quality simulations of naturalistic, spatial acoustic environments as well as recordings of real-world environments and release these two complementary benchmark task suites: NatHEAR and RealSELD. Our results demonstrate that GRAM outperforms all state-of-the-art self-supervised audio foundation models on NatHEAR and the clean, single-channel version HEAR, while using only a fraction of the training data. GRAM also shows state-of-the-art localization performance in simulated environments and generalizes efficiently to real-world recordings in RealSELD. Taken together, GRAM presents a significant advance toward robust spatial audio foundation models for real-world environments.",
      "url": "https://arxiv.org/abs/2602.03307",
      "pdfUrl": "https://arxiv.org/pdf/2602.03307.pdf",
      "titleJa": "GRAM: 現実世界環境のための空間汎用オーディオ表現"
    },
    {
      "id": "2602.03892",
      "arxivId": "2602.03892",
      "title": "Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation",
      "authors": [
        "Jinxing Zhou",
        "Yanghao Zhou",
        "Yaoting Wang",
        "Zongyan Han",
        "Jiaqi Ma",
        "Henghui Ding",
        "Rao Muhammad Anwer",
        "Hisham Cholakkal"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.",
      "url": "https://arxiv.org/abs/2602.03892",
      "pdfUrl": "https://arxiv.org/pdf/2602.03892.pdf",
      "titleJa": "セグメンテーション後の監査：言語参照型オーディオビジュアルセグメンテーションにおける参照フリーマスク品質評価"
    },
    {
      "id": "2602.03891",
      "arxivId": "2602.03891",
      "title": "Sounding Highlights: Dual-Pathway Audio Encoders for Audio-Visual Video Highlight Detection",
      "authors": [
        "Seohyun Joo",
        "Yoori Oh"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Audio-visual video highlight detection aims to automatically identify the most salient moments in videos by leveraging both visual and auditory cues. However, existing models often underutilize the audio modality, focusing on high-level semantic features while failing to fully leverage the rich, dynamic characteristics of sound. To address this limitation, we propose a novel framework, Dual-Pathway Audio Encoders for Video Highlight Detection (DAViHD). The dual-pathway audio encoder is composed of a semantic pathway for content understanding and a dynamic pathway that captures spectro-temporal dynamics. The semantic pathway extracts high-level information by identifying the content within the audio, such as speech, music, or specific sound events. The dynamic pathway employs a frequency-adaptive mechanism as time evolves to jointly model these dynamics, enabling it to identify transient acoustic events via salient spectral bands and rapid energy changes. We integrate the novel audio encoder into a full audio-visual framework and achieve new state-of-the-art performance on the large-scale Mr.HiSum benchmark. Our results demonstrate that a sophisticated, dual-faceted audio representation is key to advancing the field of highlight detection.",
      "url": "https://arxiv.org/abs/2602.03891",
      "pdfUrl": "https://arxiv.org/pdf/2602.03891.pdf",
      "titleJa": "サウンドハイライト：オーディオビジュアルビデオハイライト検出のためのデュアルパスオーディオエンコーダ"
    },
    {
      "id": "2602.03762",
      "arxivId": "2602.03762",
      "title": "Conditional Flow Matching for Visually-Guided Acoustic Highlighting",
      "authors": [
        "Hugo Malard",
        "Gael Le Lan",
        "Daniel Wong",
        "David Lou Alon",
        "Yi-Chiao Wu",
        "Sanjeel Parekh"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Visually-guided acoustic highlighting seeks to rebalance audio in alignment with the accompanying video, creating a coherent audio-visual experience. While visual saliency and enhancement have been widely studied, acoustic highlighting remains underexplored, often leading to misalignment between visual and auditory focus. Existing approaches use discriminative models, which struggle with the inherent ambiguity in audio remixing, where no natural one-to-one mapping exists between poorly-balanced and well-balanced audio mixes. To address this limitation, we reframe this task as a generative problem and introduce a Conditional Flow Matching (CFM) framework. A key challenge in iterative flow-based generation is that early prediction errors -- in selecting the correct source to enhance -- compound over steps and push trajectories off-manifold. To address this, we introduce a rollout loss that penalizes drift at the final step, encouraging self-correcting trajectories and stabilizing long-range flow integration. We further propose a conditioning module that fuses audio and visual cues before vector field regression, enabling explicit cross-modal source selection. Extensive quantitative and qualitative evaluations show that our method consistently surpasses the previous state-of-the-art discriminative approach, establishing that visually-guided audio remixing is best addressed through generative modeling.",
      "url": "https://arxiv.org/abs/2602.03762",
      "pdfUrl": "https://arxiv.org/pdf/2602.03762.pdf",
      "titleJa": "視覚誘導音響強調表示のための条件付きフローマッチング"
    },
    {
      "id": "2602.03398",
      "arxivId": "2602.03398",
      "title": "A Unified SVD-Modal Solution for Sparse Sound Field Reconstruction with Hybrid Spherical-Linear Microphone Arrays",
      "authors": [
        "Shunxi Xu",
        "Thushara Abhayapala",
        "Craig T. Jin"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "eess.AS"
      ],
      "abstract": "We propose a data-driven sparse recovery framework for hybrid spherical linear microphone arrays using singular value decomposition (SVD) of the transfer operator. The SVD yields orthogonal microphone and field modes, reducing to spherical harmonics (SH) in the SMA-only case, while incorporating LMAs introduces complementary modes beyond SH. Modal analysis reveals consistent divergence from SH across frequency, confirming the improved spatial selectivity. Experiments in reverberant conditions show reduced energy-map mismatch and angular error across frequency, distance, and source count, outperforming SMA-only and direct concatenation. The results demonstrate that SVD-modal processing provides a principled and unified treatment of hybrid arrays for robust sparse sound-field reconstruction.",
      "url": "https://arxiv.org/abs/2602.03398",
      "pdfUrl": "https://arxiv.org/pdf/2602.03398.pdf",
      "titleJa": "ハイブリッド球面線形マイクロホンアレイを用いたスパース音場再構成のための統合SVD-モーダルソリューション"
    },
    {
      "id": "2602.03245",
      "arxivId": "2602.03245",
      "title": "Mići Princ -- A Little Boy Teaching Speech Technologies the Chakavian Dialect",
      "authors": [
        "Nikola Ljubešić",
        "Peter Rupnik",
        "Tea Perinčić"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "abstract": "This paper documents our efforts in releasing the printed and audio book of the translation of the famous novel The Little Prince into the Chakavian dialect, as a computer-readable, AI-ready dataset, with the textual and the audio components of the two releases now aligned on the level of each written and spoken word. Our motivation for working on this release is multiple. The first one is our wish to preserve the highly valuable and specific content beyond the small editions of the printed and the audio book. With the dataset published in the CLARIN.SI repository, this content is from now on at the fingertips of any interested individual. The second motivation is to make the data available for various artificial-intelligence-related usage scenarios, such as the one we follow upon inside this paper already -- adapting the Whisper-large-v3 open automatic speech recognition model, with decent performance on standard Croatian, to Chakavian dialectal speech. We can happily report that with adapting the model, the word error rate on the selected test data has being reduced to a half, while we managed to remove up to two thirds of the error on character level. We envision many more usages of this dataset beyond the set of experiments we have already performed, both on tasks of artificial intelligence research and application, as well as dialectal research. The third motivation for this release is our hope that this, now highly structured dataset, will be transformed into a digital online edition of this work, allowing individuals beyond the research and technology communities to enjoy the beauty of the message of the little boy in the desert, told through the spectacular prism of the Chakavian dialect.",
      "url": "https://arxiv.org/abs/2602.03245",
      "pdfUrl": "https://arxiv.org/pdf/2602.03245.pdf",
      "titleJa": "ミチ・プリンチ ― チャカビ語方言で音声技術を教える少年"
    },
    {
      "id": "2602.02980",
      "arxivId": "2602.02980",
      "title": "WST-X Series: Wavelet Scattering Transform for Interpretable Speech Deepfake Detection",
      "authors": [
        "Xi Xuan",
        "Davide Carbone",
        "Ruchi Pandey",
        "Wenxin Zhang",
        "Tomi H. Kinnunen"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "eess.AS",
        "cs.CL",
        "eess.SP"
      ],
      "abstract": "Designing front-ends for speech deepfake detectors primarily focuses on two categories. Hand-crafted filterbank features are transparent but are limited in capturing high-level semantic details, often resulting in performance gaps compared to self-supervised (SSL) features. SSL features, in turn, lack interpretability and may overlook fine-grained spectral anomalies. We propose the WST-X series, a novel family of feature extractors that combines the best of both worlds via the wavelet scattering transform (WST), integrating wavelets with nonlinearities analogous to deep convolutional networks. We investigate 1D and 2D WSTs to extract acoustic details and higher-order structural anomalies, respectively. Experimental results on the recent and challenging Deepfake-Eval-2024 dataset indicate that WST-X outperforms existing front-ends by a wide margin. Our analysis reveals that a small averaging scale ($J$), combined with high-frequency and directional resolutions ($Q, L$), is critical for capturing subtle artifacts. This underscores the value of translation-invariant and deformation-stable features for robust and interpretable speech deepfake detection.",
      "url": "https://arxiv.org/abs/2602.02980",
      "pdfUrl": "https://arxiv.org/pdf/2602.02980.pdf",
      "titleJa": "WST-Xシリーズ: 解釈可能な音声ディープフェイク検出のためのウェーブレット散乱変換"
    },
    {
      "id": "2602.02734",
      "arxivId": "2602.02734",
      "title": "WAXAL: A Large-Scale Multilingual African Language Speech Corpus",
      "authors": [
        "Abdoulaye Diack",
        "Perry Nelson",
        "Kwaku Agbesi",
        "Angela Nakalembe",
        "MohamedElfatih MohamedKhair",
        "Vusumuzi Dube",
        "Tavonga Siyavora",
        "Subhashini Venugopalan",
        "Jason Hickey",
        "Uche Okonkwo",
        "Abhishek Bapna",
        "Isaac Wiafe",
        "Raynard Dodzi Helegah",
        "Elikem Doe Atsakpo",
        "Charles Nutrokpor",
        "Fiifi Baffoe Payin Winful",
        "Kafui Kwashie Solaga",
        "Jamal-Deen Abdulai",
        "Akon Obu Ekpezu",
        "Audace Niyonkuru",
        "Samuel Rutunda",
        "Boris Ishimwe",
        "Michael Melese",
        "Engineer Bainomugisha",
        "Joyce Nakatumba-Nabende",
        "Andrew Katumba",
        "Claire Babirye",
        "Jonathan Mukiibi",
        "Vincent Kimani",
        "Samuel Kibacia",
        "James Maina",
        "Fridah Emmah",
        "Ahmed Ibrahim Shekarau",
        "Ibrahim Shehu Adamu",
        "Yusuf Abdullahi",
        "Howard Lakougna",
        "Bob MacDonald",
        "Hadar Shemtov",
        "Aisha Walcott-Bryant",
        "Moustapha Cisse",
        "Avinatan Hassidim",
        "Jeff Dean",
        "Yossi Matias"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "The advancement of speech technology has predominantly favored high-resource languages, creating a significant digital divide for speakers of most Sub-Saharan African languages. To address this gap, we introduce WAXAL, a large-scale, openly accessible speech dataset for 21 languages representing over 100 million speakers. The collection consists of two main components: an Automated Speech Recognition (ASR) dataset containing approximately 1,250 hours of transcribed, natural speech from a diverse range of speakers, and a Text-to-Speech (TTS) dataset with over 180 hours of high-quality, single-speaker recordings reading phonetically balanced scripts. This paper details our methodology for data collection, annotation, and quality control, which involved partnerships with four African academic and community organizations. We provide a detailed statistical overview of the dataset and discuss its potential limitations and ethical considerations. The WAXAL datasets are released at https://huggingface.co/datasets/google/WaxalNLP under the permissive CC-BY-4.0 license to catalyze research, enable the development of inclusive technologies, and serve as a vital resource for the digital preservation of these languages.",
      "url": "https://arxiv.org/abs/2602.02734",
      "pdfUrl": "https://arxiv.org/pdf/2602.02734.pdf",
      "titleJa": "WAXAL: 大規模多言語アフリカ言語音声コーパス"
    },
    {
      "id": "2602.02725",
      "arxivId": "2602.02725",
      "title": "Automated Dysphagia Screening Using Noninvasive Neck Acoustic Sensing",
      "authors": [
        "Jade Chng",
        "Rong Xing",
        "Yunfei Luo",
        "Kristen Linnemeyer-Risser",
        "Tauhidur Rahman",
        "Andrew Yousef",
        "Philip A Weissbrod"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "abstract": "Pharyngeal health plays a vital role in essential human functions such as breathing, swallowing, and vocalization. Early detection of swallowing abnormalities, also known as dysphagia, is crucial for timely intervention. However, current diagnostic methods often rely on radiographic imaging or invasive procedures. In this study, we propose an automated framework for detecting dysphagia using portable and noninvasive acoustic sensing coupled with applied machine learning. By capturing subtle acoustic signals from the neck during swallowing tasks, we aim to identify patterns associated with abnormal physiological conditions. Our approach achieves promising test-time abnormality detection performance, with an AUC-ROC of 0.904 under 5 independent train-test splits. This work demonstrates the feasibility of using noninvasive acoustic sensing as a practical and scalable tool for pharyngeal health monitoring.",
      "url": "https://arxiv.org/abs/2602.02725",
      "pdfUrl": "https://arxiv.org/pdf/2602.02725.pdf",
      "titleJa": "非侵襲性頸部音響センシングを用いた自動嚥下障害スクリーニング"
    },
    {
      "id": "2602.02198",
      "arxivId": "2602.02198",
      "title": "QuietPrint: Protecting 3D Printers Against Acoustic Side-Channel Attacks",
      "authors": [
        "Seyed Ali Ghazi Asgar",
        "Narasimha Reddy"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.CR",
        "eess.AS"
      ],
      "abstract": "The 3D printing market has experienced significant growth in recent years, with an estimated revenue of 15 billion USD for 2025. Cyber-attacks targeting the 3D printing process whether through the machine itself, the supply chain, or the fabricated components are becoming increasingly common. One major concern is intellectual property (IP) theft, where a malicious attacker gains access to the design file. One method for carrying out such theft is through side-channel attacks. In this work, we investigate the possibility of IP theft via acoustic side channels and propose a novel method to protect 3D printers against such attacks. The primary advantage of our approach is that it requires no additional hardware, such as large speakers or noise-canceling devices. Instead, it secures printed parts by minimal modifications to the G-code.",
      "url": "https://arxiv.org/abs/2602.02198",
      "pdfUrl": "https://arxiv.org/pdf/2602.02198.pdf",
      "titleJa": "QuietPrint: 音響サイドチャネル攻撃から3Dプリンターを保護する"
    },
    {
      "id": "2602.01861",
      "arxivId": "2602.01861",
      "title": "RIR-Former: Coordinate-Guided Transformer for Continuous Reconstruction of Room Impulse Responses",
      "authors": [
        "Shaoheng Xu",
        "Chunyi Sun",
        "Jihui Zhang",
        "Prasanga N. Samarasinghe",
        "Thushara D. Abhayapala"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Room impulse responses (RIRs) are essential for many acoustic signal processing tasks, yet measuring them densely across space is often impractical. In this work, we propose RIR-Former, a grid-free, one-step feed-forward model for RIR reconstruction. By introducing a sinusoidal encoding module into a transformer backbone, our method effectively incorporates microphone position information, enabling interpolation at arbitrary array locations. Furthermore, a segmented multi-branch decoder is designed to separately handle early reflections and late reverberation, improving reconstruction across the entire RIR. Experiments on diverse simulated acoustic environments demonstrate that RIR-Former consistently outperforms state-of-the-art baselines in terms of normalized mean square error (NMSE) and cosine distance (CD), under varying missing rates and array configurations. These results highlight the potential of our approach for practical deployment and motivate future work on scaling from randomly spaced linear arrays to complex array geometries, dynamic acoustic scenes, and real-world environments.",
      "url": "https://arxiv.org/abs/2602.01861",
      "pdfUrl": "https://arxiv.org/pdf/2602.01861.pdf",
      "titleJa": "RIR-Former: 室内インパルス応答の連続再構成のための座標誘導型変換器"
    },
    {
      "id": "2602.01758",
      "arxivId": "2602.01758",
      "title": "Short-wave admittance correction for a time-domain cochlear transmission line model",
      "authors": [
        "François Deloche",
        "Morgan Thienpont",
        "Sarah Verhulst"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS",
        "physics.bio-ph"
      ],
      "abstract": "Transmission line (TL) models implemented in the time domain can efficiently simulate basilar-membrane (BM) displacement in response to transient or non-stationary sounds. By design, a TL model is well-suited for an one-dimensional (1-D) characterization of the traveling wave, but the real configuration of the cochlea also introduces higher-dimensional effects. Such effects include the focusing of the pressure around the BM and transverse viscous damping, both of which are magnified in the short-wave region. The two effects depend on the wavelength and are more readily expressed in the frequency domain. In this paper, we introduce a numerical correction for the BM admittance to account for 2-D effects in the time domain using autoregressive filtering and regression techniques. The correction was required for the implementation of a TL model tailored to the gerbil cochlear physiology. The model, which includes instantaneous nonlinearities in the form of variable damping, initially presented insufficient compression with increasing sound levels. This limitation was explained by the strong coupling between gain and frequency selectivity assumed in the 1-D nonlinear TL model, whereas cochlear frequency selectivity shows only a moderate dependence on sound level in small mammals. The correction factor was implemented in the gerbil model and made level-dependent using a feedback loop. The updated model achieved some decoupling between frequency selectivity and gain, providing 5 dB of additional gain and extending the range of sound levels of the compressive regime by 10 dB. We discuss the relevance of this work through two key features: the integration of both analytical and regression methods for characterizing BM admittance, and the combination of instantaneous and non-instantaneous nonlinearities.",
      "url": "https://arxiv.org/abs/2602.01758",
      "pdfUrl": "https://arxiv.org/pdf/2602.01758.pdf",
      "titleJa": "時間領域蝸牛伝送線路モデルの短波アドミタンス補正"
    },
    {
      "id": "2602.01722",
      "arxivId": "2602.01722",
      "title": "Joint Optimization of ASV and CM tasks: BTUEF Team's Submission for WildSpoof Challenge",
      "authors": [
        "Oguzhan Kurnaz",
        "Jagabandhu Mishra",
        "Tomi Kinnunen",
        "Cemal Hanilci"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS"
      ],
      "abstract": "Spoofing-aware speaker verification (SASV) jointly addresses automatic speaker verification and spoofing countermeasures to improve robustness against adversarial attacks. In this paper, we investigate our recently proposed modular SASV framework that enables effective reuse of publicly available ASV and CM systems through non-linear fusion, explicitly modeling their interaction, and optimization with an operating-condition-dependent trainable a-DCF loss. The framework is evaluated using ECAPA-TDNN and ReDimNet as ASV embedding extractors and SSL-AASIST as the CM model, with experiments conducted both with and without fine-tuning on the WildSpoof SASV training data. Results show that the best performance is achieved by combining ReDimNet-based ASV embeddings with fine-tuned SSL-AASIST representations, yielding an a-DCF of 0.0515 on the progress evaluation set and 0.2163 on the final evaluation set.",
      "url": "https://arxiv.org/abs/2602.01722",
      "pdfUrl": "https://arxiv.org/pdf/2602.01722.pdf",
      "titleJa": "ASVとCMタスクの共同最適化：BTUEFチームのWildSpoofチャレンジへの応募"
    },
    {
      "id": "2602.01634",
      "arxivId": "2602.01634",
      "title": "HuPER: A Human-Inspired Framework for Phonetic Perception",
      "authors": [
        "Chenxu Guo",
        "Jiachen Lian",
        "Yisi Liu",
        "Baihe Huang",
        "Shriyaa Narayanan",
        "Cheol Jun Cho",
        "Gopala Anumanchipalli"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "We propose HuPER, a human-inspired framework that models phonetic perception as adaptive inference over acoustic-phonetics evidence and linguistic knowledge. With only 100 hours of training data, HuPER achieves state-of-the-art phonetic error rates on five English benchmarks and strong zero-shot transfer to 95 unseen languages. HuPER is also the first framework to enable adaptive, multi-path phonetic perception under diverse acoustic conditions. All training data, models, and code are open-sourced. Code and demo avaliable at https://github.com/HuPER29/HuPER.",
      "url": "https://arxiv.org/abs/2602.01634",
      "pdfUrl": "https://arxiv.org/pdf/2602.01634.pdf",
      "titleJa": "HuPER: 人間に着想を得た音声知覚フレームワーク"
    },
    {
      "id": "2602.01547",
      "arxivId": "2602.01547",
      "title": "Attention-weighted Centered Kernel Alignment for Knowledge Distillation in Large Audio-Language Models Applied to Speech Emotion Recognition",
      "authors": [
        "Qingran Yang",
        "Botao Zhao",
        "Zuheng Kang",
        "Xue Li",
        "Yayun He",
        "Chuhang Liu",
        "Xulong Zhang",
        "Xiaoyang Qu",
        "Junqing Peng",
        "Jianzong Wang"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "The emergence of Large Audio-Language Models (LALMs) has advanced Speech Emotion Recognition (SER), but their size limits deployment in resource-constrained environments. While Knowledge Distillation is effective for LALM compression, existing methods remain underexplored in distilling the cross-modal projection module (Projector), and often struggle with alignment due to differences in feature dimensions. We propose PL-Distill, a KD framework that combines Projector-Level Distillation (PDist) to align audio embeddings and Logits-Level Distillation (LDist) to align output logits. PDist introduces Attention-weighted Centered Kernel Alignment, a novel approach we propose to highlight important time steps and address dimension mismatches. Meanwhile, LDist minimizes the Kullback-Leibler divergence between teacher and student logits from audio and text modalities. On IEMOCAP, RAVDESS, and SAVEE, PL-Distill compresses an 8.4B-parameter teacher to a compact 1.1B-parameter student, consistently outperforming the teacher, state-of-the-art pretrained models, and other KD baselines across all metrics.",
      "url": "https://arxiv.org/abs/2602.01547",
      "pdfUrl": "https://arxiv.org/pdf/2602.01547.pdf",
      "titleJa": "音声感情認識に適用される大規模音声言語モデルにおける知識蒸留のための注目度重み付け中心カーネルアライメント"
    },
    {
      "id": "2602.01394",
      "arxivId": "2602.01394",
      "title": "SSNAPS: Audio-Visual Separation of Speech and Background Noise with Diffusion Inverse Sampling",
      "authors": [
        "Yochai Yemini",
        "Yoav Ellinson",
        "Rami Ben-Ari",
        "Sharon Gannot",
        "Ethan Fetaya"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "abstract": "This paper addresses the challenge of audio-visual single-microphone speech separation and enhancement in the presence of real-world environmental noise. Our approach is based on generative inverse sampling, where we model clean speech and ambient noise with dedicated diffusion priors and jointly leverage them to recover all underlying sources. To achieve this, we reformulate a recent inverse sampler to match our setting. We evaluate on mixtures of 1, 2, and 3 speakers with noise and show that, despite being entirely unsupervised, our method consistently outperforms leading supervised baselines in \\ac{WER} across all conditions. We further extend our framework to handle off-screen speaker separation. Moreover, the high fidelity of the separated noise component makes it suitable for downstream acoustic scene detection. Demo page: https://ssnapsicml.github.io/ssnapsicml2026/",
      "url": "https://arxiv.org/abs/2602.01394",
      "pdfUrl": "https://arxiv.org/pdf/2602.01394.pdf",
      "titleJa": "SSNAPS: 拡散逆サンプリングによる音声と背景雑音のオーディオビジュアル分離"
    },
    {
      "id": "2602.01363",
      "arxivId": "2602.01363",
      "title": "Causally Disentangled Contrastive Learning for Multilingual Speaker Embeddings",
      "authors": [
        "Mariëtte Olijslager",
        "Seyed Sahand Mohammadi Ziabari",
        "Ali Mohammed Mansoor Alsahag"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "abstract": "Self-supervised speaker embeddings are widely used in speaker verification systems, but prior work has shown that they often encode sensitive demographic attributes, raising fairness and privacy concerns. This paper investigates the extent to which demographic information, specifically gender, age, and accent, is present in SimCLR-trained speaker embeddings and whether such leakage can be mitigated without severely degrading speaker verification performance. We study two debiasing strategies: adversarial training through gradient reversal and a causal bottleneck architecture that explicitly separates demographic and residual information. Demographic leakage is quantified using both linear and nonlinear probing classifiers, while speaker verification performance is evaluated using ROC-AUC and EER. Our results show that gender information is strongly and linearly encoded in baseline embeddings, whereas age and accent are weaker and primarily nonlinearly represented. Adversarial debiasing reduces gender leakage but has limited effect on age and accent and introduces a clear trade-off with verification accuracy. The causal bottleneck further suppresses demographic information, particularly in the residual representation, but incurs substantial performance degradation. These findings highlight fundamental limitations in mitigating demographic leakage in self-supervised speaker embeddings and clarify the trade-offs inherent in current debiasing approaches.",
      "url": "https://arxiv.org/abs/2602.01363",
      "pdfUrl": "https://arxiv.org/pdf/2602.01363.pdf",
      "titleJa": "多言語話者埋め込みのための因果的に分離した対照学習"
    },
    {
      "id": "2602.04883",
      "arxivId": "2602.04883",
      "title": "Protein Autoregressive Modeling via Multiscale Structure Generation",
      "authors": [
        "Yanru Qu",
        "Cheng-Yen Hsieh",
        "Zaixiang Zheng",
        "Ge Liu",
        "Quanquan Gu"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "q-bio.QM"
      ],
      "abstract": "We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.",
      "url": "https://arxiv.org/abs/2602.04883",
      "pdfUrl": "https://arxiv.org/pdf/2602.04883.pdf",
      "titleJa": "マルチスケール構造生成によるタンパク質自己回帰モデリング"
    },
    {
      "id": "2602.04881",
      "arxivId": "2602.04881",
      "title": "Contrastive Continual Learning for Model Adaptability in Internet of Things",
      "authors": [
        "Ajesh Koyatan Chathoth"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \\emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.",
      "url": "https://arxiv.org/abs/2602.04881",
      "pdfUrl": "https://arxiv.org/pdf/2602.04881.pdf",
      "titleJa": "IoTにおけるモデル適応性のための対照的継続学習"
    },
    {
      "id": "2602.04879",
      "arxivId": "2602.04879",
      "title": "Rethinking the Trust Region in LLM Reinforcement Learning",
      "authors": [
        "Penghui Qi",
        "Xiangxin Zhou",
        "Zichen Liu",
        "Tianyu Pang",
        "Chao Du",
        "Min Lin",
        "Wee Sun Lee"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "abstract": "Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.",
      "url": "https://arxiv.org/abs/2602.04879",
      "pdfUrl": "https://arxiv.org/pdf/2602.04879.pdf",
      "titleJa": "LLM強化学習における信頼領域の再考"
    },
    {
      "id": "2602.04872",
      "arxivId": "2602.04872",
      "title": "Multi-layer Cross-Attention is Provably Optimal for Multi-modal In-context Learning",
      "authors": [
        "Nicholas Barnfield",
        "Subhabrata Sen",
        "Pragya Sur"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Recent progress has rapidly advanced our understanding of the mechanisms underlying in-context learning in modern attention-based neural networks. However, existing results focus exclusively on unimodal data; in contrast, the theoretical underpinnings of in-context learning for multi-modal data remain poorly understood. We introduce a mathematically tractable framework for studying multi-modal learning and explore when transformer-like architectures can recover Bayes-optimal performance in-context. To model multi-modal problems, we assume the observed data arises from a latent factor model. Our first result comprises a negative take on expressibility: we prove that single-layer, linear self-attention fails to recover the Bayes-optimal predictor uniformly over the task distribution. To address this limitation, we introduce a novel, linearized cross-attention mechanism, which we study in the regime where both the number of cross-attention layers and the context length are large. We show that this cross-attention mechanism is provably Bayes optimal when optimized using gradient flow. Our results underscore the benefits of depth for in-context learning and establish the provable utility of cross-attention for multi-modal distributions.",
      "url": "https://arxiv.org/abs/2602.04872",
      "pdfUrl": "https://arxiv.org/pdf/2602.04872.pdf",
      "titleJa": "多層クロスアテンションはマルチモーダルな文脈内学習に最適であることが証明されている"
    },
    {
      "id": "2602.04868",
      "arxivId": "2602.04868",
      "title": "CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation",
      "authors": [
        "Yannick Denker",
        "Alexander Gepperth"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.",
      "url": "https://arxiv.org/abs/2602.04868",
      "pdfUrl": "https://arxiv.org/pdf/2602.04868.pdf",
      "titleJa": "CRoSS: 高いタスク多様性とリアルな物理シミュレーションを備えたスケーラブルな強化学習のための継続的なロボットシミュレーションスイート"
    },
    {
      "id": "2602.04863",
      "arxivId": "2602.04863",
      "title": "Subliminal Effects in Your Data: A General Mechanism via Log-Linearity",
      "authors": [
        "Ishaq Aden-Ali",
        "Noah Golowich",
        "Allen Liu",
        "Abhishek Shetty",
        "Ankur Moitra",
        "Nika Haghtalab"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "abstract": "Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets. We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.",
      "url": "https://arxiv.org/abs/2602.04863",
      "pdfUrl": "https://arxiv.org/pdf/2602.04863.pdf",
      "titleJa": "データの潜在意識効果：対数線形性による一般的なメカニズム"
    },
    {
      "id": "2602.04861",
      "arxivId": "2602.04861",
      "title": "From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures",
      "authors": [
        "Ryan Liu",
        "Eric Qu",
        "Tobias Kreiman",
        "Samuel M. Blau",
        "Aditi S. Krishnapriyan"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.chem-ph"
      ],
      "abstract": "Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an \"in-the-loop\" model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.",
      "url": "https://arxiv.org/abs/2602.04861",
      "pdfUrl": "https://arxiv.org/pdf/2602.04861.pdf",
      "titleJa": "評価から設計へ: ポテンシャルエネルギー表面平滑性メトリクスを用いた機械学習の原子間ポテンシャルアーキテクチャのガイド"
    },
    {
      "id": "2602.04850",
      "arxivId": "2602.04850",
      "title": "El Agente Quntur: A research collaborator agent for quantum chemistry",
      "authors": [
        "Juan B. Pérez-Sánchez",
        "Yunheng Zou",
        "Jorge A. Campos-Gonzalez-Angulo",
        "Marcel Müller",
        "Ignacio Gustin",
        "Andrew Wang",
        "Han Hao",
        "Tsz Wai Ko",
        "Changhyeok Choi",
        "Eric S. Isbrandt",
        "Mohammad Ghazi Vakili",
        "Hanyong Xu",
        "Chris Crebolder",
        "Varinia Bernales",
        "Alán Aspuru-Guzik"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "physics.chem-ph",
        "cs.AI",
        "cs.MA"
      ],
      "abstract": "Quantum chemistry is a foundational enabling tool for the fields of chemistry, materials science, computational biology and others. Despite of its power, the practical application of quantum chemistry simulations remains in the hands of qualified experts due to methodological complexity, software heterogeneity, and the need for informed interpretation of results. To bridge the accessibility gap for these tools and expand their reach to chemists with broader backgrounds, we introduce El Agente Quntur, a hierarchical, multi-agent AI system designed to operate not merely as an automation tool but as a research collaborator for computational quantum chemistry. Quntur was designed following three main strategies: i) elimination of hard-coded procedural policies in favour of reasoning-driven decisions, ii) construction of general and composable actions that facilitate generalization and efficiency, and iii) implementation of guided deep research to integrate abstract quantum-chemical reasoning across subdisciplines and a detailed understanding of the software's internal logic and syntax. Although instantiated in ORCA, these design principles are applicable to research agents more generally and easily expandable to additional quantum chemistry packages and beyond. Quntur supports the full range of calculations available in ORCA 6.0 and reasons over software documentation and scientific literature to plan, execute, adapt, and analyze in silico chemistry experiments following best practices. We discuss the advances and current bottlenecks in agentic systems operating at the research level in computational chemistry, and outline a roadmap toward a fully autonomous end-to-end computational chemistry research agent.",
      "url": "https://arxiv.org/abs/2602.04850",
      "pdfUrl": "https://arxiv.org/pdf/2602.04850.pdf",
      "titleJa": "El Agente Quntur: 量子化学の研究協力者エージェント"
    },
    {
      "id": "2602.04849",
      "arxivId": "2602.04849",
      "title": "El Agente Estructural: An Artificially Intelligent Molecular Editor",
      "authors": [
        "Changhyeok Choi",
        "Yunheng Zou",
        "Marcel Müller",
        "Han Hao",
        "Yeonghun Kang",
        "Juan B. Pérez-Sánchez",
        "Ignacio Gustin",
        "Hanyong Xu",
        "Mohammad Ghazi Vakili",
        "Chris Crebolder",
        "Alán Aspuru-Guzik",
        "Varinia Bernales"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "physics.chem-ph",
        "cs.AI",
        "cs.MA"
      ],
      "abstract": "We present El Agente Estructural, a multimodal, natural-language-driven geometry-generation and manipulation agent for autonomous chemistry and molecular modelling. Unlike molecular generation or editing via generative models, Estructural mimics how human experts directly manipulate molecular systems in three dimensions by integrating a comprehensive set of domain-informed tools and vision-language models. This design enables precise control over atomic or functional group replacements, atomic connectivity, and stereochemistry without the need to rebuild extensive core molecular frameworks. Through a series of representative case studies, we demonstrate that Estructural enables chemically meaningful geometry manipulation across a wide range of real-world scenarios. These include site-selective functionalization, ligand binding, ligand exchange, stereochemically controlled structure construction, isomer interconversion, fragment-level structural analysis, image-guided generation of structures from schematic reaction mechanisms, and mechanism-driven geometry generation and modification. These examples illustrate how multimodal reasoning, when combined with specialized geometry-aware tools, supports interactive and context-aware molecular modelling beyond structure generation. Looking forward, the integration of Estructural into El Agente Quntur, an autonomous multi-agent quantum chemistry platform, enhances its capabilities by adding sophisticated tools for the generation and editing of three-dimensional structures.",
      "url": "https://arxiv.org/abs/2602.04849",
      "pdfUrl": "https://arxiv.org/pdf/2602.04849.pdf",
      "titleJa": "El Agente Estructural: 人工知能分子エディター"
    },
    {
      "id": "2602.04843",
      "arxivId": "2602.04843",
      "title": "Fluid Representations in Reasoning Models",
      "authors": [
        "Dmitrii Kharlapenko",
        "Alessandro Stolfo",
        "Arthur Conmy",
        "Mrinmaya Sachan",
        "Zhijing Jin"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.",
      "url": "https://arxiv.org/abs/2602.04843",
      "pdfUrl": "https://arxiv.org/pdf/2602.04843.pdf",
      "titleJa": "推論モデルにおける流動的表現"
    },
    {
      "id": "2602.04837",
      "arxivId": "2602.04837",
      "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
      "authors": [
        "Zhaotian Weng",
        "Antonis Antoniades",
        "Deepak Nathani",
        "Zhen Zhang",
        "Xiao Pu",
        "Xin Eric Wang"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.",
      "url": "https://arxiv.org/abs/2602.04837",
      "pdfUrl": "https://arxiv.org/pdf/2602.04837.pdf",
      "titleJa": "グループ進化エージェント：経験共有によるオープンエンドな自己改善"
    },
    {
      "id": "2602.04836",
      "arxivId": "2602.04836",
      "title": "Are AI Capabilities Increasing Exponentially? A Competing Hypothesis",
      "authors": [
        "Haosen Ge",
        "Hamsa Bastani",
        "Osbert Bastani"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.AI"
      ],
      "abstract": "Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.",
      "url": "https://arxiv.org/abs/2602.04836",
      "pdfUrl": "https://arxiv.org/pdf/2602.04836.pdf",
      "titleJa": "AIの能力は指数関数的に向上しているのか？ 対立する仮説"
    },
    {
      "id": "2602.04832",
      "arxivId": "2602.04832",
      "title": "It's not a Lottery, it's a Race: Understanding How Gradient Descent Adapts the Network's Capacity to the Task",
      "authors": [
        "Hannah Pinson"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "abstract": "Our theoretical understanding of neural networks is lagging behind their empirical success. One of the important unexplained phenomena is why and how, during the process of training with gradient descent, the theoretical capacity of neural networks is reduced to an effective capacity that fits the task. We here investigate the mechanism by which gradient descent achieves this through analyzing the learning dynamics at the level of individual neurons in single hidden layer ReLU networks. We identify three dynamical principles -- mutual alignment, unlocking and racing -- that together explain why we can often successfully reduce capacity after training through the merging of equivalent neurons or the pruning of low norm weights. We specifically explain the mechanism behind the lottery ticket conjecture, or why the specific, beneficial initial conditions of some neurons lead them to obtain higher weight norms.",
      "url": "https://arxiv.org/abs/2602.04832",
      "pdfUrl": "https://arxiv.org/pdf/2602.04832.pdf",
      "titleJa": "それは宝くじではなく、レースだ：勾配降下法がネットワークの能力をタスクに適応させる仕組みを理解する"
    },
    {
      "id": "2602.04821",
      "arxivId": "2602.04821",
      "title": "Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning",
      "authors": [
        "Joydeep Chandra",
        "Satyam Kumar Navneet",
        "Aleksandr Algazinov",
        "Yong Zhang"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Urban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions -- all while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an Uncertainty-Guided Adaptive Conformal Forecaster that uses prediction uncertainty to dynamically reweight graph attention via confidence-monotonic attention, achieving distribution-free coverage guarantees; (2) CRFN-BY, a Conformal Residual Flow Network that models uncertainty-normalized residuals via normalizing flows with Benjamini-Yekutieli FDR control under arbitrary dependence; and (3) LyCon-WRL+, an Uncertainty-Guided Safe World-Model RL agent with Lyapunov stability certificates, certified Lipschitz bounds, and uncertainty-propagated imagination rollouts. To our knowledge, this is the first framework to propagate calibrated uncertainty from forecasting through anomaly detection to safe policy learning with end-to-end theoretical guarantees. Experiments on multiple real-world traffic trajectory data demonstrate that STREAM-RL achieves 91.4\\% coverage efficiency, controls FDR at 4.1\\% under verified dependence, and improves safety rate to 95.2\\% compared to 69\\% for standard PPO while achieving higher reward, with 23ms end-to-end inference latency.",
      "url": "https://arxiv.org/abs/2602.04821",
      "pdfUrl": "https://arxiv.org/pdf/2602.04821.pdf",
      "titleJa": "不確実性を考慮した等角予測と世界モデル強化学習による安全な都市交通制御"
    },
    {
      "id": "2602.04820",
      "arxivId": "2602.04820",
      "title": "Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization",
      "authors": [
        "Farzia Hossain",
        "Samanta Ghosh",
        "Shahida Begum",
        "B. M. Shahria Alam",
        "Mohammad Tahmid Noor",
        "Md Parvez Mia",
        "Nishat Tasnim Niloy"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "Human nail diseases are gradually observed over all age groups, especially among older individuals, often going ignored until they become severe. Early detection and accurate diagnosis of such conditions are important because they sometimes reveal our body's health problems. But it is challenging due to the inferred visual differences between disease types. This paper presents a machine learning-based model for automated classification of nail diseases based on a publicly available dataset, which contains 3,835 images scaling six categories. In 224x224 pixels, all images were resized to ensure consistency. To evaluate performance, four well-known CNN models-InceptionV3, DenseNet201, EfficientNetV2, and ResNet50 were trained and analyzed. Among these, InceptionV3 outperformed the others with an accuracy of 95.57%, while DenseNet201 came next with 94.79%. To make the model stronger and less likely to make mistakes on tricky or noisy images, we used adversarial training. To help understand how the model makes decisions, we used SHAP to highlight important features in the predictions. This system could be a helpful support for doctors, making nail disease diagnosis more accurate and faster.",
      "url": "https://arxiv.org/abs/2602.04820",
      "pdfUrl": "https://arxiv.org/pdf/2602.04820.pdf",
      "titleJa": "信頼性と説明可能性に優れた爪疾患分類に向けて：敵対的学習とGrad-CAM可視化の活用"
    },
    {
      "id": "2602.04813",
      "arxivId": "2602.04813",
      "title": "Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents",
      "authors": [
        "Shubham Vatsal",
        "Harsh Dubey",
        "Aditi Singh"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "abstract": "Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).",
      "url": "https://arxiv.org/abs/2602.04813",
      "pdfUrl": "https://arxiv.org/pdf/2602.04813.pdf",
      "titleJa": "ヘルスケアと医療におけるエージェントAI：LLMベースのエージェントの実証的評価のための7次元分類"
    },
    {
      "id": "2602.04811",
      "arxivId": "2602.04811",
      "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
      "authors": [
        "Jiarui Yuan",
        "Tailin Jin",
        "Weize Chen",
        "Zeyuan Liu",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "abstract": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.",
      "url": "https://arxiv.org/abs/2602.04811",
      "pdfUrl": "https://arxiv.org/pdf/2602.04811.pdf",
      "titleJa": "SE-Bench: 知識の内在化による自己進化のベンチマーク"
    },
    {
      "id": "2602.04809",
      "arxivId": "2602.04809",
      "title": "Beyond Rewards in Reinforcement Learning for Cyber Defence",
      "authors": [
        "Elizabeth Bates",
        "Chris Hicks",
        "Vasilios Mavroudis"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.",
      "url": "https://arxiv.org/abs/2602.04809",
      "pdfUrl": "https://arxiv.org/pdf/2602.04809.pdf",
      "titleJa": "サイバー防衛における強化学習における報酬を超えて"
    },
    {
      "id": "2602.04805",
      "arxivId": "2602.04805",
      "title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
      "authors": [
        "Jia-peng Zhang",
        "Cheng-Feng Pu",
        "Meng-Hao Guo",
        "Yan-Pei Cao",
        "Shi-Min Hu"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "abstract": "The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation.",
      "url": "https://arxiv.org/abs/2602.04805",
      "pdfUrl": "https://arxiv.org/pdf/2602.04805.pdf",
      "titleJa": "スキントークン：統合自己回帰リギングのための学習されたコンパクト表現"
    },
    {
      "id": "2602.04785",
      "arxivId": "2602.04785",
      "title": "Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation",
      "authors": [
        "Congjing Zhang",
        "Ryan Feng Lin",
        "Ruoxuan Bao",
        "Shuai Huang"
      ],
      "publishedDate": "2026-02-04",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "abstract": "While tabular data is fundamental to many real-world machine learning (ML) applications, acquiring high-quality tabular data is usually labor-intensive and expensive. Limited by the scarcity of observations, tabular datasets often exhibit critical deficiencies, such as class imbalance, selection bias, and low fidelity. To address these challenges, building on recent advances in Large Language Models (LLMs), this paper introduces Team-then-Trim (T$^2$), a framework that synthesizes high-quality tabular data through a collaborative team of LLMs, followed by a rigorous three-stage plug-in data quality control (QC) pipeline. In T$^2$, tabular data generation is conceptualized as a manufacturing process: specialized LLMs, guided by domain knowledge, are tasked with generating different data components sequentially, and the resulting products, i.e., the synthetic data, are systematically evaluated across multiple dimensions of QC. Empirical results on both simulated and real-world datasets demonstrate that T$^2$ outperforms state-of-the-art methods in producing high-quality tabular data, highlighting its potential to support downstream models when direct data collection is practically infeasible.",
      "url": "https://arxiv.org/abs/2602.04785",
      "pdfUrl": "https://arxiv.org/pdf/2602.04785.pdf",
      "titleJa": "チーム化、そしてトリム：高品質な表形式データ生成のためのアセンブリラインLLMフレームワーク"
    },
    {
      "id": "2602.03023",
      "arxivId": "2602.03023",
      "title": "Rethinking Music Captioning with Music Metadata LLMs",
      "authors": [
        "Irmak Bukey",
        "Zhepei Wang",
        "Chris Donahue",
        "Nicholas J. Bryan"
      ],
      "publishedDate": "2026-02-03",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "Music captioning, or the task of generating a natural language description of music, is useful for both music understanding and controllable music generation. Training captioning models, however, typically requires high-quality music caption data which is scarce compared to metadata (e.g., genre, mood, etc.). As a result, it is common to use large language models (LLMs) to synthesize captions from metadata to generate training data for captioning models, though this process imposes a fixed stylization and entangles factual information with natural language style. As a more direct approach, we propose metadata-based captioning. We train a metadata prediction model to infer detailed music metadata from audio and then convert it into expressive captions via pre-trained LLMs at inference time. Compared to a strong end-to-end baseline trained on LLM-generated captions derived from metadata, our method: (1) achieves comparable performance in less training time over end-to-end captioners, (2) offers flexibility to easily change stylization post-training, enabling output captions to be tailored to specific stylistic and quality requirements, and (3) can be prompted with audio and partial metadata to enable powerful metadata imputation or in-filling--a common task for organizing music data.",
      "url": "https://arxiv.org/abs/2602.03023",
      "pdfUrl": "https://arxiv.org/pdf/2602.03023.pdf",
      "titleJa": "音楽メタデータLLMによる音楽キャプションの再考"
    },
    {
      "id": "2602.02738",
      "arxivId": "2602.02738",
      "title": "When Noise Lowers The Loss: Rethinking Likelihood-Based Evaluation in Music Large Language Models",
      "authors": [
        "Xiaosha Li",
        "Chun Liu",
        "Ziyu Wang"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "The rise of music large language models (LLMs) demands robust methods of evaluating output quality, especially in distinguishing high-quality compositions from \"garbage music\". Curiously, we observe that the standard cross-entropy loss -- a core training metric -- often decrease when models encounter systematically corrupted music, undermining its validity as a standalone quality indicator. To investigate this paradox, we introduce noise injection experiment, where controlled noise signal of varying lengths are injected into musical contexts. We hypothesize that a model's loss reacting positively to these perturbations, specifically a sharp increase (\"Peak\" area) for short injection, can serve as a proxy for its ability to discern musical integrity. Experiments with MusicGen models in the audio waveform domain confirm that Music LLMs respond more strongly to local, texture-level disruptions than to global semantic corruption. Beyond exposing this bias, our results highlight a new principle: the shape of the loss curve -- rather than its absolute value -- encodes critical information about the quality of the generated content (i.e., model behavior). We envision this profile-based evaluation as a label-free, model-intrinsic framework for assessing musical quality -- opening the door to more principled training objectives and sharper benchmarks.",
      "url": "https://arxiv.org/abs/2602.02738",
      "pdfUrl": "https://arxiv.org/pdf/2602.02738.pdf",
      "titleJa": "ノイズが損失を低下させるとき：音楽大規模言語モデルにおける尤度ベースの評価の再考"
    },
    {
      "id": "2602.01727",
      "arxivId": "2602.01727",
      "title": "Voting-based Pitch Estimation with Temporal and Frequential Alignment and Correlation Aware Selection",
      "authors": [
        "Junya Koguchi",
        "Tomoki Koriyama"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD"
      ],
      "abstract": "The voting method, an ensemble approach for fundamental frequency estimation, is empirically known for its robustness but lacks thorough investigation. This paper provides a principled analysis and improvement of this technique. First, we offer a theoretical basis for its effectiveness, explaining the error variance reduction for fundamental frequency estimation and invoking Condorcet's jury theorem for voiced/unvoiced detection accuracy. To address its practical limitations, we propose two key improvements: 1) a pre-voting alignment procedure to correct temporal and frequential biases among estimators, and 2) a greedy algorithm to select a compact yet effective subset of estimators based on error correlation. Experiments on a diverse dataset of speech, singing, and music show that our proposed method with alignment outperforms individual state-of-the-art estimators in clean conditions and maintains robust voiced/unvoiced detection in noisy environments.",
      "url": "https://arxiv.org/abs/2602.01727",
      "pdfUrl": "https://arxiv.org/pdf/2602.01727.pdf",
      "titleJa": "時間的・頻度的アライメントと相関を考慮した選択による投票ベースのピッチ推定"
    },
    {
      "id": "2602.01645",
      "arxivId": "2602.01645",
      "title": "Membership Inference Attack Against Music Diffusion Models via Generative Manifold Perturbation",
      "authors": [
        "Yuxuan Liu",
        "Peihong Zhang",
        "Rui Sang",
        "Zhixin Li",
        "Yizhou Tan",
        "Yiqiang Cai",
        "Shengchen Li"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Membership inference attacks (MIAs) test whether a specific audio clip was used to train a model, making them a key tool for auditing generative music models for copyright compliance. However, loss-based signals (e.g., reconstruction error) are weakly aligned with human perception in practice, yielding poor separability at the low false-positive rates (FPRs) required for forensics. We propose the Latent Stability Adversarial Probe (LSA-Probe), a white-box method that measures a geometric property of the reverse diffusion: the minimal time-normalized perturbation budget needed to cross a fixed perceptual degradation threshold at an intermediate diffusion state. We show that training members, residing in more stable regions, exhibit a significantly higher degradation cost.",
      "url": "https://arxiv.org/abs/2602.01645",
      "pdfUrl": "https://arxiv.org/pdf/2602.01645.pdf",
      "titleJa": "生成多様体摂動法による音楽拡散モデルに対するメンバーシップ推論攻撃"
    },
    {
      "id": "2602.00744",
      "arxivId": "2602.00744",
      "title": "ACE-Step 1.5: Pushing the Boundaries of Open-Source Music Generation",
      "authors": [
        "Junmin Gong",
        "Yulin Song",
        "Wenxiao Zhao",
        "Sen Wang",
        "Shengyuan Xu",
        "Jing Guo"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.SD"
      ],
      "abstract": "We present ACE-Step v1.5, a highly efficient open-source music foundation model that brings commercial-grade generation to consumer hardware. On commonly used evaluation metrics, ACE-Step v1.5 achieves quality beyond most commercial music models while remaining extremely fast -- under 2 seconds per full song on an A100 and under 10 seconds on an RTX 3090. The model runs locally with less than 4GB of VRAM, and supports lightweight personalization: users can train a LoRA from just a few songs to capture their own style. At its core lies a novel hybrid architecture where the Language Model (LM) functions as an omni-capable planner: it transforms simple user queries into comprehensive song blueprints -- scaling from short loops to 10-minute compositions -- while synthesizing metadata, lyrics, and captions via Chain-of-Thought to guide the Diffusion Transformer (DiT). Uniquely, this alignment is achieved through intrinsic reinforcement learning relying solely on the model's internal mechanisms, thereby eliminating the biases inherent in external reward models or human preferences. Beyond standard synthesis, ACE-Step v1.5 unifies precise stylistic control with versatile editing capabilities -- such as cover generation, repainting, and vocal-to-BGM conversion -- while maintaining strict adherence to prompts across 50+ languages. This paves the way for powerful tools that seamlessly integrate into the creative workflows of music artists, producers, and content creators. The code, the model weights and the demo are available at: https://ace-step.github.io/ace-step-v1.5.github.io/",
      "url": "https://arxiv.org/abs/2602.00744",
      "pdfUrl": "https://arxiv.org/pdf/2602.00744.pdf",
      "titleJa": "ACE-ステップ1.5: オープンソース音楽生成の限界を押し広げる"
    },
    {
      "id": "2601.22764",
      "arxivId": "2601.22764",
      "title": "How Far Can Pretrained LLMs Go in Symbolic Music? Controlled Comparisons of Supervised and Preference-based Adaptation",
      "authors": [
        "Deepak Kumar",
        "Emmanouil Karystinaios",
        "Gerhard Widmer",
        "Markus Schedl"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "Music often shares notable parallels with language, motivating the use of pretrained large language models (LLMs) for symbolic music understanding and generation. Despite growing interest, the practical effectiveness of adapting instruction-tuned LLMs to symbolic music remains insufficiently characterized. We present a controlled comparative study of finetuning strategies for ABC-based generation and understanding, comparing an off-the-shelf instruction-tuned backbone to domain-adapted variants and a music-specialized LLM baseline. Across multiple symbolic music corpora and evaluation signals, we provide some insights into adaptation choices for symbolic music applications. We highlight the domain adaptation vs.~preserving prior information tradeoff as well as the distinct behaviour of metrics used to measure the domain adaptation for symbolic music.",
      "url": "https://arxiv.org/abs/2601.22764",
      "pdfUrl": "https://arxiv.org/pdf/2601.22764.pdf",
      "titleJa": "事前学習済みLLMは記号音楽においてどこまで到達できるか？教師あり学習と選好に基づく適応の制御比較"
    },
    {
      "id": "2601.21740",
      "arxivId": "2601.21740",
      "title": "MIDI-LLaMA: An Instruction-Following Multimodal LLM for Symbolic Music Understanding",
      "authors": [
        "Meng Yang",
        "Jon McCormack",
        "Maria Teresa Llano",
        "Wanchao Su",
        "Chao Lei"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.MM",
        "cs.SD"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLM) for audio music have demonstrated strong capabilities in music understanding, yet symbolic music, a fundamental representation of musical structure, remains unexplored. In this work, we introduce MIDI-LLaMA, the first instruction-following MLLM for symbolic music understanding. Our approach aligns the MIDI encoder MusicBERT and Llama-3-8B via a two-stage pipeline comprising feature alignment and instruction tuning. To support training, we design a scalable annotation pipeline that annotates GiantMIDI-Piano with fine-grained metadata, resulting in a MIDI-text dataset. Compared with the baseline trained on converting MIDI into ABC notation under the same instruction-tuning procedure, MIDI-LLaMA substantially outperforms in captioning and semantic alignment in question answering. Human evaluation further confirms the advantages of MIDI-LLaMA in music understanding, emotion recognition, creativity, and overall preference. These findings demonstrate that incorporating symbolic music into large language models enhances their capacity for musical understanding.",
      "url": "https://arxiv.org/abs/2601.21740",
      "pdfUrl": "https://arxiv.org/pdf/2601.21740.pdf",
      "titleJa": "MIDI-LLaMA: 記号的音楽理解のための指示追従型マルチモーダルLLM"
    },
    {
      "id": "2601.21260",
      "arxivId": "2601.21260",
      "title": "Music Plagiarism Detection: Problem Formulation and a Segment-based Solution",
      "authors": [
        "Seonghyeon Go",
        "Yumin Kim"
      ],
      "publishedDate": "2026-01-29",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Recently, the problem of music plagiarism has emerged as an even more pressing social issue. As music information retrieval research advances, there is a growing effort to address issues related to music plagiarism. However, many studies, including our previous work, have conducted research without clearly defining what the music plagiarism detection task actually involves. This lack of a clear definition has slowed research progress and made it hard to apply results to real-world scenarios. To fix this situation, we defined how Music Plagiarism Detection is different from other MIR tasks and explained what problems need to be solved. We introduce the Similar Music Pair dataset to support this newly defined task. In addition, we propose a method based on segment transcription as one way to solve the task. Our demo and dataset are available at https://github.com/Mippia/ICASSP2026-MPD.",
      "url": "https://arxiv.org/abs/2601.21260",
      "pdfUrl": "https://arxiv.org/pdf/2601.21260.pdf",
      "titleJa": "音楽盗作検出：問題の定式化とセグメントベースのソリューション"
    },
    {
      "id": "2601.20478",
      "arxivId": "2601.20478",
      "title": "On Every Note a Griff: Looking for a Useful Representation of Basso Continuo Performance Style",
      "authors": [
        "Adam Štefunko",
        "Carlos Eduardo Cancino-Chacón",
        "Jan Hajič"
      ],
      "publishedDate": "2026-01-28",
      "categories": [
        "cs.SD",
        "cs.IR"
      ],
      "abstract": "Basso continuo is a baroque improvisatory accompaniment style which involves improvising multiple parts above a given bass line in a musical score on a harpsichord or organ. Basso continuo is not merely a matter of history; moreover, it is a historically inspired living practice, and The Aligned Continuo Dataset (ACoRD) records the first sample of modern-day basso continuo playing in the symbolic domain. This dataset, containing 175 MIDI recordings of 5 basso continuo scores performed by 7 players, allows us to start observing and analyzing the variety that basso continuo improvisation brings. A recently proposed basso continuo performance-to-score alignment system provides a way of mapping improvised performance notes to score notes. In order to study aligned basso continuo performances, we need an appropriate feature representation. We propose griff, a representation inspired by historical basso continuo treatises. It enables us to encode both pitch content and structure of a basso continuo realization in a transposition-invariant way. Griffs are directly extracted from aligned basso continuo performances by grouping together performance notes aligned to the same score note in a onset-time ordered way, and they provide meaningful tokens that form a feature space in which we can analyze basso continuo performance styles. We statistically describe griffs extracted from the ACoRD dataset recordings, and show in two experiments how griffs can be used for statistical analysis of individuality of different players' basso continuo performance styles. We finally present an argument why it is desirable to preserve the structure of a basso continuo improvisation in order to conduct a refined analysis of personal performance styles of individual basso continuo practitioners, and why griffs can provide a meaningful historically informed feature space worthy of a more robust empirical validation.",
      "url": "https://arxiv.org/abs/2601.20478",
      "pdfUrl": "https://arxiv.org/pdf/2601.20478.pdf",
      "titleJa": "すべての音符にグリフ：通奏低音演奏スタイルの有用な表現を求めて"
    },
    {
      "id": "2601.20883",
      "arxivId": "2601.20883",
      "title": "VoxMorph: Scalable Zero-shot Voice Identity Morphing via Disentangled Embeddings",
      "authors": [
        "Bharath Krishnamurthy",
        "Ajita Rattani"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD",
        "cs.CR",
        "cs.LG",
        "eess.AS"
      ],
      "abstract": "Morphing techniques generate artificial biometric samples that combine features from multiple individuals, allowing each contributor to be verified against a single enrolled template. While extensively studied in face recognition, this vulnerability remains largely unexplored in voice biometrics. Prior work on voice morphing is computationally expensive, non-scalable, and limited to acoustically similar identity pairs, constraining practical deployment. Moreover, existing sound-morphing methods target audio textures, music, or environmental sounds and are not transferable to voice identity manipulation. We propose VoxMorph, a zero-shot framework that produces high-fidelity voice morphs from as little as five seconds of audio per subject without model retraining. Our method disentangles vocal traits into prosody and timbre embeddings, enabling fine-grained interpolation of speaking style and identity. These embeddings are fused via Spherical Linear Interpolation (Slerp) and synthesized using an autoregressive language model coupled with a Conditional Flow Matching network. VoxMorph achieves state-of-the-art performance, delivering a 2.6x gain in audio quality, a 73% reduction in intelligibility errors, and a 67.8% morphing attack success rate on automated speaker verification systems under strict security thresholds. This work establishes a practical and scalable paradigm for voice morphing with significant implications for biometric security. The code and dataset are available on our project page: https://vcbsl.github.io/VoxMorph/",
      "url": "https://arxiv.org/abs/2601.20883",
      "pdfUrl": "https://arxiv.org/pdf/2601.20883.pdf",
      "titleJa": "VoxMorph: 分離埋め込みによるスケーラブルなゼロショット音声アイデンティティモーフィング"
    },
    {
      "id": "2601.19702",
      "arxivId": "2601.19702",
      "title": "SAM Audio Judge: A Unified Multimodal Framework for Perceptual Evaluation of Audio Separation",
      "authors": [
        "Helin Wang",
        "Bowen Shi",
        "Andros Tjandra",
        "John Hoffman",
        "Yi-Chiao Wu",
        "Apoorv Vyas",
        "Najim Dehak",
        "Ann Lee",
        "Wei-Ning Hsu"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "abstract": "The performance evaluation remains a complex challenge in audio separation, and existing evaluation metrics are often misaligned with human perception, course-grained, relying on ground truth signals. On the other hand, subjective listening tests remain the gold standard for real-world evaluation, but they are expensive, time-consuming, and difficult to scale. This paper addresses the growing need for automated systems capable of evaluating audio separation without human intervention. The proposed evaluation metric, SAM Audio Judge (SAJ), is a multimodal fine-grained reference-free objective metric, which shows highly alignment with human perceptions. SAJ supports three audio domains (speech, music and general sound events) and three prompt inputs (text, visual and span), covering four different dimensions of evaluation (recall, percision, faithfulness, and overall). SAM Audio Judge also shows potential applications in data filtering, pseudo-labeling large datasets and reranking in audio separation models. We release our code and pre-trained models at: https://github.com/facebookresearch/sam-audio.",
      "url": "https://arxiv.org/abs/2601.19702",
      "pdfUrl": "https://arxiv.org/pdf/2601.19702.pdf",
      "titleJa": "SAM Audio Judge: 音声分離の知覚評価のための統合マルチモーダルフレームワーク"
    },
    {
      "id": "2601.19109",
      "arxivId": "2601.19109",
      "title": "Interpretable and Perceptually-Aligned Music Similarity with Pretrained Embeddings",
      "authors": [
        "Arhan Vohra",
        "Taketo Akama"
      ],
      "publishedDate": "2026-01-27",
      "categories": [
        "cs.SD"
      ],
      "abstract": "Perceptual similarity representations enable music retrieval systems to determine which songs sound most similar to listeners. State-of-the-art approaches based on task-specific training via self-supervised metric learning show promising alignment with human judgment, but are difficult to interpret or generalize due to limited dataset availability. We show that pretrained text-audio embeddings (CLAP and MuQ-MuLan) offer comparable perceptual alignment on similarity tasks without any additional fine-tuning. To surpass this baseline, we introduce a novel method to perceptually align pretrained embeddings with source separation and linear optimization on ABX preference data from listening tests. Our model provides interpretable and controllable instrument-wise weights, allowing music producers to retrieve stem-level loops and samples based on mixed reference songs.",
      "url": "https://arxiv.org/abs/2601.19109",
      "pdfUrl": "https://arxiv.org/pdf/2601.19109.pdf",
      "titleJa": "事前学習済みの埋め込みによる解釈可能かつ知覚的に整合された音楽類似性"
    },
    {
      "id": "2601.18766",
      "arxivId": "2601.18766",
      "title": "Learning to Discover: A Generalized Framework for Raga Identification without Forgetting",
      "authors": [
        "Parampreet Singh",
        "Somya Kumar",
        "Chaitanya Shailendra Nitawe",
        "Vipul Arora"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "abstract": "Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.",
      "url": "https://arxiv.org/abs/2601.18766",
      "pdfUrl": "https://arxiv.org/pdf/2601.18766.pdf",
      "titleJa": "発見することを学ぶ：忘れずにラーガを識別するための一般化された枠組み"
    },
    {
      "id": "2601.18339",
      "arxivId": "2601.18339",
      "title": "A Dataset for Automatic Vocal Mode Classification",
      "authors": [
        "Reemt Hinrichs",
        "Sonja Stephan",
        "Alexander Lange",
        "Jörn Ostermann"
      ],
      "publishedDate": "2026-01-26",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "abstract": "The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\\,\\% was achieved with a ResNet18. The dataset can be downloaded under https://zenodo.org/records/14276415.",
      "url": "https://arxiv.org/abs/2601.18339",
      "pdfUrl": "https://arxiv.org/pdf/2601.18339.pdf",
      "titleJa": "自動音声モード分類のためのデータセット"
    },
    {
      "id": "2602.02249",
      "arxivId": "2602.02249",
      "title": "Evaluating Acoustic Data Transmission Schemes for Ad-Hoc Communication Between Nearby Smart Devices",
      "authors": [
        "Florentin Putz",
        "Philipp Fortmann",
        "Jan Frank",
        "Christoph Haugwitz",
        "Mario Kupnik",
        "Matthias Hollick"
      ],
      "publishedDate": "2026-02-02",
      "categories": [
        "cs.NI",
        "cs.SD"
      ],
      "abstract": "Acoustic data transmission offers a compelling alternative to Bluetooth and NFC by leveraging the ubiquitous speakers and microphones in smartphones and IoT devices. However, most research in this field relies on simulations or limited on-device testing, which makes the real-world reliability of proposed schemes difficult to assess. We systematically reviewed 31 acoustic communication studies for commodity devices and found that none provided accessible source code. After contacting authors and re-implementing three promising schemes, we assembled a testbed of eight representative acoustic communication systems. Using over 11000 smartphone transmissions in both realistic indoor environments and an anechoic chamber, we provide a systematic and repeatable methodology for evaluating the reliability and generalizability of these schemes under real-world conditions. Our results show that many existing schemes face challenges in practical usage, largely due to severe multipath propagation indoors and varying audio characteristics across device models. To support future research and foster more robust evaluations, we release our re-implementations alongside the first comprehensive dataset of real-world acoustic transmissions. Overall, our findings highlight the importance of rigorous on-device testing and underscore the need for robust design strategies to bridge the gap between simulation results and reliable IoT deployments.",
      "url": "https://arxiv.org/abs/2602.02249",
      "pdfUrl": "https://arxiv.org/pdf/2602.02249.pdf",
      "titleJa": "近くのスマートデバイス間のアドホック通信のための音響データ伝送方式の評価"
    },
    {
      "id": "2602.02591",
      "arxivId": "2602.02591",
      "title": "VividVoice: A Unified Framework for Scene-Aware Visually-Driven Speech Synthesis",
      "authors": [
        "Chengyuan Ma",
        "Jiawei Jin",
        "Ruijie Xiong",
        "Chunxiang Jin",
        "Canxiang Yan",
        "Wenming Yang"
      ],
      "publishedDate": "2026-02-01",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "abstract": "We introduce and define a novel task-Scene-Aware Visually-Driven Speech Synthesis, aimed at addressing the limitations of existing speech generation models in creating immersive auditory experiences that align with the real physical world. To tackle the two core challenges of data scarcity and modality decoupling, we propose VividVoice, a unified generative framework. First, we constructed a large-scale, high-quality hybrid multimodal dataset, Vivid-210K, which, through an innovative programmatic pipeline, establishes a strong correlation between visual scenes, speaker identity, and audio for the first time. Second, we designed a core alignment module, D-MSVA, which leverages a decoupled memory bank architecture and a cross-modal hybrid supervision strategy to achieve fine-grained alignment from visual scenes to timbre and environmental acoustic features. Both subjective and objective experimental results provide strong evidence that VividVoice significantly outperforms existing baseline models in terms of audio fidelity, content clarity, and multimodal consistency. Our demo is available at https://chengyuann.github.io/VividVoice/.",
      "url": "https://arxiv.org/abs/2602.02591",
      "pdfUrl": "https://arxiv.org/pdf/2602.02591.pdf",
      "titleJa": "VividVoice: シーン認識型視覚駆動型音声合成のための統合フレームワーク"
    },
    {
      "id": "2602.00648",
      "arxivId": "2602.00648",
      "title": "High-Fidelity Generative Audio Compression at 0.275kbps",
      "authors": [
        "Hao Ma",
        "Ruihao Jing",
        "Shansong Liu",
        "Cheng Gong",
        "Chi Zhang",
        "Xiao-Lei Zhang",
        "Xuelong Li"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "abstract": "High-fidelity general audio compression at ultra-low bitrates is crucial for applications ranging from low-bandwidth communication to generative audio-language modeling. Traditional audio compression methods and contemporary neural codecs are fundamentally designed for waveform reconstruction. As a result, when operating at ultra-low bitrates, these methods degrade rapidly and often fail to preserve essential information, leading to severe acoustic artifacts and pronounced semantic distortion. To overcome these limitations, we introduce Generative Audio Compression (GAC), a novel paradigm shift from signal fidelity to task-oriented effectiveness. Implemented within the AI Flow framework, GAC is theoretically grounded in the Law of Information Capacity. These foundations posit that abundant computational power can be leveraged at the receiver to offset extreme communication bottlenecks--exemplifying the More Computation, Less Bandwidth philosophy. By integrating semantic understanding at the transmitter with scalable generative synthesis at the receiver, GAC offloads the information burden to powerful model priors. Our 1.8B-parameter model achieves high-fidelity reconstruction of 32kHz general audio at an unprecedented bitrate of 0.275kbps. Even at 0.175kbps, it still preserves a strong intelligible audio transmission capability, which represents an about 3000x compression ratio, significantly outperforming current state-of-the-art neural codecs in maintaining both perceptual quality and semantic consistency.",
      "url": "https://arxiv.org/abs/2602.00648",
      "pdfUrl": "https://arxiv.org/pdf/2602.00648.pdf",
      "titleJa": "0.275kbpsの高忠実度生成オーディオ圧縮"
    },
    {
      "id": "2602.00594",
      "arxivId": "2602.00594",
      "title": "Kanade: A Simple Disentangled Tokenizer for Spoken Language Modeling",
      "authors": [
        "Zhijie Huang",
        "Stephen McIntosh",
        "Daisuke Saito",
        "Nobuaki Minematsu"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "A good language model starts with a good tokenizer. Tokenization is especially important for speech modeling, which must handle continuous signals that mix linguistic and non-linguistic information. A speech tokenizer should extract phonetics and prosody, suppress linguistically irrelevant information like speaker identity, and enable high-quality synthesis. We present Kanade, a single-layer disentangled speech tokenizer that realizes this ideal. Kanade separates out acoustic constants to create a single stream of tokens that captures rich phonetics and prosody. It does so without the need for auxiliary methods that existing disentangled codecs often rely on. Experiments show that Kanade achieves state-of-the-art speaker disentanglement and lexical availability, while maintaining excellent reconstruction quality.",
      "url": "https://arxiv.org/abs/2602.00594",
      "pdfUrl": "https://arxiv.org/pdf/2602.00594.pdf",
      "titleJa": "Kanade: 音声言語モデルのためのシンプルな分離トークナイザー"
    },
    {
      "id": "2602.00560",
      "arxivId": "2602.00560",
      "title": "Edit Content, Preserve Acoustics: Imperceptible Text-Based Speech Editing via Self-Consistency Rewards",
      "authors": [
        "Yong Ren",
        "Jiangyan Yi",
        "Jianhua Tao",
        "Zhengqi Wen",
        "Tao Wang"
      ],
      "publishedDate": "2026-01-31",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "abstract": "Imperceptible text-based speech editing allows users to modify spoken content by altering the transcript. It demands that modified segments fuse seamlessly with the surrounding context. Prevalent methods operating in the acoustic space suffer from inherent content-style entanglement, leading to generation instability and boundary artifacts. In this paper, we propose a novel framework grounded in the principle of \"Edit Content, Preserve Acoustics\". Our approach relies on two core components: (1) Structural Foundations, which decouples editing into a stable semantic space while delegating acoustic reconstruction to a Flow Matching decoder; and (2) Perceptual Alignment, which employs a novel Self-Consistency Rewards Group Relative Policy Optimization. By leveraging a pre-trained Text-to-Speech model as an implicit critic -- complemented by strict intelligibility and duration constraints -- we effectively align the edited semantic token sequence with the original context. Empirical evaluations demonstrate that our method significantly outperforms state-of-the-art autoregressive and non-autoregressive baselines, achieving superior intelligibility, robustness, and perceptual quality.",
      "url": "https://arxiv.org/abs/2602.00560",
      "pdfUrl": "https://arxiv.org/pdf/2602.00560.pdf",
      "titleJa": "コンテンツを編集し、音響を維持する：自己一貫性報酬による知覚できないテキストベースの音声編集"
    },
    {
      "id": "2601.23161",
      "arxivId": "2601.23161",
      "title": "DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding",
      "authors": [
        "Jiaming Zhou",
        "Xuxin Cheng",
        "Shiwan Zhao",
        "Yuhang Jia",
        "Cao Liu",
        "Ke Zeng",
        "Xunliang Cai",
        "Yong Qin"
      ],
      "publishedDate": "2026-01-30",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "abstract": "Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.",
      "url": "https://arxiv.org/abs/2601.23161",
      "pdfUrl": "https://arxiv.org/pdf/2601.23161.pdf",
      "titleJa": "DIFFA-2: 一般的な音声理解のための実用的な拡散大規模言語モデル"
    }
  ],
  "lastUpdated": "2026-02-06T01:02:30.148880",
  "totalCount": 74
}